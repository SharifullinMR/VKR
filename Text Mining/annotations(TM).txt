English grammar books are important materials for English learning, in order to systematically analyze the themes and trends of English grammar books in the 19th and 20th centuries, unstructured data and structured information are correlated, in this paper, text mining, co-word analysis and other research methods were adopted to study 15 books named English Grammar in the Gutenberg Project, which lasted from 1812 to 1918 and had a total of 1863536 English words. Wordstat9.0.4, KH CODER3 and Excel were used to analyze the collected information. This paper presents the results of text mining, such as word frequency analysis, co-occurrence analysis, keyword analysis and part of speech analysis.
Text mining techniques has become a research hotspot in the field of power equipment defects. To summarize the research advances in the text mining techniques, this paper first analyzes the general steps and research hotspots of the techniques, and then focuses on the typical applications of text mining in the field of power equipment defects, including the construction of an ontology dictionary of electric power equipment, the evaluation of health status of the equipment based on text information, automatic classification of the defects according to the severity of the defects in the equipment, the precise extraction of defect details, the construction and application of knowledge graphs of power equipment defects. At the end of the paper, the challenges that text mining in the field of power equipment defects has to face are proposed. This work will help readers to systematically and quickly know the text mining techniques for knowledge of defects in power equipment.
As there is fast growth in digital data collection techniques it has made way for large amount of data. Greater than 85% of present day data is comprised of unsaturated and unstructured data. Determining the definite patterns and trends to examine a textual data is biggest issue in text mining The various domains associated together in data mining are text mining, web mining, graph mining, and sequencing mining. The selection of proper and correct technique of text mining enhances the hustle and by lowering the period and struggle done to mine important information. Here, we talk about text data mining, various techniques of text data mining and also application of text data mining. Text data mining is used for obtaining stimulating and fascinating designs from the unsaturated texts which are derived from various sources. It changes words, phrases and sentences of an unstructured information into mathematical value linking with the saturated information in the database and analyses it with traditional data mining techniques. Information extraction, information retrieval, summarization, categorization and clustering are the different techniques of text mining.
In today's world, the amount of stored information has been enormously increasing day by day which is generally in the unstructured form and cannot be used for any processing to extract useful information, so several techniques such as summarization, classification, clustering, information extraction and visualization are available for the same which comes under the category of text mining. Text Mining can be defined as a technique which is used to extract interesting information or knowledge from the text documents. Text mining, also known as text data mining or knowledge discovery from textual databases, refers to the process of extracting interesting and non-trivial patterns or knowledge from text documents. Regarded by many as the next wave of knowledge discovery, text mining has very high commercial values.
The analysis of unstructured data in the form of text requires careful scientific research, since the quality of the analysis affects the decision-making processes in the digital economy. The article proposes a variant of text analysis using the Text Mining technology in the R programming language. The conducted research is aimed at creating an expert assessment of the congruence of text data using the technology of unstructured data analysis.
Due to the rapid rise of digital data collection techniques, a vast amount of data has become available. Unstructured and unsaturated data account for more than 85 percent of present data. Finding acceptable trends and patterns to interpret text documents from huge amounts of data is a major challenge. The process of extracting valuable and nontrivial patterns from large amounts of text documents is known as text mining. There are a variety of strategies and tools for mining text for useful information for future forecasting and decision-making. To boost the speed and reduce the time and effort required to obtain important data, an appropriate and proper text mining methodology is applied. This paper discusses and analyses text mining techniques and their applicability in numerous areas of life. Furthermore, challenges in the field of text mining have been discovered, which have an impact on the accuracy and relevance of the results.
The volume of text that is created and formed each day is dramatically increased and increasing. This colossal sum or volume of profoundly unstructured text cannot be basically and effortlessly handled and seen by computer systems. Therefore, we can get an useful patterns from the text using an efficient and effective techniques and algorithms. Text mining is a strategy for dissecting text records to extricate valuable data and knowledge from the text document, analysis is done using Text mining. Mining contains a few exercises that assists you with deriving data from unstructured text information. Before you'll apply diverse text mining procedures, you might want to begin with text preprocessing, which is that the act of cleaning and rebuilding text information into a usable organization. This training could likewise be a center part of Natural Language handling (NLP) and it generally includes the utilization of strategies like language identification, tokenization, filtering, lemmatization and stemming to design information suitably for examination. At the point when text preprocessing is finished, after that you will apply text mining calculations to get experiences from the information. However, ensuring the excellent of chose highlights from message is a test because of the monstrous measure of insignificant (boisterous) data in message report is a major test, to guarantee the norm of found significance highlights in the message archives for portraying client inclinations because of huge scope terms and information designs.
In recent years, the use of electronic medical records becomes more pervasive. In addition to structured data, part of the unstructured data is also of great value in medical research. How to analyze these electronic documents through automation methods has already become present research focuses. For patients, the information they can provide include basic information, history of present illness (HPI) and Chief Complaint. Our research has tried using such information to identify diabetics with different complications (neurological complications, eye disease, diabetic nephropathy, diabetic foot). Our datasets come from hospital, which include 1622 electronic medical records of patients with type 2 diabetes. Among them, 678 cases were diagnosed non-insulin-dependent diabetes with neurological complications, 359 cases as non-insulin-dependent diabetes with diabetic foot, 316 cases with diabetic nephropathy, the rest 269 cases have eye disease. Our experiment exploit Text-CNN in deep learning technology as our training model, apart from this, we extracted basic information, history of present illness (HPI) and Chief Complaint from these patients as the input of our model for classification, which achieved a relatively good performance (precision: 93.6%, recall: 88.4% and F1: 90.5%). Therefore, the proposed method has been proved to be effective in the classification of common complications of diabetes patients from the EMR of type 2 diabetes patients, so as to carry out an early diagnosis and assessment for these patients.
Due to heavy use of electronics devices nowadays most of the information is available in electronic format and a substantial portion of information is stored as text such as in news articles, technical papers, books, digital libraries, email messages, blogs, and web pages. Mining the knowledge like pattern finding or clustering of similar kind of words is one of the important issues nowadays. This paper focuses on mining the important information from the text data. This paper uses the stories data set from project Guttenbergs William Shakespeare stories dataset for experimental study. R is used as Text Mining and statistical analysis tool in Ubuntu 12.04 LTS Linux Operating System. Frequent pattern mining is used to find the frequent terms, appeared in the documents and word Association among two or more words is measured at a given threshold value. Our algorithm uses cosine similarity in order to measure the distance between the words before clustering. The algorithm may be use to find the similarity between stories, news, emails. In this paper k-means and hierarchical agglomerative clustering algorithm is used to form the cluster.
Nowadays, most of information saved in companies are as unstructured models. Retrieval and extraction of the information is essential works and importance in semantic web areas. Many of these requirements will be depend on the storage efficiency and unstructured data analysis. Merrill Lynch recently estimated that more than 80% of all potentially useful business information is unstructured data. The large number and complexity of unstructured data opens up many new possibilities for the analyst. We analyze both structured and unstructured data individually and collectively. Text mining and natural language processing are two techniques with their methods for knowledge discovery form textual context in documents. In this study, text mining and natural language techniques will be illustrated. The aim of this work comparison and evaluation the similarities and differences between text mining and natural language processing for extraction useful information via suitable themselves methods.
Criminal activity detection in social network by text mining is the process of finding criminal activity by the criminals and help law text mining technique, the ability to detect hidden text from corpus documents. Text mining is process of transforming data from unstructured text to structured text which is easily perceived and processed by humans, but hard for machines to understand without designing algorithms, tools and methods in order to effectively process, such enforcing agencies to keep control of the prevailing crimes Text mining is method deriving high-quality information from raw data through the pattern devising and statistical pattern learning. Text mining is field a multidisciplinary field that relies on data mining, information retrieval, statistics, machine learning, and computational linguistics. The main thing in text mining process of analyzing and exploring is natural language processing, information retrieval, information extraction, content analysis, text clustering, and text classification. All that processes are wanted after you complete a step, the preprocess task. The importance of pre-processing task is to reduce the volume of the corpus textual documents and the tasks involved in that step are text boundary determinant, natural language specific stemming stop-word, elimination, and tokenization to remove unwanted data and handling missing data. Among this, doing the most important work is tokenization. Tokenization assist to divide the text data to individual words, open source tools become available for those interested such as spacy, NLTK with python, Gensim and many other. After that define model architecture to fit the model on the training data and evaluate this model on test simple data in order to predict values.
With the introduction of computer methods, the amount of material and processing accuracy of policy text analysis have been greatly improved. In this paper, Text mining(TM) and latent semantic analysis(LSA) were used to collect policy documents and extract policy elements from them. Fuzzy association rule mining(FARM) technique and partial association test (PA) were used to discover the causal relationships and impact degrees between elements, and a fuzzy cognitive map (FCM) was developed to deduct the evolution of elements through a soft computing method. This non-interventionist approach avoids the validity defects caused by the subjective bias of researchers and provides policy makers with more objective policy suggestions from a neutral perspective. To illustrate the accuracy of this method, this study experimented by taking the state-owned capital layout adjustment related policies as an example, and proved that this method can effectively analyze policy text.
Most common and complex diseases, such as diabetes and cancer, are influenced at some level by variation in the genome. To truly address the goal of translational research, genetic variation must be taken into consideration. Research done in public health genetics, specifically in the area of single nucleotide polymorphisms (SNPs), is the first step to understanding human genetic variation. In addition, novel methods are needed to represent and to conduct text mining over textual genotypic data sources. In this paper, we describe the development and evaluation, in the context of a genetic study, of a translational-informatics method that supports both machine-learning text mining (e.g., Conditional random fields) and automated inference for identifying key concepts (e.g., Hypotheses and results). After scaling for inter-annotator agreement, our adjusted overall precision was 64%, with a range of 48% to 80%. While other biological text mining systems have focused on named-entity recognition, the development of tools for genetic studies focusing on hypotheses and results has been relatively rare.
The peer-reviewed articles and textual data are main source of data in biology. Text mining is solution to extract information from textual data sources that are usually in bulky quantities, messy and disorganized. Dealing with this situation needs to deploy innovative methods and techniques. In this paper, we identify the current heavily used methods for biomedical text mining, their capabilities and developments, some proposed solution and how to evaluate performance. Biomedical specific challenges in text mining context have been studied with respect to proposed answers and then main future trends based on current needs and requirements have been discussed.
In the present paper, a framework for the continued supporting a systematic literature review (SLR) is proposed, which includes the application of text mining methods in order to automate the classification of scientific publications and the more in-depth analysis of their content. For this purpose, a dataset is created from the titles, abstracts and keywords of papers, included in a systematic literature review on the application of semantic technologies in bibliographic databases. Data analytics methods are applied - frequency analysis of words and word combinations; linear regression for trend exploration; text classification, where the categories are the applied semantic technologies or the researched problems in accordance with a pre-defined classification framework. The vector space model enriched with PMI (pointwise mutual information) measure is used for the classification. An assessment of the text classification performance in terms of various measures is made and the obtained results are summarized.
This paper studies the ability of Analogical proportions as a promising tool for text summarization and classification. Analogical proportions (AP) are statements of the form "a is to b as c is to d" usually denoted a : b :: c : d and expressing that "a differs from b as c differs from d", as well as "b differs from a as d differs from c". Analogical inference is based on the assumption that if four items a, b, c, d are making a valid analogical proportion and if d is unknown, this enables us to predict the value of d on the basis of the values of the triplet (a, b, c). Based on the above principle, analogical proportions have recently proven their efficiency to classify "structured" datasets. Our main interest in this work is to validate their efficiency to deal with "unstructured" datasets, especially for Arabic text summarization and classification.
Text Mining is measure in which past or recorded data is conjured utilizing various assets. The data recovery is a field where enormous measure of data is separated utilizing internet searcher to get exact and improved outcomes. As World Wide Web(WWW) gives a decent stage to assortment of data, there is need for some method to diminish time for looking through significant information and to save relentless work. Data on web by and large contains unstructured or semi organized data like content, messages, XML, HTML Pictures, MP3, Recordings and so forth likewise organizations and organizations use to save their data in text design. The new methodology called Text Mining or Information revelation is presented, which is utilized to beat issues looked by Information Retriavel(IR) for unstructured or semi organized data recovery. In this paper we present writing overview for various Content mining strategies, for example, Data Extraction, Theme following, Outline, Arrangement, Bunching, Affiliation Rule Mining (ARM) and EART with utilizations of Text Mining. Today the huge assortment of text mining devices presents the productive and compelling approach to recover fitting data from unstructured information. In this way the content mining method is turning into a significant examination zone to diminish time for looking through the unstructured data.
Descriptive feedback is a powerful tool to identify the strength and the areas that need improvement of a certain program. Text analysis using the text mining approach is the most common application in the text processing field. This study aimed to determine the potentials and the imperfections of the IT interns while taking the internship program using a Naive Bayes text classification. Text dataset from the internship program of the IT curriculum under the College of Computer Studies and Information Technology (CCSIT) of Southern Leyte State University (SLSU) was the input of the study. This study applied the text mining application using the Naïve Bayes text classification to evaluate the performance of the IT interns. Results show that the IT interns of the program were proficient and technically competent but had a problem with their communication skills. The model has obtained an acceptable accuracy rating of 87.55% in predicting the performance of the students. Based on the results, the IT interns were efficient in the IT-related task. However, there is a need to improve interns' communication skills. The management should think about a possible bridging program that will help the student to develop their communication skills.
The emergence of text mining has enabled firms to understand the requirement of consumers in real-time by mining publicly available information from Internet. More and more researchers pay more attention to improving the methods of text mining and the role of text mining has become increasingly prominent. This paper introduces the use of text mining in article screening and classification to help graduate students select more valuable articles and quickly have a clear understand of research status and content. Using data from a popular Chinese database CKNI, we find that thousands of articles appear when we use keywords to search in the database and these articles are complex and disorganized. Our study provide a method to screen and filter these articles, and extract topics for each class. Our study has a great significance to the improvement of graduate students' research ability in a short time.
Text mining, also referred to as text data mining, is the process of extracting interesting and non-trivial patterns or knowledge from text documents. It uses algorithms to transform free flow text (unstructured) into data that can be analyzed (structured) by applying Statistical, Machine Learning and Natural Language Processing (NLP) techniques. Text mining is an evolving technology that allows enterprises to understand their customers well, and help them in redefining customer needs. As e-commerce is becoming more and more established, the number of customer reviews and feedback that a product receives has grown rapidly over a period of time. For a popular asset, the number of review comments can be in thousands or even more. This makes it difficult for the manufacturer to read all of them to make an informed decision in improving product quality and support. Again it is difficult for the manufacturer to keep track and to manage all customer opinions. This article attempts to derive some meaningful information from asset reviews which will be used in enhancing asset features from engineering point of view and helps in improving the support quality and customer experience.
In this paper, we propose the novel text mining algorithm based on the deep neural network. In knowledge representation, capable of embodying the of the information content characteristic and external characteristic not only has the semantic meaning and it is interlinked, the content and the external characteristics constitute the text knowledge mining association reveals and knowledge base. This article through to the contents of a text knowledge characteristics and appearance characteristics of general different combination of co-occurrence analysis, explore the co-occurrence analysis method based on spatial distribution, time distribution and the application of the internal and the external association mapping text knowledge mining. In terms of the types of mining, not only limited to the simple knowledge classification, clustering, we found that can explore richer knowledge mining method, such as semantic rules found, trend analysis, topic tracking, etc. Under this prior basic knowledge, we propose the novel algorithm with the DNN. The experiment result proves the feasibility.
To meet the needs of intelligent government application, an intelligent e-government system based on text mining is developed, and its performance is evaluated experimentally. The tasks of the system include the realization of the correct classification of the public's question messages, the acquisition of the message hot spots and hot words, and the automatic scoring and evaluation on the quality of the message replies. The effectiveness and efficiency of support vector machines, logistic regression, random forest and naive Bayes in classifying public comments were compared and analyzed. A method for hot spot problem mining based on clustering and linear discriminant analysis is proposed. The rules for evaluating the quality of message responses based on relevance, integrity and interpretability are defined. Based on the data set provided by the eighth "Teddy cup" data mining challenge in China in 2020, the experiment and test results show that the classification accuracy and recall rate of messages are both more than 84%, which can effectively screen hot issues.
There are many people using social media to comment appearance of a various event and post own activity in every day. The business organizations and industries can use social media to improve a produce by using the technique of text mining. However, a single machine cannot perform to compute a large amount of data because it has the resource limitation such as CPU, main memory, storage and so on. Moreover, a data also consists of structured and unstructured data that must be prepared before a computation with text mining or machine learning. Therefore, the text preprocessing becomes one of the most very important tasks because it includes many steps. In this paper, we design and develop an efficient text preprocessing framework on a big data infrastructure, which is proposed to support the text preprocessing task to reduce the computation time. The framework consists of the main four modules: Data collection module, storage module, data cleaning module and feature extraction module. We collected the sentiment data from Facebook page, named “Tasty” that has more than 90 million members for the performance evaluation. The data was separated into different three sizes and each dataset was tested on different the cluster system environments. Moreover, we also compared the computation time between the proposed our framework and the single machine. As the result, our framework can reduce the computation time significantly. Thus, this framework can apply to solve the problem of the complex text preprocessing in text mining areas.
The study aimed at analyzing the keywords of the Macau Special Administrative Region's 2012 and 2013 annual policy addresses. The contribution of the study included the following two points. First, the study used the text mining method in order to explore the content of policy address. Second, the study applied the SVM (Support Vector Machine) and random forests classification analysis to explore the relationship of the keywords between the two years' policy addresses.
Traffic safety is one of the main goals when designing infrastructure or vehicles. Many entities are interested in accident data in order to identify current problems. The accurate recording and analysis of accident data, though, is a time-consuming task. New data mining methods allow for a more efficient analysis of large amounts of (unstructured) data. In this paper, accident data provided by the police department of the city of Munich is analyzed both by classical methods considering only classified accidents, as well as with the help of text mining methods, taking into account all descriptions of the accidents. The results indicate that text mining methods can give quick results identifying main problems at the given locations. Using statistical and machine learning methods for the analysis shows promising results. The results of the automatic classification of accidents into accident types based on a learned classification model using the textual descriptions is highly accurate. Retrieving structured information from the descriptions, though, requires a more concise writing of the accidents reports. We conclude that in future, data mining methods could be used to reduce the workload for police officers for both the reporting work as well as the analysis of road accidents.
In the process of scientific research, the naming of Chinese texts is very critical, which often affects scholars' cognition and judgment of things, and also has a certain impact on a series of operations such as experiments. On the other hand, in the process of professional research, many uncommon words often need to be verified by retrieving the corresponding knowledge system. The naming and recognition of these specialized words are very difficult, and there are many complex problems, such as polysemy and the nesting of wings. Therefore, under the above background conditions, this paper uses text mining method to conduct related research on the text naming of scientific research objects and uses the text mining method to construct and improve the text naming recognition algorithm accordingly. The support vector machine method and neural network model are used to build a text naming method suitable for Chinese, and good results are obtained in the application process.
This paper proposes a combination of data mining and natural language processing technology, try to analyze students' learning behavior and content in MOOCs interactive part, to dig their learning interest, difficulty, tendencies, to evaluate their homework effect, through the interaction between teachers and students, students posting, homework or answer content, preventing of cheating behavior, and so on. So that, teachers optimize the course structure, establish of incentive mechanisms to stimulate students' learning desire, improve the completion rate of the courses. This method can dig valuable information from the seemingly random mass data, optimize the corresponding key elements in the teaching model, so that, teaching works go in a virtuous circle.
In recent years, data mining and text mining techniques have been frequently used for analyzing questionnaire and review data. Data mining techniques such as association analysis and cluster analysis are used for marketing analysis, because those can discover relationships and rules hiding in enormous numerical data. On the other hand, text mining techniques such as keywords extraction and opinion extraction are used for questionnaire or review text analysis, because those can support us to investigate consumers opinion in text data. However, data mining tools and text mining tools cannot be used in a single environment. Therefore, a data which has both numerical and text data is not well analyzed because the numerical part and text part cannot be connected for interpretation. In this paper, a mining framework that can treat both numerical and text data is proposed. We can iterate data shrink and data analysis with both numerical and text analysis tools in the unique framework. Based on experimental results, the proposed system was effectively used to data analysis for review texts.
This paper aims to identify the trends in machine learning research using text mining. The researcharticles contain significant knowledge and research results. However, they are long and have many noisy results such that it takes a lot of human efforts to analyze them. Text mining can be used to analyze and extract useful information from a large number of research articles quickly and automatically. Text mining is the method of defining innovative, and unseen knowledge from unstructured, semi-structured and structured textual data. This knowldege contributed to very important information that can derive from textual data. In this paper, text mining methods are applied to detect trends of terms that occur in the research articles and how they varies over time. We collected21,906 scientific papers from six top journals in the field of machine learning published in period 1988-2017 and analyzed them using text mining. Our result analysis shows a changing trend of various terms in Machine learning research in three decades. The analysis of our study helps the upcoming researchers to explore the significant research area of machine learning.
The objective of this work is to propose a text mining based approach that supports Human Resources Management (HRM) in detecting subjectivity in staff performance appraisals. The approach detects three domain-driven clues of subjectivity in reviews, where each clue represents a level of subjectivity. A considerable effort has been directed to detecting subjectivity in opinion reviews. However, to the best of our knowledge, there is no previous work that detects subjectivity in staff appraisals. For proving our approach, we applied it to the teachers' appraisals of the Palestinian government. According to our experiments, we found that the approach is effective regarding our evaluations, where we used: expert opinion, precision, recall, accuracy and F-measure. In the first level, we reached the F-measure of 88%, in the second level, we used expert staff's opinion, where they decided the percentage of duplication to be 85% and in the third level, we achieved the best average F-measure of 84%.
This study is in the initial stage and aims to develop an automatic model to classify the best people management practices. We will use text mining techniques, considered an alternative for analysing large amounts of text. This methodology is advantageous because it allows an in-depth analysis of the data and in this way enable to point out significant trends in best practices for people management. This study also intends to contribute with companies, making possible to convert unstructured data into information easily actionable by managers helping to develop excellent working environments.
In many circumstances, it is quite challenging for researchers to look for papers that match their interests when searched from databases. More often, papers which have been searched are perused individually to precisely identify the relevant contents. In this paper, we present a prototype to test our work-in-progress of an agent-based text mining algorithm that extracts and identifies the context of potentially relevant papers published in the WWW. The prototype involves an interface that enables a user to test the algorithm and view the results. The interface links to an abstract of a paper that the algorithm mines. We conduct tests on three abstracts of selected papers from the literature. We conduct the tests on various threshold settings that filter unnecessary data. The results show that the algorithm successfully identifies the contexts of the tested abstracts with thresholds of between 30% – 60%.
Digitization makes data more readily available, leading to data inflation in recent years. However, the large amount of data makes it difficult for users to find information that suits their needs. The recommendation system is an option to provide accurate information so that it can find and determine information based on needs. This paper proposed to identify and determine the dataset, methods, and objectives of text-based recommendations in Text Mining. Based on several search strategies, datasets mainly from user reviews such as Yelp, Amazon, TripAdvisor, and IMDb are the most used datasets. The recommendation system approach used is Latent Dirichlet Allocation (LDA), Machine Learning Approach, and Hybrid Recommendation to make product recommendations, travel, tourism, and several other recommendations. This paper can be used as a basis and comparison when other researchers want to develop a text-based recommendation system in Text Mining by considering the dataset, approach, and purpose of the recommendation.
Words are the main tool of a lawyer. The analysis of a large amount of textual information makes up the lion's share of legal practice, in particular, when creating an evidence base in criminal proceedings, analyzing pre-trial decisions, analyzing court decisions in similar cases, observing judicial precedent, etc. There are many answers to essential questions hidden in these documents. Every year, the number of legal and law-related electronic text documents that need to be quickly and efficiently analyzed is growing exponentially. Using powerful Text Mining tools will help you discover important legal information and make the right legal decisions.Automatic text classification is one of the most effective tools for finding and analyzing information in jurisprudence. This article presents a Decision tree model using the Chi-square Automatic Interaction Detector (CHAID) growing method for classifying legal texts. A model for automatic classification of text documents related to criminal proceedings has been developed for the dataset of 1,800 text documents used in court proceedings in Ukraine from 2000 to 2020.
Text-mining is gaining popularity and has attained the status of cross-field technology, which brings together soft and hard sciences. This happens by virtue of its applicability in various fields, for instance, Linguistics, for language units' identification and categorization, History, for ancient texts reconstruction, Sociology, for sentiment analysis, and others. In this paper, we make a deep analysis of the methodological background of text-mining. We prove that text-mining has accumulated several methodologies, which make it flexible for various purposes and fuzzy enough to bring into proximity of human-like text comprehension and knowledge extraction. We also make a retrospective journey into the history of text-mining, providing the description of technical means that underpin its appearance and the methods which draw it nearer natural cognitive processes. The abovesaid makes the study unique, for no papers focused on the methodologic and historical background of this rapidly developing technology are available.
Historical text archives constitute a rich and diverse source of information, which is becoming increasingly readily accessible, owing to large-scale digitisation efforts. Searchable access is typically provided by applying Optical Character Recognition (OCR) software to scanned page images. Often, however, the automatically recognised text contains a large number of errors, since OCR systems are typically optimised to deal with modern documents, and can struggle with historical document features, including variable print characteristics and archaic vocabulary usage. Low quality OCR text can reduce the efficiency of search systems over historical archives, particularly semantic systems that are based on the application of sophisticated text mining (TM) techniques. We report on a new OCR correction strategy, customised for historical medical documents. The method combines rule-based correction of regular errors with a medically-tuned spell-checking strategy, whose corrections are guided by information about subject-specific language usage from the publication period of the article to be corrected. The performance of our method compares favourably to other OCR post-correction strategies, in improving word-level accuracy of poor-quality documents by up to 16%.
During a master’s or Ph.D. Thesis, starting a new project or research and development area (R&D), each investigator must get into the subject and scope of interest. For such knowledge, you must perform a literature review and explore what has been published in the area. There are several methodologies, some more scientific as systematic review, and others. Some have a defined protocol but mainly depend on the authors’ analysis. This research includes several database searches to retrieve text to analyze using text mining, to help on the initial step of review – identify keywords. When you are just starting a new R&D and you need to know the buzzwords for further and deeper investigation you can use text mining to help you. This work is a starting point to perform a state-of-the-art in health literacy area. Further than “Health literacy”, the top ten words are “University” and “School” beyond “Medicine”, “Patients”, care and “information”.
Nowadays, misdiagnoses account for a significant portion of medical errors [1]. This is due to the fact that each physician's diagnosis is different depending on the physician's knowledge, skill, and experience. In several cases, physicians may ignore uncommon diseases. Also, after the diagnosis, the physician has to provide ICD-10-CM code. This is a difficult process for most (if not all) physicians. We propose a predictive model for classifying disease from symptoms by applying text mining technique. Our research technique allows physician to diagnose and to access an ICD-10-CM code directly from symptoms. Our models are based on several classifiers such as Decision Tree, Naïve Bayes, Support Vector Machine, and Neural Network. Models from each classifier were compared using training time, predicting time, Receiver Operating Characteristic (ROC) curve, True Positive Rate (TPR), False Positive Rate (FPR), precision and accuracy. The result suggests that Neural Network gives the best TPR at 89.03%.
Text mining is a new field that attempts to bring together meaningful information from natural language text. Automatic Text categorization and summarization is the process of assigning pre-defined class labels to incoming, unclassified documents. The class labels are defined based on a set of examples of pre-classified documents used as a training corpus. This research work comprises an automatic text categorization and summarization approach to analyze the structure of input text. In this work a text analyzer is developed to derive the structure of the input text using rule reduction technique in three stages namely, Token Creation, Feature Identification and Categorization and Summarization. This analyzer is tested with sample input texts and gives noteworthy results. Extensive experimentation validates the selection of parameters and the efficacy of our approach for text classification. This work can be expanded and used in many practical applications, including indexing for document retrieval, organizing and maintaining large catalogues of Web resources, automatically extracting metadata, and Word sense disambiguation, etc.
A questionnaire text mining model is built based on latent semantic analysis (LSA). Take open questionnaire as the research object and improve Latent Dirichlet Allocation (LDA) model according to the text characteristics, a BM25-LDA model is built. By comparing the LDA model and BM25-LDA model on the open questionnaire text, we can verify that the performance of BM25-LDA mode is better than that of original LDA model.
Text classification and information mining are two significant objectives of natural language processing. Whereas handcrafting rules for both tasks has an extensive convention, learning strategies increased much attention in the past. Existing work presented concept based mining model for text, sentence mining and does not support text classification. To enhance the text clustering approach, we first presented Conceptual Rule Mining On Text clusters to evaluate the more related and influential sentences contributing the document topic. But this model might discriminate terms with semantic variation and negligible authority on the sentence meaning. In addition, we plan to extend conceptual text clustering to web documents, by assigning sentence weights based on conditional probability. Probability ratio is identified for the sentence similarity from which unique sentence meaning contributing to the document topic are listed. In this work, our plan is to implement ranking of the sentences which are calculated using the weights assigned to each and every sentences. With sentence rank conceptual rules are defined for the text cluster documents. The conceptual rule depicts finer tuned document topic and sentence meaning utilized to evaluate the global document contribution. Experiments are conducted with the web documents extracted from the research repositories to evaluate the efficiency of the proposed efficient conceptual rule mining on web document clusters using sentence ranking conditional probability [CRMSRCP] and compared with an existing Model for Concept Based Clustering and Classification and our previous works in terms of Sentence Term Relation, Cluster Object weights, and cluster quality.
Increased competitive pressure requires companies to increase their own innovative capacity. In particular, innovation management must identify and analyze the external dimensions of influence that affect the product. Although large amounts of text data are available for this purpose, there is yet no possibility of evaluating them in a structured way and linking the information obtained with each other in order to generate knowledge about the product environment. The presented method enables the derivation of innovation potentials from these text data. First, the external dimensions of influence that affect the product and are thus relevant for the innovation process are identified. Each influence dimension also defines data sources that can be used for the analysis. When analyzing the document collection from the data sources, applications from the field of text mining extract the topic areas of the available documents. Finally, a semantic network links the knowledge gained and shows the dependency of the identified topics in order to generate knowledge about future innovation potentials. The method was validated with a use-case from the sports equipment industry.
With the intelligent development of power grid equipment operation and maintenance, how to effectively use a large number of defect text records has become an important issue. Since the text is complex unstructured data, it is difficult to effectively mine defect information. To solve this problem, the new word discovery method of solidification degree-degree of freedom is used in text preprocessing to extract the word features in the defective text; further, the word2vec word vector model is used to map the word features to a multi-dimensional vector space; finally based on feature fusion Constructed an attention mechanism to optimize the convolutional neural network defect text classification model. The analysis of the calculation example makes a comprehensive comparison and analysis of the attention mechanism optimized convolutional neural network based on new word discovery and feature fusion and the traditional neural network model. The proposed method has better semantic learning ability than the traditional deep learning method and can improve the classification accuracy, which is conducive to fully mining the defect text information.
Text clustering is an important task. Generally, text document clustering methods attempt to segregate the documents into groups where each group represents some topic if the topics are same then they are belonging in the same group if the topics are different then new group can be created with the help of cluster and that different topic store in new group. In this paper has present an clustering on ontology based text mining for grouping paper or proposals and assigning that grouped proposal to reviewers systematically. It facilitates text-mining and some text extraction techniques to cluster approach based on their similarities and then to assign them to reviewer and obtain text summarization. A principled approach is proposed to develop an intelligent information system by analyzing the unstructured repair verbatim data. Construct the fault diagnosis ontology for find out the fault so that extract the irrelevant information. The three text mining technique can be used for creating intelligent information system. The proposed method is that analysis of text summarization and plagiarism analysis and find out the efficiency i.e. time complexity and increase the performance of system using cluster based approach.
Data mining is a technique of extracting hidden predictive information from large databases. The important information discovery from the data is more needed in many areas like industries, education, web mining, text mining, etc. Among this text mining is very important in social media. Text mining also referred as the process of deriving information from text. A social media platform such as Twitter is main stream which provides valuable user-generated information by publishing and sharing contents. Identifying interesting contents from large text-streams is a crucial issue in social media because many users struggle with information overload. Analyzing social audience who are interested in a company of social media is very difficult and so many text mining methods e.g. fuzzy keyword match method, Twitter LDA method and Machine learning approaches are used for solving this problem. Using the tweets of the account owner to segment followers and describing a group of high-value social audience members will assist the account owner to spend resources more effectively by sending offers to the appropriate audience which will lead to maximal marketing performance and more the return on contribution.
Data mining is a very popular research area among researchers from various disciplines. Identification of patterns related with data mining research are very popular. The main objective of this study is to identify the main and subdisciplines of data mining research using text mining techniques. This paper analyzes the highly cited research publications in the field of data mining using text mining methods. Web of Science was used as the source database for extracting the data required for the study. The top 50 articles under Information Science Library Science research area were selected as the study sample of this research. The data were coded separately using titles and abstracts of selected articles. The main disciplines and further sub-disciplines were identified. Based on the categories, term-document matrix was created. The analysis revealed that the prominent research areas related with data mining are GIS, information and bibliometric analysis. Further, it was concluded that these results based on the mining of information from the highly cited publications will be useful for researchers and publishers for academic and indexing purposed.
The exponential development in online social media allows users around the globe the possibility to share and communicate information and ideas freely in different formats of data via internet. This emerging media has become a dominant communication tool and it has been used as a communication channel in several events, especially “The Arab Spring” and BOSTON'S attack etc. In order to develop useful profiles of different cybercriminals, text mining techniques is an effective way to detect and predict criminal activities in microblog posts taking account the problems of data sparseness and semantic gap. The hashtags used on Twitter (e.g., #arabspring, #BostonAttack) contains outstanding indicators to detect events and trending topics especially to target and detect suspicious topics and eventual illegal events. Similarity approach is used in text analysis to detect suspicious posts in microblog publications. The evaluation of our proposed approach is done within real posts.
Stemming is one of the important processes in text mining. The result of stemming process gives an impact on the next text mining process. Therefore, the technique of processing text data such as text mining, natural language processing, and information retrieval for every language have different treatment, especially in the stemming process. Lately, stemming algorithm for Indonesian text has been growing rapidly. However, there is no stemming algorithms that specifically accommodate Indonesian text with slang, whereas text data from social media contains many natural languages and slangs. Therefore, the aim of this research is to improve Indonesian stemming algorithm that suitable for Indonesian text data with slang from social media. We analyzed and elaborated Porter stemmer algorithm to create a new stemmer algorithm that appropriate with Indonesian slang characteristics. This study proposed two factors as the state of the art, (1) a utilization of both Indonesian dictionary and Indonesian Slang dictionary to anticipate re-stemming an infinitive, and (2) accommodating natural languages and slangs to strengthen stemming process on word particles. We examined the algorithm with two scenarios, the first scenario used 379 of words and 20 text data for the second. Furthermore, we compared the algorithm with Porter, Nazief&Adriani, and Lucene stemmer algorithm. The result showed that the proposed algorithm could do stemming process with Indonesian slang well, with the percentage of accuracy about 88.65%. The accuracy level was increase, although the memory usage of the algorithm was no better than the others algorithm, but the duration of process was not different significantly.
Our goal is to support users who want to discover or create knowledge from a large amount of text data. Text mining is a process that extracts novel knowledge from unstructured data. A variety of applications for text mining, such as Total Environment for Text Data Mining, have been proposed. However, to the best of our knowledge, these methods are not effective at conducting text mining tasks aimed at finding novel knowledge. In this paper, we discuss characteristics of the text mining process and propose a design principle for building text mining applications based on two concepts: (1) text mining is an exploratory search task, and (2) text mining is a process for creative knowledge work.
Idea mining is a new and interesting field in the areas of information retrieval research. The thoughts of people are helpful to improve strategic decision making. This paper demonstrates the efficient computational methods of idea characterization based concept by extracting the interesting hidden data from unstructured texts which come in many forms and sizes. It may be stored in patents, publications, reports, documents, Internet etc. We briefly discussed a number of successful text mining tools and text classification to extract the idea with a combination of idea mining measures.
Vulnerabilities have always been important factors threatening the security of information systems. The endless vulnerabilities pose a huge threat to the social economy and public privacy. The vulnerability database provides abundant materials for researchers to study the threat of vulnerabilities, while mining the text information of the database and obtaining valuable information can help to grasp the severity level of the vulnerability. Based on the textual description of vulnerabilities in the database, we first use text mining to extract main features. Then we utilize principal component analysis to gather sparse features which take sparse characteristic into consideration. Finally we use XGBoost to intelligently predict the severity level of vulnerabilities and compare them with the results of other machine learning methods based on same extracted features. The experiment on real-world vulnerability text description show the effectiveness of our method.
Right now, the world is busy with the COVID-19 pandemic. Coronavirus disease (COVID-19) itself is an infectious caused by a new variant of the newly discovered coronavirus. One way to deal with the virus is to get vaccinated against COVID-19. The government through the Indonesian Ministry of Health is also promoting the procurement of this COVID-19 vaccine by bringing various types of this COVID-19 vaccine. This research was conducted to know the sentiment and perception of the Indonesian people about the COVID-19 vaccination program. To find out, this research uses the Text Mining technique using Twitter as a data source. Data processing and analysis in this research used the Naive Bayes Classifier method using Python software. The results of this study show that the sentiment and perception of the Indonesian people to vaccination against COVID-19 is positive, as evidenced by the Confession Matrix value leaning towards True Positive.
Software requirement specification is a document that can be used as a guide for developers to develop applications. This study uses SRS from the Penguin application to help determine the development of class diagrams based on use case and sequence diagrams using the text mining method. The results of this process will be calculated for similarity, after which validation and testing will be carried out using Gwet’s AC1 and Cohen Kappa. Based on the results and discussion, three artifacts were formed, namely actors from use case diagrams (AUC), objects from sequence diagrams (OSD), and class names from class diagrams (NCD). The three artifacts produce two comparisons in the formation of class diagrams. The first comparison is between AUC and NCD, with the highest cosine similarity score of 0.666. From this score, the resulting construction of class diagram component names is seller and customer. The first comparison also resulted in a score of 0.088 for Cohen Kappa and 0.756 for Gwet’s AC1. Furthermore, for the second comparison, between OSD and NCD, two results were obtained with the same score, namely 0.9. This score resulted in the formation of class component names such as seller, transaction page, revenue page, expenditure page, and penguin app system. And the second comparison has a Cohen kappa score of 0.112 and 0.926 for Gwet’s AC1 score. The results of the Cohen Kappa score, and Gwet’s AC1 can be used as recommendations for improving class names that match the actors names in use case diagram, and object names in the sequence diagram.
This paper analyzes 152 type-2 diabetes Traditional Chinese Medicine (TCM) records via text mining methods with the aim of identifying the key medicines, prescriptions and formulae when taking patients' TCM syndromes into consideration. After structuring the TCM syndrome variables according to the diagnostic scale of TCM syndrome elements, a Chinese segmentation method was adopted at the initial stage during text mining. K-Medoids method was selected to cluster the TCM records. Eventually, a FP-Growth algorithm was applied in this paper for the purpose of discovering hidden relationships between syndromes and prescriptions whose confidence values are relatively higher. In terms of the results, this research has shown 71% accuracy in test sets. Additionally, all three senior TCM doctors deem the result feasible and acceptable.
Because of large amounts of unstructured text data generated on the Internet, text mining is believed to have high commercial value. Text mining is the process of extracting previously unknown, understandable, potential and practical patterns or knowledge from the collection of text data. This paper introduces the research status of text mining. Then several general models are described to know text mining in the overall perspective. At last we classify text mining work as text categorization, text clustering, association rule extraction and trend analysis according to applications.
Text mining is a process of extracting knowledge from large text documents. A new probabilistic classifier for text mining is proposed in this paper. It uses ODP taxonomy and domain ontology and datasets to cluster and identify the category of the given text document. The proposed work has three steps, namely, preprocessing, rule generation and probability calculation. At the stage of preprocessing the input document is split into paragraphs and statements. In rule generation, the documents from the training set are read. In probability calculation, positive and negative weight factor is calculated. The proposed algorithm calculates the positive probability value and negative probability value for each term set or pattern identified from the document. Based on the calculated probability value the probabilistic classifier indexes the document to the concern group of the cluster.
Text mining is the process of extracting the word from the document which is collected from various sources. It is used to transform the text into normalized data for performing machine learning operations. Feature extraction process in the text considers various words in the text then converted into features. Traditional extraction methods extract the features without semantic meaning of the data due to different data formats. Because of the natural language representation, the text mining process uses the document as an input and evaluates the meaning and relationship between the documents. The main objective of the proposed analysis for extracting the features forms the text with various models and parameters. Bag of Words, GloVe, Word2Vec, TF-IDF and Doc2Vec models are analyzed. This analysis considers parameters like cost, number of iteration, learning rate, similarity measure and object relationship for performance measurement. This model uses the Wikipedia data source for analysis and gives raw data with high volume
When reading a document text such as news or article, people tend to read related information topic. However, when they try to use search engine for looking into it, they don't have any idea what the keywords are. Copying the whole text to the search engine's bar is not the best solution, since it will increase the searching process time but make the search engine nonoptimal. It also makes people get unrelated topic. Therefore, this research has a purpose to make an application that generates keyword from inputted text of paragraph. Despite of input the whole paragraph or text in the search bar, user can input the text in the developed application and get the best keyword based on the text. The main method that has been used in this research is a text mining. First, we perform a pre-processing which are turn the inputted text to be in lower case. The second, we remove the stop words and some characters from text. The third, we perform a tokenizing method to separate each word and store it in an array. The main step is calculating the score of each word that has been stored in array. The result of this research is three words that generated by the system from inputted paragraph text. The keywords that generated by system are the same with the method's calculation. However, this application system needs to improve more to get a better performance.
This paper analyzes the possibilities of using text mining methods to solve project management problems. Text mining is an area of artificial intelligence. The use of text mining technology allows to find useful, previously unknown information in text arrays. Due to the fact that during the implementation of projects a lot of textual data is required, presented in a certain form (paper and electronic documents, letters, between project participants, etc.) and often unstructured, technical tools are needed that can process this information and extract useful knowledge from it that can help projects achieve their goals faster. The article includes the technology of text mining itself, the main approaches and methods that are currently used in this area. Things are impossible due to the fact that it is the use of text mining algorithms that can help in solving project management problems.
A fundamental preliminary element for discovering knowledge is the event log. This discovery knowledge from the log can help us to analyze and recognize certain patterns (features) that occur during seismic events. Therefore, in this work, we propose to design the log for seismic events from the information stored in the MySQL database, through the SeisComp3 system and the information of the scientific-technical staff of Instituto Geofísico at Escuela Politécnica Nacional. The design of the mentioned log is the product of the integration of the structures of the models obtained from text, process and opinion mining applied to the primary data with the log-book and the reports of the experts in seismology available in this Institute. Thus, the log will store data that would allow identifying the features of these physical phenomena through intention mining techniques.
In this digital era most of the information is made available in digital form. For many years, people have held the hypothesis that using phrases for a representation of document and topic should perform better than terms. In this paper we are examine and investigate this fact with considering several state of art datamining methods that gives satisfactory results to improve the effectiveness of the pattern. Here we implementing pattern detection method to solve problem of term-based methods and improved result which helpful in information retrieval systems. Our proposal is also evaluated for several well distinguish domain, offering in all cases, reliable taxonomies considering precision and recall along with F-measure. For the experiment, we use Reuters (RCV1) dataset and the results show that we improve the discovering pattern as compared to previous text mining methods. The results of the experiment setup show that the keyword-based methods not give better performance than pattern-based method. The results also indicate that removal of meaningless patterns not only reduces the cost of computation but also improves the effectiveness of the system.
In this paper, an effective method for computing term association from a text corpus is presented. Two machine learning algorithms are employed to evaluate the effectiveness of the proposed method for text mining. The co-occurrence based term association method is to overcome the problem of lack of relationship between words for keyword based text mining and improve the performance of text mining. The experiments are conducted on the standard Reuter-21578 data set and 20 news group data set. Different number of associated terms are compared in the experiments. Experimental results show that the proposed method can achieve better results on different machine learning algorithms when measured by F measure.
Social media is considered one of the most effective platforms to communicate between companies and customers. Frequently, the customer of a product or service sends complaints via social media. Customers' complaint data serve as a good suggestion for companies and organizations to improve their products and services. With the increasing number of customer complaints that have entered through social media accounts, government-owned drinking water companies need a more efficient way to extract information from complaint data. In this research, text mining is used to extract information about customer complaints against drinking water companies from social media Twitter. Latent Dirichlet Allocation (LDA) and self-organizing maps (SOM) approach is applied to model complaint topics and find out which are most frequently complained. The test results indicate grouping the data into five classes is the most appropriate model. Pipes leakage are the most frequently reported topics, 27.8% of total datasets.
Many data mining techniques are used to extract the patterns from the text documents. But the challenge is using those updated patterns is still a open research issue. Most of the text mining methods generally uses the term based approaches. The main problems faced by the term based approaches is of polysemy and synonymy. This paper focuses on the implementation of an particular discovery way to discover the pattern as well as use them to retrieve the relevant document. Two important process are used i.e pattern deploying and pattern evolving is included as the pattern discovery techniques. These techniques finds to be helpful in improving the effectiveness of the discovered patterns. These discovered patterns can be used in searching the relevant and important information for text mining. To improve the effectiveness of using and updating discovered patterns according to users view, experiments on RCV1 data collection achieves user required data from a large document by removing noisy pattern by pattern evolving.
Chemical Text mining techniques have been extensively used to reveal interesting patterns and relationships between proteins, genes, disease and drug from biomedical and chemical literature. Various chemical Text mining tools were proposed for mining chemical data from various chemical databases like PubMed, Drug-Bank, etc. The exponential increase in the generation and collection of data has led to the new era of information extraction and data analysis. Mining the huge and massive literature data on conventional general purpose systems can be a tedious task which is unable to meet high computational requirements of text mining. To accelerate this process parallel Text mining operations can be performed using Graphical Processing Unit (GPU). This paper provides the literature on various Chemical text mining tools and techniques. This paper also presents a scalable framework for high performance GPU accelerated Chemical Text mining for relationship identification between chemical entities on heterogeneous platform providing a method of balancing the workload between CPUs and GPUs for Maximum utilization of diverse commodity hardware.
Topics selection for an educational material can take a lot of manual work. The manual operations can be exhaustive, especially in case of large volume of materials. In order to overcome this problem, we have proposed an automated topic selection approach, which is able to select topics automatically for any educational material with a consideration of achieving course specifications. Our research focused on text mining and n-gram analysis. In addition, filtering criteria was applied to improve efficiency and to eliminate as many irrelevant or non-critical keyphrases as possible. The proposed method was applied on educational materials in Institute of Statistical Studies and Research (ISSR), information technology and Computer Sciences department, Cairo University and in the American University of Beirut (AUB), electrical and computer engineering department, Beirut, Lebanon. The results show that the automatic selection technique is more reliable than the manual selection and reduced a lot of time and effort for course coordinators and teachers to choose the topics that will be taught and discover them automatically.
Information explosion and availability of information in various forms has changed the shape of information centres and nature of information profession and professionals. Information profession and professionals have been impacted by the exponentially increasing volumes of information available - as well as with changing attitudes and behaviour of information seeker toward electronic resources. It is very difficult to find the required pieces of information in the bundled of scattered information. The task becomes more challenging, when we find that over 90% of the information available is in unstructured and semi-structured forms, which is very difficult to search. Here text mining has come as a tool to help Information professionals to find the relevant information and deliver to its users. Text mining is used as a technology for analyzing large volumes of structured/unstructured textual documents. Text mining has very high knowledge and commercial values. The aim of Text mining is generally to strengthen decision making and internal operations processes of any organisation and generation of new domain of knowledge. These technologies help to increase the utilization of Knowledge Management (KM) systems and pro-actively help information professionals to improve their competencies and thus productivity of the organization. This article discusses the basic concept of text mining, its framework and text mining products and tools. It also discusses the benefits and challenges of text mining and examines the role of Information Professionals in Text Mining.
As a frequent fault of distribution network, the accumulated large number of trip documents are mainly handled manually, which is inefficient and has strong subjective factors. In order to solve this problem, in order to constitute a complete text causality fault phenomenon and the cause of the problem for mining the goal, put forward an intelligent power distribution line tripping fill in text mining method, the method using the distribution trip provided text narrative logic characteristics, puts forward the fusion of word segmentation and part of speech and syntactic analysis results trip provided text extraction strategy; On this basis, a two-stage filtering method is proposed. Firstly, the distributed high-dimensional vector similarity is used to realize the preliminary filtering, and then the final result of text mining is determined based on the text editing similarity. A case study based on a province shows that the accuracy of the proposed method can reach more than 72%, which significantly improves the efficiency of text processing and can initially meet the actual needs.
The approaches used in text mining procedures will be discussed in this study. This study offered a new similarity measure method that outperformed the K-NN and Naive-Bayes approaches. The proposed approach improves the accuracy of text classification methods while also implementing an effective term-weighting strategy that gives the maximum accuracy when compared to other text classification methods. We evaluated the performance of the different TF-IDF models and proposed TF-IDF to perform text classification by comparing performance parameters of KNN and Nave Bayes Classification with different Weighting Methods, Weight Information Gain, SVM, and proposed method and comparing performance parameters of KNN and Nave Bayes Classification with different Weighting Methods, Weight Information Gain, SVM, and proposed method.
In this paper, we develop a text mining algorithm that influences the identification of relevant literature studies. The algorithm consists of three processes, detection process; preparation process; and mining process. The detection process includes the determination of document language and abstract and keywords. The Preparation includes the processes, split content to paragraphs; paragraph length determination; converting text to lower case; text typography factor; content tokenization, removing stop words. Finally, the mining includes the processes, regular expression; normalization; grouping and computing frequency. The proposed algorithm would be useful in providing an alternative means of searching highly relevant content from large databases.
In our day to day life we come across unstructured data in many forms. These include books journals, audio / video files and unstructured text such as emails, web pages and documents. And these data can be a vital source in order to make informed decisions. For example in any company there is a set of people who can be identified as the paramount from among its workforce. Identifying what is common among them and identifying others like them would undoubtedly improve the output of the company. This is the basis on which this research was carried out. The central aspect of the research was to use text mining techniques to mine the data in a set of documents and identify what are the common characteristics among them and then to identify other documents which contains these characteristics.
In 2015, the United Nations proposed 17 Sustainable Development Goals, SDGs, as the guidelines for all countries in the world to promote sustainable development before 2030. Government Research Bulletin (GRB), the research projects and technical reports sponsored by government, which has long-term, numerous, complete research method, technology development and policy analysis information in Taiwan. Therefore, it is an important and effective way to explore SDG-related information from a large amount of GRB text. In this paper, a novel technologies trend and associative analysis framework which uses contextual text mining and ontology methods is proposed and applied to SDG 7, which "Affordable and Clean Energy". First, we integrate dictionary-based method and semantic textual similarity analysis algorithm to obtain a SDG 7 classifier which can exactly and quickly classify a large amount number of GRB text to SDG 7. Then, two major SDG 7 analysis procedures based on the classification results are implemented. One is using contextual text mining algorithm to obtain energy technologies trend information. The other is adopting ontology method to establish energy technologies associative analysis concept map. According to the analysis results mentioned above, we are able to efficiently incorporate the energy technology with long-term trend, energy technology associative information, and the most influential authors on the specify energy technology in order to generate a global strategy for continuous improvement in Taiwan.
Across the landscape of high technology companies, knowledge is a vital core resource at the heart of the organization. The technological complexity is steadily rising, tightening the global demands which are leading to an aggravating conflict on the product: it has to be produced faster, cheaper, more customer related and all without failures, while the change of complexity is inviting new unseen potentials for faults. On top, a new need for an agile production is arising. While todays organizational knowledge is captured and managed by an increasing number of processes and methodologies, solutions to continuously reintegrate the captured knowledge into the overall process of design and production are still scarce. This paper presents an integrated and agile process that proposes an integrative text mining architecture for design and process analytics.
Gigantic amount of content is generated every second on the web. Blog posts, tweets, videos, uploaded images maintain the rich and increasing content of the social media. Great amount of information is obtained from social media through text mining techniques which are being improved and developed continuously. Due to the increasing variety of social media, data is flooding into every business and techniques of gathering information from unstructured data becomes a challenge. Processing and analyzing of large collections of social media text is difficult by using traditional single computer methods. Fortunately, with each passing day, the development and improvements in Big Data infrastructure are promising and the analysis capabilities are also increasing. This infrastructure supports language independent researches on text mining. There can be diversified searches with language-specific definitions and assumptions. Since many of the studies on social media text mining analysis focus on English language, this paper would help enhance text mining efficiently in Turkish language which is a word order free agglutinative language. The linguistic aspects of Turkish language make text mining complicated. Suffixes can change the word meanings easily. Word stemming becomes elaborated due to indigenous letters. The intent of this article is to demonstrate the issues, methods and applications of qualitative approaches that enable text mining in Turkish language considering its compelling features with covering preprocessing, feature selection and topic clustering on text mining considering Turkish linguistic features.
Stock market has become one of the major components of economy not only in developed countries but also in third world developing countries. Making decision in stock market is not really easy because a lot of factors are involved with every choice we make. Therefore, a lot of analysis is required to make an optimal move on stock market which may involve price trend, market's nature, company's stability, different news and rumors about stocks etc. The objective of this study is to extract fundamental information from relevant news sources and use them to analyze or sometimes forecast the stock market from the common investor's viewpoint. We surveyed the existing business text mining researches and proposed a framework that uses our text parser and analyzer algorithm with an open source natural language processing tool to analyze (machine learning and text mining), retrieve (natural language processing), forecast (compare with historic data) investment decisions from any text data source on stock market. For our research we used the data of Dhaka Stock Exchange (DSE), capital market of Bangladesh.
Medical text mining has gained increasing popularity in recent years. Now a days, large amount of medical text data are daily generated in health institutions, but never refer again as it is very time consuming task. In Radiology domain, most of the reports are in free text format and usually unprocessed, hence it is difficult to access the valuable information for medical professional unless proper text mining is not applied. There are some systems existing for radiology report information retrieval like MedLEE, NeuRadIR, CBIR but very few of them make use of text associated with image. This paper proposes a text mining system to deals with this problem by using statistical machine translation approach. The System stores the text and image features to find the match report. The SVM classifier is use in SMT approach to check whether entered report present in database or not. The system will return the similar report match with the entered report from the database.
This study investigated the clusters of research topics written by undergraduate students of Informatics Dept., Satya Wacana Christian University (SWCU) from 2015 to 2019 using text mining techniques. Eight hundred and twenty-seven (827) final year project (FYP) reports' abstracts were retrieved from the University Library database. Text-mining techniques were used for identifying topics of these reports, and developed a hierarchal connection among these topics. Student reports were grouped into two domains with four groups containing nine clusters based on abstracts analysis. The clusters obtained are web & mobile applications, desktop applications, network security, network infrastructure, random generators, implementation of web service-based cryptography, cryptography in institutions, general block chippers, and block chipper with pattern. Distributions of the research topics were discussed in relation to Friedman's tower of achievement, and its implications on the curriculum.
The explosive growth of genomic data has lead to the growth of published literature of genomic text submitted by researchers all over the world. The literature database like Medline has grown tremendously in the upcoming years. Analyzing the literature for finding association of biological components has become an important domain area. The analysis of gene drug association from text has become an important research area in the field of biomedical text mining. This paper provides with a detailed study on various methods used for gene drug association discovery. The methodology for analysis of gene, drug associations related to diabetic disease using Dictionary-based term identification approach consists of two main important phases. The first phase makes use of the preprocessing text mining techniques like stop word removal, tokenization and stemming. The second phase corpus is constructed for analyzing gene drugs associations using dictionary based approach.
This paper identified the emerging developmental trends of Ghana, an African country, based on online and offline textual, video and audio data sources from 2008 to 2018. In all, about 9,501 text files were collected and where needed, transcribed from these sources and content analyzed by series of text mining processes to bring out the convention for all the combined years. Topic modeling was the main adopted text mining technique. We initiated the topic modeling with the term frequency approach. The results from the mining processes were conceptually presented in relative frequency table and visually summarized as word clouds to map meaningful findings that were logically and humanly understandable. Through this approach, 45 major developmental themes emerged of which education seems to be the most frequently discussed trend in Ghanaian politics and government. In conclusion governments can detect developmental layouts from textual data.
Digital banks are new entrants in the banking industry in the Philippines as they only started late 2018. Since then, a handful of players have and are still emerging. With more and more people becoming technologically savvy, it is very critical for financial institutions to develop a digital banking application that will stand out from the competition. This paper aims to use text mining methods to analyse digital banking application reviews. This study will perform topic modelling using LDA to explore customer concerns and will mine association rules between the digital banking features with the review score. The results will reveal which areas the digital banking application can further optimize for customer satisfaction and retention.
This tutorial will introduce attendees to the HathiTrust Research Center and its tools and services for computational text analysis research. HTRC leverages the scope and scale of the HathiTrust Digital Library collection to create opportunities for researchers to perform text data mining on subsets of the corpus. Attendees will develop skills that will allow them to conduct basic text analysis research using HathiTrust data, learn techniques for engaging with scholars on their text data mining research, and explore strategies for computational access to large-scale digital collections.
One of the foremost requirements of modern man is undoubtedly the possibility to access the desired information with minimal effort. And it is an undeniable fact that the amount of storing information digitally is increasing minute by minute. As people are moving to a digital world of information or knowledge. Whenever one wishes to collect some information, the first task is to identify the relevant points from the vastness of digitally stored data. The upsurge and flourishing of Text Mining paved the way for a new beginning in the area of Information Extraction (IE) and Information Retrieval (IR). As the term suggests, it is to mine relevant information from the text document. The text document could be un-structured or semi-structured. There exist different approaches and methods for text mining and most of these techniques are computational linguistics with python library related functions. The entitled paper attempts to provide a deliberation on the existing various concepts, techniques, pre-processing steps, applications and issues of text mining.
Knowledge discovery from textual database refers generally to the process of extracting interesting or non-retrieval patterns or knowledge from unstructured text documents. This is also called as text data mining or knowledge discovery. It can be viewed as an extension of data mining or knowledge from structured databases. At present, the stored information is increasing tremendously day by day. This is unstructured form so we can not extract the needed information. Some data mining techniques are used to extract the useful information from text documents, such as classification, clustering, visualization and information extraction. Here framework of text mining with techniques is discussed as well as benefits and limitations of text mining have been discussed.
In the light of the BERT model relies on masking part of the word to achieve bi-directional prediction of text, but does not take into account the possible association between masked and unmasked words, resulting in loss of information and long-term dependency and inadequate context-deep semantic mining in RNN, an emotion classification model based on XLNet BiLSTM is used. This paper first uses the XLNet pre-training language model to generate context-dependent word vectors to represent text information in a distributed way; Then the word vector is input into the BiLSTM network to analyze and calculate the deep semantics of the text; Finally, softmax function is used to output emotional polarity classification. Train and test the model by mining the text about the Lantern Festival on some social and media platforms. The experimental results show that the accuracy rate of the model is 0.958, and the loss rate is 0.182.
Most forms of text contain typos and many algorithms have been proposed to find them, but not much attention has been paid to understanding the occurrence of typos and their indirect significance in understanding how humans interact through text. In this paper we propose some ways in which typos can be exploited to reveal details about human interaction and closeness. We also look at how they can be used to improve automatic document classification into formal and informal types.
Twitter is one of world most famous social media. There are many statement expresed in Twitter like happiness, sadness, public information, etc. Unfortunately, people may got angry to each other and write it down as a tweet on Twitter. Some tweet may contain Indonesian swear words. It's serious problem because many Indonesians may not tolerated swear words. Some Indonesian swear words may have multiple means, not always an Indonesian swear word means insulting. Twitter has provide tweet's data by account, trending topics, and advance keyword. This work try to analyze many tweet about political news, political event, and some Indonesian famous person because the tweet assumed contains many Indonesian swear word. The derived tweets will process in text mining and then analyzed by classification process using Naive Bayes Classifier, Nearest Neighbor, and Decision Tree to detect Indonesian swear word. This work expected to discover the high accurate classification model. It means, the model can differentiate the real meaning of Indonesian swear word contained in tweet.
Enabled by the increased connectivity and ease of access to online services, an increasing number of countries are moving towards participatory decision making. In Jakarta, the Government deploys an e-participation tool to directly involve community associations and citizens in the development planning of the province. The increasing number of participation and the need of immediate response encourage the use of text mining to classify the complaints and proposals automatically. A classification model was built using Support Vector Machine (SVM) algorithm. This study was the first part of research on community complaint and proposals before text clustering and visualization. The result of model development for classifying documents showed that classification model with stemming and synonym recognition was the most accurate among others with 91.37% accuracy rate. Adding the number of training data would improve the accuracy. Based on classification result, the problem of flood and transportation became the most reported problems during January and February 2016. This result indicated that these problems need to be prioritized by the Government of Jakarta.
Eliminating all stop words from the feature space is a standard practice of preprocessing in text mining, regardless of the domain which it is applied to. However, this may result in loss of important information, which adversely affects the accuracy of the text mining algorithm. Therefore, this paper proposes a novel methodology for selecting the optimal set of domain specific stop words for improved text mining accuracy. First, the presented methodology retains all the stop words in the text preprocessing phase. Then, an evolutionary technique is used to extract the optimal set of stop words that result in the best classification accuracy. The presented methodology was implemented on a corpus of open source news articles related to critical infrastructure hazards. The first step of mining geo-dependencies among critical infrastructures from text is text classification. In order to achieve this, article content was classified into two classes: 1) text content with geo-location information, and 2) text content without geo-location information. Classification accuracy presented methodology was compared to accuracies of four other test cases. Experimental results with 10-fold cross validation showed that the presented method yielded an increase of 1.76% or higher in True Positive (TP) rate and a 2.27% or higher increase in the True Negative (TN) rate compared to the other techniques.
This study aims to read from environmental report which corporate management policy and strategy would promote the motivation of corporate behavior, especially the environment-related activities by listed companies. To do that, we analyze the relation between qualitative data and quantitative description using text mining technique. Our study aims to derive the result of more detailed analysis to confirm the change of positive factor by analyzing with text mining technique such as corresponding analysis, for environmental report in 2000-2011 in Japan. Time series analysis reveals that as CSR concept is widely disseminated, the focus of massage shifts from pollution aids to more comprehensive activities, which implicitly implies that the environmental report is currently recognized as a useful tool to effectively communicate a firm's social activities.
An abbreviation is often used in scientific communications when a concept is needed to be addressed repeatedly. Although some concepts can be described with a single word, e.g., mechanics, phrases are commonly required for more elaborated concepts, e.g., quantum mechanics. It is believed that by extracting abbreviations from full texts on a scientific topic could help expand the domain knowledge of a novice. A text mining algorithm is used to extract the abbreviations from 247 full texts on predictive maintenance - a manufacturing topic - hosted on ScienceDirect. It is found that the abbreviations extracted are useful for generating domain-specific dendrograms, which can provide valuable information during literature review on a scientific area. This paper is the second of a multi-part study, namely, performing literature review using text mining.
This work describes an investigation of formal grammar with application to text mining. It is an important area since text is the most widespread type of data and it contains a lot of potentially useful information. Unstructured nature of text requires other methods for its processing, in contrast to other types of data mining. In this work, the authors propose an original approach to text mining by making a parse tree for each sentence using regular grammar and creating an ontology and provide a demonstration of this system being implemented in a constrained scenario. This ontology can be used for different tasks, ranging from expert systems to automatic machine translation. The ontology is a network consisting of concepts linked by relations. The authors developed a new system to implement proposed approach working in different languages.
With the widespread application and vigorous development of internet technology, e-commerce is growing fast. The sales of products in leisure industry towns have also shifted from offline to online. However, for sales managers, the challenge lies in accurately predicting online sales to increase revenue. Traditional sales forecasting relies on subjective human experience and statistical methods, lacking accuracy and efficiency. Therefore, research on an intelligent analysis mechanism for online sales based on text mining is of great significance. We explored the establishment and operation of an intelligent analysis mechanism for online product sales using text-mining technology. We preprocessed sales data and constructed intelligent analysis models for online sales. The result of this study provides a foundation for intelligent analysis of product sales.
With the rapid development of Internet, we have entered an era of information explosion, there is a lot of redundant information in the Network. How to extract a useful part of this information from the massive information resources, analyzing the vast amount of information and finally get the potential knowledge we want to extract. Web mining technology came into being, and saved out the human from the information ocean. This paper will analyze the realization of Web content mining and Web structure mining, their basic algorithm principles and their application areas.
Based on the introduction of text mining software Text Analyst and Word Stat, this paper analyzes their characteristics and their application cases in text mining. The paper specifies the use requirements and considerations which help people better develop and use Text Analyst and Word Stat to do knowledge discover and management.
Multidimensional knowledge representation (MKR) is the result of integrative text mining. Analysis results from individual text mining methods such as named entity recognition, sentiment analysis, or topic detection are represented as dimensions in a knowledge base to support knowledge discovery, visualization or complex computer-aided writing tasks. Extractive text summarization is a content-oriented task which uses available information from text to shorten its length in order to summarize it. In this regard, a MKR knowledge base provides a structure which is applicable as an innovative selection instrument for text summarization. This paper introduces cross-dimensional text summarization based on dimensional selection and filtering of results retrieved from MKR knowledge base.
Discourse processing is a suite of Natural Language Processing (NLP) tasks to uncover linguistic structures from texts at several levels, which can support many text mining applications. This involves identifying the topic structure, the coherence structure, the coreference structure, and the conversation structure for conversational discourse. Taken together, these structures can inform text summarization, essay scoring, sentiment analysis, machine translation, information extraction, question answering, and thread recovery. The tutorial starts with an overview of basic concepts in discourse analysis - monologue vs. conversation, synchronous vs. asynchronous conversation, and key linguistic structures in discourse analysis. It then covers traditional machine learning methods along with the most recent works using deep learning, and compare their performances on benchmark datasets. For each discourse structure we describe, we show its applications in downstream text mining tasks. Methods and metrics for evaluation are discussed in detail. We conclude the tutorial with an interactive discussion of future challenges and opportunities.
Ask friends about a particular subject are a common situation in the daily life of a person. In virtual environments is a little more difficult. Virtual environments do not always allow face contact that helps people meet and share their experiences to answer questions about a particular subject. In this article are presented: a text mining method for obtaining the knowledge of people from forums and a recommender system that recommends people to ask them about a particular subject.
This paper studies the development history and research status of text mining. Text mining technology commonly used to obtain valuable information and knowledge. Based on the use of open source Python modules , " Art of War of Sun Tzu " was analyzed through the visualization technology to show the key contents of the book, so that everyone has a more vivid and intuitive understanding of this famous masterpiece.
Mining activities pose a major threat to the safety of mine workers due to the inherent risky nature of the job. To realize the objective of preventing fatal and non-fatal injuries, it is imperative for a mine management to analyze site specific mine safety data. This paper is the first attempt to automate the analysis of Directorate General Mines Safety (DGMS) fatality reports on Indian non-coal mines. It is done by text mining and natural language processing (NLP) techniques. The proposed method sidesteps multiple man hours of scrutinizing each report and technical expertise required in analyzing mining accidents. The authors have taken 6 years' worth of data ranging from 2010–2015 published by DGMS. The attempt revealed valuable insights into the causes of injuries such as accidents occurrence is highest in states of Rajasthan followed by Andhra Pradesh. The leading indicators of fatality included falling from heights, age between 28 to 32 years, worker class of ‘mazdoors’, and shift timings between 10 AM and 2 PM. Furthermore, results show accidents happened mostly in small scale stone mines. This will be a step forward for the Indian mining industry to extract meaningful results from more such text data quickly and accurately to make better decisions. The methodology can be extended to the Indian coal mining industry easily.
The volume of texts produced daily is growing significantly. Computers have difficulty processing and interpreting this considerable amount of largely unstructured data without using feasible techniques. Efficient and effective techniques and algorithms are important in exploring useful patterns. Text mining (TM), the process of extracting useful knowledge from text, is gaining considerable research attention. In this article, we explain the natural gradient independent component analysis (NGICA) of text classification (TC), which is an application of TM involving the preprocessing of text data followed by classification. An unsupervised framework of machine-learning classification is proposed based on NGICA for TC. A manually labelled dataset is used to assess the proposed model, and its validity is proven through experiments. The proposed model achieves an overall F-measure of 83% for TC. Results show the potential of the proposed model for Compared with the infomax method that provided 81% of an overall F-measure for TC.
Social media has become a major source of information in recent years, with millions of posts every minute if not seconds. Containing information on various topics like health, politics and sports, one cannot deny that social media has become a good leverage for the field of data analytics. The objective of this work was to apply techniques of text mining, data analytics, and machine learning to Implement a web application, HealthMine, which extracts and classifies relevant data collected from social media platforms, namely Twitter and MedHelp, following a health-related user query. This would spare the user the burden of filtering out irrelevant information and focusing more on what is relevant. However, since content on social media is user-generated, the reliability is dubitable and the advice of a certified medical practitioner is always recommended. The main purpose of the tool is to allow users to share experiences. The tool was evaluated using 1400 tweets and 1800 MedHelp posts and it was found that Naive Bayes Classifier yielded the best accuracy among other classifiers, with an accuracy of 86.3 and 76.6 for Twitter and Medhelp respectively.
Over the past decade, unstructured text data have increased significantly. Text data are utilized in various scientific research, such as sentiment analysis, semantic analysis, context extraction, or named-entity recognition. Nowadays, widely used Large Language Models (LLMs) are also based on text data. Depending on the type of task, different algorithms can be used to analyze the text data, such as classification, clustering, or the latest transformer models. In this paper, a systematic literature review of text data mining has been performed. During the research, the analysis of scientific articles was performed based on two different scientific databases: Web of Science and Google Scholar. The main aim of the research was to summarize the results of scientific researches, tasks, and methods used in text data analysis. The types of datasets and the language of the texts used in the research were also analyzed. Furthermore, the results obtained from the systematic literature that was performed allowed us to build a taxonomy of text data mining that can be helpful to other researchers.
In this paper, we conduct a preliminary study about a newly established course evaluation survey given to the students by our institution. This survey contains several free-text questions which generates more qualitative but also voluminous unstructured feedback compared to the previous Likert scale question-based survey. Our aim is to apply data mining techniques to extract knowledge from these surveys, with the goal to help educators and administrators gain insight into student sentiment and views. Specifically, we apply text mining with classification to student comments regarding their perception of the teaching as a whole, in order to categorize them as positive or negative comments. Additionally, we apply text mining with association rule mining to various student comments to extract important key terms and associations of terms in these comments. Our preliminary results are encouraging and demonstrate the usefulness of employing data mining techniques to extract knowledge from the open-ended comments in our student surveys.
The economy of Indonesia has been growing by leap and bound, and large portion of the population are shifting from lower-class consumer into the middle-class and affluent consumer socioeconomic category. That condition increases the purchasing power of middle-class consumer over the goods, including automotive products. In consequence, all of automotive products sales have increased significantly from 2009 until 2013. The raising number of automotive products sales in Indonesia was followed by the growth of consumer financing. From 2009 until 2013, 26 new business licenses have been established. As competition in the financing industry is getting tight, the financing companies have to conduct some efforts in order to retain its customer loyalty. Customer loyalty is important for business sustainability. The Structural Equation Modeling (SEM) and Self-Organizing Map (SOM) approach are applied to find factors affecting customer loyalty. Consumer's concern based on aspirations and complaints were then identified by using text-mining. SOM will cluster the aspirations and complaints of the customers into several groups. Consumer satisfaction affects customer loyalty and it also directly affected by consumer perceived value. The contribution of this research is combining structural equation modeling with text mining.
It is a big challenge to guarantee the quality of discovered relevance features in text documents for describing user preferences because of large scale terms and data patterns. Most existing popular text mining and classification methods have adopted term-based approaches. However, they have all suffered from the problems of polysemy and synonymy. Over the years, there has been often held the hypothesis that pattern-based methods should perform better than term-based ones in describing user preferences; yet, how to effectively use large scale patterns remains a hard problem in text mining. To make a breakthrough in this challenging issue, this paper presents an innovative model for relevance feature discovery. It discovers both positive and negative patterns in text documents as higher level features and deploys them over low-level features (terms). It also classifies terms into categories and updates term weights based on their specificity and their distributions in patterns. Substantial experiments using this model on RCV1, TREC topics and Reuters-21578 show that the proposed model significantly outperforms both the state-of-the-art term-based methods and the pattern based methods.
An integrated report summarizes a company's future business development, management vision, management philosophy, policies and strategies for value creation, and their processes, based on information such as their own intellectual assets (intangible assets) and financial data, which make up their strengths. In recent years, Japanese universities have started to publish integrated reports, and they are trying to promote new values that do not depend on the conventional evaluation criteria of universities. This paper evaluates whether the integrated report represents the characteristics of the university using text mining technique. The analysis shows that the integrated reports represent well the characteristics of the universities, especially those related to social contribution, and that the integrated reports can be a useful resource for university evaluation.
The establishment of traditional crafts is not only the pursuit of craftsmanship, the inheritance and innovation of traditional crafts, but also a microcosm of the history of an era. In recent years, users have also been using online forms to browse regionally specific traditional crafts. However, there are numerous and varied traditional crafts, and users do not have an intuitive and comprehensive overview of the traditional crafts in that region. Therefore, in this work, we aim to develop a visualization system for traditional crafts to help users better understand local traditional crafts and to promote the enhancement of traditional crafts and regional revitalization. For this, we utilize text mining with sentiment analysis to visualize traditional crafts designated by the Minister of Economy, Trade, and Industry (METI). Text mining of traditional crafts enables users to learn about the characteristics of local traditional crafts, and sentiment analysis of traditional crafts allows users to understand the emotional characteristics of the crafts that reflect their history. Finally, we developed a visualization system for traditional crafts and validated its usefulness, and we confirmed that users could learn about local traditional crafts comprehensively and intuitively.
Text categorization is a text mining process and it aims to discover relevant information and relationship in a huge amounts of text data. Feature extraction is an important preprocessing step of text categorization, as extracted features are used to represent texts. Several methods, feature models, and algorithms are needed to extract useful features from textual contents. One of the these methods is frequent itemset mining which is a basic data mining technique that employed to find interesting patterns in data. As the frequent itemsets (termsets) reflect strong associations between items they provide more underlying contextual semantic than an individual word. Therefore, it is used in text mining domain for different purposes (e.g., frequent itemset based text clustering). In this paper, we employ termsets for text representation, and use both binary and cardinality based approaches for termset weighting. Unlike existing studies, we use character level n-grams to represent items in a transaction in addition to the traditional bag of words model. Through our experimental results on Mod-Apte split of the Reuters-21578 dataset, we determined that performing document-transaction conversion at level of character n-grams improves the performance of the Support Vector Machine classifier.
Owing to the popularized utilization of academic search engines, such as Google Scholar or Microsoft Academic, the information that a researcher can conveniently access is unlimited. Especially in the domain of supply chain and transportation, where the academic publications are text-heavy and algorithm-driven, it is virtually impossible for one to follow the large amount of developments that is being created on a day-to-day basis. This poses a major barrier during literature review. To that end, the conventional literature-review methodology needs to be revised; text mining has strong potential in this aspect. In the earlier parts of this series of papers, technology infrastructure (Part I) and abbreviation extraction (Part II) have been studied. Both of those analyses focus on analyzing the words in a given set of documents. In Part III, moving from words to semantics, the task of automatic summarization of research papers is explored. More specifically, the TextRank algorithm is used to extract top N most important sentences from a paper, which could significantly boost the efficiency in scanning documents during literature review.
Text mining has been a popular research topic in the field of natural language processing. With the emergence of Web 2.0 and the development of social software, the amount of text generated every day has increased dramatically. The texts contain a lot of valuable information, and how to do the analysis to extract the information from that text is very important. Therefore, many researchers have explored related methods and various fields of text mining, such as sentiment analysis, text clustering, text summarization, etc., However, unlike other numerical data that can be calculated directly in terms of character performance, the calculation must be performed after vector conversion, and the words may have polysemy, which provides more challenges for text mining. Given the above challenges, various techniques are used for data preprocessing before analysis. In addition to the common statistical and discrete methods for text data, methods based on fuzzy logic provide another option for effective text analysis, Therefore, in recent years, more and more studies have added fuzzy logic to additionally capture the context semantics of individual words to improve the accuracy of natural language processing. This current survey research discusses multiple text mining methods, subfields, and application fields, covering the literature published between 2010 and 2022. It is organized based on the subtasks to be performed, the methods and text mining techniques used, and the application scenarios. At the end of this study, the key points will be discussed, and relevant suggestions for future research on text mining combined with fuzzy logic will be made.
Artificial Intelligence(AI) has been a vast area in modern science. From our day to day lives it has contributed with so many devices that actually make sense as intelligent. Human beings can operate any gadgets operating by speech, text only. One of the techniques of AI is text mining or text data mining. Text mining uses natural language processing (NLP). for processing text to make it structured. So Text mining helps by using AI technology to produce new information from the structured data. Natural language processing (NLP) is the technique of artificial intelligence (AI). This allows machines to understand human language. NLP combines the powers of linguistics and computer science to study the rules and structure of language. It also creates intelligent systems (powered by machine learning and NLP algorithms) that can understand, analyze, and extract meaning from text and speech. Natural Language Processing (NLP) in healthcare can help physicians to make better decisions. This study describes the potential of NLP to maximize the value of Electronic Medical records(EMR’s) and healthcare data, making a critical and trusted component in improving health outcomes and also focus on process, the EMRs processing and emphatically analyzes the key text data mining processing techniques. EMRs have been doing an absolute help to automated highlighting of new information in clinical notes. This study tries to give an overview of how AI techniques plays an important role in healthcare.
The use of technology really helps to maximized the effectiveness and efficiency of work expecially in the education field. Elearning is the concept of education that has begun to be widely implemented at this covid-19 pandemic to avoid the spread of transmission through social distancing. One of elearning types is essay but for large participants, it need much effort for evaluate by human rater. The inconsistency of assessment by the rater due to fatigue can also affect the quality of the assessment. Developing a system that can learn and understand on its own without having to be repeatedly programmed by humans used machine learning and computational linguistics to study the interaction between computers and human natural language used natural language processing proposed in this research. Natural language processing and text mining methods are able to provide a good assessment which is influenced by several processes, namely tokenization, stopword, stemming and support with the number of keywords, and the synonym of more complex keywords. The automated essay scoring system is proven to provide consistent and objective assessments and is able to approach human raters assessments.
Several steps need to be considered when conducting a data mining study on a text dataset, this will also involve the use of multiple techniques before achieving any results or extracting any hidden knowledge. Hence, the complexity of working with text data. One of the main steps would be the pre-processing of the text dataset, and this might include multiple techniques such as tokenisation, word stemming, stop words removal, and text vectorisation. To extract knowledge from the text data after preprocessing, depending on the use case or end goal, the application steps might include clustering of text documents, classification of the text documents, and the extraction of document topics and entities. For each of these steps there are several methods and techniques being presented in different research studies. In this paper we present a framework that would categorise, cluster and classify a corpus of unclassified documents. The framework extracts named entities and uses a linked data Knowledge Graph to assign several topics and categories to each document. Then automatically cluster the documents into groups using the K-Mean model with the Elbow and Silhouette methods. Each cluster then gets assigned to a readable name from the extracted linked data based on word frequency in each cluster.
It is a still a world-wide challenge that hypertensive patients maintain a satisfactory blood pressure control. In TCM practice, traditional physical therapy has shown beneficial to blood pressure (BP) controlling. As the amount of bio medical data in leading databases (i.e. SinoMed, etc.) is growing at an exponential rate, it might be possible to get something meaningful through the techniques developed in data mining. In this paper, focused on hypertension, we proposed an algorithm named two dimensions data slicing to mine rules of Chinese medicinal physical therapies (massage, cupping and so on). The process of mining was done in two dimensions. The one-dimension analyzes the frequencies. The two-dimension analyzes the frequencies of co-existed keyword pairs. By examining the results of these two dimensions, although some noises existed, most regular knowledge of this disease is mined out. This algorithm might be useful in mining rules in the literature of traditional Chinese medicine.
Qualitative research methodology is commonly used for education research. The main advantage is that it is capable of gaining more rigorous insights of research objectives when compared with quantitative research. However, due to the extensive time required, the sample size for a qualitative research is usually very small, and can be as small as two subjects. As a consequence, the results are difficult to be inferred to the population. Text mining methodology is a modern technique to analyze text data. It has been successfully applied to a wide variety of business problems. However, it has not yet been applied as a tool for qualitative research in education. In this study, we attempt to apply text mining method to analyze students' written reasons of a question related to the concept of variation. Four sections of an introductory statistics course participated in the study (n = 218). The results suggest text mining method can be an alternative, if applied appropriately. The strength and weakness of using text mining for qualitative research are investigated.
After viewing the literatures of text mining methods, we found that seldom have researchers studied the utility of text mining methods on massive online reviews. We draw more than 400000 reviews from an e-commerce platform and calculate the sentiment values and the frequency of feature words. Then we study the time dimension and geographical distribution of sales volume and sentiment values, as well as the geographical distribution of feature keywords. By doing this, we extract valuable information and discuss the consumer profiles of online users, coming out the conclusion that the sales volume isn't necessarily corresponding to sentiment strength and there exists differentiation in focus of people in different provinces.
This paper focuses on the pattern discovery of text and builds a visualized pattern discovery model base on TF-IDF Weight Method. The model can discover patterns in the form of hotspots, cross-distribution and development trend. With the help of relevant tools and methods the model realizes the pattern discovery in the way of visualization which has a better human-computer interaction performance. It is testified by the research related text data of the pattern discovery outcome. The results show that the model could well express the hotpot directions, cross-distribution situations and development trends. The model provides a possibility of pattern discovery especially in a visualization method.
Textual content mining is the manner of extracting significant information and insights from large volumes of unstructured textual information. Many businesses are beginning to recognize the capacity of text mining in gaining effective insights and presenting greater accurate statistics. This paper will talk the ability blessings of using text mining strategies in data analysis. It's going to present quite number possible applications, such as herbal language processing, sentiment evaluation, subject matter modeling, and predictive analytics. It also evaluation the numerous technical techniques and algorithms used for textual content mining, together with lateens semantic indexing, and will look at how text mining can be used to guide a range of various commercial enterprise targets. Sooner or later, the paper will address how text mining can help pick out useful styles and traits in unstructured information, and the way it may be utilized in tandem with conventional data processing methods to achieve higher effects. This paper will finish by way of analyzing the capacity challenges that may be encountered when using text mining and how ability dangers can be mitigated.
NA
Text mining, a section of the synthetic intelligence, is gaining grounds nowadays in terms of the applications in business and analysis. Varied sectors and domains across industries understand the potential of text mining in gaining information, mining helpful data and in enhancing the choice creating method in terms of speed and potency. In today's world, where the web area is overflowing with data, text mining technology and solutions will persuade be the turning purpose. Most of the recent researches conducted during this space focused principally on advanced or the hybrid of deep neural networks so as to induce economical and higher results. Functioning on such serious models isn't solely time taking however conjointly needs the usage of heap of resources. Therefore, to get the comparable results only basic deep learning models have been used in order to minimize the model's complexity and computational cost. For the same, RNN and LSTM based models have been used and accuracy of proposed models have been enhanced by by varying the hyper parameters and using Glove word embeddings. Moreover, a labeled dataset named Sentiment5k has also been created using semi-supervised learning approach for evaluating the performance of RNN & LSTM based models.
Text analysis using data mining technology can acquire and discover implicit knowledge, which is a process from text information description and feature extraction to knowledge formation. The spatial representation of text data and semantic information can be simplified and identified using end-to-end deep learning algorithms. This paper reviews the text analysis based on deep learning. Firstly, this article analyzes the process of the text learning, then text analysis learning models are summarized, including convolutional neural networks, recurrent neural networks, and deep learning algorithm fusion and so on. Furthermore, the applications of text analysis based on deep learning are introduced.
In unstructured text, process of looking for patterns is called text mining. Finding particular elements in documents written in natural language is a related task of information extraction (IE). Text mining has grown as an importance in study field. It is discovery of the new and past undiscovered information by computers from a variety of hand-written sources automatically. We currently live in a time marked by extreme stress and mental illnesses. A greater number of people are committing suicide as a result of the excessive levels of stress and strain because they tend to increase the amount of people who exhibit risky inclinations. Family conflict, work dissatisfaction, health issues, etc. are common sources of stress. In the modern computing environment, people are pleased to express their opinions and sentiments on social media with friends and family members by using tools like electronic communication. The suitable text mining technique can increase efficiency and cut down on the time and effort needed to retrieve relevant information. The Natural Language Processing technique which is used to extract important information from text messages is briefly discussed and analyzed in this work.
Background: The central repository of scientific models from the literature; however, the manual knowledge is research publications which also plays a selection of pertinent information from databases like crucial role in communication within the scientific PubMed, PMC, Dimension, Google scholar, and community. The public repositories like PubMed, Semantic scholar can be tedious; therefore, a robust PMC, Dimension, Google scholar, and Semantic approach like text mining can be used for this process. scholar act as a storehouse of biological systems data. Text mining can be defined as a practical approach to A substantial amount of information can be recovered extracting biologically relevant information from the in a semi-structured form in the literature. The main growing amount of published literature. It comprises obstacle to large-scale analysis of this kind of data is three main tasks: information retrieval from relevant their highly unstructured and heterogenous format, documents, extraction of information of interest, and making it even harder to extract information contained data mining, which allows identifying new within the literature. Nonetheless, this information is associations among the extracted set of information. inherently helpful in a variety of genomics and Here, we show that a text mining approach can exploit systems biology contexts. For example, it is a standard large literature databases like PubMed and PMC to practice in the genomics community to manually extract genes/proteins related to biocorrosion by curate and extract literature-derived protein-protein Sulfate-reducing bacteria(SRB). The corrosion of metal due to microbial activity is known as biocorrosion or MIC(Microbial Induced corrosion). The primary class of bacteria associated with corrosion of metals in aquatic and terrestrial habitats is Sulfur Reducing Bacteria(SRB). Biocorrosion results from collaborative interactions between the metal surface, corrosion products, and bacteria... (Show More)
Location-Based Modelling and Analysis of Threats By Using Text Mining is process of analysis of threat and criminal behavior and help to the police to find the location of the criminal activities using some text mining techniques and machine learning algorithms. Now, in digital world, Computer crime such as identity threat, cybercrime, terrorist attack, rap, kidnaping, robbery has increase because of increase a digitalization has become a significant issue for police and law enforcement agencies Many identities threat occurs through social media and that's way crime are increases. This study helps to detect the all-criminal behavior-based activity and post on social media using different techniques and construct pattern sets based on domain knowledge from social media reports or newspapers and relevant literatures. In addition, our research include previous years crime activity is use for our model to train and that data is pre-process using tokenization for meaningful data. Some techniques such as feature extraction include emoticons and synonyms, while in classification Extra tree Classifier for analyses the criminal behavior from social media.
With the increasing of online information and recourse texts, text summarization has become an essential and more favorite domain to preserve and show the main purpose of textual information. It is very difficult for human beings to summarize manually large documents of text. Text summarization is the process of automatically creating and condensing form of a given document and preserving its information content source into a shorter version with overall meaning. Nowadays text summarization is one of the most favorite research areas in natural language processing and could attracted more attention of NLP researchers. There are also much more close relationships between text mining and text summarization. According to difference requirements summary with respect to input text, established summarization systems should be created and classified based on the type of input text. In this study, at first, the topic of text mining and its relationship with text summarization are considered. Then a review has been done on some of the summarization approaches and their important parameters for extracting predominant sentences, identified the main stages of the summarizing process, and the most significant extraction criteria are presented. Finally, the most fundamental proposed evaluation methods are considered.
We conducted an analysis of incident reports from Shimane University School of Medicine Hospital using text mining techniques to efficiently analyze the cases based on the keywords they contained. The analysis targeted the text data of incident reports filed between fiscal years 2017 and 2022, with a particular focus on reports related to personal information. We employed morphological analysis on the collected text data, organizing it word by word to facilitate easier analysis. Subsequently, techniques such as word clouds and cluster analysis were utilized. Based on the results obtained, we conducted a thorough analysis and evaluation of the issues highlighted in the incidents. It was found that incidents related to USB memory often involved them being inadvertently placed in pockets and sent for laundering. Additionally, document-related incidents could be categorized into six distinct clusters.
This mini-track has five papers that are about developing systems for decision support by means of data, text, or web mining. These five papers focus on a wide range of application areas from healthcare to social media, reinforcing the fact that data, text, and web mining are effective and recently popularized tools to develop decision support systems in various domains.
Personage is an important kind of entities in study of history. Comprehensive understanding of personage biographies is beneficial for researching into historical events. This article introduces the development of a text retrieval and mining system for Taiwanese historical people - Taiwan Biographical Database (TBDB). It describes the characteristics of personages in TBDB, highlights the system architecture and preliminary achievement of TBDB, and proposes a method to recognize named entities in the personage biographies, specifically poetry societies, which achieves the recall rate 96% and the precision rate 65%. Finally, this article elaborates on the lessons learned through the creation of TBDB, and the future plans.
The report discusses the use of Text Mining algorithms such as semantic analysis of text and search for keywords to solve the problem of categorizing data entering the information system in the form of short messages in text format. An example of the application of such algorithms in the information system for processing user messages is given.
Transformer failure could occur in terms of tripping that results in an unplanned or unseen outage. A good maintenance strategy is therefore an essential component in a power system to prevent unexpected failures. In this paper, the causes of transformer failure within the power transformer systems have been reviewed. Data is obtained from the transmission substation assets from the whole of Peninsular Malaysia for the past 5 years. However, the challenge is that the problem descriptions of the datasets are all in text formats. Thus, text mining approach is chosen for the data analysis using R. This paper covers the most common steps in R, from data preparation to analysis, and visualization through wordcloud generation. This study mainly focuses on bag-of-word text analysis approaches, which means that only word frequencies per text are used and word positions are ignored. Although this simplifies text content dramatically, research and many applications in the real world show that word frequencies alone contain adequate information for many types of analysis. As a result of analysis, keywords like "leak", "lightning", "animal", "cable" and "temperature" are identified as the main causes of transformer failures based on the number of word frequency in the tripping dataset. Further enhancement could be made in the future to predict the failure beforehand using predictive analytics approaches.
Single Chinese herbal medicine (CHM) is the basic element in the formulae of traditional Chinese medicine (TCM). How to acquire knowledge of a single CHM within the framework of TCM is a meaningful task. In TCM, Radix Astragali (RA) is frequently and widely used in clinical practice. In order to explore the associating rules between prescription and syndrome on RA with text mining technique, we downloaded the data set on RA from Chinese BioMedical literature database (also called SinoMed). Then, the rules of prescription corresponding to disease and syndrome on RA were mined out by executing data slicing algorithms. The mining results were visually demonstrated with software Cytoscape 2.8. Text mining, together with artificial reading for anti-noising and validation, is an important approach in exploring the rules of prescription corresponding to disease and syndrome. The results showed that RA was usually used in treating diseases of diabetes and cancers. For them, blood stasis due to qi deficiency was the main syndrome in TCM. Moreover, the results demonstrated associations among TCM syndrome, diseases and formulae associated with RA. These associated networks represented a variety of knowledge items which embodied the associating rules between prescription and syndrome on RA.
One of the major benefits of text mining is that it provides individuals with an effective method for analyzing copious amounts of knowledge in the form of texts. Since the olden times, knowledge in medicine was established through recording and analyzing human experiences. This paper presents the first results of the use of text mining techniques to analyze online sources e.g. social networks, blogs, forums, medical literature, medical staff and patients' stories for discovering new knowledge and patterns related to diabetic disease covering diagnosis, diet, medicine, and activities. These finding are being gathered into an online knowledge repository for diabetic patients to access and better manage their diseases. In this research work, we found that the impacts of gaining informative and useful knowledge from a whole other range of data (text sources) besides the ones from medical literatures proved significant in detecting patterns in diabetic diseases that were considered to be insignificant before.
Railroad traffic safety is a major worldwide concern. Since 2000, there have been 48,083 crashes at highway-rail grade crossings in the US resulting in 6,103 fatalities, 18,851 injuries, and $302,065,336 cost of vehicle damages. Federal Railroad Administrator (FRA) seeks to improve safety and consolidate the grade crossing in risky areas. Towards the goal of better highway-rail cross safety, this paper aims to explore the reasons behind highway-rail crossing crashes by text mining within the narrative information in the crash data. The similarity of the extracted crash reasons between the 50 states and Washington DC in the USA is also calculated to create a comprehensive document for Departments of Transportation (DOTs) and railroad agencies. Important features are extracted from the FRA crash data along with two new features (geographical region and weight of narrative crash data) to classify the type of train-vehicle crashes into "train struck car" and "car struck train". The weight of narrative field was calculated using text mining techniques. Random Forest and Logistic Regression machine learning algorithms have been applied to build prediction models for the classification task. Experimental results indicate an overall accuracy of 0.86 and 0.74 for the proposed Random Forest and Logistic Regression models respectively.
Today is a time of networking or usually known as Internet. The ways in which users interact with a World Wide Web (Web) site available on Internet provides enormous data processing on the usefulness and effectiveness of Web design elements and content built in it. Selection of research projects is an important research topic in research and development (R&D) project management. With the ever-increasing quantity of text data from a variety of online sources, it is a significant task to categorize or classify these text documents into categories that are manageable and easy to understand. Hence the research and development (R&D) project management also become a great deal. Here in this paper text mining method for research project selection process which is based on the classify of data using KNN.
This Software engineering (SE) is the core research area of the software industry, and numerous algorithms and frameworks are proposed every day. Software Engineering is the discipline that keeps up with recent developments in fields and techniques such as data mining, machine learning, natural language processing, and artificial intelligence. It is difficult to manage such a massive amount of data in a software engineering repository if data processing and big data analytics are not available. This paper provides an in-depth study of data mining techniques that to improve the efficiency of the software development lifecycle. The study emphasizes the practical applications and discoveries made by text mining, clustering, classification, and other methods.
Today, with due to new revolution on the biomedical domain combined to data mining applications, research on biomedical text applications in the context of information extraction are becoming very essential. Biomedical applications are unstructured in nature, making it difficult to utilize data mining and discovery techniques to retrieve needed documents. This article introduces a based Support Vector Machine (SVM) Information Extraction from the Biomedical domain by adopting the use of text mining framework. Three main structures (Text gathering, Text Pre-processing, Data Analysis) are used in the purpose to give input/output text Pre-processing in different phases. Finally, by increasing the precision and recall rate of information retrieval, good framework performances are obtained on the Biomedical documents.
Investor sentiment will have an important influence on the trend of stock prices. There is rich information about stock reviews in the online community and investor sentiment analysis based on text mining has become a hot research topic in the field of Social Science in recent years. This thesis takes stock review in the Sina Finance and Economics Stock Forum as the research object, applying support vector machine (SVM) method to the emotional tendency of stock reviews, and then concluding BSI investor sentiment index based on the results of text mining. Finally, a multiple linear regression model is established, which testify the relationship between investor sentiment and stock price. It is found that investor sentiment index based on text mining can effectively improve the forecast of stock price.
When it comes to purchasing and making business decisions, online reviews have become incredibly valuable. The reviewer can boost brand loyalty while also assisting other customers in understanding their product experience. As internet reviews are becoming more prominent, fraudulent reviews which refer to reviews submitted by authors who are rewarded for creating fake evaluations to influence readers' perceptions, are becoming more common. This research paper aims to create a product review summarizer that generates a summary for amazon product reviews based on non-fake reviews. In the current work, we have compared supervised machine learning algorithms with SVD dimensionality reduction and a text mining approach for the summarization. The labeled amazon review dataset was used for building the model. This paper gives a novel idea of giving text summary of amazon reviews after filtering out the fake ones.
The highway is always identical to the motor vehicle which is one of the means to reach a specific location or destination, for example, they are going home or work, shopping, vacation, or those who are planning to return home. Not many people know about the road condition information along the way they will pass. Based on the lack of information on road conditions, especially in the area of East Java, it needed a Road Condition Monitoring application that can run on a mobile platform Android. Users can find out about road conditions will be passed. These applications take advantage of social media by using Text Mining System as information to be displayed to the user, so the application can be real time. The information will be displayed to users categorized into six types, including floods, traffic jams, congested roads, road damage, accidents, and landslides. The result of the evaluation process of this road condition monitoring has an accuracy of 92%. With this application, it will be monitoring road conditions in real time and the community will also feel more secure and comfortable when traveling inside and outside the city in any time.
The number of health informatics in text in electronic health records (EHRs) has increased. In parallel, the text mining of health informatics in clinical notes has also increased in the attempt to utilize big data in EHRs for better patient care. However, little is known on how to utilize the word frequencies of text in clinical notes in EHRs to investigate patient care over a per annual basis.Our methods used a tool called NimbleMiner (NM) that utilized RStudio and generate a relatively “naïve” native lexicon of Simclins (i.e., SIMilar CLINical terms) via the applications with special emphasis on patient falls, which are important events that can lead to decreased health outcomes. The identified Simclins’ frequencies in the file were then determined using a basic Python script for 1-gram words (e.g., “home”) and using Voyant Tools for n+1-gram words (e.g., “living room” or “intensive care unit”).The most frequent words (over an annual basis) in the training corpus were determined to be: “mg” (15,726); “patient” (12,945); “tablet” (10,992); “po” or per os or by mouth (8,904); “blood” (8,824). The Simclins were identified for each domain of interest, the initial prompts for the first search iteration of NM, as well as ChatGPT3.5 output for comparison purposes. There were 15,252 Simclin word frequency related to home and 2,421 related to clinical settings over one year, respectively. Simclin frequencies per year by domain were “fall” (514), “setting” (17,673), computerized provider order entry – CPOE (3,408), and medications (62,317). Furthermore, the significantly larger text of 33,769 on medications (i.e., mg, tablet, medications, aspirin, coumadin, dose, doses, prescribed, and unit_ml_solution) of the total of 46,530 or 72.6% of the corpus.The use of NM and other tools proved that text mining of the clinical notes can provide operational information of important clinical events based on word Simclin frequencies in the clinical notes over an annual basis. The resul... (Show More)
Inland waterway ship collision is a typical high-risk maritime accident which caused by many factors. In order to clarify the causal factors of collision of inland watercraft, the risk of inland river collision is analyzed. First of all, 419 ship collision accidents in the Yangtze River inland waterway from 2013 to 2017 are selected as the ext mining corpus. The human factors, ship factors, natural environment factors and management factors in the corpus database, will be applied as the target base. R language and text mining method is utilized to get a high dimensional and sparse original feature vector space set, which contains the features and the weight of the features. The chi-squared statistic is used to reduce the dimensionality of the set and 33 dimension features set which representing the ship collision risk factors are generated. Finally, the word cloud package in R language is invoked to make the date visualization, which provide a theoretical basis for the pre-control of inland waterway ship collision.
Nowadays, with the development of technology and social media, people's opinions and news about a particular product or currency is significantly expanded. In addition to expanding volumes of text data, the unstructured characteristics of them makes the analysis of these types of data with the vital challenges. In this study, the proposed method utilized different Training Set Selection (TSS) approaches like: ModelCS, ENN, MultiEdit, AllKNN, POP and MENN. Moreover, the target's features have been selected to improve the prediction of currency market (dollars, euros) based on news headlines. The evaluation of the proposed method has been done by 3-layered algorithm and the experimental results reveal that the proposed method has achieved more robust accuracies than the others.
The Internet is a house of indeterminate and ever-growing resource of information. Text Summarization has become a critical requirement for effectively managing the clogged data. The technique of retrieving relevant information from various unorganized sources is commonly known as the process of text mining. Multiple methods like the term-based approach, phrase-based approach are applied to structure data. In this research paper, extensive discussion on numerous techniques that are used to extract useful information are mentioned. The central objective of this is to highlight the issues or drawbacks of each such technique, try to resolve them, and select one such method that could be used to summarize the textual content. The algorithm used for text summarization is Sum Basic. Sum Basic is a text summarization process that creates multi-document summaries. The primary concept of the text summarization is to produce a summary, wherein the persistent occurring terms in a document should be prioritized above less frequently appearing words.
Stock Market (SM) is believed to be a significant sector of a free market economy as it plays a crucial role in the growth of commerce and industry of a country. The increasing importance of SMs and their direct influence on economy were the main reasons for analysing SM movements. The need to determine early warning indicators for SM crisis has been the focus of study by many economists and politicians. Whilst most research into the identification of these critical indicators applied data mining to uncover hidden knowledge, very few attempted to adopt a text mining approach. This paper demonstrates how text mining combined with Random Forest algorithm can offer a novel approach to the extraction of critical indicators, and classification of related news articles. The findings of this study extend the current classification of critical indicators from three to eight classes; it also show that Random Forest can outperform other classifiers and produce high accuracy.
Users on social media platforms tend to readily believe the contents of posts related to the events. However, some of these events might be fake or incredible. The spread of such events takes the form of rumors that have the potential for affecting negatively the individuals and society. To this end, in this paper we develop a text mining approach for automatic evaluation of events on social networks. We consider Twitter as a case study. Given a set of popular Twitter events along with different credibility ratings assigned manually by human annotators (i.e., crowdsourcing), we study the problem of automatically assessing the credibility of such events. The conducted experiment in this paper using events extracted from the CREDBANK dataset; a corpus of tweets annotated with human credibility judgements shows that our approach is promising. It achieves an automatic credibility assessment of events of 82.86% with Decision Tree (DT) classifier.
The problem faced by 21st systems engineers is the integration of multiple complex systems, referred to as a system of systems (SoS), where different constituent, contextual specificity, legacy, or new systems integrate to achieve an emergent mission or deliver the new desirable goal. Thus, there is a need to develop an effective cadre of systems engineers who could manage and navigate the complex system context efficiently. But, the existing body of SE literature lacks to address two fundamental questions: (1) What are the fundamental attributes of systems engineering that would impact the performance of individual system engineers? (2) What are the leading indicators for appraising the performance of an individual systems engineer? To address the gap, in this exposition we proposed a new instrument that could assess the performance of the systems engineer. This instrument is developed based on the text mining technique from a large dataset. Utilizing this instrument, the individuals would identify their weak spot and better understand personal profiles to continually emphasize their systems engineering skill development.
Social media recently are very populer in Indonesia and in the world. Luckly, this platform may express their opinion and emotion even other party especially reseacher mostly use this opportunity to find any solution for any case likely competitive business, decisions maker, and possible analitics and predictive support system. In this case our analysis is content on Twitter and Facebook which user often posted information about the crime which matters need police attention. Therefore our purpose is detection of crime rate on social media to find pattern trend of tweet number crime. This work used text mining approach for classification of tweet and post content text into 10 class of crime. The Algorithm used for classifier are Logistic Reggression, Naive Buyes, Support Vector Machine (SVM) and Decission Tree, from all the algorithm used, Logistic regression give the best accuracy of 90%
Recently, various tools for text data mining are provided. A user cannot see data from various viewpoints, because a tool is created for a single task. So, a user has to obtain multiple tools and learn how to use or how to transfer data from one to another. Also, many of those are only used for people who are accustomed to data mining analysis. TETDM (Total Environment for Text Data Mining) project, supported by Japanese Society for Artificial Intelligence, aims at constructing a total environment for combining multiple text mining techniques. General users of TETDM can input text data like news on the Internet or comments in SNS or BBS so that they could obtain new knowledge related to the input text data by using multiple techniques flexibly combining in a single total environment. In this paper, after describing TETDM components, a framework for knowledge emergence using TETDM is described. Though most of data mining systems put their weights on their process, the process from output to interpretation is not considered. Therefore, after the process from output to knowledge emergence is clearly defined, a framework for supporting knowledge creation process is described.
Over the past ten years, with the deepening and expansion of the application of Internet technology, more and more customers browse a lot of online reviews to understand the product and service reputation. Online reviews can provide decision support for consumers to purchase satisfactory product and the product customer satisfaction is mainly determined by product quality perceived value. This article will establish a new quality perceived value evaluation method by combining relatively mature test mining method with fuzzy comprehensive evaluation method. This evaluation model will be user oriented and more reflect the real needs of users. The new evaluation model will provide better decision support for consumers to buy satisfactory product.
A lot of companies carry out questionnaires. These questionnaires often have questions which need respondents to answer by free description. It is, however, inefficient for an analyzer to read whole texts to get outlines or classify them, or it is difficult to correctly analyze them without subjective biases. The authors have proposed "HK Graph" (Hierarchical Keyword Graph) which is a support tool for text mining. HK Graph can visualize the relationships among attributes and words with hierarchical graph structure based on frequency of co-occurrence. However, the result of HK Graph is not enough helpful for the analyzer to grasp the outlines of the texts and extract opinions from them, because it regards divided words as different ones unless they perfectly match and that makes the visualized result complicated. This paper presents a new visualization method for the HK Graph incorporating an aggregating words method based on concepts of words. An experiment is carried out by applying the proposed method to actual questionnaire data on disasters and studies the effectiveness of the proposed method.
The development of the Internet and computer technology has enabled the storage of digital forms of documents that has resulted in an explosion of the amount of textual data generated. We analyzed the trends in the Meteorological Yearbook of the KMA and analyzed trends of weather related news, weather status, and status of work trends that the KMA focused on. This study is to provide useful information that can help analyze and improve the meteorological services and reflect meteorological policy.
Text data mining and analysis for power grid safety hidden danger files, and in-depth study of power grid hidden danger investigation standards and norms, can help power grid enterprises to carry out hidden danger management efficiently and conveniently. Firstly, the study explains the content of the hidden problems of power grid enterprises and the direction of the study. It also summarizes the general process and methods of text mining technology, and explores the current research status of text mining technology in grid hidden danger investigation; Secondly, we observe the textual characteristics of the grid hidden danger investigation, get the visualization results as well as the main manifestations of the grid hidden danger by text mining the existing 412 grid hidden danger texts. And we finally explore the difficulties of the grid hidden danger investigation text mining as well as the possible development direction in the future.
Text mining is a growing in a unique field that attempts to convey information from natural language text. It is too strength be uncertainty characterized as the development of analysing text as well as to take out the information that is constructive for particular purposes. Compared with other kind of data stored in databases, text is unstructured, shapeless as well as difficult to deal with algorithmically. Traditionally the patterns of the text are analyzed by using PTM and FTM algorithms but this algorithms produces less accuracy. To improve the performance of pattern classification, in this paper an Enhanced Support Vector Machine based Pattern classification method(ESVMPCM) is proposed. Then ESVMPCM is applied to Reuters dataset and measure the performance using different metrics as precision, recall, F-measure and accuracy.
This study focused on the topic of predicting “proactive personality”. With 901 participants selected by cluster sampling method, targeted short-answer questions text and participants' social media post text (Weibo) were obtained while participants' labels of proactive personality were evaluated by experts. In order to make classification, five machine learning algorithms included Support Vector Machine (SVM), XGBoost, K-Nearest-Neighbors (KNN), Naive Bayes (NB) and Logistic Regression (LR) were deployed. Seven different indicators, which include Accuracy (ACC), F1-score (F1), Sensitivity (SEN), Specificity (SPE), Positive Predictive Value (PPV), Negative Predictive Value (NPV) and Area under Curve (AUC), combined with hierarchical cross-validation were also used to make the comprehensive evaluation of models. With participants' Weibo text and short-answer questions text, we proposed a new approach to classify individuals' proactive personality based on text mining technology. The results showed that short-answer questions + Weibo text datasets had the best performance, followed by short-answer questions text datasets, while the outcome of Weibo text datasets were the worst. However, it is noteworthy that Weibo text has the highest average score on the SPE, which indicated that Weibo text played an important role in identifying individuals with low proactive personality. With Weibo text, SEN was also improved compared with only applying short-answer questions text. In addition, among all three datasets, the indicator SPE is always higher than SEN, indicating this text classification approach was more competent for identifying college students with low proactive personality. As for algorithms, Support Vector Machine and Logistic Regression showed steadier performance compared with other algorithms.
The paper presents a text mining application for searching and computing the correlations between the rhythmicities of terms with high frequency appearance, using the time series model. Some theoretical basics of the model are presented, including details about preprocessing the observed texts using natural language processing techniques, followed by implementation details and graphical results.
The dialogue between pilots and air traffic controllers recorded by the cockpit voice recorder directly reflects the special situation of both sides in the scene, which can reflect the extent of the risk of the event. Based on the text data mining technology, this paper developed a corresponding analysis tool, that can realize the consistency detection for the recitation dialogue of the pilot, and once the deviation can be found, the warning information can be given.
Essential genes are crucial for the survival and growth of any organism, and therefore alteration of such genes could result in unexpected behavioral change. Identification of essential genes and their role in functioning of organisms is a basic knowledge requirement for any research, which could be manipulated to understand the mechanisms of survival and growth [1]. Several decades have witnessed the virtues and iniquities of the gram-negative facultative anaerobes, sulfate reducing bacteria (SRB) in both ecological and commercial arena. Despite of relentless increase in the number of published articles that belong to diverse research areas-from industrial biotechnology (removal of heavy metals and waste valorization) to molecular biology (genetic architecture of the genes in biocorrosion and biofilm formation on metals), not much information about the essential genes of SRB community is known yet [2]. The Desulfovibrio alaskensis G20 (DA-G20) is a well-known SRB; its genes have been annotated but have large numbers that encode for hypothetical proteins. Till date no categorization is available for the genes of DA-G20 with reference to essentiality [3]. The in-vitro prediction of essential genes relies highly on the exhaustive multi-omics strategies. In the era of big-data and artificial intelligence research, demand of abstraction and interpretation of complex relationships of biological importance using text mining has increased. Therefore, to propose an alternative and economic method, text mining is a comparable method for the prediction of the essential genes. In this study, we reported the essential genes of DA-G20 using text mining and biological network analysis. Moreover, the present work provides a foundation for the expansion of genome wide investigation and identification of essential genes in prokaryotes using machine learning and data science approaches.
This research aimed to analyze public opinion regarding Indonesia's situation in facing community restrictions (PPKM) during the Covid-19 pandemic. This research used the text mining approach to classify public opinion into two classes of the Pro and Cons classes regarding the policies, along with comparing the accuracy, precision, and recall values using two text classification methods of Naive Bayes and Support Vector Machine (SVM). The data collected were 217 tweets from Indonesia in November 2020. The Naive Bayes method showed 64% accuracy, 72% precision, and 53% recall, while the SVM method showed 63% accuracy, 70% precision, and 53 % recall. Based on these classification text methods results, researchers concluded that SVM's accuracy, precision, and recall values were not higher than Naive Bayes.
The current decision tree based electric power customer demand risk prediction method has low prediction accuracy due to the lack of effective extraction of demand features. In this regard, a risk prediction method for electric power customer claims oriented to text mining technology is proposed. Text mining technology is used to extract features from the demand data and perform text clustering analysis. The extracted data are coded and normalized to build a risk prediction model for electric power customers' demands. In the experiments, the proposed risk prediction method is verified for prediction accuracy, and the analysis of the experimental results shows that the proposed method is used to construct the risk prediction model of electric power customers' demands, which has high prediction accuracy.
Biomedical text mining is the process of extracting high quality information from biomedical text. It has a lot of applications in genetics related studies. Information about gene-disease associations is very important in drug design. Laboratory based methods for gene-disease association extraction need more effort and time. Literature mining is a good method for generating candidate set of genes. Manual methods are not suitable for literature mining as it is highly time-consuming due to the high volume of text data. Even though most of the existing gene-disease association extraction methods use abstracts, full text articles provide more association data. This paper proposes an automatic method for the extraction of disease-associated genes using the property of frequent itemsets from full text articles. The method uses rule based Named Entity Recognition(NER) method for the initial extraction of candidate genes and frequent itemset mining to find the gene-disease associations. Experimental results show that this method gives better precision compared to the existing methods.
The research aims to create an application that uses techniques from Machine Learning to extract and collate data geolocated - collected a Social Network, aiming to promote the Social Recommendation users. Existing research in the field of social recommendation deficiencies remain regarding the effectiveness of the filtered data. This paper presents a study and implementation using Text Mining techniques as a proposal for resolution of problems found in social recommendation and more effective results.
Independent component analysis is a statistical model that is used to separate a multivariate signal into additive components. Independent component analysis has gained much attention in recent years in the neural networks and signals processing fields. Several data mining applications with Independent component analysis have been considered, such as latent variable decomposition, analysis of text document data, detection of hidden signals in satellite imagery, and weather data mining. The conventional Independent component analysis search scheme is based on a gradient algorithm, which requires a predefined learning rate. Therefore, it cannot solve the convergence dilemma. To overwhelm the disadvantage, particle swarm optimization is employed in the ICA algorithm. In statistics, negentropy is used as a measure of distance to normality. The present study used a metaheuristic, particle swarm optimization algorithm that employs negentropy as a fitness function to enhance the performance of independent component analysis for the text classification model as one of the text mining applications. The proposed system was applied to a medical corpus, and two experiments were executed. Results show that the performance of the PSO-ICA algorithm is superior to the FastICA for text classification, where it achieves an overall F -measure of 89% for text classification compared with the FastICA algorithm, which provides 85% of an overall F -measure for text classification.
In this paper, we present the use of text mining and machine learning in call centers to increase the efficiency of registering customer claims and improving customer satisfaction. Our proposed method makes the process of claim registration faster and more accurate compared to experienced call center agents. Use of text mining and machine learning techniques will increase the customer satisfaction and endows the call center staff with better ways to help the customer.
The aim of the research is to ontologically identify and compare the incidence of opioid use problems (OUPs) among patients on long-term opioid therapy using text mining vs. diagnostic coding approaches.
Korea’s increasing elderly population living alone necessitates swift disaster prevention measures. This study identified heat waves as a significant threat through news title analysis and GIS-based heat index analysis. Preemptive actions are crucial, especially in high-risk areas, due to the challenges faced by the elderly living alone in acquiring information and responding quickly to disasters.
During the operation and maintenance of electrical equipment like power transformers, the information of defects or faults are usually recorded by text data. However, the method of classifying transformer defects by text data relies on manual at present, which is inefficient and uneconomical. This paper presents a recurrent convolutional neural network with Bayesian optimization, which construct a text classification model can automatically classify the power text data. Firstly, the text is preprocessed, including the establishment of the transformer's dictionary, word segmentation of defect text and then mapping the results to word vectors based on distributed representation. Furthermore, training the CNN and RCNN networks by supervised learning. It is worth mentioning that in RCNN network, the Bi-LSTM structure is used instead of the convolutional layer, which can learn the semantics of the text more effectively. In addition, in order to obtaining a better classification effect after training, the Bayesian method is used to optimize the hyper-parameters of the networks. Finally, On the test dataset, two kinds of network achieved 90% test accuracy and 92% test accuracy, respectively.
Requirement elicitation is the first step of requirement engineering where software developers focus on obtaining the users' needs and produce a requirement specification. This activity can be accomplished through conducting interviews. However, the requirement specified might not conform to the actual interview transcripts from the elicitation process due to perspective differences between the clients and developers. This study aims to propose a method to validate the requirement specification using text mining analysis. The method involves text preprocessing and text similarity analysis between the specification and elicitation transcripts. The proposed method is validated using interview elicitation transcripts and the software requirement specification (SRS) from the Baker's Corner application. The results demonstrate that the proposed method can effectively validate requirement specifications, producing comparable results to the manual validation process. This finding highlights the potential of automating the requirement validation process using text mining techniques. Additionally, the paper offers recommendations for future studies and developers to enhance the generation of software requirements.
The digital revolution and the development of communications and social media have produced billions of articles in all fields. Automatically classifying articles has a major role in developing systems and artificial intelligence applications based on textual data mining. Many methods rely on machine learning for classification, but these methods require large resources and time. This paper presents a new, simple, and fast automated method for classifying texts and articles within their domains without relying on machine learning algorithms. The proposed algorithm is based on sequential steps in textual data mining in addition to the DMOZ ontology. The proposed algorithm was applied in the form of a web service and was tested on various texts. The results showed the effectiveness of the proposed method, with an accuracy of over 90%. In the future, we suggest applying the proposed algorithm to Arabic texts and creating a specific ontology for Arabic words.
A multitude of information embedded in policy texts reflects the government's policy inclinations, objectives, and impacts. This paper conducts text mining from two dimensions: the policy instruments and implementation topics, establishing a policy text mining model based on LDA, offering a novel approach to policy text mining. By delving into 133 Chinese policy texts on Intelligent Elderly Care, the structural implementation of policy instruments and 10 implementation topics were unearthed. Using multinomial logistic regression, the implementation preferences for different topics' policy instruments were determined, providing insights for stakeholders in the Intelligent Elderly Care policy industry.
The principle of the corpus-based method is to use statistics and probability methods to build a control corpus containing various sentence patterns. During the translation, example sentences similar to the input sentence are extracted from the corpus., hence, extraction method of the machine translation equivalent pairs in the Chinese-English comparable corpus based on text mining is constructed in this paper. Firstly, the proposed research work has analyzed the theoretical basis of text mining by combining the pattern recognition models, neural networks and fuzzy model to effectively collect the features. Then, the machine translation platform is implemented. Through the testing on the accuracy, the performance of the proposed method is studied.
This paper presents a mining tool that is able to extract graphs from texts, and proposes their use in helping students to write summaries. The text summarization method is based on the use of the graphs as graphic organizers, leading students to further reflect about the main ideas of the text before getting to the actual task of writing. An experiment carried out demonstrated that the tool helped students reflect about the main ideas of the text and supported the writing of the summaries.
Only some applications that are built are successful once the client uses them. This problem occurs because there is an inconsistency between applications built with the client's needs, especially in the Software Requirements Specification (SRS) document as the reference data for software development. These losses can be avoided by implementing consistency in the contents of the SRS document. Therefore, before application development, the SRS document consistency process is crucial to match the client's needs and the developer's understanding of developing software. This study aims to propose recommendations for fixing artifacts based on the lowest value of consistency between Step Performed and Sequence Diagrams through a text-mining approach. The results of the validation and reliability tests using Gwet's AC1 formula through the Python programming language produced a Kappa Score of 0.500000, while through expert questionnaires of 0.159998. Based on the comparison of artifact consistency calculations, recommendations for fixing artifacts were made for the lowest consistency measurement values in the comparison of SP09 (d9) with SD09 (d23), which had a value of 0.840124.
Text mining can be defined as the art of extracting data from large amount of texts. It allows to structure and categorize the text contents which are initially non organized and heterogeneous. Text mining is an important data mining technique which includes the most successful technique to extract the effective patterns. The paper focuses on developing an efficient method for discovering patterns from the document. In text mining field, pattern mining techniques are used to find text patterns, such as frequent item sets, closed frequent item sets, co-occurring terms. This paper presents an innovative and effective pattern discovery technique which includes the process of pattern evolving and pattern deploying, to improve the effectiveness of using and updating discovered patterns for finding relevant and interesting information.
With the development of artificial intelligence technology, the field of education has begun to apply text mining technology to education management, teaching evaluation, and learning support. The analysis of the text generated during the learning process by educational text mining helps to better grasp the students' internal psychological characteristics such as emotions, thinking, and cognition, providing evidence support for precise teaching design. Comparing various methods of educational text sentiment analysis, we propose an educational text sentiment analysis model that combines BERT and FastText, which effectively solves problems such as polysemy and new words on the internet. Taking the teaching text data of ideological and political courses on online education platforms as the research object, the educational text sentiment analysis model combining BERT and FastText will provide more precise and efficient solutions for analyzing the dynamic changes in students' online ideological and political learning emotions.
Objective: This research focused on intrinsic biological distinctions between qi deficiency pattern and qi stagnation pattern with genes related to NEI. Methods: Using the keywords related to a disease to search literature in Pubmed. Collection and download literature related to a disease. Obtain compositions of characteristic NEI-related genes of qi deficiency pattern and qi stagnation pattern by using text-mining method to explore different bioactive materials between the two syndromes. Results: Two kinds of syndromes and their biological networks were constructed. The "qi deficiency" and "qi stagnation" genes based on NEI network were excavated. Conclusion: The genes of qi stagnation pattern relate closely with nerve and endocrine, while those of qi deficiency pattern have a close relationship with the immune system. The intrinsic biological characterizations of TCM patterns can be effectively identified at the level of NEI.
The term Text Mining, which is given to the set of techniques used for the extraction, cleaning and processing of the information in texts, has become useful to provide valuable information to other algorithms and widely used with statistical and machine learning methods. By enabling the extraction of useful insights from textual data, Text Mining has become a potent tool in decision-making and knowledge discovery across many areas, including health care, government, education and industry. R is a mature open-source programming environment that has overstepped its initial scope of application for statistical computing and graphics to be used in pretty all the Data Science knowledge Area Groups. The objective of this paper is to present review and benchmarking analysis of packages for text mining techniques with R in computational systems. The paper reviews thirteen different packages comparing them on their execution time and memory used, for which new tests have been specifically designed. The results of this approach have been intended to be used over the most common tasks carried out when analyzing texts, and comparisons included allow R users to know which packages are best for each task and to improve their performance. Text mining package (tm) stands out particularly in Tokenization and Stemming techniques, while fastTextR is the best choice for Topic Modeling and Normalization. Also in the case of the Term Frequency-Inverse Document Frequency (TF-IDF) technique, the textir package is a clear choice. The other packages will depend on whether the technique is applied to a document-term matrix (DTM) or to plain text. In addition, there are packages that perform better in runtime than in memory usage and vice versa, making the choice more difficult. Packages such as udpipe can achieve better results working in parallel. Future works will include the same analysis for parallel computing, hybrid approaches, and novel algorithms. (Show More)
A novel text association rule approach FHAR algorithm is presented. To overcome the defect of traditional keywords which does not take into account the semantic relation between keywords, FHAR algorithm in the paper is based on concept vector. The density of semantic field and the weight of meaning are used to determine the concept of the keywords, which not only adds the texts semantic, but also reduces vector dimensions, FHAR algorithm adopts improved HASH table for efficient large item set generation. The stored address of item sets is determined by a new hash function. Based on the new hash table, tree structure is constructed. When FHAR algorithm is applied to text mining, the text association rule is derived. Experiments show FHAR algorithm possesses higher efficiency and accuracy than Apriori algorithm.
Through an adequate survey of the history of the disease, Narrative Medicine (NM) aims to allow the definition and implementation of an effective, appropriate, and shared treatment path. In the present study, standard text mining (TM) techniques are applied, as a Latent Dirichlet Allocation (LDA) model for topic modeling is used to characterize narrative medicine texts written on COVID-19. In particular, the focus was mainly on the writings of patients with Post-acute Sequelae of COVID-19, i.e., PASC, as opposed to writings by health professionals and general reflections on COVID-19. The results suggest that the testimonies of PASC patients can be used for identifying shared issues to focus on to be followed and supported appropriately, even from a psychological point of view.
Nowadays, in the big data era, data processing technology facilitates us to get the utmost out of sufficient information in the data. However, few scholars apply big data technology to the field of policy evaluation. Therefore, under the Policy Modeling Consistency(PMC) index model framework, this paper thoroughly mines the valuable information in policy texts and proposes a modular policy evaluation system combining text mining and machine learning methods. The system is divided into four processing modules, including data acquisition, data processing, index evaluation construction, and score evaluation. Compared with the traditional policy evaluation methods, the modular policy evaluation system presents the advantages of objectivity, high accuracy, and high efficiency, assisting in government policies' implementation.
With the development of the information era, the application of big data technology in education has become more and more extensive. This article adopts the Hadoop platform to conduct parallel mining of educational literature on big data. The paper has analyzed the main function of text mining technology, combined Canopy and K-Means algorithm to analyze and research the educational big data literature, learned about the application of big data technology in the field of education in order to provide its services for the construction of "World-class universities and first-class disciplines" and make a contribution to the development of education.
We use text mining as a part of formative assessment in one generation of the first year undergraduate students enrolled in our Statistics course. Practical lab exercises were done in R and RStudio. During the semester, students wrote weekly assignments. There were 11 assignments. Assignments were prepared in Rmarkdown, and students added necessary code and text to the Rmarkdown document, knitted the document and submitted the generated html files. We wanted to identify students whose submissions were not substantially different from the assignment (substandard work) and groups of students who submitted similar texts (possible collusion or plagiarism). Additionally, we wanted to compare results of text mining with students' final grades. Text analyses were done in R using packages XML, RCurl, RWekaand tm. A document term matrix was created with trigrams. For each assignment, distance between submitted texts was calculated as the Euclidian distance. Hierarchical cluster analysis with complete linkage was used to identify similarities between students' assignments. The initial assignment was included for comparison. Clusters of assignments with very small inter assignment distances pointed to similarities of assignments. Students whose work was most distant from others were found to be either excellent, or failing students.
Many of the text mining applications contain a huge amount of information from document in the form of text. This text can be very helpful for Text Clustering. This text also includes various kind of other information known as Side Information or Metadata. Examples of this side information include links to other web pages, title of the document, author name or date of Publication which are present in the text document. Such metadata may possess a lot of information for the clustering purposes. But this Side information may be sometimes noisy. Using such Side Information for producing clusters without filtering it, can result to bad quality of Clusters. So we use an efficient Feature Selection method to perform the mining process to select that Side Information which is useful for Clustering so as to maximize the advantages from using it. The proposed technique, CCSI (Co-Clustering with Side Information) system makes use of the process of Co-Clustering or Two-mode clustering which is a data mining technique that allows concurrently clustering of the rows and columns of a matrix.
In this modern era, online hotel reviews have a big role considering the hotel is one of the aspects in determining the competitiveness in the tourist area, but its implementation is still rare. Regarding the government's plan to increase tourist arrivals to Indonesia, this research utilized text mining towards online hotel reviews to find useful knowledge in building the hospitality sector as an integral part of the tourism industry. Text classification technique was used to obtain sentiment information contained in review sentences through sentiment analysis, as well as clustering technique as a part of text summarization to find representative sentences that are able to describe the entire contents of the review. The main contribution of this research is to combine two techniques in text mining that have never been done before, namely the sentiment analysis and text summarization. Experiments with hotel reviews in Labuan Bajo and Bali generated surprising outcomes, where the accuracy of classification model reaches 78% and the Davies-Bouldin Index (DBI) of clustering algorithm strikes 0.071. The output of this research is expected to describe the condition of the hotel in the tourist area with a different level of tourism development so that it can contribute to improving the quality of the hotel industry as well as supporting the tourism industry in Indonesia.
This presentation describes a novel combination of text mining and quantitative structure-property relationships (QSPR) of small molecules, where text-mined ChEBI entries were retrieved by combining Europe PMC RESTful APIs in scientific workflows and matched with QSPR predictions from machine learning. Described applications include metabolomics data integration, experimental design, historical studies and business intelligence.
This study seeks to harness the power of big data through text mining the content of publicly available YouTube tutorials related to Electronic Health Record (EHR) systems. Most information about EHR systems is proprietary and hidden to the public. Nevertheless, understanding the capacity of EHRs from YouTube tutorials can offer insights to the public into health informatics and EHR functionality and usability for their patient care.We established a search strategy of the top three EHR vendors in North America (i.e., Oracle-Cerner, Meditech, and Epic Systems) from YouTube containing metadata and transcripts of EHR-related tutorials. Text mining techniques across a three-phased approach included establishing a search strategy to select the YouTube videos, auto-coding (from Voyant Tools) and manual coding (via standard usability heuristics), and analysis. The EHR components or applications in the tutorials were selected based on search strategy of total volume and quality of videos related to patient registration, scheduling, problem list, documentation, medication/order, and discharge/administration. The tutorials were analyzed for keyword frequencies, thematic patterns, and overall usability in the information presented via video transcriptions. Thematic patterns were established by heuristic categories of navigation, visibility, content, and complex interactions of learnability and memory.Preliminary findings suggest big data text mining led to analyzing 1,788,126 words via YouTube transcripts across 2280 videos. Oracle (Cerner) had a total of 799,653 words in the 1,066 videos. Meditech has 608 videos with 454,968 words. Epic has 466 videos with 436,598 words. There was a good range of content quality from medium to high, with some tutorials offering comprehensive step-by-step workflows like for medication/orders. The heuristics of content and navigation categories suggested that the tutorials covered enough material and word frequencies to be educational towards e... (Show More)
COVID-19 pandemic has created a huge impact on people around the world. Even though most of the people have encountered negative experiences, there are some people who take this as an advantage and exploit this opportunity. Difficult conditions have been adapted by companies to survive the Covid-19 situation. This paper is mainly focused to identify the business lines developed around Covid-19 and their impact on the business environment, applying text mining methodology. The analysis uses open-source software Python Anaconda which mainly focuses on the areas of business and marketing. Four different areas of intervention are identified in the results. It is found that the common factors that have been impacted are the new forms of action which majorly improve the people's quality of life.
In the rapid development of information technology, the speed of communication and communication of network information is getting faster and faster. The depth and breadth of cultural exchanges between countries in the world continue to expand, and finally form a huge and complex international relations network. Under the development trend of economic globalization, the use of text mining technology to study the international relations network can not only accurately understand the influence of different relations on the composition of international trends and hot events, but also understand the changing rules of the development of relations between countries, and provide an effective basis for cultural and economic exchanges among countries in the new era. Therefore, on the basis of understanding the text mining technology and the structure of national relations network, this paper mainly explores the method of using text mining technology to build international relations network, and then discusses the information resources obtained from it.
These studies aim to determine the degree of similarity between student and lecturer posts. The level of similarity then we validated using a lecture note. This research is a continuation of the research that we are currently doing related to the development of smart LMS with one of the supporting features of the recommendation system. Using the analysis in the forum log post discussion of this research was carried out through several stages. The first stage, the random selection of 5 classes taken by the forum data, the second stage, carried out text mining analysis from the posts of students and lecturers, the third stage, analyzing text validation using lecture notes that have been used as data sets in the form of corpus. This study uses the doc2v algorithm with vectorization. The results of this study found that the percentage of similarities between lecturers' posts, students and lecture notes only reached 49% of the target we expected at least 80%. Because discussion forums are a substitute for face-to-face sessions on face-to-face learning. On the other hands this study found that the similarity of discussion between lecturers and students on discussion forums had a significant influence on student learning outcomes (assignment) and this reinforces the need for a system of recommendations for online learning.
With the rapid development of global information and the popularization of the Internet, this new method of information dissemination has changed people's lifestyles, but Internet data as an information carrier has also shown explosive growth. Effectively organizing and extracting these data to find the information that users need faster, more accurately, and more comprehensively is a major challenge facing the field of informatics today. Text mining mainly includes methods such as text sorting, text grouping, and document summarization. Text sorting is a very important way for people to acquire knowledge and recognize things. It plays an important role in natural language editing, language understanding, machine learning, and topic recognition. Therefore, this article focuses on the deep learning of international relations text classification. The learned international relations text classification model is constructed, and the constructed model is tested. The test results show that the classification model proposed in this paper has an average classification accuracy of 90%, which is better than other classification models.
Predicting stock market behavior is a challenge that has been studied and presented several solutions in the literature. Due to technological advances, methodologies have emerged and allowed new approaches to this problem in recent years. Text mining and sentiment analysis have been widely applied in this area. On the other hand, classic solutions as time series analysis continue to be used alone or with new methods. There is still no literature review of the joint use of these methods. In this way, this study presents a systematic review with 57 selected papers using time series, text mining, and sentiment analysis applied to predict financial stock market behavior. Through this research, it was observed that the use of data from social media and internet sites is a compound source of information, providing a better prediction. However, the selection and combination of these data in a relevant way are still limitations found in the proposed models.
This research aims to develop a decision system for diagnosing diseases in dogs from behavior with text mining techniques. To help dog owners to diagnose the disease from the behavior of the pet dog in order to be able to recognize the disease that may occur or already occur of the pet dog Measure the accuracy of the data and satisfaction in using the system by using questionnaires as tools Evaluation of accuracy of data with 80 percent accuracy and accuracy in diagnosis is 80 percent accurate
In this research, extraction is carried out to change the text on an artifact to be processed through Text Mining to process and analyze the text whose results are validated, using the coefficient equation formula. There is a discrepancy between the flow in the Activity Diagram and the steps performed in the Use Case Description. Based on these problems, this study aims to extract an artifact to find similarities between the Activity Diagram and the actions taken in the Use Case Description and perform text analysis using the Text Mining method to obtain similarities, then validation and reliability are carried out. The extraction process is broken down into several documents according to the artifacts used, for Use Case Description, namely: UC01, UC02, UC03, UC04, and UC05. As for the Activity Diagram, namely: AD1, AD2, AD3, AD4, and AD5. The results obtained from this study are the highest similarity between activity diagrams and use case descriptions in sentences UC04 and AD4 with a total similarity of 0.51394086. The highest similarity between words lies in the words “request” and “application” with a complete similarity of 0.9231. Kappa's score with the Gwet's AC1 formula using Python programming is “Less than the Opportunity Agreement” with a number of −008583, while the questionnaire filled out by the Expert is “Almost perfect” with a score of 0.92246.
Text mining is the way toward analyzing of vast accumulation of composed aspect to produce new data and to change unstructured content into organized information for use in further. The need of story summarization has raised its importance as stories seems very simple but incorporate many elementary things required in natural language processing. In the event to make system more insightful, while performing summarization of stories a major issue was proof of recognizable of noun because noun may refer high level of entities. Automated summarizations have many challenges like paraphrasing, understanding context based information impact, still its important due to its need and vast areas of application in email clients, report generation, news feed, Entity timeline, story line of events, sentence compression, event understanding etc. There is demand to develop such tools which capture prominent information from stories available on the web or offline. Study shows in summarization more frequently machine learning methods are used like naive bayes, SVM, hidden Markov model etc. In this paper more emphasis is given on the aspect of conceptual dependency on verb in stories, as verbs are words that shows action or the overall Physical condition of a person helping to take out the main drama behind the stories and also in identification in noun because it refers to particular entity like people or things. The conceptual dependency is mainly used to highlight the important event from stories based on verb and noun search for Enhancement of text mining.
Advertising is a fundamental and structuring principle of Public Administration, standing out as an instrument of supervision and social control. In Brazil, public managers are required by law to be accountable and transparent to public spending, and the "Tribunal de Contas" (public controllership and finance) are responsible for overseeing and ensuring that the information provided is clear and complete. This paper presents a comparative study between the Support Vector Machine, Naïve Bayes, and Logistic Regression models to classify public spending according to the purpose of the expenditure, allowing to identify those that were omitted by managers. The constructed model detected 124 unpublished records totaling $ 3,1 million.
Text data are a major type of data in the modern society. Literatures have pointed out that more than 80% of data are in text form. It is important to study the insights from text data in addition to quantitative data. The development of text mining techniques started in the early 80's. The methodology has become much more mature in the recent years. In this article, we conduct a case study using text mining technique to analyze the patterns of the president's State of the Union address in USA. The speeches analyzed include the recent four USA presidents, Bush (1989-1992), Clinton (1993 - 2000), G.W. Bush (2001-2008), and Obama (2009-2011). Using two different text mining techniques, we identify six clusters from the 23 speeches using one technique and obtain seven topics based on the other technique.
Safety is a key monitoring point for the construction project by engineers. Accidents in the project are often accompanied by casualties, property losses and lead to negative social impact. It is vital to learn from what is going wrong in previous safety accidents to prevent the reoccurrence of similar accidents in the future. Large quantities of accident investigation report texts provide a solid foundation for accident analysis. However, accident reports are typically documented and stored as unstructured or semi-structured text data, which makes analytical work difficult. This paper proposes an analytical framework to extract accident causes from accident narratives using text mining technology. An evaluation function TF-K* based on term frequency and complex networks is proposed to identify the importance of safety risk factors. The proposed framework is validated by analysing accident reports of housing and municipal construction in China for ten years.
Nowadays, text mining technology is developed rapidly and employed by various contexts, such as healthcare, public opinion analysis, and document retrieval. Architectural design descriptions usually include large amount of massive text, which means information and knowledge are hidden. Since more and more attention are paid to the sustainable development, this paper research the main building design factors on nature aspect by using text mining. Collecting and processing 41,062 descriptions of architecture projects from website, the frequency of occurrence words with nature aspect are analyzed in 11 architecture categories respectively. The top 100 words of frequency are taken to constitute the word cloud, and the design factors on natural environment aspect could be summarized as area, light, landscape, structure, material, façade, site, water, wind, and view. This will provide architects guidance on dealing with the contradiction between building and nature. It will help us to further explore the relationship between architectural design and natural environment and realize a green and sustainable future.
The Publication Citation Network helps you visualize the distribution of journal articles or reviews. This extension helps determine the exposure of local and international university press. The relationship between one paper and another. Once formed, it is difficult to see the distribution of articles referenced in published articles. Unpublished publications may be due to complex relationships between one publication and another. Publications are widely distributed. Difficulty in determining the relationships between the objects of a publication and the need to determine the level of publication of the publication. This study consists of several parts and is only a part. In this study, we will use a text mining and social analysis approach to create a model that creates a graph that can visualize the distribution of date posts within a date post network. SINTA Science and Technology Index and data relationships using text mining on the Google Scholar website. Identify nodes in building a network of important publications. The data used is data from the SINTA Science and Technology Index website which involves publication data from Universitas Sumatera Utara lecturers from 2015 to 2020 and consists of 1730 records. The variables used included the article title, author name, institution, citation, and country. Text Mining runs in Python and generates a graph on the board.
Evolution of modern technologies allowed to store the text in various digital formats such as e-mails, e-documents, libraries, etc. The amount of text data that is produced daily is increasing dramatically. Discovering useful patterns in text that can be represented in unstructured, semi-structured or structured format is a difficult task that requires a good understanding of machine learning algorithms. Finding a suitable algorithm for text mining tasks such as classification, clustering or natural language processing is a demanding situation that tests researchers’ abilities. This paper provides an overview of the text mining process also, presents a comparison of the performance and limitations of two predictive models generated using the parametric Naïve Bayes algorithm and nonparametric Deep Learning neural network. RapidMiner data science software platform has been used for models’ implementations and e-mail classification.
Phenotyping definitions are widely used in observational studies that utilize population data from Electronic Health Records (EHRs). Biomedical text mining supports biomedical knowledge discovery. Therefore, we believe that mining phenotyping definitions from the literature can support EHR-based clinical research. However, information about these definitions presented in the literature is inconsistent, diverse, and unknown, especially for text mining usage. Therefore, we aim to analyze patterns of phenotyping definitions as a first step toward developing a text mining application to improve phenotype definition. A set random of observational studies was used for this analysis. Term frequency-inverse document frequency (TF-IDF) and Term Frequency (TF) were used to rank the terms in the 3958 sentences. Finally, we present preliminary results analyzing phenotyping definitions patterns.
Nowadays, huge amounts of information are available on social networks, blogs, websites, and digital libraries. Most of this information is in unstructured text format, so text mining approaches have become increasingly studied to process all this data. Text classification aims to automatically classify documents into predetermined categories, applying machine learning (ML) algorithms. In this paper, we collected a dataset set related to reviews of a food store in Peru and compared different vectorization models, such as Term Frequency Inverse Document Frequency (TF-IDF), Bag of Words (BoW), and classification algorithms, such as traditional ML classifiers SVM, Decision Tree, MLP, KNN, Naive Bayes and a recent approach “deep jointly informed neural networks” (DJINN) that initialize deep feedforward neural networks based on decision trees. The results show DJINN gets a F1-score higher than traditional ML, being a promising technique for text classification.
Use of software solutions as a medium for data collection, monitoring and tracking of any intervention program is increasing rapidly in real-world scenarios. In this paper, we present a case study on the use of text mining on data collected through a software system in place for tracking and monitoring industry-relevant skill training programs for the youth of rural India. Insights derived through text analysis of training data from various centers is presented. The limitations and future research potentials inferred from the data analysis are also discussed.
The emerging trends in social media have given a new path for information sharing for all types of internet users and online business operators. The contemporary emerging trends have been used in different forms to draw widespread attention from internet users. Different forms have been identified and inserting the data by different aspirants to attain their goals and aim to convey the information to the targeted audience. A huge amount of data is available in social media in different formats. The data analysis is mandatory to find the authenticity of the data and remove the spam data. Text mining methods have been introduced to analyze and summarize the data from numerous social media websites with distinct variables with open-source tools. The present paper is focusing on the emerging trends in social media text mining methodologies to extract the desirable data from the pool of social media text information associated with spam. This paper is rich in exploring different text mining applications, trend prediction in association with the social interaction theories. In this paper, Facebook, Twitter, Bloggers, and other predominant social media sites have been taken into consideration for extracting the experimental results.
Industry momentum refers to the phenomenon that buying(selling) industry portfolio from the past winner(loser) generates a positive return. In general, there is some lead-lag effect in the spread of price information due to the limited attention of investors. The industry momentum exploits the lead-lag effect due to information spillover delays within industries. In this study, We test whether industry momentum is stronger when we use information that is not attracting the attention of investors. We first perform industry classification with text mining of Annual Securities Reports describing the business activities of each company listed on the Japanese stock market. Then, we can identify groups of industry peer companies with low investor attention that is likely to cause the lead-lag effect. Such a text-based industry classification based on Annual Securities Reports is less visible to the investors but has an economic link among companies. We confirm that industry momentum base on text-based industry classification is more significant than traditional industry classification in Japanese stock market.
Summary form only given. The accumulation of biomedical literature makes it increasingly difficult for scientists to keep up with scientific advancements, requiring the development of text mining tools to collect and integrate data in a high-throughput fashion. A major challenge in biomedical text mining is how to recognize genes sensitively and accurately, and translate them to their official gene symbols. Gene symbols and their commonly used aliases and synonyms usually derive from an abbreviation of the gene's description. However, many gene symbols and alias exactly match other abbreviations commonly used in scientific literature that do not refer to genes. A systematic study on the abbreviations used in biomedical literatures should help improve the accuracy of gene recognition during text mining. A program was developed to extract all abbreviations in all free abstracts available in Pub Med from the 1960s until November 2010. We identified 198013 published abbreviations. Among them, 8602 are identical to a human gene symbol, alias or synonym in a dictionary containing 86615 entries. Of these 8602 abbreviations matching a human gene, 1095 only refer to the gene, 3581 refer to both the gene as well as another scientific term, and 3926 only refer to a another scientific term and not the gene. By checking for descriptions associated with abbreviations, whether the abbreviation refers to a gene or another scientific term can be determined. Compared to the simple use of a human gene dictionary as a guide, the presented method should provide a better solution for gene recognition during text mining by reducing false positive rates.
Text Mining is one of the computational intelligence research areas. The main goal of text mining tool is to discover knowledge which is embedded in unstructured text. The first step of text mining is to extract fact from the texts. However, to build a robust text mining tool is very complex. The first step requires the tool to process a natural language. The major challenging issue in any natural languages is the ambiguity problem. The problem may occur at lexical and phrase levels. This paper addresses ambiguity problem which occur in the preposition phrase, and presents a new technique for resolving the problem. The technique has been developed by applying possibility theory, fuzzy set, and context knowledge. The technique has been implemented and tested using a set of test cases and promising results are obtained. ()
For the existing assessment methods, there are many weakness like the complex process, the high cost and the mismatching between experts and the projects, which have an significant effect on the scientificity and efficiency. In this study, an online assessment system was proposed based on lightweight composite framework Spring+Struts+iBATIS. This system can voluntarily select the auto fit experts for the project by a text mining analysis on project declaration and study field of evaluation experts, with word segmentation tools ICTCLAS and TF-IDF algorithm. It is an effective solution to the unreasonable matching problem between project and evaluation experts. It integrated messaging platform, which greatly facilitates the communication between the administrators and assessment experts. To ensure the assessment results objective, project assessment is performed by blind review. Online assessment can overcome the defects of existing assessment methods, and make the assessment more equitable, scientific, efficiently and conveniently.
In recent years, Artificial Intelligence (AI) has emerged as a powerful force, driving innovation across various industries. In the realm of healthcare, AI, particularly Natural Language Processing (NLP), is rapidly gaining prominence as a transformative tool with the potential to reshape patient care. Natural Language Processing, a subfield of AI, has proven to be a game changer in healthcare. It plays a pivotal role in improving patient care, empowering healthcare providers, and optimizing decision-making processes. In this research project, key focus is delving into the dynamic landscape of NLP for Clinical Text Mining, highlighting its profound impact on the healthcare industry. Multiple papers have already been written on this topic. These papers focus on a lot of different topics like cancer terminologies, EHR analysis for job extraction, drug attributes etc. These papers include different databases and mention the precision of all the results as well. NLP empowers healthcare providers in several ways. It discusses how NLP streamlines healthcare workflows, leading to more precise and timely diagnosis, personalized treatment plans, and cost-effective healthcare services. The potential of NLP in healthcare is boundless, promising a future where clinical text mining is integral to delivering superior patient care. This project aims to provide an in-depth analysis of NLP’s transformative role, its current status and its evolving prospects within healthcare.
Text mining is a logical way of mining content and to know about feelings of users. Social media plays a vital role in letting people know about each other views. An ever increasing number of different online integration brands are working on Facebook, Instagram, Twitter and other social networks to furnish several directions and collaborate with multiple consumers. Social media helps different companies to enhance their businesses and get audience feedback for the betterment of their business timeline. Consequently, a lot of customer produced content is uninhibitedly accessible via web-based networking media sites. To increment upper hand and satisfactorily evaluate the focused condition of many companies, they need to work on and analyze the content that will affect their competitors who are also working on social integration networks. This paper represents a comparative analysis of five different social media pages and we have applied text mining to extract data from Facebook or Twitter of five fast food restaurants i.e. KFC, McDonald's, Burger King, Hardees and Howdy.
This study is concerned with text clustering and evaluating the clusters. Fuzzy neighborhood, a method of text mining, is used for this purpose. Five different clustering algorithms are used, they are kernel affinity propagation, kernel hard c-means, kernel fuzzy c-means, kernel hard k-means++ and kernel variable size hard c-means. These algorithms are applied to the clustering of nouns and adjectives in Chinese documents and SNS. The results from the previous four algorithms except kernel affinity propagation are evaluated by Rand index.
This paper presents a computational study of Thai legal documents using text mining and network analytic approach. Thai legal systems rely much on the existing judicial rulings. Thus, legal documents contain complex relationships and require careful examination. The objective of this study is to use text mining to model relationships between these legal documents and draw useful insights. A structure of document relationship was found as a result of the study in forms of a network that is related to the meaningful relations of legal documents. This can potentially be developed further into a document retrieval system based on how documents are related in the network.
To analyze a massive and extremely large amount of text information efficiently we use text mining techniques. It has been received a lot of attention because of the increasing demands of various organizations and companies to manage the large datasets of information which are available in text documents. In this digital world, text mining is very essential. The main goal is to discover information which is unknown, which is something till now not aware of anyone and so it has not been written down. Text mining is much similar to the web search but there are minor differences between the two of them. The major difference between text mining and web search is that in web search the browser searches the information which the user wants and that is already known that is it is written by someone else whereas in text mining we don't know exactly what we are looking for. The main objective of text mining is to discover new patterns and new trends among data, associations in entities and predictive rules, etc which combine together to extract information which is required by the user. This research is based on the exploration of the capabilities of text mining using R language which is an open source software which is basically used for statistical computing and graphics.
As a leading energy player, EDF (Électricité de France) actively works on new techniques to better understand customers' voice. In order to process unstructured and semi-structured massive text data, we have developed HETA, an application based on open source solutions which offers different text processing steps (document engineering, text analysis, clustering and visualization) on top of Hadoop. It is based on Mahout and uses sigma.js library for the visualization of the results as interactive graphs. HETA presents an ergonomic Web interface and is able to analyze any kind of unstructured (blog comments) and semi-structured (tweets, articles, etc.) massive text data. Being a modular and extensible application, HETA can be easily enhanced with the addition of advanced text mining methods.
With the overwhelming increase in the amount of texts on the web, it is almost impossible for people to keep abreast of up-to-date information. Text mining is a process by which interesting information is derived from text through the discovery of patterns and trends. Text mining algorithms are used to guarantee the quality of extracted knowledge. However, the extracted patterns using text or data mining algorithms or methods leads to noisy patterns and inconsistency. Thus, different challenges arise, such as the question of how to understand these patterns, whether the model that has been used is suitable, and if all the patterns that have been extracted are relevant. Furthermore, the research raises the question of how to give a correct weight to the extracted knowledge. To address these issues, this paper presents a text post-processing method, which uses a pattern co-occurrence matrix to find the relation between extracted patterns in order to reduce noisy patterns. The main objective of this paper is not only reducing the number of closed sequential patterns, but also improving the performance of pattern mining as well. The experimental results on Reuters Corpus Volume 1 data collection and TREC filtering topics show that the proposed method is promising.
Those who purchased the OTC drug cannot understand its efficiency. This is because most of the people do not have the knowledge of drug. This paper proposes the method of estimating the efficiency of OTC drug. The ingredient of OTC drug is a prescription drug. The prescription drugs have presented experimental results variety in order to obtain the approval of the Ministry of Health, Labor and Welfare. For this information, extracting the quantitative assessment of the efficacy OTC drug ingredients are analyzed with text mining method. Although there is much effect in OCT drug, we focus on analyzing the antipyretic effect in this paper. As a result, we succeeded in comparing the numerical effect of each component. If we carried out on various effects of this method, it is possible to express the effect of the OTC drug.
The fault diagnosis and analysis of high-speed train is of great significance for train's safe and highly efficient operation. The complexity and quantity of on-train information which is closely related with fault diagnosis and analysis is increasing rapidly as the development of high-speed railway. Onboard system fault recording files contains lots of important fault information described in short-text which is recorded by maintainers in nature language(NL). This paper makes an analysis on operation and fault information of on-board system recorded in NL based on text mining. The fault text dictionary is constructed for word segmentation, and fault text chain is defined as well as the construction method in order to process the fault recording files. At last, the method is applied to segment the on-board system fault recording files and construct fault text chains, therefore the faults of on-board equipment are analyzed and we obtained the faults' distribution.
Big Data is becoming one of the most important technology trends with a potential for a dramatic change in which organizations use information in order to improve the customer's experience and transform their own business models. Big Data as a concept is new in the world of technology and therefore requires research in technological or business sense. Management and analysis of large amount of network offer huge benefits and challenges for all organizations. The amount of information floating through social networks increases every day, and represents a rich source of data, if it is properly processed. The aim of this paper is to show on a concrete example the profit of the system implemented over Big Data from social networks using Text Mining methods and technologies, as well as semantic processing and clustering, storaging and possibility of later examination and treatment.
In the field of semiconductor manufacturing, people usually focus on the data of well-designed databases, such as values from tool sensors, inline metrology data, or WAT data. These data are well structured and easily handled by engineers for further analysis. For example, integration engineers can compared CD metrology data to WAT data to catch out the root cause of abnormal device current, or equipment engineers can check FDC data to judge PM success or not.
We herein investigate finding unusual patterns from a given string as a text. In the present paper, the pattern is expressed as a sub string of the string. The natural assumption with respect to the frequency of a pattern is that the shorter the length of the pattern, the larger the frequency of the pattern. We define a pattern to be pure if the frequencies of all of the sub strings of the pattern are the same as the frequency of the pattern. This means that the sub strings appear only within the pattern in the string. This condition is in contrast to the natural assumption. The present paper proposes three statistics for quantifying the purity of a pattern, i.e., probability, entropy, and difference, which are calculated based on the frequency of the pattern and its sub strings. Experiments using DNA sequences reveal that patterns with large probability correspond to the features of the sequences.
Hypertension becomes a major public health problem due to its high prevalence and serious complications. The complementary therapies of Traditional Chinese Medicine (TCM) play important roles in hypertension treatment. This study analyzed the application of the TCM external therapy and food therapy by text mining. Subsequently, suitable techniques and methods which were frequently used on hypertension treatment were explored. The text mining results showed that acupuncture, massage, Tuina, auricular-plaster and external application of Chinese herb medicine(CHM) were the top 5 TCM external therapy techniques, while placenta, lily root, pseudo-ginseng, peanut, sea horse, wine, radix aconiti carmichaeli, lemon, soybean and fructus crataegi were the top 10 foods used for hypertension treatment. The combinations of external therapy techniques were demonstrated in network, so does the combinations of the foods. The results provided good references for both clinical practice and medical research.
As ChatGPT has evolved, generative AI (Artificial Intelligence) has gone viral on the internet since 2022. Heated discussions on generative AI have appeared in both social media and academic field, generating massive textual data. Overwhelming media coverage of generative AI may lead to biased conception. To date, there has been no systematic analysis of how generative AI is mentioned on the internet. Moreover, little attention has been paid to demonstrating the gap in perceptions of generative AI between social media and academic field. This study seeks to focus on the following specific research questions: What are the key terms related to generative AI, what are the key term differences in social media and academic field on generative AI, and what are the topic differences of generative AI in social media and academic field? A text-mining approach supported by KH-coder was employed. The research data were drawn from two main text sources: the Sina Weibo platform and the CNKI periodical database. The results revealed statistically significant differences in key terms and topics related to generative AI between the social media and academic field. Our findings enhance the understanding of public ideas and the trend of generative AI on the internet, and provide supportive information for future studies on generative AI applications.
The use of social media has grown rapidly in the last decades. Many of these social media platforms, i.e., Twitter, do not screen or remove potentially offensive/ harmful tweets. It can cause discomfort for the users, especially if minors encounter these words while they are surfing the media. We came up with a profanity detection model with text mining that uses bag of words algorithm. Then we applied four different classifiers to see which one gets the highest accuracy score. The results show a high accuracy performance, with Random Forest classification being the highest with 97.5%.
In the biomedical domain large amount of text documents are unstructured information is available in digital text form. Text Mining is the method or technique to find for interesting and useful information from unstructured text. Text Mining is also an important task in medical domain. The technique uses for Information retrieval, Information extraction and natural language processing (NLP). Traditional approaches for information retrieval are based on key based similarity. These approaches are used to overcome these problems; Semantic text mining is to discover the hidden information from unstructured text and making relationships of the terms occurring in them. In the biomedical text, the text should be in the form of text which can be present in the books, articles, literature abstracts, and so forth. Most of information is stored in the text format, so in this paper we will focus on the role of ontology for semantic text mining by using WordNet. Specifically, we have presented a model for extracting concepts from text documents using linguistic ontology in the domain of medical.
This study aimed to quantitatively analyze the trends of environmental research that utilizes environmental spatial information using a text mining technique. The analysis was conducted on a total of 869 papers published in Korea based on 20 environmental spatial information areas, and the results showed that 1) “general environment (40.85%)” and “satellite image (24.87%)” had the highest frequency levels among the environmental spatial information areas, and 2) a total of 80 relevance rules were generated on environmental spatial information, indicating that “general environment” generated the highest number of correlation rules (17) with environmental spatial information such as “satellite image” and “digital map”.
Recently, combined quantitative and qualitative analysis has become popular for research. In studying careers, subjective and objective information are ideal for assessing individual career development and are relevant in career counseling. This paper measures career adaptability by combining text mining and item response theory (IRT), with college students' self-reported career adaptability as a subjective measure and responses to questionnaire items as an objective measure. The two are combined under a Bayesian framework. Additionally, the validity of text categorization and IRT, combined with model measurement, were explored; text categorization results were used as prior information when estimating IRT capability parameters to test whether adding prior information can improve accuracy. This study draws the following conclusions: (1) The text classification method had the highest sensitivity in 300-person samples; however, the text-IRT method had the best predictive effect, high reliability, and unique advantages in accuracy. (2) In 600-person samples, the text classification method had the best predictive effect. The effect was relatively good, with unique advantages in identifying low career adaptability. However, this must be selected according to actual needs. If the accuracy requirement is high and sensitivity can be sacrificed, the text-IRT method is more appropriate. (3) The text-IRT method is more suitable for 900 subjects when accuracy, sensitivity, and specificity need to be considered, and text classification is best when identifying low career adaptability. (4) Sample size influenced accuracy, specificity, and the negative predictive values of text classification, as well as the sensitivity of IRT and text-IRT methods.
Tourist perception plays a crucial role in managing tourist destinations. Traditional analysis relies on questionnaires, but the widespread use of new technology has made the "digital footprints" that tourists leave online an increasingly important source of information for understanding their perception. It is imperative to consider this source of information. Compared to the traditional survey questionnaire approach, digital footprints offer a wider range of data sources that can be acquired more conveniently on larger scales with more extensive content. In this paper, we utilize tourists' comments about tourist destinations shared on the internet as our primary data source. Using the "weiciyun" platform’s text analysis and visualization functions, we aim to provide an intuitive and concise approach for students to learn the principles and methods of text analysis. This lays a solid theoretical and practical foundation for realizing the full potential of tourists' perceptual data in managing tourist destinations.
With the number increase of biomedical literatures, biomedical relation extraction discovery from the literature represents a new challenge for researchers in recent years. Then, a system that automatically extracts the related genes to the targeted disease is required. In this paper, we explore text mining and pattern clustering for relation extraction of breast cancer and related genes. It can be considered an unsupervised method and labeled data is not necessary. We firstly extract the candidate genes related to breast cancer by checking the window distance between the appearance of genes and breast cancer in a sentence. Then, two different clustering approaches (simple clustering and K-means clustering) are applied for finding the candidate association words that indicate the relationship between breast cancer and genes. The comparison experiment demonstrates that simple clustering is superior to K-means clustering in this task.
With the digital transformation, life started depending on the digital world. Hence, there is a massive amount of unstructured textual data produced and accumulated faster. Such data used in many applications such sentiment analysis, topic modeling, summarization, classifications, and clustering. However, researcher's wastes time and effort in collecting data and constructing a dataset to examine and evaluate their models or algorithms. In the Universal Language of English, there are many benchmark datasets. The Arabic language lacks datasets in many domains. This paper introduces an Arabic dataset called AraDS. AraDS consist of three Arabic datasets namely; Arabic Dataset on Herbal Treatments for Diabetes (ADHTD), Arabic Multi-Classification Dataset (AMCD), and the Arabic Sentiment Dataset - Khat (ASDK). These datasets were collected from social media platforms such as YouTube. It contains user-generated comments and video metadata. AraDS publicly available dataset. The assessment of the annotation process has been carried out and evaluated using four methods. They are Cohen's kappa, three Arabic native speaker annotators, accuracy, and F-measure. AMCD and ASDK datasets are balanced, whereas ADHTD is an imbalanced dataset. Two datasets are sided with binary classes and the third one is a multi-class dataset. These datasets can be beneficial for many data, text mining, and sentiment analysis researchers to apply methods and algorithms.
The Ministry of Education, Culture, Sports, Science and Technology (MEXT) published “Compendium Student Guidance - Seito Sidou Teiyou” in 2010. The compendium aims to activate student guidance and counseling in every primary and high school. However, reading and understanding this compendium is difficult for those unfamiliar with school teaching. Additionally, in December 2022, this text was substantially revised to reflect the increasingly complex and changing environment for students. This compendium is a frequent candidate for the teacher employment examination. Only seven months after the publication of the new edition, students will have to take a challenging exam. It takes time for university faculty to read complex documents and understand their moot points, and it is difficult to explain them in a way that is easy for students to understand. Therefore, this article analyzes the document using text-mining techniques with KH Coder software, SQL, and R language. In particular, it will discuss how to facilitate its understanding for university students taking the teacher employment selection examination.
With the emerging technologies and latest trends in the information technology sector, machine learning can be said as one of the most powerful means in developing various software which are helping the business as well as the industries, the government, and private organizations, which shows the change in the emergence of this technology in the healthcare sector. Healthcare can be said to be the most important aspect of the 21st century. With the spread of various diseases and also the pandemic which had hit the world recently the healthcare system must come up with efficient drugs which are liable and are for a sure cure to the disease. Proposed research involves use of various machine learning tools and deep learning techniques. The algorithm uses text mining and NLP for extraction of chemical data from the related research papers and a drug dictionary is created. This dictionary contains specific targeted chemical data related to drugs used for treating ovarian cancer and will help in suggesting personalized drugs to patients. Our primary focus is ovarian cancer. Ovarian cancer is not only deadly to women but also very tough to get the correct diagnosis. It is considered the third most crucial cancer within Indian women.
Visual text analytics has recently emerged as one of the most prominent topics in both academic research and the commercial world. To provide an overview of the relevant techniques and analysis tasks, as well as the relationships between them, we comprehensively analyzed 263 visualization papers and 4,346 mining papers published between 1992-2017 in two fields: visualization and text mining. From the analysis, we derived around 300 concepts (visualization techniques, mining techniques, and analysis tasks) and built a taxonomy for each type of concept. The co-occurrence relationships between the concepts were also extracted. Our research can be used as a stepping-stone for other researchers to 1) understand a common set of concepts used in this research topic; 2) facilitate the exploration of the relationships between visualization techniques, mining techniques, and analysis tasks; 3) understand the current practice in developing visual text analytics tools; 4) seek potential research opportunities by narrowing the gulf between visualization and mining techniques based on the analysis tasks; and 5) analyze other interdisciplinary research areas in a similar way. We have also contributed a web-based visualization tool for analyzing and understanding research trends and opportunities in visual text analytics.
Sentiment analysis in text mining is known to be a challenging task. Sentiment is subtly reflected by the tone, affective state or emotion of a writer's expression in words. Conventional text mining techniques which are based on keyword frequency counting usually run short of accurately detecting such subjective information implied in the text. In this paper we evaluated several popular classification algorithms, along with three filtering schemes. The filtering schemes progressively shrink the original dataset, with respect to the contextual polarity and frequent terms of a document. In general the proposed approach is coined hierarchical classification. The effects of the approach in different combination of classification algorithms and filtering schemes are discussed over three sets of controversial online news articles where binary and multi-class classifications are applied.
This paper presents a text mining analysis as a part of evaluation for healthcare support system, which is designed and developed from view of previous research results and problems. The system is an e-mail text-based communication system independent with sorts of terminals, based on mailing-list system and approved various communications by one-stop account. The system offers users' availability for checking their own data which is reflected by every day's reply data, and daily mails have many variation because they are not automatically sent but nurses writes and sends every day, which let e-mails not routinely but humane and improved. We have validated the system by the operation test, where we have evaluated and discussed the results.
The technology of natural language processing in the task of constructing a psychological portrait is considered. An algorithm for machine learning based on a recurrent neural network is proposed, which allows you to automatically determine the psychological portrait from a person’s speech, translated into text format.
Text mining is a popular research topic with application areas ranging from security to media and marketing. More specifically, text mining has been applied in biomedical area for the categorization of radiology reports, which is a challenging problem due to their free-text and unstructured format. State-of-the-art in radiology report mining has mostly focused on English text, while studies on Turkish reports are scarce. Accordingly, in this work we propose to employ text mining for categorization of Turkish radiology reports. We automatically remove header and footer of the reports, apply frequency analysis on the remaining report text, and perform categorization of reports to anatomical regions using pre-selected keywords. The accuracy of the proposed solution is measured as 84.3% over a 66-report test set.
There has been a rapid rise in the number of users getting connected online via social networking sites. To communicate with other users and share their thoughts and opinions, online users' tend to use texts in the form of blogs, posts, tweets, messages, reviews, comments etc. Thus, there has been an immense possibility complemented with a wide gamut of research in the field of Opinion Mining or Sentiment Analysis by using textual information from online communities. Hence, there is an extensive need for different text classification algorithms and approaches to classify texts and predict sentiments correctly so as to comprehend the emotional state of the user. We have varied algorithms for text classification for predicting emotional traits. In this paper, we are proposing a novel dictionary-based algorithm that uses lexicon-based approach for opinion mining and calculates the sentiment polarity levels. Our algorithm is different from other lexicon-based algorithms in the context that it uses the three degrees of comparisons viz. positive, comparative and superlative degrees on words; for each of the positive and negative sentiment words. Our system yields an Accuracy of 81% and an F-score of 0.874 on the test dataset which is quite moderate and can be fairly accepted.
The development of information technology and smartphones has caused production of many data around us. In every second million of new data is created in the form of text, audio, image and even videos. This environment then has triggered big data analytics demand. One of big data that is produced daily is data on the history of healthcare services in hospitals. Important new information can be retrieved through this huge dataset, especially concerning the patient symptoms, drug usage and new diseases report. In this study, text processing technique is applied on text data of patient medical record data from public hospital during 2017 till 2019 regarding the patient symptoms and the disease classification. Naïve Bayes Classifier and Random Forest algorithms are used to classify diseases in medical record data with 19 diseases in preprocessing data. A list of modified Indonesian stop words was used to filter the symptom sentences. The result indicates that the Random Forest classification algorithm can achieve the highest accuracy of around 99.9%, better and more accurate than the Naïve Bayes classification algorithm. This experiment shows that our proposed method provides a robust system and good accuracy for classifying medical record data with many diseases.
With the incredible increase in the amount of text data, the need to develop efficient methods for processing and analyzing it increases. In this context, feature extraction from text is an urgent task to solve many texts mining and information retrieval problems. Traditional text feature extraction methods such as TF-IDF and bag-of-words are effective and characterized by intuitive interpretability, but suffer from the «curse of dimensionality», and they are unable to capture the meanings of words. On the other hand, modern distributed methods effectively capture the hidden semantics, but they are computationally intensive, and uninterpretable. This paper proposes a new concept-mining-based text vectorization method called Bag of weighted Concepts BoWC that aims to generate representations with low dimensions and high representational ability. BoWC vectorizes a document according to the concepts’ information it contains, where it creates concepts by clustering word vectors, then uses the frequencies of these concept clusters to represent document vectors. To enrich the resulted document representation, new weighting functions are proposed for concept weighting based on statistics extracted from word embedding information. This work is a development of previous research in which the proposed method was developed and tested on a text classification task. In this work, empirical tests were extended to include tuning the parameters of the proposed method and analyzing the effect of each on the efficiency of the method. The proposed method has been tested in two data mining tasks; document clustering and classification, with five various benchmark data sets, and it was compared with several baselines, including Bag-of-words, TF-IDF, Averaged word embeddings, Bag-of-Concepts, and VLAC. The proposed method outperforms most baselines in terms of the minimum number of features and maximum classification and clustering accuracy.
The objective of this study is to provide a systematic literature review related to text mining, clustering, and sentiment analysis by trying to find more appropriate technics, methods, algorithms, and procedures in this field with special emphasis on lower resource languages such as Albanian language. In this study, we have analyzed more than 170 papers that are published in the last decade from relevant databases such as IEEE, ACM, Scopus, and Springer among others. This paper aims to identify the advantages and disadvantages of each approach and underline some of the major remaining obstacles and challenges in this area.
Often repositories of systems engineering artifacts at NASA's Jet Propulsion Laboratory (JPL) are so large and poorly structured that they have outgrown our capability to effectively manually process their contents to extract useful information. Sophisticated text mining methods and tools seem a quick, low-effort approach to automating our limited manual efforts. Our experiences of exploring such methods mainly in three areas including historical risk analysis, defect identification based on requirements analysis, and over-time analysis of system anomalies at JPL, have shown that obtaining useful results requires substantial unanticipated efforts - from preprocessing the data to transforming the output for practical applications. We have not observed any quick 'wins' or realized benefit from short-term effort avoidance through automation in this area. Surprisingly we have realized a number of unexpected long-term benefits from the process of applying text mining to our repositories. This paper elaborates some of these benefits and our important lessons learned from the process of preparing and applying text mining to large unstructured system artifacts at JPL aiming to benefit future TM applications in similar problem domains and also in hope for being extended to broader areas of applications.
Internet has become the world's largest information repository, especially the explosive growth of the text data on the web, the disadvantages that it need much more time to acquire and update web pages, and is not high precision have become more obvious. The text mining algorithm based on focused crawler is proposed in this paper, it classifies and integrates the whole web pages by topic using topic crawler algorithm as much as possible, which greatly improves the retrieval ability of the web pages, naive bayes algorithm is adopted on this basis, which realizes the text mining processing of the web data. The experimental results show that the algorithm has good feasibility and higher recall ratio and precision ratio of the web pages.
Pattern mining is a fundamental topic in data mining area. Many pattern mining techniques, such as closed and maximal pattern mining have been proposed for different applications. However, when calculating the frequency of a pattern, the existing techniques treat each word equally. For example, although the word `pie' in `I love eating pie.' is quite different from `pie' in `american pie', `pie' in `american pie' will still be added up to the counts of `pie' when calculating its frequency. Therefore, this paper aims to overcome the drawback to find the valid patterns tailored to text mining. We will approach pattern mining from a different perspective and introduce a novel problem of frequent semantic pattern mining. We then propose an algorithm to solve this problem via suffix array sorting. The algorithm can be implemented to run in linear time. Compared with traditional pattern representations, our results show the semantic patterns extracted are more than 13% compact. Also, classifier built on these features is no less or more powerful.
The aim of machine learning is to solve a given problem using past experience or example data. Many machine learning applications are using now-a-days already. More aspiring problems can be handled as more data become accessible. Here. in this context we learn in detail about text mining as a multi-dimensional field which involves the closely linked areas or sections like 1. Retrieving information, 2. Machine learning concepts shortly termed as ML, 3. Statistics, 4. And finally Computational linguistics and specifically to be mentioned, data mining. With the use of sample data or previously gained experience, machine learning is included into computers to enhance or improve a performance decisive factor. In this context we have detailed a model up to some level of constraints, and learning is the processing of a main content to enhance the parameter of the form using the training or sample data or previously gained experience. This may be designed to gain knowledge from the given data, or use the effect for changes in the future, or both. These learning techniques also helps us to make solutions to various bugs which includes vision, speech recognition, and robotics. We take the example of the main analysis of preprocessing of tasks and procedures, then classification, then clustering, information extraction and finally visualization.
The issue of low productivity growth and reliance on foreign labour are major concerns in Singapore. Traditionally, identifying productivity issues usually involves an expert's opinion or market surveys. These methods might be biased or labour intensive. In this paper, we consider an alternative method, a data-driven approach using text mining. Various text mining techniques are applied on the data collected from 87 companies that participated in an operational excellence programme. The results indicated that even though the set of productivity issues for companies residing in Singapore are similar, the severity of the problems differs across different types of industries. Furthermore, we also found that companies from different industries tend to adopt different methods to solve their productivity issues.
With the wide application of computer multimedia technology in university English language teaching (EL T)., the automatic evaluation system (AES) of English composition has become the breakthrough point for English teachers to reform ELT means and methods. Because of the openness of the composition itself and the subjectivity of the marking staff in the marking process., it is always difficult to ensure the consistency of the marking standards. This article proposes an automatic assessment algorithm for English composition based on big data text mining, which provides theoretical model support for the development of intelligent auxiliary tools for English composition training. The simulation results show that the accuracy of this algorithm has obvious advantages, reaching more than 93%, compared with the support vector machine (SVM) algorithm and the random forest (RF) algorithm. Therefore, the automatic essay evaluation (AEE) model based on big data text mining is a reasonable and feasible assessment model, which is of great significance for the innovation of AEE system. Using this model to effectively assist manual assessment can truly test students' practical language application ability, assist students' English composition writing training and help students effectively improve their writing ability.
Text Mining is a set of techniques that analyzes large masses of data, extract relations that are unknown beforehand, and provide solutions to help decision-making. Text mining had been used extensively to analyze English text. However, text mining has only been used recently in analyzing Arabic text. As a result the objective of this paper is to present the current state of Arabic text mining. A systematic review has been performed to collect the papers published on the analysis of Arabic text mining. More than one hundred papers were used in our review from different reliable sources, and then they were classified according to their specific domain, and classified again according to the specific techniques used. This paper also provides quantitative analysis of publications according to publication type, year, category, and contributors.
Much useful information on Ligusticum wallichii (LW) could be obtained from published literature by text mining technique. In this study, the data set on LW was downloaded from Chinese BioMedical literature database (SinoMed). Then, association rules among diseases, traditional Chinese medicine (TCM) syndromes, formulae and herbs on LW were investigated by text mining technique. These rules include TCM syndromes to diseases, formulae and combinational herbs, respectively. Diseases related with formulae including LW were mined out by executing data slicing algorithm. Finally, the results were visually demonstrated with Cytoscape 2.8 software. The main features from the mining data were: (1) LW was frequently used in treating cerebral infraction; (2) Blood stasis due to Qi deficiency was the main syndrome in TCM clinical practice; (3) Angelica sinensis was the first herb to combine with LW according to co-occurrent frequency; (4) Associated with LW, networks of TCM syndromes-diseases, formulae-diseases, TCM syndromes-formulae, and TCM syndromes-combinational herbs were constructed. These associated networks represented a holistic thinking of Chinese medicinal therapy, which might embody association rules among diseases, syndromes, formulae and herbs on LW.
Ecology is a hot topic nowadays. Taking care of it is essential to our existence on Earth. There is plenty of work being done to improve the ecological situation. But a small group of avid supporters can come only so far without a consensus in the society. One way to raise public awareness of the ecology question is through the news. Anybody familiar with the scientific method knows: you cannot improve what you cannot measure. Hence the topic of this paper. We attempt to measure the general awareness of the ecology-related topics through text mining of Ukrainian news articles.
In the languages, the occur of words are indicated about meaning of contents in text. Generative models for text, such as the topic model, have the potential to make important contributions to the statistical analysis of large document collections, and the development of a deeper understanding of human language learning and processing. In this paper, we proposed a novel method for building Vietnamese topic model based on core terms and conditional probability. With this approach, we reduced cost of time for building corpus. After that, we perform with Vietnamese text classification and the experimental show that, this corpus will help text classification system really effectively than traditional methods, higher accuracy and reduced complex data processing.
Acupoint application therapy is one of the Traditional Chinese medicine (TCM) external therapies, which has been used for coronary heart disease (CHD) treatment for a long time. The prescription of acupoint application therapy includes both Chinese herbal medicines (CHMs) and acupoints. This study aims to reveal the basic rules of acupoint selection in acupoint application therapy on CHD treatment by text mining study. Results showed that on CHD treatment, the most frequently used acupoint was Danzhong (RN17), followed by Xinshu (BL15), Neiguan (PC6), Zusanli (ST36) and Zhiyang (DU9). Acupoints closing to the heart or with specific functions were more frequently used. Moreover, Xinshu, Danzhong and Neiguan were of the top 3 relationships with other acupoints, while Zusanli and Sanyinjiao (SP6) were in the lower grade. The combination of Xinshu-Danzhong-Neiguan could be regard as the basic acupoint prescription of acupoint application therapy on CHD.
In the current digital landscape, particularly on the web, there is an increasing demand for improved methods of controlling and organizing written documents. Automatic text classification, a crucial aspect of text mining, plays a significant role in managing systems effectively. The accurate weighting of keywords greatly influences the outcomes of text-mining techniques. In this article, we present an enhancement to the TFIDF weighting method, incorporating two important factors: the distribution of keywords throughout the entire text and giving priority to the opening and conclusion paragraphs based on the fuzzy method. To assess the effectiveness of our proposed approach, we classified 500 texts into 5 distinct categories. Using the proposed method has led to the improvement of 4%, 3%, 13%, 2%, and 5% in each of the parameters F-measure, Recall, Precision, FP, and TP. The results demonstrate the advantages of our method, highlighting its superior performance.
Through in-depth excavation and analysis of tourist network texts in many rural tourist spots, the effectiveness of the ecological civilization construction function of rural tourist destinations is analyzed, and it can be found that tourists usually follow the basics of perceptual cognition, attitude and consciousness generation, emotion and behavior transformation Logic, realize the internalization of ecological values, perform topological clustering on the reference network, and then use the text mining method to extract the topic of the network community, combine the relationship between the network communities in different time periods to analyze the technological evolution path, and add materials Take the manufacturing field as an example to conduct empirical research and increase the efficiency by 7.63%.
The majority of existing knowledge is encoded in unstructured texts and is not linked to formalized knowledge, like ontologies and rules. The potential solution to this problem is to acquire this knowledge through natural language processing (NLP) tools and text mining techniques. Prior work has focused on the automatic extraction of ontologies from texts, but the acquired knowledge is generally limited to simple hierarchies of terms. This paper presents a polyvalent framework for acquiring more complex relationships from texts and codes them in the form of rules. Our approach starts with existing domain knowledge represented as OWL ontology and SWRL "Semantic Web Rule Language" rules by applying NLP tools and text matching techniques to deduce different atoms as classes, properties etc. This is to capture the deductive knowledge in the form of new rules. We evaluate our approach thereafter by applying it on medical field more precisely Gynecology specialty, showing that this approach can generate automatically and accurately SWRL rules for the representation of more formal knowledge necessary for reasoning.
Scientific exhibition in Taiwan has been held by National Taiwan Science Education Center for more than 55 years, more than 15,000 works has been accumulated. It also becomes a famous competition for K-12 students. In order to provide recommendation of research topic to the participating students, domain experts require the growth topic and related industry from the past works. However, with the long history of the scientific exhibition, the domain experts cannot interpret all of the works in the short term. Therefore, through the computer to explore and summarize a large number of works becomes an emerging technology. In this study, we applied text-mining technology, and designed an expert rules as a computer-enabled methodology to explore the correlation between scientific exhibition and industry in the earth science and physics category. In the result, you can observe the computer programming and real estate development industries were the most growing research topic in scientific exhibition.
The aim of this paper is to study methods of agglomerative hierarchical clustering which are based on the model of bag of words with text mining applications. In particular, a multiset theoretical model is used and an asymmetric similarity measure is studied in addition to two symmetric similarities. The dendrogram which is the output of hierarchical clustering often has reversals. If we have a reversal, to obtain clusters from the dendrogram becomes difficult. Then, we show the condition that dendrogram have no reversals. It is proved that the proposed methods have no reversals in the dendrograms. Examples based on Twitter and Wikipedia data show how the methods work.
Nowadays there is an increasing trend in computers used for learning Islamic knowledge from Indonesian Translation of AL-Quran (ITQ). As a result, substantial knowledge is stored in the form of unstructured text on chapters (surah) of ITQ. Text mining is exciting research area incorporated with information extraction, natural language processing, information retrieval, and data mining. It tries to discover knowledge from unstructured text. Text mining on ITQ is an ability to process Indonesian text into sentences (ayat) or documents (surah), interpret its text meaningfully, and identify as well as extract relationship among concept to directly answer the question of interest. This paper presents a review of concepts, searching and question answer (SQA) applications, and issues on text mining for ITQ. We reviewed the research papers highlighted some of the problems, gaps, critical challenges in this area and proposed some future research directions. Review method is composed of three phases: planning, conducting, and reporting the review. The results of this review show most existing research on text mining based on complexity, ambiguities, and optimizing for the SQA system application. Finally, this review can be beneficial to researchers in this area to carry it to the next level.
The unstructured text of railway signal equipment failure records important information such as the failure cause and failure phenomenon of the signal equipment. Most of them are stored in Word, Excel, etc. The traditional technology cannot explore the important value contained in the text data. In order to convert the analysis of the fault causes of the signal equipment recorded in the text into knowledge that can serve fault diagnosis, this paper uses the BiLSTM+CRF model to realize named entity recognition and analyzes 638 fault texts of railway signal equipment in a railway field from 2021 to 2022. The accuracy of the model reaches 83.38%, which shows that the named entity recognition model of railway signal fault equipment has a high evaluation standard and can be applied to the extraction of signal equipment fault entities based on text mining.
The fast spreading of coronavirus name covid19, generated the actual pandemic forcing to change daily activities. Health Councils of each country promote health policies, close borders and start a partial or total lockdown. One of the first countries in Europe with high impact was Italy. Besides at the end of April, one country with a shared border was on the top of 10 countries with more total cases, then France started with its own battle to beat coronavirus. This paper studies the impact of coronavirus in the poopulation of Paris, France from April 23 to June 18, using Text Mining approach, processing data collected from Social Network and using trends related of searching. First finding is a decreasing pattern of publications/interest, and second is related to health crisis and economical impact generated by coronavirus.
This minitrack encompasses papers of a quantitative, theoretical or applied nature that focus on:    Content Mining of Social Media -- discovery of patterns from the text, images, audio, video and other data generated by Social Media sites Structure Mining of Social Media -- social network analysis of the node and connection (graph) structures underlying Social Media sites
This paper presents a combination of text classifier and word centrality for analyzing product reviews. The product reviews collected from the Amazon website consist of 10,374 office product reviews. The classifier selected Random Forest algorithm and word centrality measures are Degree Centrality (DC), Centrality (BC), Closeness Centrality (CC), and Eigenvector Centrality (EC) and set threshold values ranking from 0.2-1.0. The result showed that Random Forest algorithm combining the world centrality was higher than the Random Forest alone with a threshold greater than 0.2. Classification accuracy is 85.50% to 87.50%.
Non-alcoholic fatty liver disease (NAFLD) is a kind of prevalence diseases. Traditional Chinese medicine (TCM) has better efficacy on treating NAFLD. But there are also not known about the critical pathogenesis and the corresponding biological factors. Regarding this, we addressed a text mining approach to analyze the pattern profile, rule of medication, and the pathological factors of NAFLD from the opening database (SinoMed and PubMed). Based on canonical data source, we have our data treatment scheduled in 4 steps: (1) data retrieving, (2) data pretreating, (3) data analyzing, and (4) data visualization. And according to the TCM theory of formulae-pattern-disease' correlation, we partly understand the possible TCM pathogenesis of NAFLD which linked biological process of lipid metabolism disorder, inflammation, and metabolic regulation confusion.
Complete reporting of Experimental Meta-data (EM) is necessary for reproducing and understanding biomedical experiments and results. Experimental Metadata Reporting Checklist Questions (EMR-CLQs) have been designed and used by journals as guidelines to capture EM and evaluate the quality of the reporting. Automatically answering EMR-CLQs is necessary to check completeness and clarity of EM, which can be useful for the peer-review process. Moreover, automatically extracting the EMR-CLQs answers can be used to search the relevant literature for the meta-data analysis process in an efficient way. This paper shows the possibility of answering different types of EMR-CLQs automatically by understanding the structure of both EMR-CLQs and the biomedical article. A text mining model (rule-based approach) based on the information extraction techniques and the structure of the biomedical articles and the EMR-CLQs, is proposed as a first model in the biomedical reproducibility domain to answer EMR-CLQs automatically. The model was used to answer five EMR-CLQs of two different types automatically; Main and Attribute questions. We evaluated the feasibility of the model against gold-standard data of 58 full-text articles annotated by domain experts. The results are showing the possibility of answering the EMR-CLQs automatically with a mean f-measure of 75% and 73% for development and testing datasets, respectively.
Text data plays an imperative role in the biomedical domain. As patient's data comprises of a huge amount of text documents in a non-standardized format. In order to obtain the relevant data, the text documents pose a lot of challenging issues for data processing. Topic modeling is one of the popular techniques for information retrieval based on themes from the biomedical documents. In topic modeling discovering the precise topics from the biomedical documents is a challenging task. Furthermore, in biomedical text documents, the redundancy puts a negative impact on the quality of text mining as well. Therefore, the rapid growth of unstructured documents entails machine learning techniques for topic modeling capable of discovering precise topics. In this paper, we proposed a topic modeling technique for text mining through hybrid inverse document frequency and machine learning fuzzy k-means clustering algorithm. The proposed technique ameliorates the redundancy issue and discovers precise topics from the biomedical text documents. The proposed technique generates local and global term frequencies through the bag-of-words (BOW) model. The global term weighting is calculated through the proposed hybrid inverse documents frequency and Local term weighting is computed with term frequency. The robust principal component analysis is used to remove the negative impact of higher dimensionality on the global term weights. Afterward, the classification and clustering for text mining are performed with a probability of topics in the documents. The classification is performed through discriminant analysis classifier whereas the clustering is done through the k-means clustering. The performance of clustering is evaluated with Calinsiki-Har-abasz (CH) index internal validation method. The proposed toping modeling technique is evaluated on six standard datasets namely Ohsumed, MuchMore Springer Corpus, GENIA corpus, Bioxtext, tweets and WSJ redundant corpus for experimentation. The... (Show More)
This paper aims to analyze affective expressions in articles of popular science by text mining with the keywords “Cancer” and “Immunity”. This study selects 145 articles from the website of a magazine and segmented them into 410,919 terms. And the study uses an automatic system to classify the terms into vocabulary categories, selecting the affective terms with specific vocabulary categories. The results show those the affective terms in the analyzed articles of popular science are not significantly differential with year and decade. But there is a significantly negative correlation with the quantity of articles that are published in the same year. That is, the more articles are published the less proportion of affective terms to the summary terms occurs on the articles.
In this knowledge era, plethora of textual information is growing rapidly which is usually semistructured or unstructured data collected and stored in various databases. Discovery of knowledge from this available database is not simple. Thus, the automatic feature selection approach is very much necessary in the processing of this unstructured data. The Feature selection approach focuses towards processing unstructured information into relevant information, helps to understand and visualize the data, it also reduces the training and processing time of huge amounts of data. Feature helps in caarying out the text mining task effectively and accurately. Text mining offers various methods to fetch the interested data from vast databases. Feature selection proves to have a vital role in this process. In this paper, bread model is proposed that processes text document using the input termset. Based on first principles of instruction application methodology, the model phases are implemented that provides the effective results.
Now, in the Pandemic era, all people use online technology for all-purpose such as messenger application for all their activities. Text message is very fast to give respond but it is difficult to understand about felling or emotion people because in messenger cannot show their face. In online learning, teachers can send a text using a chat room with their students for learning but teachers cannot understand their student condition such as feelings or emotions. The problem that if students are bad emotion, they are very difficult to give them learning, especially in messenger's application, for example, chat room in online learning. The purpose of this research solves this problem by building a system that text documents in the chat rooms will process using an Artificial intelligence algorithm to know about the emotions of people in the chat room. The system uses 3 algorithms: naïve Bayes, fuzzy logic, and NPC. The system was built in existing online learning and embedded in the chat rooms in online learning. The result of testing of the system was 70% accuracy to determine the emotion-based in a text document in the chat room of online learning.
In the area of medicine, x-rays are very useful to check if the patient suffers from brain death. Their diagnosis is made using free text. This type of record difficult the process of making qualitative analysis in order to automatically detect possible brain problems. This project aims to make qualitatively and quantitatively analysis of Brain Computed Tomography (CT) diagnosis using text analysis tools as is Natural Language Processing and Text Mining. In this work a set of related words that can means patterns in CT reports was detected. The dataset was provided by the Centro Hospitalar do Porto-Hospital de Santo António and it contains information about patient deaths and CT done to the brain. With the analysis made, a new research and analysis perspectives of structured and unstructured texts in this field was opened.
It is well known that acupuncture treatment has an effect on patients with ischemie cerebrovascular disease. This study was aimed to summarize the basic laws of acupoints of ischemie cerebrovascular disease in acupuncture treatment using text mining techniques. First, we proposed the text-mining-based method to collect related literatures about acupuncture treatment of ischemie cerebrovascular disease from Chinese Biomedicai Literature Database, and then the ACCESS database was constructed. Second, structured query language was applied to data processing as well as data stratification algorithm was adopted to analyze the basic laws of acupoints of ischemie cerebrovascular disease in acupuncture treatment. Final, 62,903 documents were retrieved and the results showed that the higher frequency of single point in the treatment of ischemie cerebrovascular disease were sorted in descending order as the following: Renzhong(570), Burong(205), Fufen(186), Youmen(84), Shuifen(58), Baihui(57), Zusanli(55). Moreover, the higher frequency of paired points in the treatment of ischemie cerebrovascular disease were sorted in descending order as the following: Baihui with Dazhui(21), Neiguan with Zusanli(21), Quchi with Zusanli(21), Baihui with Qubin(10), Neiguan with Renzhong(9), Neiguan with Sanyinjiao(9), Neiguan with Yinjiao(9). These findings suggest that the method of text mining can be used to analyze basic acupoint laws through statistical frequency of acupuncture treatment of ischemie cerebrovascular disease and to provide a reference for clinical acupoints.
Text mining is the progression of originating high superiority information from text. As the majority information is presently accumulated as text, text mining is alleged to enclose a high profitable potential significance. The preprocessing steps are performed individually based on their need and there is no common framework for preprocessing. This paper focuses on designing preprocessing framework for text mining application by gathering the frequently used preprocessing steps. This framework focuses on three major preprocessing tasks Expansion, Removal and Tokenization (ERT). The ERT takes corpus as input and generate the list of tokens; the tokens has been equipped to perform all learning algorithms. ERT framework helps to perform all the preprocessing steps quickly and correctly.
Depression is regarded as the major cause of disability and a leading cause of suicide. Social media has recently risen to prominence as a leading means of disseminating information online. The majority of people use the internet to express their opinions, ideologies, and personal experiences. Depression has an effect on how people communicate on social media as evidenced by the language they use. The detection of mental illness has become a major concern as a result of the massive increase in mental health awareness. In this paper, we examine Twitter users’ tweets to detect any factors that may reveal depression. For this purpose, we use Text Mining and Natural Language Processing techniques. We employed CNN + LSTM classification technique and obtained an accuracy of 92%. We have also compared our model with Logistic Regression and TF-IDF classifiers.
Mining opinions from online reviews is an essential step in obtaining the overall sentiment of a product. Deep learning procedure is applied over various fields. User ratings are huge for recommender structures since they consolidate various kinds of energetic information that may influence the exactness of the suggestion. In this work, a deep learning model is utilized to process the user remarks and to create a potential user rating for user comments is proposed. To start with, the system uses sentiments to create a feature vector as the input nodes. Further, the framework tools reduce the noise in the dataset to recover the classification of information mining. To finish, Deep Belief Network (DBN) and sentiment analysis reaches data learning for the approvals.
Power distribution equipment is vulnerable to comprehensive impacts such as natural disasters, human activities, tree barriers, and bird damage during operation. However, the volume of power distribution equipment is large while the online monitoring coverage is low. Inspection is one of the main means to obtain its operating status. In order to make full use of patrol text, enhance the operation stability of power distribution equipment, and reduce operation and maintenance costs, a classification method for cable inspection texts based on word embedding and recurrent convolutional neural network is proposed to identify the environmental risks of cables. First, the inspection text was preprocessed by eliminating the punctuation marks and annotations, with the word vector model pre-trained. Then the recurrent convolutional neural network (RCNN) model was built and the text was embedded into feature vector with the pre-trained word vector model, by which the RCNN model was trained. Finally, the classification model were evaluated by F1 value and accuracy rate. The example analysis shows that the F1 value of circuit patrol text classification and recognition evaluation index is as high as 0.8771, which indicates that the word embedding and circular convolutional neural network model proposed in this paper can effectively extract the local features and global features of patrol text, achieve accurate risk level classification, and provide algorithm support for the distribution cable Big data analysis system.
The development of human society is inseparable from the processing of information and data. It is precisely the development of information technology and computing science that enables humans to have the ability to extract value and laws from data. Especially with the development of deep learning, the depth and the breadth of data have gradually increased, making the ability to mine potential information from data more and more powerful. This paper discusses the application of data mining algorithms based on deep neural networks. It builds a TextCNN network, especially for text classification tasks, so that the content can be vectorized in natural language processing so that the entire sentence can be input into the CNN network as a two-dimensional vector for convolution. In this way, the semantics of the input data can be further analyzed and extracted. It is hoped that the research in this paper will provide some help and enlightenment for applying deep neural networks in data mining algorithms.
To explore the regularity of clinical medication of Gastro-esophageal reflux disease (GERD) through text mining approach. Literatures on GERD in SinoMed(Chinese biomedical literature service system) were collected. Results: BANXIA, CHAIHU and HUANGLIAN were commonly used Chinese herbs in sequence from high to low, the network of Chinese herbs, symptoms and patterns could be established. Conclusion: The commonly used Chinese Herbs are coincided with the treatment rules of regulating the function of liver and stomach as well as adjusting cold and heat and the networks are corresponded with each other in treating GERD and could be verified through text mining approach.
There are many languages in the world, each one of them has its own characteristic. This makes languages a very interesting research, especially in text mining field. Written language need to be preprocessed first to get its normalized form. Abbreviated words are one of the problems in text mining. System cannot process the text optimally due to the different meaning the abbreviated words may have. This research objective is to develop Indonesian abbreviated words dataset, which can then be used to normalize any abbreviated words in Indonesian. Crowdsourcing was selected as a method to develop the dataset, because only human is able to translate abbreviated words into normal form of the words. From 1170 sentences that were tested, 1063 sentences were answered correctly while 107 sentences were answered incorrectly by the respondents. This research accuracy is about 90.85%. However, there is still a problem which occurred when the abbreviated word has more than one meaning/normal form, thus unique keywords are needed to determine which meaning is most accurate.
The dispatch operation ticket system contains much real-time information about the power system. To make this information available for better dispatch control for power systems, a new text mining method of dispatching operation ticket system is constructed in this paper, which is based on a graph partition spectral clustering algorithm. According to the content of order tickets, they are divided into different types, and the same type of order tickets are stored in a group. The relationship between each group is modeled and analyzed, and the bipartite graph is introduced to achieve a good clustering effect among heterogeneous objects. The Laplacian matrix in the spectral clustering algorithm is used for the Graph Segmentation, and the matrix feature vectors and data point set are obtained, completed by dimension feature data set. After that, the clustering result is obtained, which represents the result of dispatching operation ticket checking text mining used for the real-time power system dispatching. The simulation results show that the proposed method can accurately mine the operation content of the power system overhaul and repair management process, which is efficient and convenient to meet the regulation needs of power system operation.
A large number of idle defect and fault texts are accumulated in the process of long-term operation, maintenance and repair of power secondary equipment. In order to realize the effective utilization and deep mining of defect texts, a method of defect text classification of secondary equipment based on BiLSTM-Attention is proposed. Firstly, the text is cleaned according to the natural language characteristics of the historical defect texts of the secondary equipment. Secondly, the text content is vectorized through Word2Vec model, and the semantic information of the text is extracted in depth through BiLSTM-Attention to realize the accurate classification of the defect degree of the equipment. Finally, an example is built through the historical defect texts of a power company. The result shows that the proposed model can accurately classify the defect text of power secondary equipment and help operation and maintenance personnel to judge the defect degree quickly and accurately.
This study aimed to investigate the quality performance of beauty e-commerce services in Indonesia to find some appropriate improvements according to customer perceptions. Identification is conducted based on electronic service quality dimensions using text mining approaches, namely multiclass classification, sentiment analysis, and text network analysis methods. We gathered data from Twitter as a form of user generated content, because it is essential for companies to perceive what customers feel and need. This study utilized Sociolla, the leading beauty e-commerce in Indonesia, as an object. The results indicate several factors considered necessary by customers, i.e., efficiency, system availability, fulfillment, and responsiveness obtained from classifying customer opinions using the Naive Bayes Classifier model with an accuracy rate of 89%. The study also discovered many complaints regarding incompatible goods stock information on the website, app crash problems, difficulties in tracking orders, and difficulties in selecting payment methods. Moreover, several services are to be maintained, including faster delivery, more discounts, promos, and giveaway events, as well as quick response from customer service in handling complaints.
The purpose of this article is to present the approach to Text Mining using special software. Authors consider possibility of using of k-means with different similarity measures and cluster validity evaluation. The authors have suggested k-means method with Bregman divergence for text documents clustering. Results show it is efficient in comparison of similar methods.
One problem of exploiting information captured by an OO system model such as class models is mapping user queries on the model. This is particularly due to the informal character of the user queries. This paper presents an approach based on text mining for identifying and interpreting user queries. This would enable a matching between the interpreted user query and the corresponding entities specified by the system model. Therefore, navigation expressions in OCL can be derived from the system model that have a formal character and are able to answer the user questions against the class model.
The existence of technological sophistication and the use of the internet can be done with various things such as using social media, e-commerce, and others. Tokopedia is a marketplace where e-commerce activities can be carried out. In its development, Tokopedia experienced several adverse events such as data leaks, a decrease in monthly website visits in 2020, and the performance of the Tokopedia brand, which was still below other marketplaces. Based on this incident, Tokopedia experienced a decline which could make the view of Tokopedia less favorable, such as trust. This study aimed to see how Twitter users trust Tokopedia based on trust factors in e-commerce: perceived privacy, perceived security, perception of website quality, perceived risk, and internet experience. Text mining methods were used in this study, i.e., multiclass classification and sentiment analysis. The study results show that trust in Tokopedia is still lacking. There are results from the ecommerce factor where perceived privacy and risk have the most negative percentages among other factors. These results can be of particular concern in knowing the recommendations for marketing tactics that Tokopedia should carry out. In addition, this study is fruitful in seeing how the application of big data can be helpful in management regarding trust using available technology, such as the application of text mining to analyze reviews on a topic.
With geological big data becoming a focus of geoscience research, the vast amount of textual geoscience data provides both opportunities and challenges for data analysis and data mining. In fact, it does not seem possible to meet the demands of the big data age through the traditional manual reading for information extraction and gaining knowledge. In this paper, a workflow is proposed to extract prospecting information by text mining based on convolutional neural networks (CNNs). The aim is to classify the text data and extract the prospecting information automatically. The procedure involves three parts: 1) text data acquisition; 2) text classification based on CNN; and 3) statistics and visualization. First, the large amount of available text data was acquired based on geoscience big data acquisition methodologies. After text preprocessing, the CNN was used to classify the geoscience text data into four categories (geology, geophysics, geochemistry, and remote sensing), with each category consisting of three levels of text scales (word, sentence, and paragraph). Second, the word frequency statistics, co-occurrence matrix statistics, and term frequency-inverse document frequency (TF-IDF) statistics were for words, sentences, and paragraphs, respectively, which aimed to obtain the key nodes and links derived from the content-words. Finally, the deep semantic information of the big data mining of relevant geoscience texts was visualized by word clouds, knowledge graphs (e.g., the chord and bigram graphs), and TF-IDF statistical graphs. The Lala copper deposit in Sichuan province was taken as a test case, for which the prospecting information was extracted successfully by the developed text mining methodologies. This paper provides a strong basis for research into establishing mineral deposits prospecting models based on logical knowledge trees. In addition, it shows the great potential of this method for intelligent information extraction within geoscience big data. (Show More)
This research work intends to classify the texts associated with bullying contents in social media, especially twitter by using the text mining process. A Multi-Modal Detection and classification of Cyberbullying media is developed in the study. This model integrates textual, and metadata to identify the cyberbullying media in case of social networks. The process involves two phases training and test the cyberbullying data, where natural language processing (NLP) is applied as the pre-processing tool and then particle swarm optimisation is used as feature selection process. Finally, the study applies decision tree classifier to classify the instances associated with cyberbullying and after classification, these features are combined with text instances to detect the performance of the proposed model. The simulation is conducted to test the detection rate of the classifier than the existing methods. The results show that the proposed method achieves higher rate of classification and detection accuracy than the existing methods.
As the global energy demand is increasing, the share of renewable energy and specifically wind energy in the supply is growing. While vast literature exists on the design and operation of wind turbines, there exists a gap in the literature with regards to the investigation and analysis of wind turbine accidents. This paper describes the application of text mining and machine learning techniques for discovering actionable insights and knowledge from news articles on wind turbine accidents. The applied analysis methods are text processing, clustering, and multidimensional scaling (MDS). These methods have been combined under a single analysis framework, and new insights have been discovered for the domain. The results of our research can be used by wind turbine manufacturers, engineering companies, insurance companies, and government institutions to address problem areas and enhance systems and processes throughout the wind energy value chain.
Companies who sell their products through the e-commerce model frequently request that their clients review, survey or give a feedback for the products that they have bought. Companies that extensively use the e-commerce model depend on the large number of customer reviews that are collected for each product through the Customer Relationship Management (CRM) Application. CRM Managers find it very difficult to go through each and every review feedback, and to track and manage them. It becomes difficult to comprehend what the customer has actually liked and how they have experienced the product. The current CRM Feedback/Review Systems either contain closed end questions which can only generate numerical data or open end questions which generate responses which is an unstructured textual data. These textual data can be grouped together into Neuro Linguistic Programming predicates to generate insights into customer's experience. We can capture the customer's perceptual experience in most appropriate manner; which can be analyzed and used to retain customer's loyalty towards the Company.
Educational data mining is an emerging trend, concerned with developing techniques for exploring, and analyzing the huge data that come from the educational context. EDM is poised to leverage an enormous amount of research from data mining community and apply that research to educational problems in learning, cognition and assessment. In recent years, Educational data mining has proven to be more successful at many of these educational statistics problems due to enormous computing power and data mining algorithms. This paper surveys the history and applications of data mining techniques in the educational field. The objective is to introduce data mining to traditional educational system, web-based educational system, intelligent tutoring system, and e-learning. This paper describes how to apply the main data mining techniques such as prediction, classification, relationship mining, clustering, and social area networking to educational data.
With increasing risks faced by financial firms and markets, text mining is playing more and more important role. However, because of the complexity between a text and the consequences caused by the text, it is hard for traditional text mining techniques to meet the needs. In order to solve this problem, we focus on the employment of intelligent agents to enhance the capability of text analysis. In our system, various classes of intelligent agents are proposed for financial risk early warning. The text mining agent gives influence measurement, the relationship agent generates relationship measurement between the text and the target, and the reliability agent measures whether the text is trustable or not. Collaboration within such agents is able to produce more effective and accurate early warning messages.
This paper studies a system composed of metallic and non-metallic mineral extracting companies, logistic suppliers, public agencies, and a trade association providing services. The strategic texts of the system are analyzed with the multicriteria of the macro-environment of the mining industry economic, technological, social, and environmental, with Latent Dirichlet Allocation (LDA) modeling and with Kmeans, to determine the topics and strategic words that could characterize it with a digital business ecosystem. The results show the presence/absence of critical factors that increase the level of competitiveness and sustainability.
High costs in health care and everlasting need for quality improvement in care delivery is increasingly becoming the motivating factor for novel predictive studies in health care informatics. Surgical services impact both the operating theatre costs and revenues and play critical role in care quality. Efficiency of such units relies extremely on effective operational planning and inventory management. A key ingredient to such planning activities is the structured and unstructured data available prior to the surgery day from the electronic health records and other information systems. Unstructured data, such as textual features of procedure description and notes, provide additional information while structured data alone is not sufficient. To effectively utilize textual information using text mining, textual features should be easily identifiable, i.e., without typographical errors and ad hoc abbreviations. While there exists numerous spelling correction and abbreviation identification tools, they are not suitable for the surgical medical text as they require a dictionary and cannot accommodate ad hoc words such as abbreviations. This study proposes a novel preprocessing framework for surgical text data to detect misspellings and abbreviations prior to the application of any text mining and predictive modeling. The proposed approach helps extract the most salient text features from the unstructured principal procedure and additional notes by effectively reducing the raw feature set dimension. The transformed (text) feature set thus improves subsequent prediction tasks in surgery units. We test and validate the proposed approach using datasets from multiple hospitals' surgical departments and benchmark feature sets.
In many text mining applications, information from Document is present in the form of Text along with Side Information or Metadata. Examples of this side information include links to other web pages, title of the document, author name or date of Publication which are present in the text document. Such metadata may possess a lot of information for the clustering purposes. But this Side information may be sometimes noisy. Using such Side Information for producing clusters without filtering it, can result to bad quality of Clusters. So we use an efficient Feature Selection method to perform the mining process to select that Side Information which is useful for Clustering so as to maximize the advantages from using it. The proposed technique makes use of the process of Two-mode clustering which is a data mining technique that allows producing groups by Clustering both Text and Side Information.
There has been a lot of research on the application of data mining or knowledge discovery in financial market predictions. In those researches, various data mining techniques are applied to predict stock price trends, index values, currency exchange rates, volatilities etc. However, most of the existing studies in this area are based on numeric and structured data for example, historical price quotes, financial statements, interest and tax rates or with other quantifiable figures. Studies on mining textual and unstructured information like news, recommendation and comments from experts, postings from online forums and chat rooms, personal blogs and so on seems to be only an emerging area of study. The available textual data on the internet is usually of huge quantity and is much more informative than purely numeric information. This information helps people to identify the market behavior from a piece of financial news but also understand the reason why the market behaves this way. The purpose of this thesis is to explore this emerging research area and to give comprehensive experiments and comparisons on applications of various textual data mining techniques on financial market predictions.
The epidemiological outbreak of a novel coronavirus (2019-nCoV or Covid-19) in China, and its rapid spread, gave rise to the first pandemic in the digital age. Derived from this fact that has surprised humanity, many countries started with different strategies in order to stop the infection. In this context, one of the greatest challenges for the scientific community is monitoring (real time) the global population to get immediate feedback of what is happening with the people during this public health contingency. An alternative interesting and affordable for the materialization of the aforementioned is the social media. In a social network, the people can act as sensors that provide information not only of personal data, including health, but also data derived from their behavior. This paper aims to analyze the publications of people in Mexico using a text mining approach. Specifically, Mexico City is presented as a case study to help understand the impact on society of the spread of Covid-19.
In this paper, we propose a novel ontology-based approach for text mining of EMR information retrieval. The advantage of this approach is that it is capable of handling numerous variations in nature text which essentially refer to the same identity, as well as inferring implicit information from the plain text, which are both important in data mining of medical records. We applied the approach to text mining of EMR documents for stroke patients in a Chinese medical hospital. A benchmark study on an independent test set shows that the proposed pipeline can accurately extract the vast majority of useful information from the EMR documents, including the implicit ones through ontology inference. We also carry out a primary statistical analysis on a sample EMR set to illustrate the utilization of the approach on medical studies.
There is growing interest in mining software repository data to understand, and predict, various aspects of team processes. In particular, text mining and natural-language processing (NLP) techniques have supported such efforts. Visualization may also supplement text mining to reveal unique multi-dimensional insights into software teams' behavioral processes. We demonstrate the utility of combining these approaches in this study. Future application of these methods to the study of teams' behavioral processes offers promise for both research and practice.
Since the beginning of mankind, there has been a need to display knowledge in such a way that it could be easily understood. In today's highly technological world we face an overwhelming amount of information from a variety of sources, namely: business applications such as customer comments and communications, trade publications, internal research reports and competitor web sites. Most of this knowledge is stored in textual format so in order to conceptually visualize this information, we need to extract the knowledge it contains and then try to comprehend it. However, one of the characteristics of textual information is its unstructured format which is not readily assimilated and understood. This paper presents a summary of current techniques used in information visualization. We present some of the main visual text analytics systems, highlighting their techniques, in addition to providing a brief overview of visual text analytics, semantic analysis of text and ontology that could be used to facilitate the process of cognitive visualization.
As the fifth most common malignancy, liver cancer has a high incidence. Chinese herbal medicine (CHM) has been an integral part of traditional Chinese medicine (TCM) for thousands of years. Many herbal formulae have been used in treating liver cancer and proved to be safe and effective in some degree. Yet the principles of treating liver cancer with CHMs are hard to manage due to the complexity of TCM theory and flexibility of TCM clinical practices. In this study, a practical text mining method was deployed based on a comprehensive collection of literatures in order to explore the treatment principles. Networks of TCM patterns and CHMs which were most frequently used in liver cancer treatment were built-up and analyzed; three major treatment principles were explored from 53,499 records of literature: (1) benefiting qi or replenishing blood; (2) promoting blood circulation for removing blood stasis; (3) clearing heat and promoting diuresis. These provided an accessible way for understanding the treatment principles for liver cancer with CHMs.
In recent years, many successful machine learning applications have been developed. Classification & Clustering is one such. This application is cross-disciplinary, now that it is based on data mining algorithms on the technical side and on graphemes and morphophonemic on the linguistic side. It will thus map the correspondence between grapheme 〈y〉 and related phonemes via morphemes in a given context. The grapheme 〈y〉 is often realized as an approximant /j/ (a consonant phoneme), as in 〈yacht〉 or as vowel / i / as in 〈racy〉, or a diphthong / ai / as in 〈sky〉, etc. The objective, that is to say, of this text analysis is to map the various articulators//phonemic realizations of the grapheme 〈y〉. This experiment will thus help the study the occurrence of 〈y〉 in different positions in words, word initially, finally or elsewhere, as in these examples. The training data (or corpus) chosen for this experimentation is a set of English literary texts [Harry Potter part 1, part 2, The complete works of William Shakespeare By William Shakespeare, The Adventures of Sherlock Holmes By Arthur Conan Doyle, (A Christmas carol) By Charles Dickens's. As for the tools, the alphabet recognizer is used for retrieving information and categorizing the data as noun, adjective, etc. Then, GATE Developer, the Text engineering tool is used to analyze the dataset and to derive the output with statistical data of global distribution 〈y〉 as in all the input documents. This project is particularly relevant because it offers, as will be demonstrated, a solution to problems due to lack of one-to-one correspondence between spelling and pronunciation in English, as for instance, in the context of language pedagogy. Glossary of technical terms (of both linguistics & computing) is appended, now that the paper is inter-disciplinary.
The paper proposes an algorithm of text data representation for time-series econometric modeling. We link two areas in empirical econometrics, text mining and time-series modeling, and provide a multi-step methodology for processing qualitative text data for quantitative time-series models. We also present an empirical example of text data processing of research article meta-data in economics and show its applicability to sentiment analysis using annual data from 1933-2015
In the era of technology, the amount of textual data has dramatically grown and increased. It is also getting to be more complex in its nature every day. The ability to manage, analyze, summarize, and understand this data remains a challenging task that requires new techniques to deal with automatically organizing, searching, indexing, and browsing large collections of documents. Text classification is one of text mining areas, which is the process of classifying the text into predefined classes or topics. We developed a tool for Arabic text classification using parallel programming framework. The tool is called Parallel Arabic Text Classifier (PATC). It analyzes a labeled corpus of Arabic text that is input by the user and subsequently builds a text classifier. PATC consists of three major stages; (1) Preprocessing: PATC will normalize and stem the Arabic corpus before using it to train the classifier, (2) Training or Building the Classifier: The classifier will be trained with a user-uploaded, annotated Arabic corpus, and (3) Testing or Classifying: this stage will predict the class of a new document based on the trained classifier. This classifier is built using an approach that associates each label with frequent words using MapReduce distributed programming model. The classifier was evaluated using an Arabic corpus. The accuracy of the classification was around 80% using single-label measures, while it was in the high 90s% using multi-label measures.
It is a long histroy that use of acupuncture treatment coronary heart disease (CHD) which belongs to Xiong Bi in Traditional Chinese Medicine (TCM). In this paper, application of acupuncture on CHD treatment was analyzed by text mining. The study was focused on the fields of meridian, acupoints, acupuncture method and constructed the associated network of them. By analyzing the network, we found that Pericardium Meridian of Hand-Jueyin was the frequently used meridian combined with other meridians and Neiguan was the frequently used acupoint combined with other acupoints for acupuncture treatment on CHD. Electroacupuncture was the common used method and often applied in the meridian of Hand-Jueyin and Hand-Shaoyin. The combinations of meridians, acupoints and methods were demonstrated in network. Results could be helpful for both clinical practice and medical research.
Text mining defines generally the process of extracting interesting features (non-trivial) and knowledge from unstructured text documents. Text mining is an interdisciplinary field which depends on information retrieval, data mining, machine learning, parameter statistics and computational linguistics. Standard text mining and retrieval information techniques of text document usually rely on similar categories. An alternative method of retrieving information is clustering documents to preprocess text. The preprocessing steps have a huge effect on the success to extract knowledge. This study implements TF-IDF and singular value decomposition (SVD) dimensionality reduction techniques. The proposed system presents an effective preprocessing and dimensionality reduction techniques which help the document clustering by using k-means algorithm. Finally, the experimental results show that the proposed method enhances the performance of English text document clustering. Simulation results on BBC news and BBC sport datasets show the superiority of the proposed algorithm.
This study aimed to analyze sentiment opinions to find out the opinions of users on an e-commerce Web. The method used was through analyzing text reviews obtained from customers on an e-commerce website. The algorithm used was k-medoid clustering.
Future-oriented technology analysis (FTA) is a term derived from a collective description given to the range of technology-oriented forecasting methods and practices by a group of futures researchers and practitioners
The amount of biomedical literature is vast and growing quickly, and accurate text mining techniques could help researchers to efficiently extract useful information from the literature. However, existing named entity recognition models used by text mining tools such as tmTool and ezTag are not effective enough, and cannot accurately discover new entities. Also, the traditional text mining tools do not consider overlapping entities, which are frequently observed in multi-type named entity recognition results. We propose a neural biomedical named entity recognition and multi-type normalization tool called BERN. The BERN uses high-performance BioBERT named entity recognition models which recognize known entities and discover new entities. Also, probability-based decision rules are developed to identify the types of overlapping entities. Furthermore, various named entity normalization models are integrated into BERN for assigning a distinct identifier to each recognized entity. The BERN provides a Web service for tagging entities in PubMed articles or raw text. Researchers can use the BERN Web service for their text mining tasks, such as new named entity discovery, information retrieval, question answering, and relation extraction. The application programming interfaces and demonstrations of BERN are publicly available at https://bern.korea.ac.kr.
One of the common learning activities used in educational levels and disciplines is essay writing. The problems of the essay writing activities are time-consuming, concerns in producing immediate result and/or feedback from teachers to students, and the teachers tend to be subjective in grading the essay activities. The study aims to apply the preliminary approach forautomatically generating the domain concept ontology in essays using OntoGen and applied natural language processing algorithms using NLTK (Natural Language Tool Kit) that enhance the teachers essay grading.
In the text preprocessing of text mining, a stop-word list is constructed to filter the segment results of the text documents so that the dimensionality of the text feature space can be cut down primarily. This paper summarized the definition, extraction principles and method of stop-word, and constructed a customizing Chinese-English stop-word list with the classical stop-word list based on the difference of text documents' domain. Three different filter algorithms were designed and implemented in the process of the stop-word filter and their efficiency was compared emphatically. The experiment indicated that the hash-filter method was the fastest.
The growing incidents of counterfeiting and associated economic and health consequences necessitate the development of active surveillance systems capable of producing timely and reliable information for all stake holders in the anti-counterfeiting fight. User generated content from social media platforms can provide early clues about product allergies, adverse events and product counterfeiting. This paper reports a work in progress with contributions including: the development of a framework for gathering and analyzing the views and experiences of users of drug and cosmetic products using machine learning, text mining and sentiment analysis; the application of the proposed framework on Facebook comments and data from Twitter for brand analysis, and the description of how to develop a product safety lexicon and training data for modeling a machine learning classifier for drug and cosmetic product sentiment prediction. The initial brand and product comparison results signify the usefulness of text mining and sentiment analysis on social media data while the use of machine learning classifier for predicting the sentiment orientation provides a useful tool for users, product manufacturers, regulatory and enforcement agencies to monitor brand or product sentiment trends in order to act in the event of sudden or significant rise in negative sentiment.
Electronic Medical Record (EMR) is an important element of information technology in healthcare sector. EMR is an electronic record containing health-related information on patients that can be created and managed by authorized physician and staff in a healthcare service organization. EMR is a framework for determining diagnosis and treatment. EMR has free text and unstructured format which makes it more difficult to extract the hidden information as a decision support system. This study performs classification from Indonesian EMR for clinical decision support system (CDSS) in classifying patient diagnosis using Term Frequency-Inverse Document Frequency (TF-IDF) for feature extraction and Support Vector Machine (SVM) for classifier method. SVM is a powerful algorithm in high-dimensional data such as in textual data processing. The focus diagnoses classified in this paper are tuberculosis, cancer, diabetes mellitus, hypertension, and chronic kidney which have high prevalence rates in Indonesia. The model is built by considering the kernel function and the use of stopword removal or without stopword removal. The result showed that TF - IDF and SVM method could be used effectively to predict diagnosis with stop word removal. Classification performance increased with stopword removal on all SVM kernels with accuracy in linear kernel 89.91 %, polynomial kernel 90.58%, RBF kernel 90.75%, and sigmoid kernel 91.03%.
Along with the increasing activity on social media, hate speech is getting out of control. Hate speech detection can be done by utilizing text mining technology. There have been many hate speech detection studies conducted. To identify and analyze research trends, data sources, methods and features used in hate speech detection, this systematic literature review was created. Until early 2020, the topics of hate speech were found, including hate speech against minorities, religion, women, the general election agenda, and politics. Sources of data that are widely used to be used as datasets come from twitter. Hate speech is not only classified into HS (hate speech) and Non-HS (non-hate speech) but can be further classified into racism, sexism, offensive, abusive, threats of violence and others. Of the 38 studies that meet inclusion and exclusion, there are 26 algorithms and 28 features that have been used to detect hate speech. However, these methods and features do not necessarily guarantee a good hate detection performance. Hate speech classification performance is also influenced by the dataset, the features chosen, the number of classes and mutually exclusive classes.
Preprocessing is an important step in information retrieval and text mining. In this study, we examined the impact of stemming on clustering Turkish texts. We used two datasets compiled from web sites of Turkish news agencies, and performed extensive experiments. We empirically show that there is no significant evidence that stemming always improves the quality of clustering for texts in Turkish. However, when stemming is used, dimensionality of the document-term matrix dramatically decreases without inversely affecting the clustering performance. As a result, it is highly recommended to apply stemming for clustering Turkish texts.
This paper explores opinion mining using supervised learning algorithms to find the polarity of the student feedback based on pre-defined features of teaching and learning. The study conducted involves the application of a combination of machine learning and natural language processing techniques on student feedback data gathered from module evaluation survey results of Middle East College, Oman. In addition to providing a step by step explanation of the process of implementation of opinion mining from student comments using the open source data analytics tool Rapid Miner, this paper also presents a comparative performance study of the algorithms like SVM, Naïve Bayes, K Nearest Neighbor and Neural Network classifier. The data set extracted from the survey is subjected to data preprocessing which is then used to train the algorithms for binomial classification. The trained models are also capable of predicting the polarity of the student comments based on extracted features like examination, teaching etc. The results are compared to find the better performance with respect to various evaluation criteria for the different algorithms.
Since Harvard University emphasized the important characters of college students, the service learning courses or activities is driving in educational system especially in higher education in order to building up the students global caring characters. This paper used the text mining technology to analyze the diary of the pre-service teachers attending the service learning activities. The purpose of this paper is to appear and analyze the service learning activities in order to finding out the learning outcome of students as well as providing a reflection of the service learning activities. The significance of this study is to provide a reflection and strategy form the practical data driven and using qualitative text mining technology. In other words, this study combined the Ground theory and text mining technology to find a storyline for reflection and to suggest some rare and important factors for improving the service learning activities.
Automatic extraction of relations between gene mutations and cancer entities occurring in the cancer literature using text mining can rapidly provide vital information to support precision cancer medicine. However, mutation-cancer relation extraction is more challenging than general relation extraction from free text, since it is often not possible without cancer-specific background knowledge and thus the model replies on a deeper understanding of complex surrounding tokens. We propose a deep learning model that jointly extracts mutations and their associated cancers. Background knowledge comes from two different knowledge bases which store different types of information about mutations. Given the different ways in which knowledge is stored in these two resources, we propose two separate methods for embedding knowledge, namely sentence-based knowledge integration and attribute-aware knowledge integration. The evaluation demonstrated that our model outperforms a number of baseline models and gains 96.00%, 92.57% and 94.57% F1 scores on three public datasets, EMU BCa, EMU PCa, and BRONCO, thus illustrating the effectiveness of our knowledge integration approach. The auxiliary experiments show that our models can utilize more informative text from the KBs and link the mutations to their corresponding cancer disease although the input text provides insufficient context.
In recent years, several text mining models have been developed for supporting the process of early and accurate cancer diagnosis. This study provides a comparative analysis of existing text mining algorithms for cancer diagnostics. This study will support researchers and practitioners when choosing the most accurate algorithm when diagnosing specific types of cancer.
Occupational accidents are a serious threat to any organization. Occupational accidents in steel industry sector remain a threat as workforce is exposed to different kinds of hazards due to the workplace characteristics. In this study, a unique method is proposed by developing a text mining based prediction model using fault tree analysis (FTA), and Bayesian Network (BN). Free unstructured accident dataset for a period of four years has been used in this study. Text mining approach results in finding the basic events concerning each of primary causes. The basic events, in turn, are utilized in building FT and BN diagram that could predict the occurrence of accidents attributable to different primary causes. The model, so developed, can be considered adequate with 87.5% accuracy. Furthermore, sensitivity analysis is performed for the validation of the model.
The number of users and amount of data transfer are increasing per each minute with the rapid growth of social network platforms on the web while the users have no certain knowledge of each other. Thus, with the overwhelming spread of the internet and such bulk of data, people find it arduous to identify valid comments. Establishing a genuine and more accurate trust becomes harder if classical processing is used especially with the presence of profitable, oriented, devious and narrow-minded comments. Various methods have been employed so far to evaluate reliable users most of which combine trust algorithms, subject classification, and comment mining methods. Researches reveal that the majority of social network users firstly take into account an overall number of public trust standards such as the number of friends, followers, followings, and likes of individuals in order to trust them. However, a malicious user could manipulate this trust by building virtual qualities. Accordingly, this study supplies a dictionary of malicious words and weighs them by combining trust standards and text mining users' tweets. It is intended to identify malicious users and analyze their behavior to proceed a more accurate trust within distributed execution in Spark environment for providing a quicker call. The results of this study show that the suggested method benefits from a high diagnostic accuracy.
Until recently, study program accreditation is still the standard for evaluating the feasibility and quality of the education process worldwide. Normally, in the process of accreditation, one specific study program needs to prepare a report document that represents the whole process of education, especially from the standpoint of quality, and then submit it to the National Board of Accreditation. Self-assessment and evaluation are the keys to creating a proper accreditation document. In the case of Indonesia, the accreditation process is run by the National Board of Higher Education Accreditation (called BAN-PT). The study program must prepare and submit a document that represents the overall process of education by following the national standard and format determined by BAN-PT. In this study, we hypothesized that the text mining technique can be used to evaluate the accreditation document by analyzing the English terms that are used in the document. The analysis is done by scoring the number of English terms in one accreditation document and selecting the proper terms that are used. We hypothesize that the higher the score is obtained by the document the higher the national rank of accreditation was obtained by the document. In this study, 24 accreditation documents with approximately every document consisting of 200 pages within similar study programs were analyzed. A text processing technique was implemented to filter and clean the documents from unimportant words. English phrases were detected from each sentence in the document and then continued by extracting English terms further filtering to get the English terms is then performed. The selected English terms are then scored from each accreditation document. The validation process is done by mapping the scoring result with the Accreditation Rank obtained by the document. The result showed that the higher the score based on the English terms obtained by one document. The higher the Nasional Accreditation ranks... (Show More)
Social media platform like blogs, social networks and content communities have become valuable resource for mining the sentiments. In fact information gained from the social platform like Facebook and twitter has been expressed to be more important to industries, market, news, public opinion organization and many more. Sentiment analysis in a text is a procedure in which the polarities like positive and negative of a specified text is determined. In this project we are using twitter comments as the text input. The proposed method consists of two phases called testing and training phase. For extracting the features we are using the sentiwordnet. The distance based classifier is used to classify the features. Finally analyses of the sentiments are done by polarities classified by the distance classifier.
Defect factors and their relevant rules can be analyzed in depth by processing defect records which are often expressed in the form of text data. However, considering that defect text consists of both structured and unstructured data, it is necessary to excavate structured information from unstructured data. In this paper, a text mining method based on semantic framework technology is introduced to transform unstructured defect description into structured information such as components and defect attributes. Then, a deep analyzing model of a power equipment defect is established, which provides a scheme of defect mining based on historical defect texts. Case studies prove that the proposed deep analysis method has a guiding significance for equipment upgrading, selection and maintenance.
The recent diverge in Data Science research studies towards Geoinformatics is well justified. Geographical dimensions are constantly captured by data collection engines in different shapes and forms. Data mining methods however, expect highly structured and organized datasets. Organizing data is especially challenging for geographical forms. Another critical form of data that is difficult to structure is text. Subsequently, the continuous rise in content through social media and the Internet of Things (IoT) has made Text Mining more relevant than ever before. Geographical data are mostly presented (if longitudes and latitudes data are not present) through text (Country, State, City). This paper is focused on mining through bodies of text that have a direct relevance to a geographical location, and extracting knowledge from that text for a better understanding of a certain chosen political/social/economical topic. In the novel method presented in this manuscript, geographical data is coupled with textual data to enable insights and correlations that are not possible otherwise. Additionally, the method mines for sentiments and assigns quantifiable sentiment scores. In geo-enabled mining of text, issues such as Emojis and stop-words (to, from, the, and) pose a technical challenge; the method presented in this paper addresses that as well. Besides the mentioned descriptive data insights (user's location, tweet test, age, language, gender, and number of tweets), this paper introduces a method that describes geographical patterns within text (using cosine distance measures and hierarchical clustering). Finally, experimental work is presented, results are recorded, evaluated, and used to define conclusions and future directions.
Fault dependency (D)-matrix is a systematic diagnostic model [7] to capture the hierarchical system-level fault diagnostic information consisting of dependencies between observable symptoms and failure modes associated with a system. Constructing a D-matrix from first principles and updating it using the domain knowledge is a labor intensive and time consuming task. Further, in-time augmentation of D-matrix through the discovery of new symptoms and failure modes observed for the first time is a challenging task. Here, we describe an ontology-based text mining method for automatically constructing and updating a D-matrix by mining hundreds of thousands of repair verbatim (typically written in unstructured text) collected during the diagnosis episodes. In our approach, we first construct the fault diagnosis ontology consisting of concepts and relationships commonly observed in the fault diagnosis domain. Next, we employ the text mining algorithms that make use of this ontology to identify the necessary artifacts, such as parts, symptoms, failure modes, and their dependencies from the unstructured repair verbatim text. The proposed method is implemented as a prototype tool and validated by using real-life data collected from the automobile domain.
Customer satisfaction is insufficient. It applies to all service industries including train services. Apart from weather conditions and safety issues, the challenges faced by train services are improving passenger comfort, sense of well-being, and emotional satisfaction. How to understand and satisfy the customer emotional needs is critical. Conventional methods such as survey and interview sometimes bring shortcomings. Hence, this study proposes the integrated approach of text mining, Railqual, Kano model, and Kansei Engineering (KE) in train services. Text mining is inserted in the KE methodology to refine the more representative Kansei words and service attributes experienced. The finding shows that there were 8 final Kansei words, namely, clean, extraordinary, comfortable, spacious, modern, friendly, cool, and cheap. Related to critical train service attributes, there were 3 items i.e., comfortable temperature in train, politeness of staff, and good quality meals served in train. Surely, the continuous scheduled air conditioner maintenance, training “dealing with people” for staff, and food supplier evaluation should be prioritized.
By tremendous extension of Internet, information is the most important forms of tourism Big Data. The powerful way to express tourist thinking, text mining information has great capability to stimulate innovation in tourism professionals. Over the past decades, various text mining methods are proposed and implemented in tourism to enhance value describing techniques, construct tourism proposal models, generate tourism profiles, and establish process to monitor tourist's markets. The success of these technologies has been accelerated by advances in Natural Language Processing (NLP), ML, and DL. Understanding the complexities by these different approaches and tourism information sources, this analysis explores text mining methods which has been implemented to recent tourism big data analysis and tries to provide an up-to-date overview. Various data analyzing designs, content-based NLP methods of topic removal, text categorization, description of emotions, text clustering related to tourists' text mining and requests in tourism profiles, picture description of travel destinations, market requests, etc. The upcoming years will provide development rules for new tourism big data applications.
The purpose of Text Mining is to process unstructured (textual) information, extracting meaningful numeric indices from the text, and, thus, make the information contained in the text accessible to the various data mining (statistical and machine learning) algorithms. Information can be extracted to derive summaries for the words contained in the documents or to compute summaries for the documents based on the words contained in them. Hence, we can analyze words, clusters of words used in documents, etc., or we can analyze documents and determine similarities between them or how they are related to other variables of interest in the data mining project. This topic that might be integrated with massive files of very disparate joke clusters, bound only because someone thought that they were funny or someone had an interesting thing for light-bulb jokes. Therefore, there are probably tons of doubles and triples and what you have in this collection, and there's just not enough time in the day to sort them, manually. This paper is going to analyze the different variation and analysis of funny or humorous words using data mining techniques.
This paper mainly uses data mining and statistical analysis to mine network data and survey data, and studies the transformation and development of K9 education and training institutions under the double reduction policy of Xuzhou City. First, we analyzed the user groups of education and training institutions through text mining and other network big data technologies. The results show that: users pay more attention to family education and strengthen quality education; Then, based on the analysis of the questionnaire data, the results show that the research age, parents' wishes, environmental impact, strength of education and training institutions, and expected demand have a significant impact on whether user groups choose education and training institutions. Finally, it proposes that education and training institutions should transform to camp education, arts and sports education, science education and other quality education.
Decision making for education at universities calls for objective analysis using reliable data. In addition to commonly used methodology, it is inspirational to introduce innovative approaches to large educational data analysis. In this study, we apply text mining techniques to analysis of a bibliography database for detecting temporal differences in academic subjects at higher education. Our approach is designed on the assumption that if automatic text categorization is effective in detecting topical distinction in book titles in different subject areas, it should also be effective to detect time-oriented distinction in the same subject area, wherein state-of-the-art books are sufficiently different from classical books. The experimental results in this study suggest that our proposed approach is feasible to detect and visualize transition in book titles of university libraries in Japan.
A large amount of data is generated every day; it represents both a challenge and opportunity to understand statistics from existing and new data (datasets/documents). Urdu is Pakistan's national language and is also the official language of six Indian states. A vast number of books, documents, datasets are present in the Urdu language. To examine their content, we have no proper application/system that helps to understand and analyze their data with complete statistics, graphical representation options, dataset topics, and coverage of the dataset. We proposed a solution for Urdu-language datasets text mining, which includes comprehensive file statistics, better understandings using graphs/charts, topics extractions, comparison with other datasets, and findings related words with any specific word. We have manually compiled and cleansed 9 million Urdu words dataset. A new list of stop words for Urdu, consisting of 271 words that mainly occur in Urdu, is constructed. A unique Urdu words list composed of 0.2 million is made. System algorithms are optimized for text analysis, and their results are auspicious. This is the pioneering text mining system developed entirely for Urdu language text that covers various aspects of data analysis for the Urdu language.
The study aims to explore the UX issues of electric vehicle (EV) sound by mining qualitive think aloud text data. Electric vehicle, which is more eco-friendly than traditional vehicle, becomes a popular development trend in the automotive industry. The user’s preference for sound can have a great influence on EV purchases. Think aloud is widely-used method to collect users’ thoughts and needs. 40 participants join in the experiment by speaking their own opinions on EV sound while driving. Then, text mining is conducted to explore UX issues from qualitative think aloud data. The UX labels are generated according to word frequency, and ten UX labels are divided into five UX aspects. Sentiment analysis is also taken and the associated words with UX labels are generated. Finally, insights based on the five UX aspects (speed, mode, vehicle component, environment, and sound type) on EV sound have been drawn. The outcomes can suggest implications for EV sound designing.
In this paper, we introduced a searching platform called BCISearch, a web-portal for the collection of molecular information linked to breast cancer using text mining technology including two types of information: static and dynamic information, involving with six categories entities and their relationships: protein, DNA, RNA, Cell-type, Cell-line, Virus. BCISearch could search 248997 Proteins, 71358 DNA, 7724 RNA, 58891 Cell-line, 871 Virus, 31698 Cell-type. The BCISearch Experimental approach is promising for develop biomedical text mining technology. The BCISearch would assist researcher to understand breast cancer etiology in genetic factors. The searching platform is available at http://210.28.186.168:8080/BCISearch.
In recent years, with the increasing popularity of the Internet, the number of Internet users has reached 854 million in China, and social platforms have diversified and developed rapidly. Internet users can express their opinions on social hot spots through various ways. This paper takes word2vec model and K-means algorithm as the core, crawls three kinds of texts from two platforms: Bilibili.com and Zhihu.com, and collects 28816 texts with more than 1.35 million words. Through cleaning and noise reduction, Jieba package carries out word segmentation, word frequency calculation for preliminary topic analysis. Furthermore, word2vec model is constructed, and K-means algorithm is combined with human-computer cooperation to achieve more accurate text clustering. Results the user expression differences of different platforms were compared, which can be used for public opinion evolution analysis. Accurate user feedback can promote the output of high-quality content and increase the user stickiness of the platform. The improved k-means algorithm also improves the credibility of text clustering.
Reviewer assignment to papers is one of the most significant stages of the scientific publishing process. It determines whether high-quality scientific papers are accepted for publication, while low-quality papers are rejected or sent to be improved to the required level. This in turn affects the status of academic journals and the level of scientific papers in general. A large number of researchers deals with issues of improving and speeding up of the reviewer assignment procedure, frequently using intellectual methods. However, insufficient attention is given to comprehensive reviewer assessment in their assignment to papers. A method of reviewer assignment decision support in an academic journal based on a joint use of multicriteria assessment and text mining is proposed in the paper. Calculation of an integral indicator with the use of additive folding of weighted reviewer’s indicators is at the core of the method. Text mining of manuscripts and reviewer’s papers is utilized to determine value of one of significant indicators. The proposed method allows to assess reviewers not only by authority and expertise, but also allows to take into account their work in the role of a reviewer, deciding how good they are in this role, with the use of previously collected statistical information refers to the carried out reviewing. The method has been successfully tested on data of peer-reviewed academic journal "Modeling, Optimization and Information Technology".
Classification and sentiment analysis and research is one of the most important issues in recent times, especially in the realm of language is Persian. Identify the user sentiments about an issue through a process of expressing a sense of individual and output data analysis work is in the context of Hijab of natural language processing and computational linguistics, artificial intelligence is a subsidiary. The need to categorize and production data sources in our field and that's the dress code, according to the news raised in this context and user reaction to the news texts, to provide a data set with an algorithm that on it has been implemented. The algorithm proposed in this study using text mining techniques and algorithms applied weight to words and try to identify the type of polarization of the comments gathered from official sites built in 1000. Analysis on the words in the comments feature has been conducted. It focused on the importance of the word in a sentence.
This paper aims to design a system model that analyzes the unstructured data inside the posts about electronic products on social networking websites. For the purposes of this study, posts on social networking websites have been mined and the keywords are extracted from such posts. The extracted keywords and the ontologies of electronic products and emotions form the base for the text-mining model which is used to understand online consumer behavior in the market.
The time spent by users are almost two or more hours looking for papers that generates the possibility to make a search engine to optimize and precision in the results. This works purposes a better classification of research papers, the architecture works with a database of knowledge related with the topics of programming, databases and operating systems. That's the initial work of a classification using text mining techniques to search into the documents with natural language contained and get the best words of their content to get a database knowledge, that's the first step to get the desired knowledge also the proposed work use the same engine to make searches classifying the information introduced by the final user and searching in the correct cluster.
Governmental administrations frequently release various types of policies to enforce specific rules, laws, or economic stimulus plans. For a specific policy, there are many potential target individuals, companies and organizations. However, it is not always timely for a company to get to know the policies that are suitable to it. It is necessary to develop efficient policy recommendation methods to help companies catch up with useful policies right the first time. In this paper, we propose a policy recommendation method based on text mining. This method consists of three phases: 1) Policy structure division; 2) Attribute extraction; 3) Matching and recommendation. Since most of policy texts contain too many contents that might be suitable for multiple different types of companies, policy text is firstly divided into text fragments, and each text fragment is then divided into multiple elementary discourse units (EDUs) which are used as the basic recommendation unit with optimal granularity. Secondly, by combining a Named Entity Recognition (NER) based extractor with a rule-based extractor and a grammar-based extractor, we extract attribute entities and logical relations between these entities from each EDU. Thirdly, on the basis of the extracted attribute entities and logical relations, we calculate the matching score between each fragmented policy text and each company, and these scores are taken as the sorted criteria of policy recommendation. The policy recommendation results cover policy text fragments, related governmental administrations, attribute entities and logical relations, links to the entire policy text, and so on. On the basis of a data set collected from real world, the effectiveness of the proposed method is validated by the experiments of two different scenarios: 1) there is a list of policy texts, and it is needed to recommended suitable policies to a specific company; 2) there is a list of companies, and it is needed to help them to find suitable ... (Show More)
This paper describes text mining technique for automatically extracting association rules from collections of textual documents. The technique called, Extracting Association Rules from Text (EART). It depends on keyword features for discover association rules amongst keywords labeling the documents. EART system ignores the order in which the words occur, but instead focusing on the words and their statistical distributions in documents. The system based on Information Retrieval scheme (TF-IDF) for selecting most important keywords for association rules generation. It consists of three phases: Text Preprocessing phase (transformation, filtration, stemming and indexing of the documents), Association Rule Mining (ARM) phase (applying our designed algorithm for Generating Association Rules based on Weighting scheme GARW) and Visualization phase (visualization of results). Experiments applied on Online WebPages related to the cryptography. The extracted association rules contain important features.
This paper presents the findings of a research project concerned with extracting the themes currently being researched within the Software Engineering field by automatically processing a collection of papers published thus far in 2015. Natural Language Processing techniques have been applied to extract the key features of each publication, with publications then being clustered with similar papers to identify their common themes and highlight the key themes currently being researched within the Software Engineering domain. The most dominant active theme was Education, with a cluster of papers concerning the teaching of software engineering, while other relevant topics included Components and Requirements.
Entity extraction is widely used in power fault diagnosis to extract valuable knowledge from a large amount of power fault data, supporting fault diagnosis and prevention. Traditional entity extraction methods mainly rely on fixed rules and procedures, which cannot be dynamically learned or changed with new data or knowledge. When dealing with complex and variable power plant fault data, traditional methods have limitations. This paper proposes a text mining-based entity extraction method that updates the model by learning fault data characteristics. This makes the model more adaptive and flexible to cope with complex and variable data. We compared and analyzed experimental results based on real fault data. The results show that our method effectively extracts fault entities from power plants and has a high accuracy and recall rate. This has a significant reference value for improving the efficiency and accuracy of power plant fault processing.
With the wide adoption of social networks, people are accustomed to post their ideas and thinking via these platforms. Tweets or comments online usually come with individual sentiment, which are time consuming to be analyzed by human labor. This study encapsulates a prototype Chinese sentiment mining system and takes a global hotel reviewing website TripAdvisor as the evaluation sample. The proposed sentiment mining model is compared with logistic regression and support vector machine models based on their performances. This proposed model outperforms LR and SVM in all datasets in terms of classification accuracy and F-measure. An additional module embedded in proposed system enables expansion of novel or undefined terms to the dictionary referred (NTUSD). With this Word2Vec-based module, the system further improves accuracy while reduces both type I and type II error for at least 5%.
In the theory of traditional Chinese medicine, deficiency pattern is a distinguished one among patterns in rheumatoid arthritis. As for the explanation of deficiency pattern in rheumatoid arthritis, traditional Chinese medicine explains the deficiency in organs of both liver and kidney. As for the modern medicine, no specific factor available to explain it. In this paper, we propose an approach through data mining to explore the biological basis of deficiency pattern in rheumatoid arthritis. In this approach, the first step is to find the formula in traditional Chinese medicine in the treatment of rheumatoid arthritis. Then, list out the top three diseases which can be regulated by this formula. After that, we can find the networks of biological basis existing among all these three diseases by data mining. By analyzing these networks, directly or not, the deficiency pattern in rheumatoid arthritis might be caused by the chronic inflammation.
Due to popularity of social networks, smartphones and mobile Internet usage in Thailand, Thai people, especially young adults have been utilizing social network such as Facebook on the move via their smartphone in daily basis. The study proposes a framework that are attempting to incorporate social network Thai text data with online marketing survey using computer-assisted self-interview questionnaire in order to develop better, information-richer marketing segmentation. The merits of the study are to reduce manpower and errors in traditional offline survey, to offer easier access to the young adults, to retrieve and utilize social network data by combining them with lifestyle factors to develop market segmentation with better understanding of the sophisticated customers.
The Global tourism competitiveness report 2017 has placed India in 40th position in the global. In many significant domains or pillars, India falls far behind compared to other nations. Many studies related to tourist satisfaction has apparently revealed the role of quality of accommodation, Service quality of hotel personnel, Affordability, Variety in tourist accommodations etc acts as important pull factor in deciding tourist behavior. In this backdrop, an attempt to revisit service quality perception of foreign travelers about hotels is of paramount importance. To create a favorable perception in the minds of foreign travelers and to encourage positive word of mouth of such attribute is essential. To know and improve those attributes this study is done, and from the study, we found that the attributes such as rooms, housekeeping, and food have more negative reviews from the customers. Thus the hotels should improve on these attributes.
Regulation is an essential element for the industrialization of technologies, and the first and most basic step to facilitate the regularity of regulations is to classify the regulations. Classification of regulations helps both the regulated and the regulating parties. Regulatory agencies will be able to structure regulations for the appropriate users or challenges; the regulated citizens or companies will be able to know the appropriate rules and regulations they must abide by. This research aims to detect the relationship between technology and regulation in the medical field with text-analysis of medical device guidelines. In the past, we conducted a simple dichotomization according to whether they contained disease-related terms (e.g. disease name, body part) or not. The dichotomization was experimented on a set of medical device guidelines in Japan and technologies listed in the preceding technology forecast which lead to the patterning of the relationship between technology forecasts and guideline formulation. In this research, we utilize the classification we explored in our past research, and use available text mining tools on medical device guidelines in Japan to detect the relationship between technology and regulation within guideline documents.
Nowadays, text mining is a long studied field in science, the vast amount of text resources available has made scientist explore several domains through many different techniques. One of the main processes in text mining are cleaning, reduction and transformation before the application of a classification algorithm. Preprocessing has a large impact in classification algorithms because text is an unstructured form of data with very large number of dimensions and therefore it can be seen as a very sparse matrix. These characteristics that make text so complex are addressed by preprocessing algorithms which extract the main data features. We present a work with a comparison of the performance of the different preprocessing algorithms for a classification problem in two datasets written in Spanish and English.
Urban planners have been striving to enhance citizens' participation in urban planning processes. In Indonesia, the Government of Jakarta developed an e-participation tool called e-Musrenbang to directly involve public in submitting complaints and proposals. However, there was no systematic method used by the Government to analyze and map a lot of complaints and proposals data. The big number of complaints and proposals requires data mining approach to automating the process of extracting knowledge from the data set. This study aimed to perform text clustering to obtain the specific topics of community reports and to visualize the intensity of each topic in the urban area. This study is a subsequent stage of research, after urban problem classification. The results of text clustering showed that majority of public complaints and proposals are associated with drainage and roadwork. This research showed the spread of public reports intensity in the urban area by visualizing the text clustering results using Geographic Information System.
This paper studies the environmental perception of the intertextual artistic conception of ancient poetry and the display space of the text mining algorithm, and mainly discusses the problem of how to implement the intertextual interpretation of classical poetry. The similarity of allusions, the similarity of syntax, and the consistency of destiny are the main ways to implement the intertextuality of classical poetry. The common language form elements are displayed in different texts. The text mining technology is used to divide the Guanzhong area into Chang'an through visual analysis. Five spatial display types, including the ancient capital, Xinggong Bieyuan, Shanshui Xingsheng, Pastoral Artistic Conception, and Xiongguan Remains, and further analyze the emotional characteristics of the human landscape, the spatial distribution characteristics of the landscape, and the spatial semantic network characteristics, and the efficiency is increased by 7.2%.
Text mining is a knowledge-intensive task. Text mining practitioner finds many analytics, statistics and linguistics concepts very difficult to deal with and it is compounded by additional difficulties inherent to the user's native language and culture. Correspondence analysis (CA), SVD plot and cross-correlation analysis (CCA) are used to analyze concepts and words, correlated in time and to create a warning/forecast mechanism. We apply CA and CCA to Toyota's quality control problems related sudden acceleration of their cars. The predictive model a “cross correlated autoregressive” created helps to determine the number of lags (weeks) which could have been used for the detection of Toyota Accidents.
With many security events taking place in recent years, scientists have realized vulnerability management is an important field and it brings critical effects to many information systems and services. One topic in this field is to identify similarity relationship between vulnerabilities. It can help us to alarm potential attacks. In this paper, we propose a text mining approach to compute a similarity score between two vulnerabilities based on their text description. It consists of two steps: preprocessing and similarity score computation. Experimental results based on an annotated vulnerability dataset have proved the effectiveness of our approach.
Management control and monitoring of production activities in intelligent environments in subway mines must be aligned with the strategies and objectives of each agent. It is required that in operations, the local structure of each service is fault-tolerant and that large amounts of data are transmitted online to executives to make effective and efficient decisions. The paper proposes an architecture that enables strategic text analysis on the Internet of Things devices through task partitioning with multiple agent systems and evaluates the feasibility of the design by building a prototype that improves communication. The results validate the system's design because Raspberry Pi can execute text mining algorithms and agents in about 3 seconds for 197 texts. This work emphasizes multiple agents for text analytics because the algorithms, along with the agents, use about 70% of a Raspberry Pi CPU.
Currently, the data that dealt by traditional text clustering methods is small. Text representation model that text clustering used is traditional vector space model (VSM). The traditional text clustering has defect of low efficiency when processing big data. The quality is bad when using traditional VSM model for text representation. To solve these two problems, this paper puts forward a VSM model for text representation, which is improved by Term Frequency(TF), and using Mahout to text clustering. Experimental results show that use Mahout for text clustering has a higher efficiency than traditional text clustering; this novel VSM model can improve the text clustering quality.
CISTI (Iberian Conference on Information Systems and Technologies) is a technical and scientific annual event dating back to 2006, whose purpose is to present and discuss knowledge, new perspectives, experiences and innovations in the Information Systems and Technologies domain (IST). The dynamics associated with IST together with the growing interest of the global community in CISTI have resulted in many topics and articles published in each conference, justifying a comprehensive analysis of the literature published. In this study, we present such analysis encompassing the last four editions, between 2013 and 2016, and a total of 677 articles. To accomplish such challenge more efficiently, we adopted text mining. We assessed through topic modeling how the unveiled research trends are aligned with the main conference themes. Data-driven empirical research has proven it is still a hot subject for researchers. Likewise, education and learning are also playing a significant role in CISTI's contributions. Notwithstanding, Internet and social media are highly relevant topics for the conference, although not figuring as major themes. On the other side, health is receiving less attention. Thus, this study can lead to recommendations for future CISTI's themes, in addition to providing an overview of current research trends.
In traditional Online Examination System, only objective type questions are assessed and according to that marks are given to the student. However, this technique lacks the capability of evaluating descriptive answers. In university examinations, there are many types of question included for evaluation of the students. Therefore, the automated system must be capable of evaluating the descriptive answers. The online examination system checks the student answer by matching the answer with predefined set of answer. The predefined answers are saved on the server and evaluation is done automatically using the automatic assessment tools. Here the machine learning approach is used to solve this problem using text mining. Measuring the similarity between, sentences, words, documents and paragraphs is an important component in various tasks such as text summarization, information retrieval, automatic essay scoring, document clustering, and machine translation and word-sense disambiguation. In this system JSON is used for transferring data between web application and server, serving as an alternative to XML.
Information extraction (IE) and knowledge discovery in databases (KDD) are both useful approaches for discovering information in textual corpora, but they have some deficiencies. Information extraction can identify relevant sub-sequences of text, but is usually unaware of emerging, previously unknown knowledge and regularities in a text and thus cannot form new facts or new hypotheses. Complementary to information extraction, emerging data mining methods and techniques promise to overcome the deficiencies of information extraction. This research work combines the benefits of both approaches by integrating data mining and information extraction methods. The aim is to provide a new high-quality information extraction methodology and, at the same time, to improve the performance of the underlying extraction system. Consequently, the new methodology should shorten the life cycle of information extraction engineering because information predicted in early extraction phases can be used in further extraction steps, and the extraction rules developed require fewer arduous test-and-debug iterations. Effectiveness and applicability are validated by processing online documents from the areas of eHealth and eRecruitment.
organizations strive for innovation in order to be economically competitive, meet customer needs, or improve their efficiency. Numerous approaches have been developed in recent years for this purpose. One is idea mining, which is technically based on text mining but aims to capture (new) innovation ideas from predominantly user-generated content on the Internet. In addition to weblogs and blogs, major social networks such as Twitter, Facebook, and Instagram are also used as data sources. These sites contain an immense amount of data in the form of user comments in unstructured text form. This data can relate to organizations or their offerings, such as products and services, and can be collected, analyzed, and evaluated using text mining techniques. In this paper, we present a concept for collecting large amounts of unstructured text data that is processed (semi-) automatically and from which valuable innovation ideas can be derived. The concept is integrated into an employee-driven digital innovation process. Furthermore, we shed light on the benefits but also the challenges of (employee-driven) idea mining and thus provide further starting points for research and aspects to be considered for practice.
This survey deals with the problem of evaluating the submissions to crowdsourcing websites on which data is increasing rapidly in both volume and complexity. Usually expert committees are installed to rate submissions, select winners and adjust monetary rewards. Thus, with an increasing number of submissions, this process is getting more complex, time-consuming and hence expensive. In this paper we suggest following text mining methodology, foremost similarity measurements and clustering algorithms, to evaluate the quality of submissions to crowdsourcing contests semi-automatically. We evaluate our approach by comparing text mining based measurement of more than 40'000 submissions with the real-world decisions made by expert committees using Precision and Recall together with F1-score.
Earlier techniques of text mining included algorithms like k-means, Naïve Bayes, SVM which classify and cluster the text document for mining relevant information about the documents. The need for improving the mining techniques has us searching for techniques using the available algorithms. This paper proposes one technique which uses the auxiliary information that is present inside the text documents to improve the mining. This auxiliary information can be a description to the content. This information can be either useful or completely useless for mining. The user should assess the worth of the auxiliary information before considering this technique for text mining. In this paper, a combination of classical clustering algorithms is used to mine the datasets. The algorithm runs in two stages which carry out mining at different levels of abstraction. The clustered documents would then be classified based on the necessary groups. The proposed technique is aimed at improved results of document clustering.
Digital libraries, journals and conference proceedings repositories are a great source of information. These sources are very useful for the purpose of research and development. This paper presents an overview of text mining and its application towards information extraction from literature. In this study, we used word cloud, term frequency analysis, similarity analysis, cluster analysis, and topic modeling to extract information from multi-domain research articles. Cloud computing and big data are new emerging trends. So it is important to extract useful patterns and knowledge from published articles in these domains and discover the relationship between them. Therefore, a total of two hundred research articles published from 2010 to 2018 in these domains, were selected. The source of these articles is high impact factor journals from reputed publishers namely IEEE, Springer, Wiley, Elsevier, and ACM. It is a cross-domain analysis in cloud computing and big data domains to find the latest trends, related topics, tools, terms, and author affiliation from extracted data. This study identifies the ten major areas of big data using cloud computing, fourteen factors towards cloud adoption, and hurdles in adoption. Moreover finding shows that IEEE has more sources for subject cloud computing application towards big data, then comes Springer, Wiley, and Elsevier. Furthermore, it has been observed in the analysis that the number of articles in these domains increased from 2013 onward.
With the rapid development of Internet technology, social networks have gradually become an important platform for people to communicate and share health topics. As one of the most common health problems, chronic diseases have a negative impact on people's quality of life and physical and mental health, and social networks have become an important source of health information for patients with chronic diseases. In order to better mine and understand the large number of online health information resources generated by users, this paper constructs a framework to identify chronic disease-related topics that users mainly discuss on social networks, and analyze the emotions expressed by users.This research supplements the research on chronic diseases in social networks in theoretical methods, and enriches research in related fields to a certain extent. In practical applications, it helps medical staff quickly understand the core needs of patients and guide patients to pay attention to content related to their own diseases. At the same time, it helps the health and medical departments to formulate relevant policies to improve the national medical service system.
For text data, feature dimension reduction is a very significant and important work for simplifying document representation and enhancing computation of learning algorithm. There are usually two main dimension reduction strategies, feature extraction and feature selection. Feature extraction is to create new features to represent documents, whereas feature selection will return a subset of words as features. Comparing two strategies, feature extraction has powerful capacity in reducing dimensionality, but it will lost intuitive semantic for documents. Feature selection has perfect interpretability for text contents, and specially is significant for text dimension reduction, but it is still a difficult work to design a suitable measure for feature evaluation. In this paper we present a new feature selection method called class subspace feature selection (CSFS) method. We utilize PCA feature extraction method to capture lower dimensional class subspaces, and then base on the subspaces to choose the most relevant features to the subspaces. The feature words chosen by our method can approximate the class subspace which has lower dimensionality and also owns intuitive semantic understanding for the class. The experimental results on three text data sets show the effectiveness of our proposal feature selection method.
Text mining is an important and popular data mining topic, where a fundamental objective is to enable users to extract informative data from text-based assets and perform related operations on the text, like retrieval, classification, and summarization. For text classification, one of the most important steps is feature selection, because not all the features in the text dataset are useful for classification. Irrelevant and redundant features should be removed to increase the accuracy and decrease the complexity and running time, but it is often an expensive process, and most existing methods using a simple filter to remove features, which might potentially loose some useful ones because of feature interactions. Furthermore, there is little research using particle swarm optimization (PSO) algorithms to select informative features for text classification. This paper presents an approach using a novel two-stage method for text feature selection, where with the features selected by four different filter ranking methods at the first stage, more irrelevant features are removed by PSO to compose the final feature subset. The proposed algorithm is compared with four traditional feature selection methods on the commonly used Reuter-21578 dataset. The experimental results show that the proposed two-stage method can substantially reduce the dimensionality of the feature space and improve the classification accuracy.
The popularity of ride-hailing services in the form of smartphone application as a transportation solution has become center of attention. The convenience offered has made many people use it in daily life and discuss it on social media. As a result, ride-hailing service providers utilize social media for capturing customers' opinions and marketing their services. If customers' statements about ride-hailing services are analyzed further, service providers can get insight for evaluating their services to meet customers' satisfaction. Text mining approach can be useful to analyze large number of posts and various writing styles to extract hidden information. Furthermore, by applying topic modeling, service providers can identify the important points that were spoken by customers without previously giving label or category to the text. Latent Dirichlet Allocation was used in this study to extract topics based on the posts from ride-hailing customers published on Twitter. This study used 40 parameter combinations for LDA to get the best one to obtain the topics. Based on the perplexity value, there were 9 topics discussed by customers in their posts including the top words in each topic. The output of this study can be used for the service providers to evaluate and improve the services.
Misclassification of bug reports inevitably sacrifices the performance of bug prediction models. Manual examinations can help reduce the noise but bring a heavy burden for developers instead. In this paper, we propose a hybrid approach by combining both text mining and data mining techniques of bug report data to automate the prediction process. The first stage leverages text mining techniques to analyze the summary parts of bug reports and classifies them into three levels of probability. The extracted features and some other structured features of bug reports are then fed into the machine learner in the second stage. Data grafting techniques are employed to bridge the two stages. Comparative experiments with previous studies on the same data -- three large-scale open source projects -- consistently achieve a reasonable enhancement (from 77.4% to 81.7%, 73.9% to 80.2% and 87.4% to 93.7%, respectively) over their best results in terms of overall performance. Additional comparative empirical experiments on other two popular open source repositories confirm the findings and demonstrate the benefits of our approach.
The research conducted in this paper presents a detailed analysis of the latest research publications related to Data Science using information retrieval and text mining approach. The database used in this study was created by collecting the latest research papers from well-reputed Journals and Conference proceedings published by IEEE and Springer. This comprehensive study shows the significance of information retrieval and text mining in the identification of key insight from textual documents.
The rapid development in network technology and the widespread use of the Internet have led to an increased reliance on the exchange and sharing of online information. However, the sheer volume of data has resulted in fragmented and disorganized information about individuals. This study employs social media analysis and text mining techniques to analyze characters from the novel “Wuthering Heights.” Initial data collection from social media undergoes pre-processing to eliminate noise through N-gram analysis, the establishment of Term-Document Matrix (TDM), tokenization and segmentation, and normalization. Feature selection from the pre-processed data involves applying Chi-square and Least Absolute Shrinkage and Selection Operator (LASSO). A performance of feature subsets, extracted by these tools, is evaluated using Support Vector Machine (SVM). Subsequently, Latent Semantic Analysis (LSA) and K-means algorithm with Singular Value Decomposition (SVD) are employed to establish selected keywords into comprehensible concepts.
Text mining is the process of discovery of interesting knowledge in text documents, many data mining techniques have been proposed for mining useful patterns in text documents. It is challenging issue to accurate knowledge in [1] text documents to help uses to find what they want. In existing information system they provided many term based methods to solve this challenge. As we know the term-based methods suffers from the problem of polysemy and synonymy. The polysemy means the word has multiple meanings and synonymy is multiple words having the same meaning. In propose to use pattern or phrase based approach so it will perform better than the term based approach. In the proposed approach can improve the accuracy of evaluating term weights because discovered patterns or most specific than whole documents. In this paper we focus on innovative and effective pattern discovery techniques as well as architecture of proposed system. Then we will study how pattern taxonomy model and deploying model is useful for the effective pattern discovery for Text Mining.
Organizations frequently report problems finding skillful people to cover their most knowledge intensive vacancies. Being software engineering positions some of the such kind of jobs, there is a considerable gap between job postings and hiring skillful engineers in many software engineering organizations. In this paper, we will introduce the prototype of a web application that helps identifying Technical Knowledge (TK) in software development, to serve as a tool in the hiring process of software engineering positions, and in talent management. The purpose of this tool is to do an initial screening when opening a job position. All this is accomplished using Natural Language Processing (NLP) and Text Mining (TM) to analyze unstructured text in resumes and curriculum. We propose a way to use NLP and TM to identify knowledge profiles for Software Engineering Positions.
News is important information to know what events occur every day among the community. The Internet makes news spread wider and faster. So that it can help people read news anywhere and anytime easily. But there are often many news on internet that are still not categorized according to the news content. Grabbing method will takes news' important information from online media. The news that has been taken will be categorized according to the news content using the text mining method. Word weighting or Term Frequency is one text mining method that can be used to categorize news because this method has high accurate, easy, and efficient results. This is proven by the results of research testing reaching an average accuracy of 95.2%.
In the agile application development environment, automatically identifying relevant components in a large complex software system for software maintenance is still remain a research problem with the proliferation of software applications. Earlier, concept mining with formal concept analysis was one of the commonly applied techniques for legacy software systems of small to medium size. Recently, text mining is being widely used for locating features or concerns in a large complex software system. Nevertheless, the literature study reveals that combining text mining with other techniques always yield better accuracy in locating features. Even though it is efficient, applying formal concept analysis on the large systems poses limitation due to its exponential time complexity in constructing concept lattices. In this research work, a model is devised to combine text mining and concept mining for large systems. The unsupervised machine learning technique, Latent Dirichlet Allocation modeling also called as Topic Modeling is used to reduce the feature space on which K-Means clustering is applied to cluster the related documents and formal concept analysis is carried out on individual clusters. Three open source software systems namely JEdit, ArgoUML and JabRef are considered for the experimental study. The empirical evaluation of feature location measure of the proposed model shows a significant improvement in terms of accuracy, scalability, flexibility and efficiency over the contemporary methods existing in the literature.
With the development of weblogs and social networks, many news providers share their news headlines on different websites and weblogs. One of the main text mining topics is how to classify news into different groups. This study aims to classify news into various groups so that users can identify the most popular news group in the desired country at any given time. Based on Term Frequency-Inverse Document Frequency (TF-IDF) and Support Vector Machine (SVM), a news classification method was proposed. The proposed approach is comprised of three different steps: 1) text preprocessing, 2) feature extraction based on TF-IDF, and 3) classification based on SVM. The proposed approach was evaluated using two BBC datasets and five groups of 20Newsgroup datasets. The classification precisions were obtained as 97.84% and 94.93% for BBC and 20Newsgroup datasets respectively. These are very desirable results in comparison with other classification methods.
With the development of information technology, a large number of equipment management systems have been used in the aviation field. These systems generally have failure management functions. After the failure occurs, it is necessary to input the text information of equipment failure in time, such as “combination switch damage”, “intake pipe rupture” and other failure phenomena. In addition, the corresponding failure causes and troubleshooting methods also need to be fully recorded. Therefore, as the system has been used, a large number of equipment failure texts will be generated. If we can make full use of these unstructured data and discover the knowledge through text mining, it will be of great significance to fault analysis and maintenance decision-making.
Epilepsy diagnosis can be an extremely complex process, demanding considerable time and effort from physicians and healthcare infrastructures. Physicians need to classify each specific type of epilepsy based on different data, e.g., types of seizures, events and exams' results. This work presents a text mining approach to support medical decisions relating to epilepsy diagnosis and classification in children. We propose a text mining process that, using patient medical records, applies ontologies and named entities recognition as preprocessing steps, then applying K-Nearest Neighbors as a white-box lazy method to classify each instance. Results on real medical records suggest that the proposed framework shows good performance and clear interpretations, albeit the reduced volume of available training data.
The purpose of this paper is to investigate the graph-based representation of the data required for the vector space model (VSM) and PMI (pointwise mutual information)-enriched VSM used for text mining (e.g. text classification). The transformation of a dataset containing free text reviews for online stores in a graph-based form is described and its format that allows to be used by Neo4j graph database management system is proposed. Queries for retrieving the data required for training text mining models are considered; the steps and the actions for their modification when receiving new data are specified. The advantages of the proposed graph-based representation in regard to the maintenance and the extraction of current data needed for retraining data mining models are summarized, in order to prevent loss of performance of results from the execution of the respective data mining task.
Inequalities and exclusion from education were exposed and worsened during the COVID-19 pandemic; however, it forced us to recognize the need to make equality, equity, and social inclusion policies effective for all. Scientific and technological solutions to global threats depend on the formation of the maximum number of qualified human resources, which entirely relies on enabling everyone to acquire, update, and improve their knowledge, skills, and competencies through lifelong learning and higher education. To guarantee inclusive and quality education for all (UN Sustainable Development Goal 4) is hard to achieve at higher education or post-secondary levels. This research aims to provide an overview of the achievements and challenges that higher education institutions (HEI) face in fulfilling the requirements of students with disabilities (SWD). We analyzed a database of 104 abstracts from reviews of SWD in HEI published in Scopus-indexed journals between 2018 and August 2022. After data preprocessing, the text mining analysis on the corpus was visualized in word clouds and graphs. From the results, we could identify that providing access to facilities and information still dominates the research on inclusive education, and visual disability is the most frequently analyzed. The graphs reveal published research on undergraduates with disorders like Autism Spectrum (ASD), learning disorders, and visual, hearing, physical, intellectual, and psychosocial disabilities. The authors also evidenced the lack of information on the barriers and needs of SWD in HEI and potential future research to address them. Concerning the strategies to attend and care for SWD inside the classrooms, the graphs highlight Universal Design as a promising trend leading to inclusivity in higher education. The results and analyses in current research provide essential information to educational stakeholders and decision-makers inside institutions so that they can take action to embrace diversity... (Show More)
Feature selection is regarded as an important task in data mining. The applications of machine learning eliminate irrelevantly, redundant features so that the learning performance is improved. A novel feature selection method for unsupervised text clustering, that is, binary multi-verse optimizer algorithm (BMVO) is proposed in this paper. A new application of the MVO algorithm is introduced via this method, which selects important text features. Then, these important features are tested using a k-means clustering algorithm to enhance performance and lessen the cost of the proposed algorithm computational time. The BMVO performance is examined on 6 datasets that are published including Classic4, Wap, tr41, tr12, 20Newsgroups, and CSTR. Based on the measures of the evaluation, the obtained results showed that the BMVO performance has outperformed the rest of the comparative algorithms.
This paper presents a novel approach to automate the process of extracting topic and main title from a single-document short text. The proposed approach uses online text mining and Natural Language Processing techniques. The title of any text provides an efficient way to concisely grasp the overview of the contents in the text by giving a glance on its main heading only, which is quicker than reading the summary. In this paper, three different mechanisms have been proposed, implemented and compared to find the best approach for automatic extraction of a topic that is more relevant to the overall event explained in the text. The proposed system is evaluated against fifteen news articles from New York Times. The significance of the paper is twofold: Firstly, these automatic topic extraction techniques can be used further for document classification, document relevancy and similarity, summarization, comprehensive grasp of any event and finding novelty in out sized and scattered text data by scanning titles. Secondly, it can be used as a road map for the new researchers by using this detailed analysis of various data mining techniques. The experimental results show that the Nouns are more related, reliable, and suitable words for finding the topic of the text.
With the comprehensive development of the smart grid, along with the large amount of operational data generated by the operation of the power grid, a lot of attention has been paid to the short-text information about the secondary electrical equipment failures that have occurred. This article analyzes the fault data that occurs during the operation of the secondary equipment. With reference to the general process of Chinese natural language processing, considering the overall characteristics of the fault information, the LDA topic model is used to generate a topic text model of short text data. For local features, use the Word2Vec word vector model is used for characterization. Finally, convolutional neural network (CNN) is used to categorize the fault categories of the information, and a short text classification model of secondary equipment in smart grid based on the LDA topic model and CNN is proposed. The example results show that the proposed Chinese short text classification model can improve the classification accuracy, and the classification effect is also considerable.
Bike-sharing has experienced rapid growth in China since 2016. Notwithstanding the fast expansion, or possibly because it grew too fast, some bike-sharing companies have experienced setbacks and failure in 2018. This paper aims to close the gap due to the insufficient analysis on the influencing factors of the bike-sharing system problems in China. The study first used text mining of bikes-sharing related Chinese news reports and social web discussion boards. Subsequently, the study used association rule mining to explore the relationship between keywords generated by text mining of the original data sources. The results of news reports mined keywords show that problems with shared bikes deposits were closely related to the complaints of customers. In the context of social web discussion boards, the keywords relations implies that the users concern about the possible collapse of the bike-sharing companies and related management and economy issues. The information existed before the bike-sharing companies financial failure news took place. Our results also show that the association rule mining relationship of major keywords in news reports and social media can be an early warning sign of the financial failure of sharing bikes companies.
Text mining is vital for knowledge cultivation, keeping this in perspective we have focused on developing a system which uses a full parser for analyzing the text, grammar towards the biomedical arena. We proposed a preprocessor to overcome the shortcomings of full parsing and modules to handle the partial outcome. The developed system, not only has the viability to be maintained easily, but also can adapt itself for a particular domain. In the primary experiment, out of 131 argument structures extracted from 96 sentences, 32 were extractable, 33 with ambiguity and the remaining 66 (non-extractable) for which partial result was determined. The work produced better result than the other full parser with reduced count of failure in extraction and ambiguity.
Bacterial biofilms are complex surface attached communities of bacteria glued together by extracellular polymeric substance (EPS) matrix, secreted proteins, and extracellular DNAs [1]. Biofilm show reduced growth rates and metabolism. Biofilm formation is a survival mechanism that provides with better options compared to their planktonic counterparts. It impart bacterial communities stronger ability to grow in oligotrophic environments, greater access to nutritional resources, and enhanced syntropic interactions as well as greater tolerance towards environmental stress [2]. Biofilm play a detrimental role in many areas such as healthcare, food industry, water distribution systems, oil and gas industry etc. The composition of biofilm microbial community is varies depending on the environment in which it is formed. Biofilms are stratified formations where deeper layers maintain anoxic conditions. These anoxic niches promote the growth of certain specific groups, including sulfate reducing bacteria (SRB), that use the surface (usually metal) as resources for their survival.
In view of the coexistence of “difficulty in employment” for logistics job seekers and “difficulty in recruitment” for enterprises, this work uses web crawler technology to collect a total of 17,086 pieces of data from recruitment websites, and uses web text mining to segment Chinese recruitment data text, using BERT pre-training depth model Process text clustering and sentiment analysis for unstructured information, and use complex network tools to visually interpret the relationship between job and demand characteristics. The analysis results provide professional development help for logistics talents, so that colleges and universities can provide specific suggestions for transporting outstanding logistics talents to enterprises through the market demand more clearly on the training direction of students.
Natural language processing has set off another development boom in the field of artificial intelligence. Intelligent question answering, machine translation, and dialogue systems are all important research contents in the field of natural language processing. Among them, text mining is an important branch in the field of natural language processing, including text classification, named entity recognition and other technologies. In actual research and engineering projects, obtaining a large number of reliable text data sets is the guarantee for these tasks. In the industrial field, the production of text data sets is usually done by manual sorting and labeling, but manual labeling consumes a lot of manpower and time, making the text data set not ideal owing to the different subjective thoughts of each person. Consequently, it can not meet the needs of the actual industrial field. This paper proposes an automatic annotation method based on the number of named entities for text datasets, which can quickly obtain a large number of text datasets in the field of terrorism, and ensure the smooth progress of research and engineering projects.
With the improvement of social consumption level and the rapid development of the Internet, the application of recommendation system is becoming more and more extensive. Du to the complexity of Chinese language, the traditional recommendation system cannot grasp the user's sentiment tendencies well. In this paper, we establish a recommendation system with text mining technology. The proposed system uses the improved logistic regression in sentiment analysis to get user's sentiment score. Moreover, we build an item-feature matrix to calculate the feature similarity of the items, enhanceing the accuracy of item similarity. The experimental results demonstrate the effectiveness of our proposed system.
Web mining plays vital role in day-to-day applications to improve intelligence of web in the context of business must be able to identify useful business intelligence. To achieve our model in web engineering, we are using mining techniques for next generation business intelligence development. In this research our approach identifies the weblogs error reports using comprehensive algorithms, applies the mining techniques to detect noisy and integrates the different models, finally our information patterns satisfies the need of client inputs. For web engineering retrieval system, list of web log bugs and web architecture, the system uses mining techniques to explore valuable web data patterns in order to meet better projects inputs and higher quality web systems that delivered on time. Our research uses association and machine learning applied to web architecture model pertaining to source code mining implementation tools improves software debugging business rules for novel projects and also presents strategies for efficient study text, graph mining. Presents the Geo Tracking system to identify messages from terrorist or threat persons and also from hackers detects the negative rates and improves the high positive which increases the quality of Government Private and Public sectors.
This paper mines the keywords of related literature through word frequency data, and integrates the keywords. Using the word frequency analysis method, the research directions, research methods, and English language structural modeling of text mining at home and abroad in the past ten years are analyzed through the statistics of high-frequency keywords. Estimate peak analytical power in real time, and build a unit-unified dynamic power model. The experimental results show that compared with the model based on peak analysis, the total number of English instructions can better reflect the power consumption of English language structure 6%
The problem of plagiarism the recent years has been intensified by the availability of information in digital form and the accessibility of the electronic libraries through the Internet. As a result, plagiarism detection has been transformed into a big data analytics problem since the number of digital sources is extravagant and a new document needs to be compared with millions of other existing documents. In this paper, a text mining methodology is proposed that can detect all common patterns between a document and the documents in a reference database. The technique is based on a pattern detection algorithm and the corresponding data structure that enables the algorithm to detect all common patterns. The methodology has been applied in a well-defined dataset providing very promising results identifying difficult cases of plagiarism such as technical disguise.
This paper applies data mining technology to search for Korean/Korean web pages. After a brief introduction to data mining and information management technology, it mainly studies Korean word segmentation technology, topic prediction, and tunnel crossing strategy. The detailed work is as follows: The word segmentation task is the basis of natural language processing. The first task for Chinese, Japanese, Korean and other characters in various tasks is word segmentation. In terms of Korean/Korean word segmentation, traditional matching-based word segmentation methods often require manual construction of a large dictionary, which does not have the ability to recognize words that are not in the dictionary, and the accuracy of word segmentation is not very high.
Knowledge Management became the focus of scientific study during the second half of the 20th century. Unstructured data in project management documents hold critical and time-based information about how the project is going-on. However, retrieving useful information from this data is still challenging task. The proposed approach presents a Naïve Bayes text mining approach to identify early warnings of failure in a project lifecycle in advance. The paper focused on a construction industry project to analyse the effectiveness of this approach. Hence, the primary focus of this research was to identify the lack of various project management aspects. The implemented system scanned critical management documents including Minutes of meeting to identify these so-called early warnings. The technique was evaluated against unseen Meeting Minutes PDF files labelled via expert feedback. The system reported 80.43% of the identified early warnings to be lack of onsite materials followed by lack of keen commitment to project milestones and lack of staff skills and training.
In the software development stage, there are needs analysis activities carried out through elicitation activities, which will produce requirements specification documents. In this document, there is a discrepancy in the perception of elicitation results between developers and clients regarding software requirements based on two methods, namely: elicitation and the Requirement Statement. There are specific steps to achieve this goal, namely: determining the similarity between the software requirements specifications and the elicitation results, analyzing the text contained in the elicitation results using the text mining method, and testing the validity and reliability of the research results through the use of Gwet's AC1 formula. The results obtained are the document with the highest similarity value is the 6th document, namely "The manager can approve items that the supervisor has approved," and the 8th document, namely "Admin can approve items that supervisors and managers have approved." The resulting similarity figure is 0.927587, which means that the similarity level is very high from the number, which is close to number 1.
The world’s consideration has been attracted to ongoing occasions in Afghanistan due to the potential for them to extend to Iraq and Syria, just as the horrendous treatment of the residents. There is as of now little review to decide whether individuals in Western and Eastern countries consider the new occasions in Afghanistan to be a psychological oppressor bunch and a wellspring of fear, or regardless of whether they see it in an unexpected way. Online media has as of late acquired ubiquity as a device for person-to-person communication and data trade. Twitter is microblogging website that permits clients to send and get brief instant messages (tweets) of 140 characters. These messages can be used for opinion mining to check whether there is a distinction in impression of recent developments in Afghanistan between people from Western and Eastern countries. In this study the opinion analysis of the tweets is conducted to understand whether individuals utilized almost similar words while tweeting about the new occasions in Afghanistan, having similar positive and pessimistic words reflects how individuals consider this association a gathering of psychological oppressors paying little heed to where they are from, and most clients consider the new occasions in Afghanistan to be a wellspring of danger and dread paying little heed to where they are from.
Destination image has gained increasingly attention of tourism researchers. However, little attention has been devoted to study the difference in perception of destination image among different demographic groups using the big data analytics in general, the text mining approach in particular. Therefore, in this paper, we develop a method based on Latent Dirichlet Allocation (LDA) to extract cognitive destination image from travel blogs. Furthermore, we explore the differences in cognitive image and travel options among different gender groups. Through experiments, we extract 26 topics by LDA and find that the female has significantly preferences to natural landscape and rural scenery than the male. Meanwhile, the male prefers historical sites than the female. In terms of the travel options, we find the female usually travel with travel agency, while the male travel by self-drive. Our study contributes to the tourism planning, tourism positioning and marketing for different target groups.
This paper describes a system which surveys the French Presidential Election trends from Twitter's discussions. This system carries out the automatic collection, evaluation and rating of tweets for evaluating the trends. The objective of this paper is to discuss the variety of issues and challenges surrounding the perspectives regarding the use of Social Network Analyses and Text Mining methods for applications in politics. The article will first present a review of the literature in Social Media and Text Mining analysis for political purposes and then describe the work done for our system for collecting and manipulating Twitter's data.
Text mining discover and extract useful information from documents, whenever increase the size and number documents leads to redouble features. The huge features for the documents adds challenge to text mining called high dimension. The aim of this proposed study is minimize the high dimension of the documents, and improve Arabic text mining using clustering. In order to achieve this goal, we propose to applied coreference resolution technique using the clustering algorithms k-mediods and k-means. This study uses the similarity metrics Euclidean and Cosine. The system implements using a corpus contains on 200 sport news Arabic. Finally, evaluation measures are used including (Precision' Recall and F-measure) to evaluate our system.
This study explored the application of interview robots on recruitment process. By adopting techniques including web crawling, text mining, and natural language processing, this study developed an effective system that matches job candidates with recruiters. The designed system analyzed electronic résumés in Traditional Chinese, on which the words were graded according to the job market on the Internet and implemented with techniques related to big data. The results demonstrated that the designed system identified the current demand on talent-seeking and quickly presented candidate rankings for a specific position, thereby fulfilling the needs of both job-hunting candidates and talent-seeking recruiters.
In power grid, operating and maintaining engineers have recorded plenty of operation data and fault or defect texts of distribution transformers, which contain asset health information. However, less information is effectively exploited in devices condition assessment. Aiming at improving the decision-making for condition based maintenance, a practical condition assessment method based on multi-source information fusion for distribution transformers is proposed in this paper. Firstly, an assessment index system based on real time and statistical operation data of transformers is introduced. Secondly, through HMM-based text preprocessing and the machine learning of relativity, the text mining technique realized the key information extraction from transformer's fault and defect elimination record texts for condition assessment. Finally, after synthesizing the subjective initial index weight, equipment's aging and the “Short Board Effect”, the optimized condition assessment model based on multi-source information fusion for distribution transformer is established. An operating transformer example reveals that this model is effective and accurate, comparing with the other available models. It shows that with this method, the transformer's accurate health status and evolution trend could be reflected much more timely and rigorously.
Primarily, the need for automatic text categorization and medical diagnosis was the start of Multi-label classification. Multi-label classification received a great attention and used in several real world applications The demand of its applications increased to cover additional fields like functional genomics, music, biology, scene, video etc. For example, a text document may belong to many subjects or topics like Scientific, Cultural, or Politics. There exist a variety of multi-label classification algorithms developed based on two basic approaches: algorithm adaptation and problem transformation. Our contribution consists to present an analysis and experimental comparison of 4 problem transformation algorithms applied to two text benchmark datasets using 4 evaluation measures. In the experimental study, each problem transformation method is applied against all 54 classifiers found in the MEKA software in order to find the classifier that gives the best performance for each dataset and classification method.
A system for the automated construction of the components of the vulnerability metrics and a quantitative assessment of the severity of a vulnerability based on the analysis of its textual description using Natural Language Processing (NLP) tools is proposed. The issues of increasing the efficiency of assessing the danger of detected vulnerabilities of software and hardware for the appropriate response and taking the necessary measures to ensure the required level of protection of objects and systems by information security specialists are considered.
Multimedia database can be define as a collection of storage and retrieval systems, in which large amount of media objects are created, modified, searched and retrieved, where as Multimedia is the combination of text, image, graphics, animations, audio and video. The extension of database application to handle multimedia objects requires synchronization of multiple media data streams. Multimedia data mining refers to the extraction of implicit knowledge, data relationships, or other patterns which are not stored in multimedia files explicitly. The system's overall performance in retrieval can be increase by indexing and classification of multimedia data with efficient information fusion of the different modalities is mandatory. Apart from text retrieval, the current waves in web searching and multimedia data retrieval are the search for and delivery of 3D scenes, images, music and video. The content-based multimedia information retrieval provides new techniques and methods for searching various multimedia databases over the world.
The development of social media has changed the way that travelers visit sightseeing spots. In tourism and hospitality industry, to enhance the revisit intention of passengers is an important issue for the purpose of increasing margin. In recent years, related researches had focused on the customers' revisit behaviors and factors. But, few studies have investigated the related issues that travelers do not want to visit again. Failure to revisit may bring a great damage to the company's revenue in the future. To avoid the occurrence of these injuries, a text mining approach will be employed to discover the reason why customers don't revisit from online textual reviews in social media. In this work, we attempt to define the candidate factors that may influence the non-revisit, and then use two feature selection methods, decision tree and Support Vector Machines -Recursive Feature Elimination (SVM-RFE) to find the crucial factors. Experimental results could be provided to travel service providers to improve service quality and effectively avoid future impact on passengers no longer visiting.
Accessibility is a critical aspect to be considered by college library in order to facilitate users in searching library collections. The Library of Universitas Indonesia, as one of Asia's largest library with more than 1,500,000 book collections, should also concern about accessibility to balance its numerous collections. UI-ana collections or works produced by and associated with Universitas Indonesia; in particular theses (undergraduate and graduate theses) and dissertations are one of the largest numbers of collections in Universitas Indonesia's Library. However, the current collection's management system was still based on the submission of the collection in Universitas Indonesia's Library. Since these collections are arranged with no exact criterion, it is harder for users to find theses and dissertations with the same topic. Therefore, management of these collections based on certain criterion is extremely needed to facilitate users in searching these collections. This research aims to determine the categories that can represent theses and dissertations through abstract text mining of each collection in 2005–2015 with a clustering algorithm, namely Self-organizing Map. This study found 139 categories which will be used to classify theses and dissertations of Universitas Indonesia.
In the 21st century we have seen tremendous growth in the users who are accessing internet. At the same time users across the universe are giving preferences to the regional languages while accessing internet. It is seen that there is a huge scope for doing categorization in regional languages, considering the fact that search engine has worked on many regional languages. Moreover, super-fast acceleration in the information technology has driven the variety of applications of text categorization. We have chosen Marathi as a regional language for automatic text categorization. Categorization defines various classification and clustering techniques and Label Induction Grouping (LINGO) is one of these algorithms used for categorization of Marathi text documents. This uses Single Value Decomposition (SVD) for dimension reduction. We are proposing a modified LINGO algorithm which is considering morphology of Marathi text and Principle Component analysis (PCA) for improving results.
Background: Systematic literature reviews (SLRs)are an important component to identify and aggregate research evidence from different empirical studies. One of the activities associated with the SLR process is the selection of primary studies. The process used to select primary studies can be arduous, particularly when the researcher faces large volumes of primary studies. Aim: An experiment was conducted as a pilot test to compare the performance and effectiveness of graduate students in selecting primary studies manually and using visual text mining (VTM) techniques. This paper describes a replication study. Method: The same experimental design and materials of the previous experiment were used in the current experiment. Result: The previous experiment revealed that VTM techniques can speed up the selection of primary studies and increase the number of studies correctly included/excluded (effectiveness). The results of the replication confirmed that studies are more rapidly selected using VTM. We observed that the level of experience in researching has a direct relationship with the effectiveness. Conclusion: VTM techniques have proven valuable in the selection of primary studies.
Failure Mode and Effects Analysis (FMEA) is one of the basic and the most commonly used techniques in reliability analysis and management. Design FMEA is the application of the FMEA method in product development phase and have shown remarkable results in various engineering design fields. DFMEA reports usually contain a lot of text information about quality and reliability improvement opportunities for new product development. However, it can be very difficult and non-intuitive to fully understand these text information for design improvements. To address this challenge, we propose to apply text mining methods in DFMEA reports to discover the hidden reliability information. Specifically, three types of hidden information are investigated, which include the correlation of the failure related features in the potential failure mode or failure cause, the classification of failure modes and causes, and the optimal set of improvement activities according to the effect and cost metric proposed in this paper. Among them, one feature represents a failure characteristic or a location of the potential failure mode or failure cause (e.g. wear, break, bend, surface, insert, stem). These three pieces of information can be further integrated to product verification and validation process for effective risk identification and mitigation in a more economic and effective way. Finally, we illustrate the application of the text mining methods in DFMEA report for a subsystem in a power generation system within a diesel engine.
The development of World Wide Web has made it difficult for a user to understand textual data coming from diverse sources. Automatic classification of short text i.e. tweets classification, news headlines classification etc. is one of the severe problem for user to get the right information related to his/her interest. In order to cope with the problem of short text classification, in this paper, we have proposed a probabilistic framework for short text classification. Proposed classification model is composed of three major modules i.e. pre-processing of unstructured text, learning of probabilistic model and the classification of unseen data by using learned model. This framework is trained and tested by using news headlines dataset containing six different news categories i.e. politics, sports, business, weather, showbiz and terrorist. During the experimental evaluation 92.41%classification accuracy is achieved by using bigram as feature which demonstrates the effectiveness of proposed short text classification approach.
The amount of job advertisement data is rapidly growing, and this rich dataset is expected to have implications for the employment market, sector trajectories, and the education sector. Most significantly, human resources (HR) data has never previously been examined with the lens of tech mining for science and technology analyses. This article is the first to examine job advertisement data considering research and development (R&D) progress and requirements, and hereafter, we refer to this as HR mining. The aim of this article is to use HR mining with the purpose of R&D and human capital intelligence using the job advertisement data of Turkey for the 2015-2017 period. The method of this study follows classification as part of the preprocessing step to determine R&D, engineering, and high-tech industry-related job advertisements. Afterward, we use clustering methods to identify areas where key human capital is required, and investments are made by R&D-oriented companies. The results show that it is possible to identify sector-oriented skill requirements and that the significance of the R&D skills varies. For the case of Turkey, we can clearly show the national human capital and R&D by identifying nine key clusters that indicate R&D progress and directions.
In traditional Chinese medicine, rules of Li-Fa-Fang-Yao is of critical importance in clinical practices. Li-Fa-Fang-Yao, which means principles, methods, formulae, and Chinese herbal medicines respectively, indicate the four basic steps of diagnosis and treatment: determining the cause, mechanism and location of the disease according to the medical theories and principles, then deciding the treatment principle and method, and finally selecting a formula as well as proper Chinese herbal medicines. In this paper, focused on major depressive disorder, we explored the rules of Li-Fa-Fang-Yao within the framework of traditional Chinese medicine. Through calculation, three clusters of Li-Fa-Fang-Yao on major depressive disorder were found based on the syndrome differentiation. What's more, these three clusters can also be validated by textbooks of traditional Chinese medicine.
Electronic patient records (EPR) are rich in texts, where almost all the decision making processes of medical staff are written. Thus, mining in EPR is important for acquision of decision making process and diagnosis. In this paper, as a first step, we focus on text mining for discharge summaries, which include the compact explanation for the patient's admission. a record of her complaints, physical findings, laboratory results and radiographic studies while hospitalized; a list of changes in her medications at discharge; and recommendations for follow up care. Text mining process consists of the following four processes: first, morphological analysis is applied to a set of summaries and a term matrix is generated. Second, correspond analysis is applied to the classification labels and the term matrix and generates two dimensional coordinates. By measuring the distances between categories and the assigned points, ranking of key words will be generated. Then, keywords are selected as attributes according to the rank, and training examples for classifiers will be generated. Finally, learning methods are applied to the training examples. Experimental validation shows that random forest achieved the best performance and the second best was the deep learner with a small difference, but decision tree methods with many keywords performed only a little worse than neural network or deep learning methods.
Online Customer Reviews (OCRs) make it difficult for firms to examine them due to their number, diversity, pace, and validity. The big data analytics study predicts OCR reading and its usefulness. Titles with positive emotion and sentimental reviews with neutral polarity attract more readers. Online merchants may use this work to build scale automated processes for sorting and categorizing huge OCR data, benefiting vendors and consumers. Current OCR sorting approaches may prejudice readership and usefulness. Python crawled, processed, and displayed data using Natural Language Processing (NLP). The crawling dataset collected literature using a Pubmed Application Programming Interface (API) module. Natural Language Toolkit (NLTK) processed text data. Tokens were processed into bigrams and trigrams using n-grams. According to study abstracts, West Java has the most stunting research. Text mining and NLP may enhance oral history and historical archaeology. Text mining algorithms were intended for enormous data and public texts, making them inappropriate for historical and archaeological interpretation. Text analysis can effectively handle and evaluate vast amounts of data, which may substantially enrich historical archaeology study, especially when dealing with digital data banks or extensive texts.
Text data pre-processing has become essential in research fields like Information Retrieval (IR), Natural Language Processing (NLP), and text mining. It extracts valuable and nontrivial information from unstructured text data. Text data plays an essential role in the digital era as various types of information and data are being generated in an unstructured way. In this work, Multi-level Secret Sharing Scheme (MSSS) based novel approach has been proposed to ensure the security of the most informative and sensitive text data extracted through text preprocessing steps from the text document. The proposed model employs tokenization, stop word removal, and stemming as preprocessing techniques, which help examine text documents. Three types (electronic mail, whats-app messages, and text messages) of datasets have been contributed to this work for performing the experiments, where the model achieves 100% correlation between original and reconstructed text.
Mammary gland hyperplasia (MGH) is a kind of endocrine disorder diseases, which is usually characterized by lump and pain in breast and an increased risk event of breast cancer. The complementary therapies of Traditional Chinese Medicine (TCM) play important roles in MGH treatment. This study analyzed the application of the TCM external therapy and food therapy by text mining. Subsequently, suitable techniques and methods which were frequently used on MGH treatment were explored. Results showed that external application of Chinese herb medicine, acupuncture, massage, catgut embedding, and magnet therapy were the top 5 TCM external therapy techniques, while fritillaria Oyster, alga, trionyx, kelp, arrowhead, pseudo-ginseng, lemon, leonurus, and hawthorn were the top 10 foods used for MGH treatment. The combinations of external therapy techniques were demonstrated in network, so does the combinations of the foods. The results provided good references for both clinical practice and basic research.
This paper presents a model for classifying ICD-10 TM using machine learning and information retrieval. The scope of this research take systematic approach for translating diagnosis from medical records to ICD-10 TM is proposed. First, an information retrieval is used to find similarity word in Thai and English diagnose. Then, machine learning approach is applied to classify ICD-10 TM by training models using Naïve Bayes algorithm. The result shows that our proposed approach can accurately classify ICD-10 TM in Thai-English diagnose at 81.41%.
This paper aims to delve into detailed reviews using text mining to support the perception of potential customers toward organic foods. We attempted to analyze more than 1500 reviews comments, and reviews on social media and e-commerce websites and suggest appropriate marketing strategies that can help organic food vendors in effective sales. The study used a descriptive research design, and data relevant to the study objectives were collected using secondary sources, including YouTube, e-commerce sites, and the Scopus database. Text mining was used to collect data relevant to the research objectives, suitable text analysis was performed using KH coder, and a co-occurrence network was used to present the results. The inferences on the perception of potential customers were analyzed, and key terms such as pesticides, chemicals, way of farming, natural fertilizers, health consciousness, price, quality, comparisons, time, and health were all predominant factors that led to their perception. By analyzing customers’ perceptions, this study provides relevant suggestions for the organic food market, which would help in designing a customer-centric strategy to promote organic food products effectively.
The authors discuss the problem of distributed knowledge acquisition for the construction of complete and consistent knowledge bases in integrated expert systems, in particular dynamic integrated expert systems, via sharing of knowledge sources of different topologies (databases as electronic media, experts and problem-oriented texts). This work is focused on models and methods of distributed knowledge acquisition from databases as additional knowledge sources and automation of the process via intelligent program environment. It is discussed in context of dynamic integrated expert systems development, so the problem of temporal knowledge acquisition is also mentioned. Special standard design procedure for dynamic integrated expert system construction and for combined knowledge acquisition method is reviewed, which provides synchronization of knowledge acquisition processes in distributed variant. This procedure uses technologic knowledge base of intelligent planner of AT-TECHNOLOGY workbench and special program tools.
Today there is a huge amount of information from a lot of various resources such as World Wide Web, news articles, e-books and emails. On the one hand, human beings face a shortage of time, and on the other hand, due to the social and occupational needs, they need to obtain the most important information from various resources. Automatic text summarization enables us to access the most important content in the shortest possible time. In this paper a query-oriented text summarization technique is proposed by extracting the most informative sentences. To this end, a number of features are extracted from the sentences, each of which evaluates the importance of the sentences from an aspect. In this paper 11 of the best features are extracted from each of the sentences. This paper has shown that use of more suitable features leads to improved summaries generated. In order to evaluate the automatic generated summaries, the ROUGE criterion has been used.
With the in-depth development of China's education reform, the combination of educational public opinion and emotion analysis is conducive to the discovery of the evolution law of public sentiment and related factors, and provides ideas for the guidance of public opinion of relevant government departments. This paper proposes an analysis model of public opinion on the education policy network based on text mining, obtains network comment contents through Octopus crawler, preprocesses the data, and introduces the calculation formula of emotion heat, on which the evolution process of public opinion is divided into incubation period, outbreak period, spreading period and decline period. Combined with visualization and emotional heat maps of provinces and ages are constructed, so as to grasp the spatial-temporal evolution law of educational public opinions. Taking the postponement of the college entrance examination in 2020 as an example, the empirical analysis proves that the online public opinion analysis model of education policy can reflect the evolution law.
Essay assignments are one of the effective ways to find out the understanding of students about the material that has been studied in both high school and university. Of course, the essay is different from multiple choice questions which are easily automated to assess, while the essay has a text narrative that needs to be known and maintained in its meaning. This article aims to make an automatic assessment of essay assignments by using text mining techniques to maintain the meaning in the text. One process in text mining is a stemming process that returns words to their basic word form. The stemming process is important because stemming words are verbs that store the meaning of the text. This article discusses the Porter and Cosine Similarity Algorithms in the stemming process. Based on the experimental results of 35 essay assignments, the automatic assessment of the essay has an accuracy rate of around 97% compared to the results of the expert assessment, with the difference in value between automatic assessment and expert judgment 2.62 points.
Nowadays, people are aware that many of the social changes and problems facing humanity cannot be solved by economic development or technological change alone. In recent years, many researchers have called for more attention to be given to social demand and social change. Although the concept of social innovation is still vague, solving social challenges through social innovation is considered to be important. At the same time, researches have been conducted on the importance of social enterprises and social entrepreneurs as players in social innovation. In addition to these researches, the roles and activities of firms in promoting social innovation should be focused on. This paper explores Japanese firms' social innovation activities by analyzing newspaper articles. The results show that firms tend to undertake social innovation by connecting their regular or core businesses with social innovation activities rather than building an innovative way to create social innovation.
Ratings and reviews are always the major consideration factor by online course seekers before they join the course. However, it can be time-consuming to read all the information especially the course reviews. In this research work, our objective is to propose a text analytics pipeline that includes text cleaning, text lemmatization, sentiment analysis, text mining, and visualization that can help course seekers to gain a quick insight into the courses as well as enables them to make a quick comparison between multiple courses. The proposed text analytic pipeline was created in Python Jupyter Notebook. Three different Python-related courses were chosen for the study. The proposed text analytics pipeline solution was proved able to achieve our research objective. It can help course seekers to gain a quick insight including the positive and negative reviews into the courses as well as enables them to make a quick comparison between multiple courses. The n-gram analysis and word cloud generated were sufficient to provide an accurate and informative glance into the course. However, it fell short on sentiment analysis especially in detecting the negative reviews.
Nowadays, large volumes of text data are being produced in real time due to expansion of communication. It is necessary to organize this data for exploitation and extraction of useful information. Text classification based on the topic is one of the efficient solutions to this problem. Efficient algorithms are applied for text classification if they address high dimensional data. In this paper, a novel neural network classifier is adapted to classify the texts and compared to some similar techniques. To this end, firstly preprocessing is conducted on the words, which includes five steps. After that, the feature extraction phase is applied consisting of three steps, with the Principle Component Analysis (PCA) method used for dimensionality reduction. Finally, a supervised learning method, namely the Multi-Level Fuzzy min-max neural network classifier (MLF), is used for text classification. The simulation results of the experiments, compared to three other principle methods, indicate that MLF method has a high accuracy of 94% for the Reuters-21578 dataset and 95% for the 20 newsgroup dataset. In addition, the execution time of the proposed method is less than other supervised learning algorithms.
Information mining is an assignment which is utilized to discover the concealed example or data to break down any subject. These days a ton of research is going on web mining i.e. to mine the web assets to discover the example or shrouded data. In our research the main aim is to perform the text mining over the real time data to predict the result of election that which party will win the state or national election held in India. In our work we get the data from twitter where the citizens of India give the opinion about the political parties and the analysis of these sentiments is done to conclude the result.
Automatic Text Classification is a semi-supervised machine learning task that automatically assigns a given text document to a set of pre-defined categories based on the features extracted from its textual content. This paper attempts to automatically classify the textual entries made by bloggers on various sports blogs, to the appropriate category of sport by following steps like pre-processing, feature extraction and naïve Bayesian classification. Empirical evaluation of this technique has resulted in a classification accuracy of approximately 87% over the test set. In addition to classifying the textual entries of sports blogs, it is proposed that the extracted features themselves be further classified under more meaningful heads which results in generation of a semantic resource that lends greater understanding to the classification task. This semantic resource can be used for data mining requirements that arise in the future.
A Network Service Provider can receive different network related problems and complaints from various communication channels regarding their service activity in certain regions. One of the major and most important communication channels is customer call center. Detecting network related problems that customers are notifying during these calls are significant in order to provide solutions and increase customer satisfaction. However, due to sheer volume of the call records that are converted to text, it is quite difficult to analyze whole data using traditional approaches. In this paper, we study a topic modeling approach for detecting network related problems from call center text data. The analysis results demonstrate that for a major broadband service providers' personal Internet at home tariff, most of calls received in a customer call center is related to information, whereas the second majority of all calls are related to faults that are network related issues. These results signify the existence of network and service related issues in service providers' infrastructure.
With increasing complexity of the social systems under surveillance, demand grows for automated tools which are able to support end users in making sense of situational context from the amount of available data and incoming data streams. This paper presents MOSAIC (Multi-Modal Situation Assessment and Analytics Platform), a semantically integrated system which aims at exploiting multi-modal data analysis comprising advanced tools for text and data mining, criminal network analysis, and decision support. The aim is to provide, from an enriched context, an understanding of behaviour of the system under surveillance thus supporting authorities in their decision making processes. Specific measures and algorithms have been developed to support analysts in retrieving, analysing, and disrupting criminal networks, identifying offenders that pose the greatest harm aligned with domain-specific strategies, as well as enabling the investigation of intervention strategies. A case study is provided in order to illustrate the system in practice.
At the beginning of 2020, 2019-nCoV caused serious adverse effects on China and the world economy. Due to the sudden drop in passenger traffic during the epidemic, the catering industry has become one of the industries most affected by the epidemic. How to maintain survival on the basis of reasonable prevention and control of epidemic situation has become a major problem facing the catering industry. This research collects the reviews and reports of catering companies during the epidemic, and uses ROST Content Mining software to extract high-frequency feature words and analyze the main risk factors. Then this paper builds a social semantic network between high-frequency words and analyzes the mechanism of risk. Finally, based on the different stages of the epidemic, the prevention and control model of the epidemic risk in the catering industry is constructed to provide feasible suggestions for catering companies to deal with the risk of the epidemic, safe resumption of production, transformation and upgrade.
The field of education has been affected by globalization and the constant increase of online courses. The high number of students enrolled in these learning environments and their constant interaction with platforms generate a large amount of data that is difficult to handle with traditional methods of data analysis. The permanence of students in these courses poses challenges aimed at raising their level of commitment and motivation. Several articles with this approach have been identified in the literature analyzed in this work. Some of them are related to the application of text mining techniques aimed at analyzing the interaction of students in these environments. This interaction is based on entries included in discussion forums, emails or interaction in social networks. In this article, we explore the interaction of students through text mining techniques in different student interaction environments in a massive open online course (MOOC). The research focuses on the calculation and analysis of the frequency of terms, the analysis of concordances and groupings in n-grams.
Nowadays, there is an increasing tendency for using acronyms in technical texts, which has led to ambiguous acronyms with different possible expansions. Diversity of expansions of a single acronym makes recognizing its expansion a challenging task. Replacing acronyms with incorrect expansions will lead to problems in text mining procedures, namely text normalization, summarization, machine translation, and tech-mining. Tech-mining involves exploring and analyzing technical texts to recognize the relations between technologies. This paper is aimed at proposing a method for building a dataset that meets the requirements for training acronym disambiguation models in technical texts. In this paper, challenges in automatic acronym disambiguation are presented. We have proposed a method for building the dataset and the accuracy of the acronym disambiguation model is 86%.
Now a day Social media communication become to important factor for business operation. Several Customer prefers to post their comment, suggestion, complaints about company's products and services to online media such as Facebook, Twitter, Social web board because it easy way to blast to public and increases pressure to product owner for responding. This is one factor that cooperate need to be concern and manage responding to customer services that match to customer requirements by analyzes customer suggestion on social media vice versa they can detect negative feedback or complaints early which, able to prevent their reputation. This study was collected text that contains customer suggestion on insurance services from various online social media and extract some specific word via Thai text segmentation and coverts text to Vector Space Model (VSM) based on TF-IDF. We performs experiment by used 800 records of textcrawler and implement two clustering models algorithm which include K-Means and Self-Organization Map (SOM) for clustering suggestion text into three cluster groups as follow Cluster_0 is about to customer feedback on Car Insurance Policy, Car Insurance Premium or Insurance Renewal, Cluster_1 is contains customer feedback on insurance claim services, Cluster_2 is about customer enquired general information. We use “Davies-Bouldin index” method[3] for evaluating both clustering algorithms. A result of experiment shows that K-Means has a significant performance higher than SOM. Finally, The benefit of this study able to help insurance company improve their products and services and increase customer satisfaction and retention strategies planning.
Application distribution platforms (or mobile app stores) provide a space for users to submit feedback or ratings for downloaded software. These platforms have become very important for both software providers and users in communication. The really image of what users' demands can be obtained from the analysis of large-scale data of user feedback. This study analyzes 4,480 user feedbacks from a health and fitness-tracking app in the Google Play with text mining. The result of this study shows that the users of health and fitness-related apps are concerned about their physical activity records and physiological records. The records include track, distance, time, and calories burned during jogging or walking...etc. Besides, the connection between the mobile device and wearable devices is very important for users.
In text mining, clustering and classification are the most important techniques to extract information from textual data. These are techniques which allow us to identify similar groups of textual documents or build classification models based on some similarity. This paper presents a hybrid model based on clustering and classification techniques to recommend research articles to researchers. Since the process of literature review is time consuming, our aim is to automate this process and recommend the most relevant research articles based on users' research field preferences.
The selection of business charts based on data attributes and the purpose of visualization is an important consideration in improving accessibility of data interpretation and decision-making. However, the data visualization tools currently in usage are limited to the provision of numerous charts, and the selection of semantical-sensitive charts still falls back on personal experience and subjectivity. Therefore, the purpose of this study is to recommend proposed methods for improving the accuracy and efficiency of business charts in data usage by developing rules in accordance with data attributes and the purpose of visualization without specific knowledge required on the usage of business charts. For achieving the purpose, the definition and use cases of business charts were collected through web crawling, and keywords characterized each business chart were extracted through TF -IDF analysis. Subsequently, keywords matrix was created according to the associations of these extracted keywords with each business chart and used as recommendation rules.
The present outbreak as consequence by coronavirus covid19 has generated an big impact over the world. South American countries had their own limitations, challengues and pandemic has highlighted what needs to improve. Peru is a country with good start with quarantine, social distancing policies but the policies was not enough during the weeks. So, the analysis over April is performed through infoveillance using posts from different cities to analyze what population was living or worried during this month. Results presents a high concern about international context, and national situation, besides Economy and Politics are issues to solve. By constrast, Religion and Transport are not very important for peruvian citizens.
The total amount of text data information available on the web has been remarkably increasing the data accumulating and augmenting each day. Data and information that are available in high volume are not represented in a structured form that remains suitable for text processing. Text data mining is a subfield of data mining which aims at exploring the useful information from the recorded resources. Text data mining has the following key challenges namely high dimensionality, distance measures between data, achieving quality and classifier accuracies. The research work has attempted to addresses the key text data mining challenges by proposing high-dimensionality reduction novel techniques based on feature similarity functions. In the proposed design, the feature similarity measures are used to group features into clusters. From these feature clusters, an optimal transformation matrix is obtained using which the high dimension text corpus is projected to its equivalent low dimension. This low dimensionality text corpus can be used to implement text clustering and text classification efficiently.
For the question of information security vulnerabilities discovery, the parallel vulnerabilities discovery method is given based on the CAPEC, CWE, CVE and other open source database and text mining. Firstly, we can extract the association vulnerability CWE under the same attack mode, then from CWE associated with CVE based on open source database. That can help us to analyze the potential parallel relationship of the multiple vulnerabilities. Secondly, the vulnerability description information will be vectorized, so that the software system is able to intelligent processing to vulnerability data. That is different from the query based on keyword matching, analyzes the similarity between the multiple vulnerabilities according to the threshold from the training set, and computes the parallel relationship between the multiple vulnerabilities and discovery the parallel vulnerabilities. Finally, this method is correct and effective by the experimental verification and in practice. According to this method, we are able to repair other parallel vulnerabilities when finding a vulnerability is exploited. An advantage of our method is that is applied to network defense.
Social media such as Twitter create space to explain the thoughts and opinions on various topics and different events, millions of users can share their ideas in this Micrblog, Therefore Twitter is converted as a source to exploration of information; make a decision and an analysis of sentiment. There is a sense in all of the texts, but it is more important to provide strategies for obtaining suitable forecasting and optimized usage of information for forecasting sentiment. Also Twitter information follows the stream model. In this model, data were arrived at high speed to destination. As a result, data mining algorithm should be able to predict user feeling in immediate time under limited space and time. The purpose of this paper is to examine the previous works online analysis of sentiment on Twitter.
Nowadays there is an increasing interest in the area of unstructured data analysis. The vast majority of unstructured data belongs to unstructured text data. Retrieving useful information from huge volume of unstructured text data is very challenging task. Text mining is a thought-provoking research area as it tries to discover knowledge from unstructured text. This paper deals with methods used for handling unstructured text data in particular document classification problems. Most document classification methods based on term vector space model of representation of unstructured textual data. The term vector space model is easy to implement, provides uniform representation for documents. However feature space for a large collection of documents can reach millions and be sparse. One of the issues is to reduce the dimension of the term-document matrix. In this research we proposed an approach for reduction of term vector space in KNN algorithm.
In the present day working scenario, job recruitment has become a time-consuming process for the HR department. Not only that, the results of the prevailing recruitment system are often unsatisfactory, as reflected in the frequent job changes, employee dissatisfaction and overall inefficiency. Through the proposed model, we plan to simplify and automate the process with an additional stress of psychometric tests. Psychometric tests prove to be useful in mapping the personality, aptitude and qualities of candidates with the jobs they are applying for. Text mining is performed on the answers, based on a scoring mechanism that serves to produce a shortlist for a given job. This model can work for several recruitment areas and jobs, as need be.
Sentiment Analysis (SA) employed for detecting, extracting, and classifying people opinions about an issue. Social media is a channel to show people opinions and thoughts. This study aimed to detect and classify Indonesia public opinion from Twitter written in Indonesia language for trade relations between Indonesia and China topic with text mining techniques. The result was the model that detected and classified sentiment of public opinion into negative, neutral or positive sentiment. The sentiment detected by lexicon-based and rule-based sentiment analysis. VADER was chosen as a tool for sentiment analysis lexicon-based. The text classification process was a training stage for the model. The experiment revealed SVM classifier performed higher accuracy value than Naïve Bayes, 67.28% and 64.68% respectively.
In order to supervise the bidding activities efficiently and to put forward the accurate standard audit opinions, this paper constructs a model of bidding auxiliary audit intelligent algorithm based on the text mining in Python language. The length and frequency of key words are normalized and optimized. The optimized model shows good characteristics in consistency between prediction and reality, and it can help auditors to identify fraud effectively in bidding process with the quantitative and scientific methods, which is meaningful to improve quality and efficiency of audit in bidding.
The failure of Lehman Brothers, a United States investment bank, on September 15, 2008, had a significant impact on the Japanese economy. The Japanese life-time employment of a traditional employment pattern and seniority system began to slowly deteriorate. A new employment pattern for laborers thus came into force and became conspicuous. Specifically, the number of the part-time employees, who were atypical, the just-in-time employees, and the special employees increased. An atypical employment pattern is causing instability, given the case that payment and recognition are not commensurate with work and that the strong high production causes a negative image. It is disturbed by the atypical employment issues at most companies. Therefore, education by an enterprise is needed; this is ideally achieved in the short term through high efficiency. High-quality education contributes to a company indirectly through the helpdesk staff. The value of the consumer satisfaction will raise an existence value of the helpdesk. This study focuses on the factor extraction method to improve consumer satisfaction by using text mining to helpdesk question and answer.
The Internet is a major source of online news content. Current efforts to evaluate online news content, including text, story line and sources is limited by the use of small-scale manual techniques that are time consuming and dependent on human judgments. This article explores the use of machine learning algorithms and mathematical techniques for Internet-scale data mining and semantic discovery of news content that will enable researchers to mine, analyze and visualize large-scale datasets. This research has the potential to inform the integration and application of data mining to address real-world socio-environmental issues, including water insecurity in the Southwestern United States. This paper establishes a formal definition of framing and proposes an approach for the discovery of distinct patterns that characterize prominent frames. Our experimental evaluation shows that the proposed process is an effective and efficient semi-supervised machine learning method to inform data mining for inferring classification.
In text mining field, The KNN (K Nearest Neighbors) is one of the oldest and simplest methods of text classification. But it is known to be sensitive to the distance (or similarity) function used in classifying a test instance, this disadvantage can cause low classification accuracy and limit the KNN classifier's utilization in text classification in text mining. In this paper, we introduce Mahalanobis distance in text classification area, and proposed an algorithm (MDKNN) base on this theory. Experiment show that our method has comparable or better performance than KNN Classifier and Naïve Bayes classifier in text classification.
The paper presents the findings of an industry-based study in the utility of text categorization. The purpose of the study is to explore new approach to evaluate service quality of customer complaint handling. The industrial research setting is a large China insurance company. The text categorization methodologies are used in this research including nature language processing and machine learning. In addition, a multiple filtration algorithm using synonyms (near-synonym) dictionary, term frequency and Chi-square statistic is adopted in feature selection. The experimental result shows that text categorization can be used to evaluate the service quality by text data from customer complaint handling. The method can solve automatic evaluation problems in customer complaint handling management.
Since text mining saves a large amount of information in text format, it has a very high potential application. One of the main applications of text mining is to classify texts in subject order. In this paper, we tried to propose a aarianew method in order to increase classification accuracy and efficiency, by considering different methods of Persian text classification. We used a number of 5330 news of Hamshahri data collection, for classification. In pre-processing of texts for removing stop words, we proposed a new method by using entropy of words. To extract the feature, word frequencies, and Tf-idf methods have been used. K nearest neighbor algorithm, Naive Bayes classification, and mixture of classifiers, have been used to classify texts, by using combinational classification and mixture of experts. Implementation of proposed method has caused a 15 percent improvement comparing to the previous works done on this data collection, by presenting entropy in pre-processing and also mixture of classifiers. In the best condition, scientific and cultural news has gained 96.36 percent classification accuracy.
In earlier work, we assessed, analyzed and improved the teaching of programming through pair work. In this study, we evaluated the changes in learning behavior that were confirmed due to these improved teaching methods. As a result, we confirmed that knowledge is shared during pair work between highly skilled learners and learners of lower skill levels.
Digital means of communication have become the primary source of information for the majority of population. Given their availability through various platforms digital contents have an important role in shaping of the public interests and opinion. Every now and then a part of it becomes occupied with a new concept that resonates for a certain time, creating corpora of contents that persist on the platforms readily available for the consumers to reach them. It is critical to objectively determine what kind of discourse these text corpora created. In order to examine one such case, related to the use of the term "hibridni rat" (en. hybrid war), this research conducted a study that applied contemporary text analysis techniques. The process consisted of several stages: initially, having investigated the research phenomenon, the analytical problems were defined. This was followed by the identification of the digital contents posted under the national domain of the Republic of Croatia, their retrieval, structuring and pre-processing. Thus, the digital corpora suitable for analysis were created and they were subjected to processing by using the text mining techniques. The results of this process have provided an enhanced insight into the relatively large corpora of digital texts that are difficult for a reader to review, grasp on, and extract the useful information about the phenomenon being investigated just by using the traditional means of browsing the contents. The proposed approach for text fetching and analysis is a major contribution of this study. At the same time the described procedure is relatively easy to reproduce so it can be used in the analysis of texts available in digital format that are related to other phenomena.
One important problem in Bioinformatics is the discovery of new interactions between cellular lines and chemical compounds. In silico methods for cell-line screening are fundamental to optimize cost and time in the drug discovery processes. In order to build these methods, we need to computationally represent cell lines. Current methods for modeling cell line interactions rely on comparing genetic expression profiles. However, these profiles are usually unknown. In this work, we present a method to characterize and represent cell lines by text processing the related scientific literature. We collect abstracts of scientific papers about cellular lines from Cellosaurus and PubMed. These documents are then represented as TF-IDF vectors. We build a data set for classification with the document vectors having the cell line identifier as the target class. We then apply a multiclass SVM classification method. We use Support Vector Domain Description to describe and characterize each cell line with its corresponding hyperplane obtained with a one-vs-rest training. We evaluated several configurations of classifiers, using micro-averaged precision as metric to choose the best classifier, and were able to differentiate cellular lines from a set of 200+.
This paper deals with the problem of automatic topic identification of noisy Arabic texts. Actually, there exist several works in this field based on statistical and machine learning approaches for different text categories. Unfortunately, most of the proposed methods are effective in clean and long texts. In this research work, we use an in-house dataset of noisy Arabic texts, which are collected from several Arabic discussion forums related to 6 topics. In this investigation, we propose a graph approach called LIGA for topic identification task. This approach was firstly introduced for language identification field. Moreover, we propose two other extensions in order to enhance LIGA performances. The experiments undergone on the Arabic dataset have shown quite interesting performances, reaching about 98% of accuracy.
Wound healing is a complex biological process characterized by intricate cellular and molecular interactions. Understanding the underlying mechanisms and the effects of different biological entities, such as genes, proteins, and medications, on the cellular and biological functions of wound healing is of paramount importance for the development of effective therapeutic interventions. In this paper, we present a text-mining approach aimed to explore and unravel the complex regulatory relationships of genes, proteins, and medications with the biological mechanisms of wound healing. Our approach relies on a set of predefined dependency rules to capture the relationships between biological entities and their target functions from text. By leveraging advanced AI technology like Generative Pre-trained Transformer 4 (GPT-4), also known as ChatGPT, we evaluate the accuracy and quality of the extracted relations. We present a detailed discussion of the encouraging preliminary results that validate the efficacy of our model in identifying potential therapeutic targets in the complex biological system.
The amount of data being generated during recent times has been exponentially huge. The data mainly comprises of unstructured data in the form of textual information like emails, tweets, articles etc. To gain information from these textual data, traditional way of analyzing cannot be used. There is a need for efficient techniques for analyzing these data. Text mining is defined as the process of transforming this unstructured data into understandable and meaningful information. Text Mining is a subfield of Artificial Intelligence which aims to automatically process the data and gain insights from the huge voluminous data. In this paper, several techniques used for classifying the data have been discussed. An overview about the dimensionality reduction methodology and how it can enhance the categorization process has been highlighted. It also aims with a future research scope in extending this categorization process along with dimensionality reduction procedures.
In this paper we discuss the case study approach to learning Text Mining techniques. We propose a novel framework that supports case-based learning and implements the SMART-goal setting methodology.
Information Retrieval (IR) involves several disciplines to improve the search quality. In this paper, we focus on terminology extraction which includes critical tasks especially for highly ambiguous languages like Arabic. Properly retrieving a list of relevant terms for a given domain and enrich it is a persistent problem. Therefore, and in order to improve the coverage of terminology extraction and enrichment, we must strengthen our research by text-mining technologies based on well-founded methods. In this article, we present a new Arabic terminology extraction and enrichment approach which exploits the corpus structure as a first step to extract a minimal terminology and uses text-mining techniques to enrich it in a second step.
Technology infrastructure (TechInfra) refers to metadata describing an academic field, such as journals & conferences, authors, publications and organizations. Understanding the TechInfra is often the first step in performing a literature review on a particular topic. In this paper, a study is conducted to retrieve TechInfra for a topic in supply chain management, namely, last mile logistics. Google Scholar is used as the primary tool for data collection. The first 1,000 results returned by Google Scholar are downloaded as HTML files. Subsequently, various application programming interfaces (APIs) - e.g., ScienceDirect, IEEE, CrossRef APIs - are used to enhance the data quality. Some plots are used to provide visualization of TechInfra of last mile logistics.
Today, it is estimated that over 80% of data is unstructured and mostly in text format. Contributing to it is the rise of Internet usage and content from individuals, media, as well as academic, commercial, industrial, and financial organizations and corporations [2], [3], [11]. Thus, an indispensable need for text classification or categorization in order to distill information from this overwhelming amount of data. Additionally, as more corporations outsource their services, web services have become preferable due to its versatility and ease of integration to existing hardware and software. This project aims to create a reusable text classification service for Bahasa Indonesia, a language spoken by over 20 million people, yet any text processing for it is still uncommon, inaccessible, or costly. This text classifier will use Naïve Bayes Classification, a simple yet preferable method due to its computational simplicity and effectiveness. To test the functionalities and efficacy of the machine, the study used data from web articles, reaching an accuracy of 83.75%.
Clinical sources of information are markedly increasing in both volume and variety. A significant portion of the valuable data resides in the unstructured or semi-structured clinical text of documents stored in disparate repositories or embedded in HL7 messages. Clinical documents such as discharge summaries, prescriptions, lab reports, and free-form physician notes are filled with abbreviations, acronyms, misspellings, and ungrammatical phrases. However, synoptic reporting methods are restrictive for health care practitioners who wish to express critical and comprehensive patient information in electronic medical records. Furthermore, they have been superseded by systems that use natural language processing (NLP) to extract clinical concepts from free-form text. To address the growing need for efficient NLP solutions that can handle the volume and variety of clinical text, we have developed an optimized rules-based clinical concept extractor called TRACE (Tactical Rules-based AQL Clinical Extractor) using the Annotation Query Language (AQL). We present the experience we have gained applying text mining tools to this challenging domain, as well as a comparison of our solution to cTAKES (clinical Text Analysis and Knowledge Extraction System), an open-source clinical text miner, on a set of prescription documents. We also describe how efficient and scalable clinical text mining techniques will improve several of our company's offerings.
This research proposes a new analysis approach for economic phenomena, including data from news and social networks as external information to predict commodity values (LBMA GOLD and Brent oil) and the USD/COP currency, clas-sify the sources of information and model multi-agent systems. Information was collected from 166 news sources through RSS and Twitter for 8 months (from October 2020 to May 2021). Linear regressions and assembly machine learning techniques such as XGBoost and Random Forest are used to predict daily changes. The analysis is complemented with the construction of a socio-inspired multi-agent system that evolves using external information, at the end presents patterns typical of complex systems.
High-utility pattern mining, the process of searching highly relevant patterns in databases. Pattern mining has gained popularity as a study area due to its many appliance in fields like bioinformatics, text mining, product recommendation, e-learning, and web click stream analysis. Effective decision- making requires high utility patterns, and demand for these patterns has grown recently. By classifying HUPM algorithms into a wide taxonomy, current comprehensive survey offers a overall, broad and strategic summary of advanced approaches in UPM. With a discussion of their benefits and disadvantages, a detailed analysis of current high-utility pattern mining techniques is offered. The study also covers current issues and practical difficulties in this field.
The huge collection of text available represents a remarkable challenge to process and exploit it in many fields. Therefore, there is a multitude of articles that are being proposed to summarize text automatically. More accurate and higher performing models are still required for text summarization. It is one of the most common tasks of text mining. In this paper, a novel Graph-based Growing self-organizing map for Single Document Summarization (GGSDS). GGSDS is an unsupervised extractive summarization approach composed mainly of five tasks: text pre-processing, document representation, sub-topics identification, sentence ranking and finally summary generation. The entire text of a document is represented in GGSDS by one accumulative graph. The choice of this representation model supports the extraction of all required features as to achieve the most suitable summary of text, especially the shared phrases between sentences. The impact of the sub-topics on the accuracy and comprehensiveness of the generated summary is taken into account in the design of GGSDS model. For this purpose, G-GSOM is employed to cluster sentences into clusters to represent the sub-topics of text. Next, sentences are scored using TextRank algorithm under the assumption that when a sentence has more relation with others, it is considered as more important and more representative to a sub-topic. Finally, the sentences with the highest score in each cluster are selected for generating the summary. Experimental results showed that GGSDS generated summaries of single documents with more than 80% accuracy of two datasets. Furthermore, these summaries covered most of the sub-topics of the documents.
This subject study the problem of the classification of texts. Classification (or categorization) of texts is one of the most common natural language processing (NLP) tasks. The objective of automatic classification is to automatically classify documents into categories that have been defined either beforehand by an expert, it is then a question of supervised classification or categorization, or automatically, it is then a question of unsupervised classification or even clustering. In our case, the categories are already defined, so we used supervised learning algorithms. The structure of the report consists of four chapters. The initial chapter provides an overview of data mining and text mining, which includes automatic natural language processing, definition, operation, fields of research, and applications. The second chapter discusses natural language processing (NLP) in detail, including its definition and workflow. The third chapter focuses on the fundamentals of text classification, such as learning techniques and the process of text classification. Finally, the last chapter is devoted to implementation and includes the development tools used as well as the realization of text classification.
Cyber frauds are a major security threat to the banking industry worldwide. Malware is one of the manifestations of cyber frauds. Malware authors use Application Programming Interface (API) calls to perpetrate these crimes. In this paper, we propose a static analysis method to detect Malware based on API call sequences using text and data mining in tandem. We analyzed the dataset available at CSMINING group. First, we employed text mining to extract features from the dataset consisting a series of API calls. Further, mutual information is invoked for feature selection. Then, we resorted to over-sampling to balance the data set. Finally, we employed various data mining techniques such as Decision Tree (DT), Multi Layer Perceptron (MLP), Support Vector Machine (SVM), Probabilistic Neural Network (PNN) and Group Method for Data Handling (GMDH). We also applied One Class SVM (OCSVM). Throughout the paper, we used 10-fold cross validation technique for testing the techniques. We observed that SVM and OCSVM achieved 100% sensitivity after balancing the dataset.
This paper develops and estimates an interest rate model with investor attitude factors, which are extracted by a text mining method. First, we consider two contrastive attitudes (optimistic versus conservative) towards uncertainties about Brownian motions driving economy, develop an interest rate model, and obtain an empirical framework of the economy consisting of permanent and transitory factors. Second, we apply the framework to a bond market under extremely low interest rate environment in recent years, and show that our three-factor model with level, steepening and flattening factors based on different investor attitudes is capable of explaining the yield curve in the Japanese government bond (JGB) markets. Third, text mining of a large text base of daily financial news reports enables us to distinguish between steepening and flattening factors, and from these textual data we can identify events and economic conditions that are associated with the steepening and flattening factors. We then estimate the yield curve and three factors with frequencies of relevant word groups chosen from textual data in addition to observed interest rates. Finally, we show that the estimated three factors, extracted only from the bond market data, are able to explain the movement in stock markets, in particular Nikkei 225 index.
A Data offers many facilities to the end users such as software, organization and platform go on. In this paper, we study about the wisely mining knowledge of social media. Social media becomes much popular from the health care information and Biomedical. This information is commonly shared so healthcare is improves and costs is decrease using opinion which is generated by user. We suggest investigation framework that give attentions on side effects of drugs and also focus on positive and negative response. To improve health care some Clinical documents are mostly useful because it's are free-text data sources. Clinical documents containing information related to symptoms and valuable medications. To extract a Data from large dataset it's become a very popular because users get various ideas from this filtered data. All Data Mining and Knowledge mining become popular because user are process on data and getting information of different area like health, Social, etc. After data processing we focus on users positive and negative opinions. We count this opinions and find out which medication is good, to decide this we also find out the side effects of the medications. Further we focus on the symptoms of the cancer patient. By taking the expert doctors suggestion, we list out the medication of the cancer according to the symptoms and we provide this medication or treatment to the user on our forum. We can expand our research into Data and Knowledge mining of social media and takes the users' views on various drugs of cancer. This daily updated data helps to pharmaceutical industry, doctors, hospitals, and medical staff, for effective future treatments.
Text Mining is an emerging area of research where the necessary information of user needs to be provided from large amount of information. The user wants to find a text P in the search box from the group of text information T. A match needs to be found in the information then only the search is successful. Many String matching algorithms available for this search. This paper discusses three algorithms in unique pattern searching in which only one occurrence of the pattern is searched. Knuth Morris Pratt, Naive and Boyer Moore algorithms implemented in Python and compared their execution time for different Text length and Pattern length. This paper also gives you a brief idea about time Complexity, Characteristics given by other authors. The paper is concluded with the best algorithm for increase in text length and pattern length.
We describe SyntacticDiff, a novel, general, and efficient edit-based method for transforming sequences of words given a reference text collection. These transformations can be used directly or can be employed as features to represent text data in a wide variety of text mining applications. As case studies, we apply SyntacticDiff to three quite different tasks, including grammatical error correction, student essay clustering and analysis, and native language identification, showing its benefit in each case. SyntacticDiff is completely general and can thus be potentially applied to any text data in any natural language. It is highly efficient, customizable, and able to capture syntactic differences from a reference text collection at the sentence, document, and subcollection levels. This enables both a rich translation method and feature representation for many text mining tasks that deal with word usage and syntax beyond bag-of-words.
The application of corporate sustainability in triple bottom line has been a critical issue to enterprises. It is a future trend for enterprises working and self-discoursing on their corporate sustainability, and there are many reports about enterprises' efforts on corporate sustainability in developed economy. The research purpose of this study is (1) to explore the three different industrial sectors' context of factors of Taiwan enterprise which empathizes on corporate sustainability report in economic, environment, and social dimensions, and (2) to understand how different the leaders' viewpoints of enterprises in three different industry sectors toward corporate sustainability are, namely; finance, insurance and real estate; wholesale, retail and catering industry; transport, storage and communication industry. This study applies the text mining as a tool in the information system to explore the annual corporate sustainability reports in Taiwan. As the findings, it can learned that (1) the industrial context of Taiwan enterprises on corporate sustainability report in economic, environment, and social dimensions; and (2) there are signification differences in leaders' viewpoints in corporate sustainability report from the three different industries sector.
In this paper we mainly focus on the techniques of data mining such as clustering, classification etc. In today's strategy it becomes a hectic task to gather, analyze and extract huge amount of datasets. So we use many efficient methods for the practical integration of the data. Some of the main techniques are fuzzy set theory, approximate reasoning, genetic algorithms etc. It is also useful for transformation to many fields and also decision making. It also enhances Knowledge discovery database(KDD) for retrieving the information from any kind of formats like graph, flow chart, video etc. This mainly focuses on the data mining methodologies to handle the huge amounts of data in logical and systematic manner.
Text Summarization is the technique of extricating notable data from the first content archive. In this procedure, the separated data is produced as a consolidated report and introduced as a clearly expressed rundown. Text Summarization can be broadly classified into Extractive Summarization and Abstractive Summarization. This paper describes an algorithm based on a combination of both these approaches. Initially, essential sentences are identified and stitched together to form a consolidated report. The significance of a sentence is chosen in light of measurable and semantic highlights of sentences. This shorter representation is then passed through an Encoder-Decoder model to generate a concise summary representing the whole article. The proposed model is capable of effectively creating a concise summary, which is semantically and linguistically correct, by understanding the whole content and reletting it in its own words. The proposed methodology focuses only on the relevant sentences and passes it to the Bi-Directional RNN for identifying and representing the core idea of the article.
This paper introduces three classic models of statistical topic models: Latent Semantic Indexing (LSI), Probabilistic Latent Semantic Indexing (PLSI) and Latent Dirichlet Allocation (LDA). Then a method of text classification based on LDA model is briefly described, which uses LDA model as a text representation method. Each document means a probability distribution of fixed latent topic sets. Next, Support Vector Machine (SVM) is chose as classification algorithm. Finally, the evaluation parameters in classification system of LDA with SVM are higher than other two methods which are LSI with SVM and VSM with SVM, showing a better classification performance.
Investors have to deal with an increasing amount of information in order to make beneficial investment decisions. Thus, text mining is often applied to support the decision-making process by predicting the stock price impact of financial news. Recent research has shown that there exists a relation between news article sentiment and stock prices. However, this is not considered by previous text mining studies. In this paper, we develop a novel two-stage approach that connects text mining with sentiment analysis to predict the stock price impact of company-specific news. We find that the combination of text mining and sentiment analysis improves forecasting results. Additionally, a higher accuracy can be achieved by using finance-related word lists for sentiment analysis instead of a generic dictionary.
Web mining is the application of data mining technique to automatically discover and gathered information from web documents and services which can be in structured, unstructured or semi-structured form. It is used to understand user behaviour, evaluate the effectiveness of a particular web and find out the relevant and efficient results from the web. Accuracy and relevance of information extracting from the web is the most significant issue of concern for the realization of web mining. The idea is to improve the accuracy and relevance of information extracting from the web. This paper proposes a novel model for improving the web mining, we hypothesize that web mining is Semantic-Synaptic web mining. Semantic-Synaptic web Mining interlinks the web of data to different data sources at low entropy (Information Theory). This paper combines the best ideas from the semantic web and synaptic web at low entropy and constructs the architecture of Semantic-Synaptic web mining.
Text mining applications became important in various intelligent tasks. Text documents are the most materials that record many important procedures in various worldwide organizations and different people cultures. Text poetry is an important type of people culture and education domains media. Arabic text poems classification is a few experimented fields, however, it has an important presence and special influence. Both new and ancient Arabic poetry has the same unique approach for rhythmical harmony measure, which can be used for identifying Arabic poems types. Deep learning as a machine learning method has many distinctive achievements in many areas, as well as, text classification tasks. In this paper, Arabic poetry text is categorized. A customized feature selection is proposed, which is fused with a clustering technique for enhancing models efficiency. Deep learning has experimented alongside two popular machine learning techniques; support vector machine and decision tree. The proposed feature extraction method has achieved high accuracy with all three techniques. The results are better than many related works.
A text mining based identification model for urban rail transit system infrastructure fault analysis is proposed for the Fault analysis and provide basis for fault prevention, control and diagnosis. The proposed model uses the Naive Bayes algorithm is used to segment the long text data which has been preprocessed, and identifies the cause of infrastructure fault. The analysis results are displayed clearly in the form of word clouds. The proposed model was experimentally verified by using 15 240 records of AFC equipment faults (defects) records from a certain urban rail transit group. in 2018.
Customers' conversations on social media could shed light into their experience, opinions, feelings, and concerns. To gain valuable knowledge about customers, it becomes increasingly important for businesses to collect, monitor, analyze, summarize, and visualize user-generated data on social media. However, analyzing social media data is challenging due to the vast increase of social media data. In this paper, we present a workflow approach of using established natural language preprocessing, text mining and sentiment analysis techniques to analyze social media data. Then we conducted two experiments to verify the effectiveness of our workflow approach by mining online movie reviews. The results show that our workflow approach is effective in analyzing social media data.
Financial Text Documents is one of the research domain build variety of applications. Mostly text processing techniques solve financial text problems. Dimensionality reduction process one the major challenging in text processing. Text features retained it helps to performing defines text clustering. Text clustering of similarity measure treats as the similarity between two text documents. The Main objective this paper defines and design suitable similarity measure motived from [18]-[21] and proposed similarity measure improves the previous measure [34]. This processes representation of financial system features initialized process based defined as Clusters.
Text mining is a growing field of applications, which enables the analysis of large text data sets using statistical methods. In recent years, exponential increase in the size of these data sets has strained existing systems, requiring more computing power, server hardware, networking interconnects, and power consumption. For practical reasons, this trend cannot continue in the future. Instead, we propose a reconfigurable hardware accelerator designed for text analytics systems, which can simultaneously improve performance and reduce power consumption. Situated near the last level of memory, it mitigates the need for high-bandwidth processor-to-memory connections, instead capitalizing on close data proximity, massively parallel operation, and analytic-inspired functional units to maximize energy efficiency, while remaining flexible to easily map common text analytic kernels. A field-programmable gate array-based emulation framework demonstrates the functional correctness of the system, and a full eight-core accelerator is synthesized for power, area, and delay estimates. The accelerator can achieve two to three orders of magnitude improvement in energy efficiency versus CPU and general-purpose graphics processing unit (GPU) for various text mining kernels. As a case study, we demonstrate how indexing performance of Lucene, a popular text search and analytics platform, can be improved by an average of 70% over CPU and GPU while significantly reducing data transfer energy and latency.
Dependency Structure Matrix (DSM) is a square matrix, used to map the dependencies between each entity/element. Such dependencies are examined to make out impacts. In general, college libraries maintain e-books for various departments, as students, research scholars in these days have started to refer e-books in most of the times. Since we don't have hard copies for some of the books, we go for searching e-books. This paper primarily focuses on finding related/linked/correlated/interrelated e-books that for a targeted e-book based on table of contents of each book. Related books will hold desired content that we require. Extract content page to ascertain the classes of e-books, ontology that provides inferences, regarding them. Subsequently, uses this ontology to discover the dependencies between each book and construct DSM which is used to represent related books as a result of comparing table of contents of each book. Subsequent to construction of DSM, it is converted into graph structure along with dependency level for easy analysis and find out. During arrival of fresh e-book to library, DSM is automatically updated by mining PDF and the graph is also updated along with their dependency level.
The present paper discusses the use of text mining to support the design of a pedagogical agent that mediates synchronous online discussions of academic texts by undergraduate students of English as a foreign language. The pedagogical agent proposed here has the instructional role of a tutor collaborator that participates in the chat discussion following mediation strategies grounded on sociocultural theory to assist the collective writing. Furthermore, we propose a pedagogical agent model that uses text mining techniques to identify when students deviate from the main topics that should be addressed in their discussions in a real time chat. Another important function enabled by the use of the text mining tool involves the assessment of the discussion relevance in relation to the base text, which supports the pedagogical agent decision towards a more adequate intervention. The conception of using text mining to guide the pedagogical agent in the mediation of the students' discussions has been based on previous research that has already shown how this particular mining tool could support educators' work in the evaluation of essays and of students' contributions in discussion forums. Preliminary results of this study are also presented showing the agent's potential to foster students' online conversations.
Bali Island is the most popular tourist destination in Indonesia. Bali needs to make continuous quality improvements of its tourism industry by devoting particular attention to the hotel as an integral part of tourism. Through hotel user reviews, hotel managers gained insight about the hotel condition that was perceived by the users. based on online reviews in Tripadvisor.com, this study used text mining approach and aspect-based sentiment analysis to obtain hotel user opinion in the form of sentiment. Aspect-based sentiment analysis is able to provide information that is not provided by the typical sentiment analysis. To perform these tasks, this study tries to apply the Recursive Neural Tensor Network (RNTN) algorithm, which was commonly used for classifying sentiment in sentence level. With the average accuracy of 85%, the proposed algorithm performed well in classifying the sentiment of words or aspects. Moreover, the output can be used for evaluation in improving the quality of the hospitality industry as well as supporting the tourism industry in Indonesia.
Text classification (TC) is a task that assigns a text to one or more classes and predefined categories. Constructing text classifiers with high accuracy is a vital task in biomedical field, given the wealth of information hidden in unlabelled documents. Because of large feature spaces, traditionally discriminative approaches, such as logistic regression and support vector machines with n-gram and semantic features have been utilized for biomedical text classification. In this study, we propose Recurrent Convolution Neural Networks (RCNN) based automated technique for classifying protein-protein interaction (PPI) articles. In RCNN model we utilized a recurrent structure to detain the contextual information from word embedding features. Max pooling layer was configured to extract important semantic keywords from the text. We evaluated our approach on two benchmark PPI datasets BioCreative II and BioCreative III. An experimental results show that RCNN based protein-protein interaction classification approach performs better than other state of the art approaches.
The text mining techniques are utilized to extract interesting knowledge or information from the text documents. The process of finding accurate knowledge in text documents represents the main challenge for the users. Since many existing techniques of text mining encompassed term-based approaches, these techniques face the issues of synonymy and polysemy. Over the years, the researchers have discovered the assumption that pattern-based methodologies must provide better performance than the term based ones. This paper proposes an effective and innovative technique for pattern discovery that contains the pattern taxonomy model and Levenshtein edit distance algorithms for improving the efficiency of utilizing and updating the obtained patterns to find interesting and relevant information. This system shows that the accuracy of PTM is 98.09% for a specific dataset.
During the last decade, Peruvian government started to invest and promote Science and Technology through Concytec(National Council of Science and Technology). Many programs are oriented to support research projects, expenses for paper presentation, organization of conferences/ events and more. Concytec created a National Directory of Researchers(DINA) where professionals can create and add curriculum vitae, Concytec can provide official title of Researcher following some criterion for the evaluation. The actual paper aims to conduct an exploratory analysis over the curriculum vitae of Peruvian Professionals using Data Mining Approach to understand Peruvian context.
The Internet continues to grow at a phenomenal rate and the amount of information on the web is overwhelming. It provides us a great deal of information resource. Web text feature extraction is considered as the main problem in text mining. We use Vector Space Model (VSM) as the description of web text and present a novel feature extraction algorithm which is based on the improved Bean optimization Algorithm (BOA). This algorithm will greatly improve the efficiency of web texts processing.
Software Bug Localization (SBL) is a task of locating the buggy source code. There are various ways of doing SBL and one of them is static SBL which utilizes the power of Text Mining (TM) in association with software repositories. Most of static SBL models are based on Information Retrieval (IR) methodology in which bug report works as a query and source code as database. In this paper we review state of the art SBL models which uses text mining techniques as their back bone in conjunction with other techniques. Essential features are extracted and summarized with the help of tabular representation. Aim of doing this study is to find the gaps in previous SBL models for proposing a novel SBL model in future.
The quality of discovered related features in text documents are describing based on user preferences. For the reason that of large scale terms and data patterns. Most existing popular text mining and classification methods have adopted term-based approaches. Most of the problems are occurred in polysemy and synonmy. Over the years, there has been repeatedly held the hypothesis that pattern-based methods should achieve better than term-based ones. Big challenge is how to effectively use large scale patterns vestiges a hard problem in text mining. In this paper, the robustness is used to discuss the characteristics of a model for describing its training sets is distorted or the application environment is altered. A new model robust if it still provides satisfactory performance regardless of having its training sets are altered or changed. To make a breakthrough in this challenging issue, this paper presents a pioneering model for weight feature discovery. It discovers both positive and negative patterns in text documents as at a higher level features and deploy them over low-level features. The terms also classify into categories and updates term weights depends on their specificity and their distributions in patterns. Significant experiments using this model on RCV1, TREC topics and Reuters-21578 significant experiments using this model on RCV1, TREC topics and Reuters-21578 demonstrate that the proposed model significantly outperforms both the state of the term-based methods and the pattern based methods.
This study utilizes text mining techniques, including sentiment analysis, analytical methods, and machine learning models (such as Support Vector Machine, Random Forest, and Naive Bayes), to delve into customer perceptions of healthy and natural food. Through an analysis of user sentiments on diverse social media platforms such as Twitter, Facebook, and weblogs, we explore consumer viewpoints. Our findings reveal that consumers generally hold favorable views towards organic food, with sentiment variations across different sources. Additionally, we investigate whether fresh produce is perceived as healthier, tastier, and more organic compared to conventionally grown alternatives. The outcomes highlight the growing popularity of organic food and suggest further investigation into overall consumer perceptions of organic produce.
This work presents frances, an integrated text mining tool that combines information extraction, knowledge graphs, NLP, deep learning, parallel processing and Semantic Web techniques to unlock the full value of historical digital textual collections, offering new capabilities for researchers to use powerful analysis methods without being distracted by the technology and middleware details. To demonstrate these capabilities, we use the first eight editions of the Encyclopaedia Britannica offered by the National Library of Scotland (NLS) as an example digital collection to mine and analyse. We have developed novel parallel heuristics to extract terms from the original collection (alongside metadata), which provides a mix of unstructured and semi-structured input data, and populated a new knowledge graph with this information. Our Natural Language Processing models enable frances to perform advanced analyses that go significantly beyond simple search using the information stored in the knowledge graph. Furthermore, frances also allows for creating and running complex text mining analyses at scale. Our results show that the novel computational techniques developed within frances provide a vehicle for researchers to formalize and connect findings and insights derived from the analysis of large-scale digital corpora such as the Encyclopaedia Britannica.
In recent years, Big Data analytics has gained widespread recognition and been applied in various fields, including marketing, finance, and medical domain. Text analytics, a subfield of Big Data analytics, has been a trending topic due to the emergence of fast improving Natural Language Processing (NLP) techniques and the increasing amount of text data on the web, such as blogs, forums, social media, and reviews. Leveraging text data presents an excellent opportunity for enterprises to acquire valuable insights and make data-driven decisions. However, there is no one-size-fits-all solution for companies adopting Text analytics. Strategies for implementing algorithms and tuning parameters may vary depending on different domains and scenarios. This paper introduces a web-based workflow system called Text Analytics and Retrieval Flow (TARFlow) for non-technical people to make use of NLP and machine learning techniques without programming, enabling them to concentrate on generating insights.
In the Constitution of India, a provision is made for each of the Indian states to choose their own official language for communicating at the state level for official purpose. The availability of constantly increasing amount of textual data of various Indian regional languages in electronic form has accelerated. So the Classification of text documents based on languages is essential. The objective of the work is the representation and categorization of Indian language text documents using text mining techniques. Several text mining techniques such as naive Bayes classifier, k-Nearest-Neighbor classifier and decision tree for text categorization have been used.
The function of social media is now transforming to become source of information, even to support electronic word of mouth (e-WOM). Many companies, including ride-hailing service providers, can capture customers' opinions for the purpose of evaluating their products and services. Text mining can be useful to analyze great number of comments from ride-hailing customers in social media. Furthermore, by applying sentiment analysis, service providers can define the service categories which are good and still needing improvement. Customers' comments were taken from Twitter, and text classification method was used to classify the comments based on six predefined categories and their respective polarity. The accuracy of the classification model was 86% which was good to classify the text data. The output of this research is expected to give insight for ride-hailing service provider to understand customers' perspective about the services so that it will be easier to evaluate and improve their services based on the categories in this study.
With the advent of social media, our online feeds increasingly consist of short, informal, and unstructured text. This textual data can be analyzed for the purpose of improving user recommendations and detecting trends. Instagram is one of the largest social media platforms, containing both text and images. However, most of the prior research on text processing in social media is focused on analyzing Twitter data, and little attention has been paid to text mining of Instagram data. Moreover, many text mining methods rely on annotated training data, which in practice is both difficult and expensive to obtain. In this paper, we present methods for unsupervised mining of fashion attributes from Instagram text, which can enable a new kind of user recommendation in the fashion domain. In this context, we analyze a corpora of Instagram posts from the fashion domain, introduce a system for extracting fashion attributes from Instagram, and train a deep clothing classifier with weak supervision to classify Instagram posts based on the associated text. With our experiments, we confirm that word embeddings are a useful asset for information extraction. Experimental results show that information extraction using word embeddings outperforms a baseline that uses Levenshtein distance. The results also show the benefit of combining weak supervision signals using generative models instead of majority voting. Using weak supervision and generative modeling, an F1 score of 0.61 is achieved on the task of classifying the image contents of Instagram posts based solely on the associated text, which is on level with human performance. Finally, our empirical study provides one of the few available studies on Instagram text and shows that the text is noisy, that the text distribution exhibits the long-tail phenomenon, and that comment sections on Instagram are multi-lingual.
With the growing importance of textual data processing, sentiment analysis which is a field of text mining, has been widely researched. But it is insufficient for the detection of human emotions. Emotion detection, an extension of sentiment analysis, has proven to be one of the most important areas in text mining, especially in the field of human-computer interactions. The recent works on emotion detection primarily focus on facial expressions, voice, audio and gestures. However, the content on the web is mostly text-based and it becomes difficult to capture the human emotions in the absence of facial and audio aspects in the data. Therefore, there is a need to design efficient mining techniques for processing textual data. Traditional approaches overlook disambiguation and ignore the presence of multiple emotions in text. In this paper, we propose a hybrid model which uses rules, sentiments and context for the disambiguation of words by using sentence transformers which recognize the various emotions involved by using natural language processing, sentence embeddings, BERT and similarity techniques so as to overcome such shortcomings. Our work ensures that Ekman's emotions along with neutral emotion are identified such that multiple emotions are tagged precisely based on the context. This hybrid method has proven to be far superior than existing approaches for the detection of multiple emotions.
The Naval Air Systems Command (NAVAIR) produces and supports highly complex aircraft weapons systems which provide advanced capabilities required to defend U.S. freedoms. Supporting said complex systems such as the MV-22/CV-22 aircraft requires being able to troubleshoot and mitigate complex failure modes in dynamic operational environments. Since an aircraft is comprised of multiple systems designed by specialty sub-vendors and subsequently brought together by an aircraft integrator, diagnostics at the aircraft level are usually “good enough” but not capable of 100% fault isolation to a single component. Today's system components must be highly integrated and are required to communicate via high speed data-bus conduits which require precise synchronization between systems. Failure modes of aircraft are identified via design, analysis and test prior to fielding of the weapon system. However, not all failure modes are typically known at the time of system Initial Operational Capability, but rather are found in the field by maintainers/pilots and then subsequently mitigated with aircraft engineering changes or system replacements. Also, the requirement for increased capabilities can drive the need for new systems to be integrated into an aircraft system that may not have been considered in the initial design and support concept. There is a plethora of maintenance action detail collected by pilots, maintenance officers (MO) and engineers that can and should be used to identify failure mode trends that come to light during the operational phase of an aircraft. New troubleshooting techniques can be developed to address underlying failure modes to increase efficiency of future maintenance actions thus reducing the logistics trail required to support the aircraft. The elements available for analysis are maintenance results input by the MO/pilot, (including free form comments regarding problems and resulting actions), Built-In-Test (BIT) fault codes recorded during a flight... (Show More)
This paper involves deriving high quality information from unstructured text data through the integration of rich document representations to improve machine learning text classification problems. Previous research has applied Neural Network Language Models (NNLMs) to document classification performance, and word vector representations have been used to measure semantics among text. Never have they been combined together and shown to have improved text classification performance. Our belief is that the inference and clustering abilities of word vectors coupled with the power of a neural network can create more accurate classification predictions. The first phase our work focused on word vector representations for classification purposes. This approach included analyzing two distinct text sources with pre-marked binary outcomes for classification, creating a benchmark metric, and comparing against word vector representations within the feature space as a classifier. The results showed promise, obtaining an area under the curve of 0.95 utilizing word vectors, relative to the benchmark case of 0.93. The second phase of the project focused on utilizing an extension of the neural network model used in phase one to represent a document in its entirety as opposed to being represented word by word. Preliminary results indicated a slight improvement over the baseline model of approximately 2–3 percent.
This paper uses text mining techniques in order to carry out an analysis of the monetary policy decisions of the National Bank of Romania (NBR). The analysis highlights the main topics of debate of the NBR Board that revolve around inflation, reference rates for monetary policy, rising consumption and the current account deficit and the need to keep current monetary policy rates constant in the regional economic context that characterized the year 2019 and the massive shift in the beginning of 2020 with the start of the coronavirus outbreak.
Now a days, huge amount data has generated on the internet and it is important to extract useful information from that huge data. Different data mining techniques are used to extract and implement to solve divers types of problems. In the era of News and blogs, there is need to extract news and need to analyze to determine opinion of that news reviews. Sentiment analysis finds an opinion i.e. positive or negative about particular subject. Negation is a very common morphological creation that affects polarity and therefore, needs to be taken into reflection in sentiment analysis. Automatic detection of negation from News article is a need for different types of text processing applications including Sentiment Analysis and Opinion Mining. Our system uses online news databases from one resources namely BBC news. While handling news articles, we executed three subtasks namely categorizing the objective, separation of good and bad news content from the articles and performed preprocessing of data is cleaned to get only what is required for analysis, Steps like tokenization, stop word removal etc. The currently work focuses on different computational methods modeling negation in sentiment analysis. Especially, on aspects level of representation used for sentiment analysis, negation word recognition and scope of negation and identification.
The information momentum available on social media is an appropriate environment for identifying users' reactions and attitudes towards a particular topic, products, or any issues. To analyze this data and extract useful information, machine learning algorithms are used to categorize data into predefined categories. Analyzing data in the Arabic language is a challenge, and few studies focus on Arabic text mining. This paper focuses on sentiment analysis of Arabic tweets, in which, it conducts a performance comparison between three machine learning classifiers; Logistic Regression (LR), K-Nearest Neighbors (KNN) and Decision Tree (DT). Four Arabic text datasets are used in the experiments to evaluate the performance of the classifiers. For comparing purpose, we used four evaluation metrics: recall, precision, f-measure, and accuracy. The results show that the Logistic Regression achieves a better accuracy rate in the case of large datasets (93%) compared with the other classifiers. LR showed more improvement by increasing the volume of data, unlike other classifiers that recorded a noticeable decrease in accuracy in the last database (74% for KNN and DT when applying on 100K reviews dataset). Also, KNN and LR classifiers outperform DT classifier when applying them on small datasets such as AJGT and ASTD datasets.
Online hotel reservation system is commonly used for travel planning. We usually refer to users' reviews before making reservation. However, most of reviews are not classified appropriately for travel objectives so far, and make it difficult to read whole reviews promptly. For those backgrounds, this paper provides analyzing method for use reviews to find out the characteristics of expressions according to their purposes, and examined some methods to present beneficial information at the time of reservation. Moreover, we extract the differences of user reviews by areas and each accommodation through text mining. From the results of our analysis, we find differences between the expressions and the evaluations each accommodation.
Concept extraction is one of the important parts in ontology learning approach in order to construct ontology. This step becomes an inevitable process in such ontology development and becomes a seed to the next step in the approach. Studies in unstructured text especially in Malay are quite little due to the lack of availability extraction tools to extract relevant information. In addition, studies in Malay unstructured text based on Al-Quran is very rare compared to English and Arabic text. In this paper, an early experiment was carried out to extract concepts from Malay translation of Quranic texts using several rules-based formulated by Malaysian government body that responsible for Malay language. The study focuses only noun phrases in the Malay Quranic Text which involves only twenty- two short chapters or surah that comprises from Surah Ad-Dhuha to An-Nas. There are two criteria that have been tested in terms of symbols and stop words. The result of noun phrases has been extracted range from a combination of one word to twelve words. It shows that all the extracted noun phrases are not sufficient enough to extract most of the important concept in Al-Quran.
Electronic health record (EHR) serves to capture the patients' medical conditions and detailed visits information. The structured part of EHR can be used to serve administrative and financial management of patients. However, the unstructured part, that contains the interventions applied to the patient and transcribed in textual format, have unexplored knowledge due to the narrative format. Unlike conventional data mining techniques that can be applied to structured databases, applying large and automated analysis on clinical notes can potentially provide better support for medical decision making for an individual patient or for collective of patients. It can also allow for discovery of emerging associations to explore relationships among the patients using the data stored in the unstructured text of the EHR.
The field of law in Morocco is behind on progress to other fields, where the archiving of legal documents is done manually and the access to the information is increasingly complex. In the scope of this work, we investigate the application of text classification methods to support law professionals, by proposing a system able to automatically select the class of a judgement. We will build four models which are Naïve Bayes, K-Nearest Neighbor, Support Vector Machine and Decision Tree to classify Arabic legal documents, which are tried by the Moroccan court.
The development of the World Wide Web it is no longer feasible for a user can understand all the data coming from classify into categories. The expansion of information and power automatic classification of data and textual data gains increasingly and give high performance. In this paper five important text classifications are described Naive Bayesian, K-Nearest Neighbour, Support Vector Machine, Decision Tree and Regression. Which are categorized the text data into pre define class. The target of the paper is to study different classification techniques and finding the classification accuracy for different datasets. An efficient and effective text documents into mutually exclusive categories.
The Great East Japan Earthquake caused a fear of large earthquakes, tsunamis, and radiation disasters caused by nuclear accidents in the Japanese public. After a disaster, such as an earthquake, people in the disaster region continue to live an unstable life and are exhausted both mentally and physically. To more quickly regain stability, it is necessary to address the topics of anxiety. The purpose of this research is to use a feelings dictionary and text mining technique to study effective extraction methods for determining the issues causing anxiety in disasters. Therefore, we collected a massive amount of information consisting of questions and answers posted before and after three recent large Japanese earthquakes to an open site (i.e., a website in which anyone can post questions and answers freely) and analyzed the issues causing anxiety. The analysis results suggest that the topics of the questions can indicate the causes of anxiety at the time of disaster.
It is known as Benford's law that the distribution of the first digits forms a specific shape for natural numerical datasets. Deviation from the Benford's distribution indicates the irregularity of the dataset. However, it does not tell any clue to interpret the reason of irregularity. The present paper constructs a search engine of cells that appear in tables by correlating a cell with the words in the title of row or column or in the explanation of the table. We generate an exhaustive dataset of cells for testing irregularity by enumerating the search conditions. We applied the method to the number of applicants, the number of candidates, and the number of successful applicants in each department of 565 private universities in Japan. We confirmed the effectiveness of the proposed method by extracting the characteristics of the irregular datasets.
The document classification task is one of the widely studied research fields on multiple domains. The core motivation of the classification task is that the manual classification efforts are impractical due to the exponentially growing document volumes. Thus, we densely need to exploit automated computational approaches, such as machine learning models along with data & text mining techniques. In this study, we concentrated on the classification of medical articles specifically on common cancer types, due to the significance of the field and the decent number of available documents of interest. We deliberately targeted MEDLINE articles about common cancer types because most cancer types share a similar literature composition. Therefore, this situation makes the classification effort relatively more complicated. To this end, we built multiple machine learning models, including both traditional and deep learning architectures. We achieved the best performance (≈82% F score) by the LSTM model. Overall, our results demonstrate a strong effect of exploiting both text mining and machine learning methods to distinguish medical articles on common cancer types.
Twitter is one of social media with more than 500 million users and 400 million tweets per day. In any written tweet of Twitter users it contains various emotions. Most research on the use of social media classifies sentiments into three categories that are positive, negative, and neutral. However, none of these studies has developed an application that can detect user emotions in the social media, particularly on Twitter. Hence, this research developed a text mining application to detect emotions of Twitter users that are classified into six emotions, namely happiness, sadness, anger, disgust, fear, and surprise. Three main phases of the text mining utilized in this application were preprocessing, processing, and validation. Activities conducted in the preprocessing phase were case folding, cleansing, stop-word removal, emoticons conversion, negation conversion, and tokenization to the training data and the test data based on the sentiment analysis that performed morphological analysis to build several models. In the processing phase, it performed weighting and classification using the Naive Bayes algorithm on the validated model. The process for measuring the level of accuracy generated by the application using 10-fold cross validation was done in the validation phase. The findings showed that this application is able to achieve 83% accuracy for 105 tweets. In order to get a higher accuracy, one requires a better model in training data.
The news ecosystem has changed in the modern years from outdated print media to social media sites. Because social media platforms enable us to absorb news much more quickly and with less restrictive editing, fake news is disseminated at an astonishing rate and scale. More and increasingly individuals are using social media as the world becomes more digital since it makes connecting with others relatively simple. But false information is misguiding people. Although fake news is simple to propagate, its effects can be disastrous. Bogus news has frequently resulted in uncontrolled circumstances that killed numerous people. People with limited education might quickly become acclimated to bogus news. Instead of checking the accuracy of the information, they accept what is provided to them. This can be overcome by using Text mining, Statics concepts which can detect fake news more precisely when compared to several machine learning methods. This proposed system is on the analysis of fake news and detection of hatred news using t-distributed Stochastic Neighbor Embedding(t-SNE) to check dimensionality reduction and topic modeling using Latent Dirichlet Allocation(LDA) and data pre-processing using Latent Semantic Analysis (LSA).
Digital village construction plays a vital role in realizing rural revitalization and agricultural and rural modernization. Quantitative analysis of policies related to rural digital construction can provide important bases for the formulation and improvement of policies in the future. This paper adopts text mining method and combines social network analysis to study 213 digital village construction policy texts issued by national government departments from 2007 to 2021, and deeply analyzes the content characteristics and evolution rules of such policies. The results show that several small cooperative groups have been formed among the policy issuing agencies. They gradually refine the specific arrangements and measures of rural digitization construction on the basis of division of labor and cooperation. With the promotion of digital information technology, the government's formulation of policies related to rural digital construction has formed a change process from point to point. The policy focus has changed from agriculture to smart agriculture, emerging industry and information service industry, making the policy coverage more comprehensive and more in line with the actual situation of local villages. In the future, policies related to digital village construction pay more attention to the difficulties of development and the consolidation of construction achievements while comprehensively developing.
These days, the amount of virtual facts to be had has accelerated dramatically, making it difficult for people to manually examine and interpret this sizable quantity of information. To address this project, text mining techniques have emerged as a treasured solution for automatic records evaluation. Textual content mining is the procedure of extracting treasured insights and understanding from textual content-based totally records assets. It involves numerous strategies together with herbal language processing, machine learning, and statistical evaluation to pick out styles, relationships, and tendencies inside unstructured textual content statistics.one of the key advantages of textual content mining is its potential to manner and analyze massive volumes of textual content records in a brief time frame. This makes it especially beneficial for companies that deal with a high extent of unstructured text records, inclusive of social media posts, client critiques, and emails. By using automating the evaluation of this records, textual content mining lets in for a greater green and accurate understanding of consumer preferences, sentiment, and conduct. Further to its performance, text mining additionally affords a complete and unbiased evaluation of textual content facts. Not like manual evaluation, that's subjective and liable to human blunders, textual content mining is predicated on algorithms to extract insights, decreasing the danger of bias and mistakes. That is especially crucial for groups that need to make data-pushed choices primarily based on correct and independent data.
The deep transformation induced by the World Wide Web (WWW) revolution has thoroughly impacted a relevant part of the social interactions in our present global society. The huge amount of unstructured information available on blogs, forum and public institution web sites puts forward different challenges and opportunities. Starting from these considerations, in this paper we pursue a two-fold goal. Firstly we review some of the main methodologies employed in text mining and for the extraction of sentiment and emotions from textual sources. Secondly we provide an empirical application by considering the latest 20 issues of the Bank of Italy Governor's concluding remarks from 1996 to 2015. By taking advantage of the open source software package R, we show the following: 1) checking the word frequency distribution features of the documents; 2) extracting the evolution of the sentiment and the polarity orientation in the texts; 3) evaluating the evolution of an index for the readability and the formality level of the texts; 4) attempting to measure the popularity gained from the documents in the web. The results of the empirical analysis show the feasibility in extracting the main topics from the considered corpus. Moreover it is shown how to check for positive and negative terms in order to gauge the polarity of statements and whole documents. The evaluation of these synthetic indexes is quite relevant for increasing the transparency of the central banks' communications.
Text mining is a data mining technique to find hidden things from a set of data in the form of text. One of the things that can be obtained with text mining is opinion or sentiment, whether it is positive or negative. Positive sentiment is used as a reference for a subject or object from a collection of texts to be recommended. Java Island is the city with the most population in Indonesia with a variety of culinary delights. This study aims to analyze sentiment recommendations on culinary data from food or cuisine in big cities on the island of Java. Four big cities were selected, namely Jakarta, Bandung, Yogyakarta, and Surabaya. The data source is a tweet of culinary arts of the four cities from Twitter in Indonesia. The Sastrawi Library is used as a text mining data processing tool in Indonesia. The results obtained are the majority of positive sentiments from all cities. The cumulative sentiment value of the four cities is 54%, meaning that the big cities on the island of Java have good culinary delights and deserve to be recommended to be enjoyed.
In academic institutions, commercial enterprises, research centers, technology-heavy businesses, and government funding agencies, maintaining consistent data is a major difficulty. For an entity, which might be anything from an object to a place or thing, most data are irregular. These days, to identify significant patterns that represent the data, entity links in a dataset are investigated by text mining or data analytics. With this knowledge, alternatives are then taken. Analytics creates data and finds patterns by turning words into numbers. In the end, better data organization results in better conclusions. However, classifying and processing each piece of data by hand is difficult. As a result, in the domain of Natural Language Processing (NLP), which looks at grammatical and lexical patterns, intelligent text processing systems have emerged. Before mining, it's imperative to examine and comprehend the nature of the data. Text categorization requires automation because of the increasing volume of data and the requirement for accuracy or precision. It is an interesting study opportunity to develop automatic classifying texts with deep learning methods to handle difficult NLP tasks with semantic constraints. Text categorization is founded on data analytics, which can facilitate information discovery. The majority of the advantages can be obtained by applying these insights to emerging applications that support decision-making, improve resources. Improved techniques for parameter optimization demonstrating effective knowledge discovery will be the focus of future research studies.
As a notoriously lethal human disease, cancer has obtained much concern for a long time. There have accumulated huge amounts of literature and experimental data on cancer-related research. It is impossible for people to deal with these texts manually to discover novel information and knowledge. However, text mining has an advantage of extracting previously unknown and understandable knowledge from large amounts of texts, and forming well-defined knowledge, providing the possibility to fully taking use of the existed texts. With the proceeding of biomedical research, people have gradually realized that complex biological functions and the phenomenon of life are the results of complex interactions among a variety of biological entities, such as protein. Deeply studying protein interaction network is essential to understand life. We, adopting reinforcement learning idea, put forward an algorithm for protein interaction network constructing. With the algorithm, nodes are used to represent proteins and edges denote interactions. During the evolutionary process, a node selects with which nodes in the network it tends to interact. Keep selecting and carrying on iteration, until eventually attaining an optimal network. The network is the result of the dynamic nature of learning behavior. As a malignancy, prostate cancer has been concerned for a long time. We attain biological texts from PubMed and establish a prostate cancer protein interaction networks by the proposed methods. The results show that our proposed method is pretty good. Network topology analysis results also show that the network node degree distribution is scale-free.
The need for designing Arabic text mining systems for the use on social media posts is increasingly becoming a significant and attractive research area. It serves and enhances the knowledge needed in various domains. The main focus of this paper is to propose a novel framework combining sentiment analysis with subjective analysis on Arabic social media posts to determine whether people are interested or not interested in a defined subject. For those purposes, text classification methods- including preprocessing and machine learning mechanisms-are applied. Essentially, the performance of the framework is tested using Twitter as a data source, where possible volunteers on a certain subject are identified based on their posted tweets along with their subject-related information. Twitter is considered because of its popularity and its rich content from online microblogging services. The results obtained are very promising with an accuracy of 89 %, thereby encouraging further research.
Opinion Mining is a valuable knowledge resource to understand the collective opinions and to take better decisions. It is a Natural Language Processing (NLP) task that decides whether a text expresses positive or negative sentiment. Web contents are increasing rapidly and providing a huge number of information. It is an important research issue to analyze and organize these enormous information for better knowledge extraction. In this paper, we emphasis on opinion mining for Bangla text using web based diverse data. We apply both Linear and Nonlinear Support Vector Machine as machine learning technique and N -gram method to classify Bangla documents collected from social media sites. Most of works in this arena take a single word as a vector. Instead of thinking a single word as a vector, we used one vector containing more than one words using N-gram. N-grams of texts are extensively used in text mining and natural language processing tasks. We found better results using N-grams for different values of n.
This study at first used the text mining method to analyze the keywords of the Chinese news reports related to Macan's gambling industry from June to September 2012. The study got 19 major keywords at the first step. In order to comprehend the influence of each keyword in each document, the study applied the Fruit Fly Optimization Algorithm to evaluate the Keywords Frequency Composite Function. The contributions of the study included the following two points: First, the study applied the text mining method to get the 19 major keywords and found the public opinion focus during the time frame. Second, the study found the Fruit Fly Optimization Algorithm as a useful tool to evaluate the Keywords Frequency Composite Function.
Classification plays a vital role in many information management and retrieval tasks. This paper studies classification of text document. Text classification is a supervised technique that uses labeled training data to learn the classification system and then automatically classifies the remaining text using the learned system. In this paper, we propose a mining model consists of sentence-based concept analysis, document-based concept analysis, and corpus-based concept-analysis. Then we analyze the term that contributes to the sentence semantics on the sentence, document, and corpus levels rather than the traditional analysis of the document only. After extracting feature vector for each new document, feature selection is performed. It is then followed by K-Nearest Neighbour classification. The approach enhances the text classification accuracy.
The tax gives an important role for the contributions of the economy and development of a country. The improvements to the taxation service system continuously done in order to increase the State Budget. One of consideration to know the performance of taxation particularly in Indonesia is to know the public opinion as for the object service. Text mining can be used to know public opinion about the tax system. The rapid growth of data in social media initiates this research to use the data source as big data analysis. The dataset used is derived from Facebook and Twitter as a source of data in processing tax comments. The results of opinions in the form of public sentiment in part of service, website system, and news can be used as consideration to improve the quality of tax services. In this research, text mining is done through the phases of text processing, feature selection and classification with Support Vector Machine (SVM). To reduce the problem of the number of attributes on the dataset in classifying text, Feature Selection used the Information Gain to select the relevant terms to the tax topic. Testing is used to measure the performance level of SVM with Feature Selection from two data sources. Performance measured using the parameters of precision, recall, and F-measure.
Recently, deep learning has achieved impressive success in text mining and Natural Language Processing tasks. Bert is one of the remarkably rewarding deep learning models that is employed in a variety of NLP classification tasks such as intent and topic detection, question answering, sentiment analysis, hate speech detection, and so on. Plenty of studies have implemented different models of classification using pre-trained Bert models. Fine-tuning is done by adding either a simple fully connected layer, BiLSTM, convolutional layers, or a combination of them. Each of those models has fine-tuned Bert for a specific task. The results do not always approve neither the efficiency of using complex fine-tuned models of Bert nor the generalization of them. In this study, we extensively inspected various Bert-based fine-tuning models for different text classification tasks. Several types of fine-tuning Bert models varying in their classification layer are implemented and the performance of them is meticulously investigated. The implemented fine-tuning models are using alternatively deep learning networks such as convolutional networks and BiLSTM. The output layer of each model is studied to receive entirely varying inputs coming from the distinct layers of Bert. We conducted considerable experiments to find the most general outperforming model. We discover that adding a simple dense layer to the pre-trained Bert model, as a classifier, surpasses other types of deep neural network layers in the investigated tasks. We examine different values of hyperparameters to find the optimized combination providing the highest performance.
Microbial interaction network is the foundation to understand the structure and function of microbial communities. However, there is currently less a comprehensive dataset of microbial interaction network. Using text mining technology, available microbial interaction knowledges could be extracted automatically from unstructured biomedical text data. However, the existing biomedical relation extraction tasks can only identify binary relations among microorganisms without differentiating complex interaction types. In this paper, we proposes a computational framework for the task of multi-type microbial relation extraction based on transfer learning. Transfer learning models were applied on large-scale unlabeled texts from PubMed, and predicted 2,132 standardized multi-type microbial interaction relationships among 682 bacterial species.
The proportion of the stock range that is devoted to spare parts is often considerable in industrial context. Accordingly, even small improvements in forecasting spare parts demand might lead to substantial cost savings. Time series analysis has been the most popularly applied method in the prior spare part demand forecasting models. However, these approaches need to be improved in terms of prediction accuracy. In this study, we gathered component consumption data including structured and unstructured data from a spare part management information system in military logistics. We proposed demand forecasting models based on data mining and text mining techniques. The results show that our approach can improve the prediction performance compared to that of existing approaches.
Based on a large number of texts produced by railway traction power supply equipment during service, which is difficult to analyze and apply directly, a text information mining system framework for railway traction power supply equipment operation and maintenance is proposed. This paper covers the process of Data pre-processing, information extraction, analysis and application, and studies the key technologies of Chinese text segmentation, information extraction, status evaluation, etc. Finally, the system architecture and related key technologies are verified by actual traction power supply equipment operation and maintenance text data.
Researchers have collected Twitter data to study a wide range of topics. This growing body of literature, however, has not yet been reviewed systematically to synthesize Twitter-related papers. The existing literature review papers have been limited by constraints of traditional methods to manually select and analyze samples of topically related papers. The goals of this retrospective study are to identify dominant topics of Twitter-based research, summarize the temporal trend of topics, and interpret the evolution of topics withing the last ten years. This study systematically mines a large number of Twitter-based studies to characterize the relevant literature by an efficient and effective approach. This study collected relevant papers from three databases and applied text mining and trend analysis to detect semantic patterns and explore the yearly development of research themes across a decade. We found 38 topics in more than 18,000 manuscripts published between 2006 and 2019. By quantifying temporal trends, this study found that while 23.7% of topics did not show a significant trend (P = 0.05), 21% of topics had increasing trends and 55.3% of topics had decreasing trends that these hot and cold topics represent three categories: application, methodology, and technology. The contributions of this paper can be utilized in the growing field of Twitter-based research and are beneficial to researchers, educators, and publishers.
The amount of content on the web has increased dramatically since the Internet began providing users with the ability to produce content. Initial work on original text production has aimed at publishing the given data by putting in a certain mold. The most obvious example of this is the analysis reports on sporting events. However, preparing an original text compiled with general information about a subject has become a subject of interest to scientists as well. Although Neural Networks and Markov models were used previously for original text production, the original text generation process and comparison of the success rates weren't done using the Turkish language and the academic publication data repository dataset. In this study, it was tried to create summary information/original content about a specific subject by using Wikipedia TR for the Turkish language and the data pool created with hundreds of thousands of academic publications. In the study, texts were produced with Markov Model and LSTM, which were previously proposed, and the results are comparatively shared in detail. In the evaluation study, the performance of the proposed method was examined, and the correctness of the techniques was evaluated concerning syntactic accuracy and semantic preservation. The results are evaluated by presenting a mixture of original and machine-generated texts to the actual user for the success test of the proposed method. The success rate of the results is calculated with accuracy, recall, and f-measure. The results are very promising because it has been observed that the method can produce accurate and quality representations.
We propose a framework for adapting text mining models that discovers low-rank shared concept space. Our major characteristic of this concept space is that it explicitly minimizes the distribution gap between the source domain with sufficient labeled data and the target domain with only unlabeled data, while at the same time it minimizes the empirical loss on the labeled data in the source domain. Our method is capable of conducting the domain adaptation task both in the original feature space as well as in the transformed Reproducing Kernel Hilbert Space (RKHS) using kernel tricks. Theoretical analysis guarantees that the error of our adaptation model can be bounded with respect to the embedded distribution gap and the empirical loss in the source domain. We have conducted extensive experiments on two common text mining problems, namely, document classification and information extraction, to demonstrate the efficacy of our proposed framework.
External expert review analysis is a critical important task in funding agencies, such as Beijing Natural Science Foundation (BNSF). In this paper, a novel research framework is proposed for expert review analysis using reason mining. In the proposed framework, evaluation criteria ontology is firstly constructed, and then data pre-processing is carried out to refine the reviews. Finally, reason mining is offered to give the evidence of review scores and comments. Experimental results show the effectiveness and efficiency of the proposed framework. The proposed framework can be used in funding agencies to improve the evaluation quality of expert reviews for research project selection.
This paper introduces an innovative fusion of Latent Dirichlet Allocation (LDA) with fuzzy matching, aiming to elevate the expressiveness of topics in text mining. Traditional LDA techniques, while effective, often fall short in capturing the intricate semantic nuances and contextual variations of keywords within a given document. In this enhanced version, we propose a methodology that seamlessly integrates LDA with fuzzy matching algorithms, offering a more refined approach to topic modeling. The key distinction lies in the enhanced model's ability to not only identify keywords but also accommodate their diverse forms and variations, thus creating a more comprehensive and expressive representation of topics. The integration of fuzzy matching enriches the association between topics and the underlying text by considering partial matches and accounting for variations in spelling, structure, and context. This enhancement ensures a more accurate reflection of the intricate relationships between topics and the surrounding text, fostering a deeper understanding of the document's content. To validate the effectiveness of our proposed approach, we conducted extensive experiments on diverse textual datasets. Results demonstrate that the enhanced model consistently outperforms traditional LDA in terms of capturing topic expressiveness. By revealing the subtle nuances and contextual richness within documents, our methodology contributes to a more nuanced and insightful interpretation of textual data. This paper provides a significant advancement in the field of text mining, offering researchers and practitioners an enhanced tool for uncovering the hidden layers of meaning embedded in large corpora of text.
Cyberbullying has become intensive field of research, due to its major impact on society. Most researchers analyze causes and consequences of cyberbullying, however, only few try to improve software to reduce or stop cyberbullying, and make Internet a safer place. In this article, current review of efforts in cyberbullying detection using web content mining techniques is presented.
As an important means to extract valuable information from massive data, big data mining technical has been widely used in commercial and scientific research fields. In recent years, with the continuous acceleration of China’s globalization process, a large amount of OFDI data have emerged in business websites and related databases. Firstly, based on CiteSpace visualization software, this paper makes a bibliometric analysis of the research results of "China’s OFDI" in SSCI database and carries out text mining on the influencing factors of OFDI according to keywords co-occurrence network map and timezone map. Then, for further analyzing the factors affecting the survival of overseas subsidiaries, this paper uses R language web crawler technology to grab the list of overseas investment enterprises from the website of China’s Ministry of Commerce and constructs a small database of “going out” enterprises. Finally, the life table method and Cox proportional hazards regression model are used to make statistical and empirical analysis on the survival of overseas subsidiaries. The research shows the average survival duration of overseas subsidiaries is only 4.55 years, and the host country factors and the characteristics of the enterprises themselves will affect their long-term development.
This study focuses on extracting Unified Modeling Language artifacts for evaluation related to the consistency of interaction design and functionality flow of an application in the software development process. Specifically, the study focuses on extracting two types of artifacts, namely Activity Diagram and Use Case Description, with emphasis on the Step Performed section. As these two artifacts have varying rules and formats, the study proposes a text mining approach to extract that artifacts while standardizing the extraction format to enable the evaluation. The software requirement specification document for the Sipranta software development project is used as the research object, which focuses on managing growth and development data for toddlers in Integrated Health Posts (Posyandu). The extraction results for each artifact comprise eleven sets of extracted information, labeled as documents D1 to D11 for Activity Diagram and D12 to D22 for Step Performed. The reliability of the extraction results is assessed using two approaches: Python programming and expert questionnaires and is found to be in the “Almost Perfect Agreement” category with respective values of 0.8127 and 0.8930. The similarity measurement using cosine similarity based on the extraction results indicates that the two artifacts are aligned and consistent, with similarity scores above 0.85.
The Endesa Company is the main power utility in Spain. One of the main concerns of power distribution companies is energy loss, both technical and non-technical. A non-technical loss (NTL) in power utilities is defined as any consumed energy or service that is not billed by some type of anomaly. The NTL reduction in Endesa is based on the detection and inspection of the customers that have null consumption during a certain period. The problem with this methodology is the low rate of success of these inspections. This paper presents a framework and methodology, developed as two coordinated modules, that improves this type of inspection. The first module is based on a customer filtering based on text mining and a complementary artificial neural network. The second module, developed from a data mining process, contains a Classification & Regression tree and a Self-Organizing Map neural network. With these modules, the success of the inspections is multiplied by 3. The proposed framework was developed as part of a collaboration project with Endesa.
Court judgments from several nations have recently been published and made accessible online for study, adding to the growing number of online resources available today. As a recommendation, the legal databases can be incorporated into the training of crime scene investigators and made available as a learning resource. A text mining method was used to identify relevant documents, including crime scenes, and categorize them according to the kind of crime committed. The results from the statistical data support the possibility of useful information about crime scenes and murder cases in legal documents.
The main objective is to ensure the standards of every call center for its performance, services and get insights into customer's emotions involved with the company using different data mining techniques. As the call center is the second most important thing just after the actual product it becomes very important to deal with the agent's performance and different methodologies to improve services and decrease the volume of the call flow by improving services. To achieve this, have to make sure that the agent is performing his/her task appropriately and to validate Text mining will be used to be able to monitor every call made or received by an agent. The ability to get customer's emotional insights for every issue raised by customers and escalated by the agents and these issues will be termed as Focus points. Interaction mining will be the technology used for extracting emotional insights. So firstly when agents make an outbound call to the customer or will receive an inbound call by the customer, Speaker diarisation is used to solve the problem of who spoke when during the call to perform data mining techniques live, by converting speech to text for text mining and use emotion analysis for interaction mining. After converting speech to the text, different key-points are compared and the customer's emotional input to generate a report for every call.
Samtla (Search And Mining Tools with Linguistic Analysis) is an online integrated research environment designed in collaboration with historians and linguists to facilitate the study of digitised texts written in any language. It currently supports the research of two corpora: the Genizah collection held by the Taylor-Schechter Genizah Research Unit in Cambridge University, and a collection of Aramaic incantation texts from late antiquity. In contrast to standard search engines and text mining systems that rely on the bag-of-words representation of text, Samtla provides the retrieval and discovery of fuzzy text patterns/motifs (aka “formulae” to historians), which is achieved through applying a character-based n-gram statistical language model built on top of a powerful generalised suffix tree data structure. This paper brie y describes the major components of Samtla and their underlying techniques.
This article presents a study using Brazilian publicly listed companies' financial statements footnotes, in order to verify if these documents can provide information to predict variations in the debt of the corresponding firms. From a text mining perspective, we built classification models by assigning a class to each company in a period of time based on the variation of the debt to equity ratio. We conducted experiments using two different classifiers: random forest and support vector machine. The caret package in R language was used for this work. When we employed the random forest classifier, we got accuracies 4 percent points greater than the baseline accuracy, suggesting that the textual content of financial statements has some potential use for debt prediction.
Usage of social media is becoming an inescapable part of all of us. When people with similar interest interact with each other they eventually lead to a discussion. While a person expresses his point of view then there is an impact of his words those who are listening by having some perceptions in their mind. How a person perceives or accepts any view of some other person also has an impact on their thinking as well. They either have a positive impact or a negative impact on them, and this impact leads to some positive or negative comments by that person. And at certain stages the negativity in the group increases so much that it leads to some ill-impacts on the society.
Finding similar diagnoses for the same region are vital for patients. In this paper, we aim to find the similarity radiology reports based on bag-of-words (BoW) and Fuzzy C-Means Clustering methods. A double-layer structure is applied. Firstly, extracting features from data BoW method is applied and then Fuzzy C-Means algorithm is performed to cluster the blocks into the similar cluster and the non-similar cluster. 457 radiology reports were examined which were collected from a research and education hospital in Istanbul. Data were tested according to the 23 regions and 137 diagnosis. By the opinion of the radiologist a vocabulary consists of these regions and diagnosis were created. Experimental results on data sets have shown that for the standard documents BoW and Fuzzy C-Means Clustering can be used to find similarity.
As critical and sensitive systems increasingly rely on complex software systems, identifying software vulnerabilities is becoming increasingly important. It has been suggested in previous work that some bugs are only identified as vulnerabilities long after the bug has been made public. These bugs are known as Hidden Impact Bugs (HIBs). This paper presents a hidden impact bug identification methodology by means of text mining bug databases. The presented methodology utilizes the textual description of the bug report for extracting textual information. The text mining process extracts syntactical information of the bug reports and compresses the information for easier manipulation. The compressed information is then utilized to generate a feature vector that is presented to a classifier. The proposed methodology was tested on Linux vulnerabilities that were discovered in the time period from 2006 to 2011. Three different classifiers were tested and 28% to 88% of the hidden impact bugs were identified correctly by using the textual information from the bug descriptions alone. Further analysis of the Bayesian detection rate showed the applicability of the presented method according to the requirements of a development team.
In the context of Business-to-Business (B2B), an understanding of inter-organizational success factors and their impacts is crucial for effective strategic management. Several studies regarding those success factors and their influences have been conducted and published as articles. We aim at applying existing techniques, especially data mining, to automatically classify relevant sentences describing an influencing relationship between success factors. This paper presents the experiment method and results to find the optimal data mining workflow for our classification task. In particular, we apply several well-known data mining techniques based on different control factors. Then all discovered models are evaluated and compared to find the optimal data mining workflow. The main contributions include (i) the application of data mining for discovering success factors and their relationships, and (ii) the optimal workflow as a standardized flow for further similar classification tasks. The major challenge of this work is that there exists no mature corpus in this context, and hence our approach is implemented without a supporting corpus. The result shows that the models derived from the workflows that consider a section where a sentence is located perform better than the others in term of average performance. Furthermore, we found that the Support Vector Machine (SVM) performs better than other classifiers.
The vast numbers of digitised documents containing historical data constitute a rich research data repository. However, computational methods and tools available to explore this data are still limited in functionality. Research on historical archives is still largely carried out manually. Text mining technologies offer novel methods to analyse digital content to identify various types of semantic information in these documents and to extract them as semantic metadata. Methods range from the automatic identification of named entities (e.g., people, places, organisations, etc.) to more sophisticated methods to extract information about events (e.g., births, deaths, arrests, etc.), allowing users to greatly increase the specificity of their search. We have created an extended model of event interpretation to allow searches to be refined based on various discourse facets, including isolating definite information about events from more speculative details, distinguishing positive and negative opinions and categorising events according to information source. We present ISHER as an example of a multi-faceted, semantically oriented system for searching news articles from the New York Times, dating back to 1987. We explain how our extended event interpretation model can enhance search capabilities in systems such as ISHER, including the identification of contrasting and contradictory information in news articles.
Urdu language is used by approximately 200 million people for spoken and written communications. The bulk of unstructured Urdu textual data is available in the world. We can employ data mining techniques to extract useful information from such a large, potentially informative base data. There are many text processing systems available to process unstructured textual data. However, these systems are mostly language specific with the large proportion of systems applicable to English text. This is primarily due to language-dependent preprocessing systems, mainly the stemming requirement. Stemming is a vital preprocessing step in the text mining process and its primary aim is to reduce grammatical words form, e.g., parts of speech, gender, tense, and so on, to their root form. In the proposed work, we have developed a rule-based comprehensive stemming method for Urdu text. This proposed Urdu stemmer has the ability to generate the stem of Urdu words as well as loan words that belong to borrowed languages, such as Arabic, Persian, and Turkish, by removing prefix, infix, and suffix from the words. In the proposed stemming technique, we introduced six novel Urdu infix words classes and a minimum word length rule to generate the stem of Urdu text. In order to cope with the challenge of Urdu infix stemming, we have developed infix stripping rules for introduced infix words classes and generic stemming rules for prefix and suffix stemming. We also present a probabilistic classification approach to classify Urdu short text. Different experiments are performed to demonstrate the effectiveness and efficacy of the proposed approach. Comparison with existing state-of-the art approaches is also made. Stemming accuracy results demonstrate the adoptability of the proposed stemming approach for a variety text processing applications.
This paper introduces a novel methodology to extract core concepts from text corpus. This methodology is based on text mining and social network analysis. At the text mining phase the keywords are extracted by tokenizing, removing stop-lists and generating N-grams. Network analysis phase includes co-word occurrence extraction, network representation of linked terms and calculating centrality measure. We applied our methodology on a text corpus including 650 thesis titles in the domain of Industrial engineering. Interpreting enriched networks was interesting and gave us valuable knowledge about corpus content.
Useful information from a text document can be extracted using text mining techniques. Many text mining methods are generally uses term or keyword based approach. They all have advantages, but get stuck into the issue of synonymy: two terms have the same meaning and polysemy: one term has many meanings. So phrase based approaches are suggested, but they are not performing better than term based approaches. Instead of term based methods pattern mining methods are suggested, but there is still an open research issue of utilizing and modifying these discovered patterns. Pattern mining methods suffer from pattern misinterpretation and low frequency problem. This paper presents an effective technique to extract patterns that applies the pattern taxonomy model to get close sequential patterns. Pattern deploying process is used to reduce the pattern misinterpretation problem. Pattern co-occurrence matrix (PCM) is applied get the important relationship between close sequential patterns and removes patterns with zero co-occurrence weight. PCM and Inner Pattern Evolution process gives the benefit of reducing low frequency problem. This increases effectiveness of extracting interesting and accurate to the topic information.
This research paper has focused on the concept of Text mining and “Natural Language Processing or NLP” using Deep Learning. Text mining, also known as “Text Analytics”, employs Artificial Intelligence techniques and Machine Learning algorithms to convert uncontrolled corpus into standardized and normalized databases for further research. Text mining and NLP are two scientific topics that are mutually exclusive. These fields both deal with extracting text from unstructured materials, such as data that cannot be categorized or sorted numerically. NLP is more intriguing than text mining, according to Google Trends. Despite this, internet access has resulted in a dramatic increase in the number of documents that contain natural languages. To deal with the influx of documents, text mining and natural language processing (NLP) are required. Early computerized methods to language research centred on automation linguistic analysis method and building foundational technologies like language processing, language processing, and language processing. This research paper aims to evaluate the NLP in enhancing speech and text mining. In order to get information regarding this topic, researchers have used secondary data collection method to gather effective information from different journals and articles.
Female Daily Network is a company engaged in social media. Female Daily has social media to share experiences using beauty products called Female Daily. Female Daily has regulations not to use the Female Daily Platform to promote, sell products and services on social media platforms in Female Daily. However, users on Female Daily sometimes violate these rules in their posts and cause other users to be annoyed about it. Admins at Female Daily have difficulty identifying users who violate these rules and ban their posts containing product sales due to the limited number of admins with the number of posts that enter each day. Text mining can also overcome this problem by determining the classification automatically by creating a system that carries out the learning process from the available post words. Algorithms that can be used to carry out the text mining process in this research are Support Vector Machine (SVM), Naïve Bayes (NB), Decision Tree (DT), and Random Forest (RF). This study uses a combination of feature extraction, contextual features, and data balancing. This study uses research scenarios to analyze feature extraction, contextual feature usage, and data balancing. The best algorithm seen from the recall value in the combination of algorithms and features of this research is the Random Forest TF-IDF Unigram and uses additional contextual features to detect money and selling words with balanced data. The recall value of 88.37% is obtained from the results of the combination of these algorithms and features.
Data mining of myths, legends and folk tales in the context of artificial intelligence is completely studied in this paper. The core text correlation data structure all is limited, the text correlation data, has the very major difficulty in the computer daily treating processes, therefore, the text correlation content is unable through the data correlation excavation technology to carry on the solution and processing, must first carry on prompt processing to the text. Hence, the paper gives the novel ideas on information mining. We propose that SPARK has its own technical applications in the core process processing, graphics technology, machine learning, NoSQL query and so on by virtue of many years of practical experience in a big data application. We make the proper combinations of different methods to achieve the global optimal. The simulation reflected effectiveness.
Higher education has a central role for nurturing sustainable mindsets for the future well-being of society and the environment. That is why knowledge generation, teaching, and social innovation are the main components of sustainable development goals. In this light, recent research that aims to search for best content and practices in education for sustainability has increasingly focused on the analysis of sustainability coursework, programs, and projects. However, several studies demonstrate the low presence of sustainability contents in higher education curriculum across the world, pointing to an urgent need to integrate sustainability in its varied dimensions (ecological, economic, and social) into the curriculum. In order to address the issues above, this paper will assess the correspondence of sustainability topics of interest for society at large and for university students through the use of text mining techniques. For this analysis two types of textual data were used. The first was extracted from newspaper's articles from the Asahi Shimbun, which is a reference that largely reflects the sustainability views of society. The second was extracted from students' reports written about sustainability during the courses of interdisciplinary sciences of the Liberal Arts and General Education of Kyoto University. Both types of textual data were analyzed and the results were visualized according to the frequency and co-occurrence of appearance of words. The results demonstrate that, after university learning, students became aware of their own individual role in nearby spheres of action. Moreover, the results show that the sustainability views of students and the society partially overlap with different degrees of intensity, from dissimilar contextual backgrounds. Comprehensively, while the university courses further emphasize biocentric and ecocentric views, in contrast, society's views tend to focus on topics related to policy making and the role of enterprises. (Show More)
The Web has made possible many advanced text-mining applications, such as news summarization, essay grading, question answering, and semantic search. For many of such applications, statistical text-mining techniques are ineffective since they do not utilize the morphological structure of the text. Thus, many approaches use NLP-based techniques, that parse the text and use patterns to mine and analyze the parse trees which are often unnecessarily complex. Therefore, we propose a weighted-graph representation of text, called Text Graphs, which captures the grammatical and semantic relations between words and terms in the text. Text Graphs are generated using a new text mining framework which is the main focus of this paper. Our framework, SemScape, uses a statistical parser to generate few of the most probable parse trees for each sentence and employs a novel two-step pattern-based technique to extract from parse trees candidate terms and their grammatical relations. Moreover, SemScape resolves co references by a novel technique, generates domain-specific Text Graphs by consulting ontologies, and provides a SPARQL-like query language and an optimized engine for semantically querying and mining Text Graphs.
The text mining of micro-blog topic information can effectively obtain the attention degree of internet users for news events. It is of great significance in the field of public opinion monitoring and analysis. At the situation of the algorithm of traditional frequent word set is suitable for long text information clustering, this paper proposes to mine top-K frequent corpus in short text database and then to divide micro-blog topic texts covering the same frequent word sets into the same cluster. Combined with the largest frequent word-sets for similarity calculation, the overlapped document is re-divided to achieve micro-blog short text clustering. The experimental results of micro-blog topic dataset and the comparison with K-means clustering algorithm show that the proposed algorithm can effectively solve the sparseness and high-dimension problem of micro-blog topic short text clustering and greatly improve the micro-blog short text clustering effect.
The aim of this paper is to examine the Islamic State of Iraq and Syria (ISIS) recruitment actual behavior through social media. The ISIS has targeted Indonesia as the main source of participants due to the large of Muslim community with active internet users. Despite many ways to connect with social media like Twitter, Facebook, Instagram, and YouTube using videos and images, this paper is selected text as the main of source of materials. This paper is deployed a qualitative technique to analyze related literature and then it was combined with media tracking systems from texts posted on Twitter, Facebook, Website, and Telegram. The text mining was captured words that frequently appears that mentioned their support to the ISIS accounts from 2011 to 2017. This paper found that Indonesian tends to pay more attention to the existence of ISIS in Syria. This paper suggests that the Indonesian related agency to closely monitor the accounts which positively associated with ISIS terrorism and propaganda and take necessary action to clean up the massive haywire issues.
Text documents clustering is a popular unsupervised text mining tool. It is used for partitioning a collection of text documents into similar clusters based on the distance or similarity measure as decided by an objective function. Text clustering algorithm often makes prior assumptions to satisfy objective function, which is optimized either through traditional techniques or meta-heuristic techniques. In text clustering techniques, the right decision for any document distribution is done using an objective function. Normally, clustering algorithms perform poorly when the configuration of the well-formulated objective function is not sound and complete. Therefore, we proposed multi-objectives-based method namely, combine distance and similarity measure for improving the text clustering technique. Multi-objectives text clustering method is combined with two evaluating criteria which emerge as a robust alternative in several situations. In particular, the multi-objective function in the text clustering domain is not a popular, and it is a core issue that affects the performance of the text clustering technique. The performance of multi-objectives function is investigated using the k-mean text clustering technique. The experiments were conducted using seven standard text datasets. The results showed that the proposed multi-objectives based method outperforms the other measures in term of the performance of the text clustering, evaluated by using two common clustering measures, namely, Accuracy and F-measure.
This paper presents an approach to fake review detection, essentially for online hotel reviews, by combining the review-based approach and reviewer-based approach. Different Natural Language Processing techniques such as tokenization, lemmatization, vectorization, etc. are used to extract insightful features from the review text data. After text mining, the data is used to train different classification models using machine learning algorithms that detect fake reviews. After evaluating the models, a comparison is made based on the performance metrics. Furthermore, a web based user interface is created to provide a platform that combines the knowledge of the input user information with the chosen machine learning model to perform fake review detection on the input data.
The text clustering is one of core problems in text mining and information retrieval field, clustering algorithm is divided into four categories: the partitioned clustering algorithm, the hierarchical clustering algorithm, density-based clustering algorithm, as well as intelligence clustering algorithm. However, most clustering algorithms cannot meet the demand of speed and self-adapting about text clustering. This paper proposed a text clustering algorithm based on find of density peaks. The algorithm was implemented by the calculation of text distance and density, which was in accordance with calculation of the text vector similarity. SVM was used to express text to obtain the vector mapping for the similarity calculation. The next work was the finding of the local density and the distance from points of higher density of each text, removing the noise points, selecting the cluster center. The remaining points were assigned into the cluster which its nearest cluster center represented. According to several sets of contrast experiment, the density-based text clustering has an advantage of reliability and robustness.
Text is the main carrier of information on the Internet. More and more researchers are committed to mining textual information for great value. In terms of education, text mining can greatly help teachers and students to improve their writing. Computers can assist teachers in grading and classifying students' argumentative essays into three kinds: effective, adequate, ineffective. In this paper, we use data consisting of students' discourse and annotation provided by Kaggle platform. We utilize DeBERTa for Predicting Effective Arguments. We represent related work in section II, and introduce our methodology and experiment in section III and IV. To evaluate our experiment's performance, we do compared competitions. The predicting can be converted into a classification problem. We use the cross entropy as the metrics. Our Deberta-large owns the lowest metric 0.619 among these models, which is 0.007, 0.114, 0.030 lower than BERT, RoBERTa respectively, which shows our model has the best performance among them.
The hospitality industry is highly competitive, and in today's social media era, knowing customers' changing expectations has become essential. Accordingly, this study conducts aspect-based opinion mining (ABOM) on customer reviews to gain insights into these evolving perceptions and opinions. The study analyses user reviews from six prominent global chain hotels in South Asia, extracted from Tripadvisor. com. Employing Python for a robust text mining approach, we execute aspect-based opinion mining to extract nuanced sentiments expressed by users. Leveraging the Recursive Neural Tensor Network (RNTN) algorithm, widely used for sentence-level sentiment classification, we achieve a commendable average accuracy of 88.6% in categorising sentiments associated with specific words or aspects. This technical endeavour underscores the significance of aspect-based opinion mining and highlights the contributions of the proposed algorithm in illuminating insights into users' perceptions of hotel conditions. The obtained results stand to drive advancements in sentiment analysis techniques and provide a foundation for enhancing the quality of the hospitality industry in South Asia, thereby bolstering the region's tourism sector.
The large amount of text that is generated daily on the web through comments on social networks, blog posts and open-ended question surveys, among others, demonstrates that text data is used frequently, and therefore; its processing becomes a challenge for researchers. The topic modeling is one of the emerging techniques in text mining; it is based on the discovery of latent data and the search for relationships among text documents. In this paper, the objective of the research is to evaluate a generic methodology based on topic modeling and text network modeling, that allows researchers to gather valuable information from surveys that use open-ended questions. To achieve this, this methodology has been evaluated through the use of a case study in which the responses to a teacher self-assessment survey in an Ecuadorian university have been studied. The main contribution of the article is the inclusion of clustering algorithms in order to complement the results obtained when executing topic modeling. The proposed methodology is based on four phases: (a) Construction of a text database, (b) Text mining and topic modeling, (c) Topic network modeling and (d) The relevance of the identified topics. In previous works, it has been observed that the human interpretative contribution plays an important role in the process, especially in phases (a) and (d). For this reason, the visualization interfaces, such as graphs and dendograms, are of critical importance for researchers in order allow topic to efficiently analyze the results of the topic modeling. As a result of this case study, a compendium of the main strategies that teachers carry out in their classes with the aim of improving student retention is presented. In addition, the proposed methodology can be extended to the analysis of the unstructured textual information found in blogs, social networks, forums, etc.
Game reviewing is one of the method for game users and critics to comment and discuss about a game. Game developers and marketers could use game reviews as insights to assist on designing a better game by specifying quality requirements and providing better game marketing. Usability and problems are major concerns of users and game developers since these quality affects users' satisfaction and opportunity to increase market share respectively. However, manually extracting game usability qualities and problems from a large number of game reviews is challenging since most reviews often use natural language and may not be well-organized. This research presents a method for extracting and classifying game usability qualities and problems from game reviews using text mining technique. This research is composed of 2 main phases. The first phase is specifying game usability qualities and problems categories which also included keyword corpus creation for each category. The second phase is gathering, extracting, analyzing and classifying game reviews. The gathered game reviews will be extracted and classified into each usability qualities and problems category. Game usability qualities classification will also include sentiment classification to further specify positivity and negativity of each sentence. Accuracy metric will be used for classification evaluation. In addition, a game review summary on usability qualities and problems will be created for developers to improve game quality.
The purpose of this study is to investigate the research themes and trends in the field of Human-Computer Interaction (HCI) in the last twenty years between 1998-2018. For this purpose, an automated text mining based content analysis has been implemented using probabilistic topic modeling on 33830 peer-reviewed journal articles published between 1998-2018, and that are indexed by SCOPUS database. As a result of this analysis, 53 topics specifying the themes and trends in the HCI field was uncovered. Besides, these discovered topics categorized within themselves, and a systematic taxonomy mapping the landscape of HCI research and practices in the past 20 years was presented. This taxonomy contains six main categories: "machinery systems", "HCI body of knowledge", "feature identification", "brain-computer interfaces", "interaction", and "medical". This study is expected to contribute to HCI research, practices, and investments in the future.
Simulation is a common method for studying the behavior of complex systems and revealing the mechanism of the system. However, complex systems have many parameters, non-linear interactions, and complex evolutionary dynamics. It is difficult to reveal the mechanism of complex systems. Especially complex system simulation experiments may produce a large amount of data. How to summarize the macroscopic mode of the system, identify key factors, and discover the relationship between input and output variables, still lacks an effective method. This paper proposes an integrated framework for simulation modeling and data mining, which combines data mining and simulation modeling to conduct iterative experimental exploration and analysis of complex systems. Data mining techniques were used in multiple stages of modeling and simulation, including: ETL on raw data, text mining and process mining to build conceptual models, uniform experimental design to generate simulation data, and clustering of simulation data to identify system macro patterns, use stepwise regression, neural network, etc. to build a meta-model of a complex system. The introduction of data mining can improve the ability and efficiency of complex system modeling and simulation.
Social networks have become a convenient and effective means of communication in recent years. Many people use social networks to communicate, lead, and manage activities, and express their opinions in supporting or opposing different causes. This has brought forward the issue of verifying the owners of social accounts, in order to eliminate the effect of any fake accounts on the people. This study aims to authenticate the genuine accounts versus fake account using writeprint, which is the writing style biometric. We first extract a set of features using text mining techniques. Then, gtraining of a supervised machine learning algorithm to build the knowledge base is conducted. The recognition procedure starts by extracting the relevant features and then measuring the similarity of the feature vector with respect to all feature vectors in the knowledge base. Then, the most similar vector is identified as the verified account.
Nowadays, there are a large number of models in the scientific literature capable of recognizing and extracting gene mentions from a given text. Several data sets have been developed to facilitate the learning process of these models. However, very few models are able to increase their knowledge and performance progressively from new annotated text but also to take into account the granularity of the input text of the model. Our proposed solution, GenNER, is a method for recognizing gene/protein mentions from free text. GenNER relies on continuous learning and a text granularization algorithm as input to the model, which allows it to achieve better performance. Its evaluation process was done around BioCreative II annotated datasets; we obtained an average F1-score of 0.9704, which outperforms current methods.
Translingual Text Mining (TTM) is an innovative technology of natural language processing for building multilingual parallel corpora, processing machine translation, contextual knowledge acquisition, information extraction, query profiling, language modeling, contextual word sensing, creating feature test sets and for variety of other purposes. The Keynote Lecture will discuss opportunities and challenges of this computational technology. In particular, the focus will be made on identification of language pair phenomena and their applications to building holistic language model which is a novel tool for processing machine translation, supporting professional translations, evaluation of translingual systems efficiency and also for improving the process of teaching foreign languages regardless of the level of students' proficiency. Some components incorporated to machine translation systems rely specifically on language-pair phenomena like for example language recognizer, or word deliminer. Nowadays, declarative programming is becoming widely used just for describing language pair phenomena by graphical formalism. Translation irreversibility represents a unique language and system independent asymmetrical translation innovative technology, which will be presented during the Keynote lecture.
TCM medical cases in records are free text with much valuable data and clinical terms, how to recognize and extract these clinical terms automatically is a valuable work. TCM medical records obtained from Guangdong Provincial Hospital of Chinese Medicine are segmented to single word and labeled with five labeling features(words in sentence, grammatical property of words, words in clinical dictionary, set phrases acting on neighbor context, and set phrases acting on far distance.), and divided into training sets and testing sets. Training sets are also handled with outputted labeling (labeling of symptoms or signs, TCM diagnosis, TCM syndrome type, Chinese medicines (drug), and Names of TCM prescriptions.). In order to evaluate abilities of labeling features on improving clinical terms recognition with CRF, three indicators (recognition Precision (P), recognition Recall (R) and F-score (F)) are defined, and three comparisons are given: comparisons of individual labeling features, comparisons of combined labeling features, and comparisons of combined features in different diseases. The results show that, "grammatical property of words" is the best labeling features in all individual labeling features. Multi-combined features have higher scores than individual labeling features on improving clinical terms recognition. The combined mode of "grammatical property of words", "words in sentence", and "words in clinical dictionary" may be the most suitable labeling features. Multi-combined labeling features can improve term recognition with CRF model for text mining in TCM medical cases.
In this paper a system for voice of customer analysis is proposed, which will produce strong rules to help organization to take business decisions. It uses parallel association rule mining for rule generation and data usually tends to be very huge so partitioning is done on the basis of sentiment of customer. For this purpose text mining algorithm is used which extracts information from unstructured data. On these partitions of data association rule mining algorithm is applied which determines strong association rules and kept in a database. Domain experts can use these rules to take business decisions which can help an organization to have a better understanding of customer's all needs and wants.
Automatic text summarization is one of the important challenges of natural language tasks. It will help the readers save time to get the important information from a lengthy document automatically. Sentiment Analysis is the process of identifying and categorizing opinions expressed in a piece of text computationally to determine whether the writer's attitude towards a particular topic, product, etc., is positive, negative, or neutral. This research focuses on improving the summarization accuracy with sentiment analysis of movie review posts using RapidMiner operators. The first model of summarization is built using the Aylien Text Analysis extension. The proposed second model is built using the Text Processing extension. For both these methods the sentiment analysis is done using the same Aylien Text Analysis extension for evaluating the summarization results. An accuracy of 90% is achieved for sentiment analysis using the first model and 96% for the second model.
Most word embedding methods are proposed with general purpose which take a word as a basic unit and learn embeddings according to words' external contexts. However, in biomedical text mining, there are many biomedical entities and syntactic chunks which contain rich domain information, and the semantic meaning of a word is also strongly related to those information. Hence, we present a biomedical domain-specific word embedding model by incorporating stem, chunk and entity to train word embeddings. We also present two deep learning architectures respectively for two biomedical text mining tasks, by which we evaluate our word embeddings and compare them with other models. Experimental results show that our biomedical domain-specific word embeddings overall outperform other general-purpose word embeddings in these deep learning methods for biomedical text mining tasks.
Stemming is a technique used to reduce inflected and derived words to their basic forms (stem or root). It is a very important step of pre-processing in text mining, and generally used in many areas of research such as: Natural language Processing NLP, Text Categorization TC, Text Summarizing TS, Information Retrieval IR, and other tasks in text mining. Stemming is frequently useful in text categorization to reduce the size of terms vocabulary, and in information retrieval to improve the search effectiveness and then gives us relevant results. In this paper, we propose a new multilingual stemmer based on the extraction of word root and in which we use the technique of n-grams. We validated our stemmer on three languages which are: Arabic, French and English.
We propose a novel general-purpose hybrid method comprising topic modeling and Class Association Rule Mining (CARM) for text classification in tandem. While topic modeling performs dimension reduction, association rule mining aspect is taken care by Apriori and Frequent Pattern(FP)- growth algorithms, separately. In order to illustrate the effectiveness of the proposed method, malware prediction using two publicly available datasets of API calls has been performed. The proposed model has generated highly accurate class association rules and Area Under the Curve (AUC) compare to the extant models in the literature. With the help of statistical significance test, it is concluded that the performances of both proposed hybrid models, i.e., topic modelina with FP-2rowth and Apriori, are same.
Previous scholars always made an effort to make various formulations that were used to categorize and calcify hadith. At present, the process of categorization or classification is facilitated by the process of text mining technology. In the study of text mining itself, there are various kinds of tools and methods or algorithms that can be used and also help provide maximum results in the process of mining information from a text. An example is the Decision Tree C4.5 and K-Nearest Neighbor algorithm. Based on that, the author wants to make research and this final project to compare the performance resulting from the classification process of text documents using Decision Tree C4.5 and K-Nearest Neighbor algorithm for the classification of Imam At- Tirmidzi hadith. With this research, it is expected to be knowledgeable about the process of classifying text documents along with the performance of the two algorithms. Based on testing that has been done, the Decision Tree C4.5 algorithm produces an average accuracy value of 70.53% with an average processing time of 0.083 seconds. While the K-Nearest Neighbor algorithm produces an average accuracy value of 66.36% with an average processing time of 0,03 seconds.
Assessments of digital maker activities increasingly rely on automatically analyzing student-created products and their components, such as their textual output. In particular, recent learning analytics research has proposed incorporating text analytic feedback for facilitating students' virtual reality (VR) content creation, though lacking direct empirical evidence from student-created artefacts. Thus, this study examined the relationships between metrics on text in student-created VR content and their learning performance. VR narration scripts and performance scores were collected from 102 students in a maker-based general education course. Results of statistical testing and text mining show that high and low-performing students demonstrated significant differences in such metrics as word counts, vocabulary sizes, and frequent unigrams and bigrams. This study makes methodological and practical contributions in the domains of maker education and learning analytics.
Multiple sclerosis is an autoimmune disease that causes psychological impacts and severe physical disabilities, including motor disabilities and partial blindness. This work introduces an early detection method for multiple sclerosis disease by analyzing transcriptomic microRNA data. By transforming this phenotype classification problem into a text mining problem, multiple sclerosis disease biomarkers can be obtained. To our knowledge, text mining methods have not been introduced previously in transcriptomic data analysis of multiple sclerosis disease. Hence, this work presents a complete predictive model by combining consecutive transcriptomic data preprocessing procedures, followed by the proposed KmerFIDF method as a feature extraction method and linear discriminant analysis for dimensionality reduction. Predictive machine learning methods can then be obtained accordingly. This study describes experimental work on a transcriptomic dataset of noncoding microRNA sequences denoted from relapsing-remitting multiple sclerosis patients before fingolimod treatment and after six consecutive months of treatment. The experimental results of the predictive methods with the proposed model report sensitivity, specificity, F1-score, and average accuracy scores of 96.4, 96.47, 95.6, and 97% with random forest, 92.89, 92.78, 93.2, and 94% with support vector machine and 91.95, 92.2, 93.1, and 94% with logistic regression, respectively. These promising results support the introduced model and the proposed KmerFIDF method in transcriptomic data analysis. Moreover, comparative experiments are conducted with two referenced studies. The obtained results show that the average reported accuracy scores of the proposed model outperform the referenced literature work.
Wikis are widely used collaborative environments as sources of information and knowledge. The facilitate students to engage in collaboration and share information among members and enable collaborative learning. In particular, Wikis play an important role in capstone projects. Wikis aid in various project related tasks and aid to organize information and share. Mining project Wikis is critical to understand the students learning and latest trends in industry. Mining Wikis is useful to educationists and academicians for decision-making about how to modify the educational environment to improve student's learning. The main challenge is that the content or data in project Wikis is unstructured in nature. The data formats are in both text and images. In this work, we propose an automated project Wiki mining solution that leverages data mining, text mining and optical character recognition techniques for discovering insights from Project Wikis. The results of mining process are presented as visual summaries which can be useful for the capstone project coordinators and academicians for education pedagogy decisions. We use dataset from Singapore Management University, School of Information Systems' undergraduate capstone projects for our solution evaluation. We evaluated our model on 314 capstone projects over a period of 8 years.
In order to recover the value of short texts in the operation and maintenance of power equipment, a short text mining framework with specific design is proposed. First, the process of the short text mining framework is summarized, in which the functions of all the processing modules are introduced. Then, according to the characteristics of short texts in the operation and maintenance of power equipment, the specific design for each module is proposed, which adapts the short text mining framework to a practical application. Finally, based on the framework with the specific designed modules, two examples in terms of defect texts are given to illustrate the application of short text mining in the operation and maintenance of power equipment. The results of the examples show that the short text mining framework is suitable for operation and maintenance tasks for power equipment, and the specific design for each module is beneficial for the improvement of the application effect.
Text mining is a process to extract data based on text. The source of text mining should be coming from social media users' activities; e.g. posting, fans page, group, hashtag, tweet, etc. As raw data and information, the text mining sources are able to be gold information to be operated for academic analyzing purposes. This paper proposes the Python-based model to scrape text from Twitter user's tweets and then analyze them to see the commerce and tax-income potency in purposely Indonesia. The mathematical and statistical methods are technically operated. The constructed model finally exhibited. Also, the information on sale-behavior and tax-income forecasts are able to be portrayed.
Data visualization in big data analysis, facilitates decision makers to see analytics visually represented, that empowers them to understand difficult concepts or identify new patterns effortlessly. Existing visualization tools are capable of visualizing only structured data, but more than 80% of business information is in unstructured format. Hence there is a need for an end-to-end tool that could structure, summarize and visualize Unstructured-Data. With this notion, a flexible tool is being developed that could be applied to any domain accepting the domain-specific and user-specific attributes as inputs and plot graphs accordingly. The Unstructured-Data is obtained by web mining. Further, an effective attribute-based opinion mining algorithm has been proposed which enables the user to make well-informed decisions focusing on the attributes he is concerned most.
The clustering algorithm based on density is widely used on text mining model, for example the DBSCAN(density-based spatial clustering of application with noise) algorithm. DBSCAN algorithm is sensitive in choose of parameters, it is hard to find suitable parameters. In this paper a method based on k-means algorithm is introduced to estimate the ε neighborhood and minpts. Finally an example is given to show the effectiveness of this algorithm.
The development of text mining algorithms is far reaching and thus, new application areas arise, inter alia in the framework of applying modern information technologies (e.g. big data analytics) in order to answer social and economic (research) questions (digital humanities). In this paper existing algorithms to reveal results in the context of benchmarking scientific research clusters are combined. For conducting the analysis, two data corpora are gathered. The first data corpus contains abstracts of publications according to the used application case of the Cluster of Excellence Tailor-Made Fuels from Biomass 'TMFB' at RWTH Aachen University. The second data corpus -- based on the analysis of the first data corpus -- contains content-related abstracts of research belonging to the relevant scientific community. In the framework of processing the gathered data, information extraction, information retrieval and named entity recognition among others are utilized for analyzing the publications in order to derive useful findings. Furthermore association mining findings are introduced. As an excerpt of the results, the identification of thematic alignments and developments over time are presented for both corpora and are subsequently compared.
With the advancement of science and technology, algorithms for data mining are constantly being introduced but most of the algorithms focus on solving the problem of how to reduce the dimension of variables. Nowadays, the amount of data is accumulating and increasing rapidly, and how to compress the amount of data becomes an important topic. Attribute-oriented induction (AOI) is a method of longitudinally compressing data. At present, there are many derivative methods to solve the problem of multi-value-attributes and ordered data. However, few researchers have focused on unstructured text data. Nowadays, the most common way to process text data for classification and induction is to use Latent Dirichlet Allocation (LDA) and generate attributes of the topic. However, the attributes of the topic do not consider whether they exist or not a potential hierarchical relationship. As the topics of text data are hierarchical, we focus on unstructured data such as comment content, transform the comment content into concept datasets, and use AOI to construct a reasonable hierarchical concept. Through induction cost, the final level of induction is determined, too. Finally, the induction rate of the topics generated by the LDA and AOI methods is evaluated to solve the problem of data compression with multiple events and hierarchical relationships among concepts.
Free-form, unstructured and semi-structured textual data has become increasingly more prevalent in the telecommunications industry, with service and equipment providers alike. Some typical examples include textual data from customer care tickets, machine logs, alarm and alerting systems, and diagnostics. There is a growing business need to rapidly and automatically understand the underlying key topics and categories of this bulk collection of text. With the present mode of operation of relying on domain experts to analyze textual data, there is a clear need to apply text analytics to automate the process. Difficulties arise due to the jargon-filled and fragmented, incomplete nature of textual data in this field. In this paper, we propose a domain-agnostic, unsupervised approach that deploys a multi-stage text processing pipeline for automatically discovering the key topics and categories from free-form text documents. Using anonymized datasets retrieved from actual customer care tickets and system logs, we show that our approach outperforms traditional text mining approaches, and performs comparably to manual categorization tasks that were undertaken by domain experts with full system knowledge.
Science is collected and digitized and then stored in various forms, for example, websites, blogs, videos, scientific articles, books, journals, social media, and so on. Then with this, it becomes increasingly difficult to find what to look for and what is needed. At this time the tools to search, organize, and understand a large amount of collected information will be expensive and a lot of time to do the research. With some structured data and unstructured data, it will be difficult to get relevant information as desired. One of the techniques in this field is text mining, also known as text mining, one of which is topic modeling.Based on the results of testing conducted on four nationally accredited information technology journals, the resulting group of topics is topics discussed in several theories and implementation of information technology, especially in the field of information systems and informatics. The use of lemmatization and Term Frequency for research with Indonesian data on topic modeling is quite effective as seen from the results of preprocessing text data. Most relevant topics generated from each iteration of terms do not exceed the value of 50% for each topic, meaning that their relevance is not too dominant in specific discussions about information technology but in certain data cases that cause evenly the composition of terms in each journal topic document.
The Bert model is a pre-training model based on deep learning. It has refreshed the best performance of 11 NLP missions as soon as it appears, and it also has a wide range of applications. The text data of people's livelihood governance is huge, and there is a large amount of unstructured data, which makes the traditional text analysis and mining technology increase in the space-time complexity of computing; so it is very important to choose appropriate text classification technology. Here we propose to use the Bert model in combination with the Bayesian network to achieve one new classification of text. That is, the Bayesian network is used to perform the classification of two categories first, and we can get the approximate category range of each text, and then the Bert model is used to classify the text into specific categories. The combination of these two methods can greatly reduce the errors caused by the classification defects of using only one of the methods. Thereby achieving an improvement in the accuracy of text classification.
Nowadays, many sources of information that can be used by the government to improve the problem handling in society. One alternative that can be used is gathering feedback from society through social media. Twitter has become a popular social media based on microblogging that allows the user to express their opinion and condition that happen and occur around them. Along with the massive number of users that express their feedback on Twitter, there is so much tweet that we must analyze to gather the citizen's complaint data manually, which is not an easy task to do. In this research, we offer text mining using Naïve Bayes classifier for classifying tweet automatically so that we can classify public's feedback to the government much faster. Also, from the classification, we can give a prediction to the government about what society is thinking about government, based on the keyword that we use. This research use 3000 tweet dataset regarding feedback for Pemerintah Kota Surabaya (Surabaya's City Government) that contains complaint or non-complaint tweet. The dataset was split into 160 data of complaint tweet and 140 non-complaint tweets. This research generates systems that can classify tweet automatically with 82.5% of accuracy.
The paper is to dig out the topics contained in customer service chat records, which can provide guidance for the design and optimization of customer service automatic question answering system. In order to mine the interactive short texts of customer service chats, preprocessing and feature selection are carried out in the paper. Finally, the topics are obtained by LDA model. The results of the model show that topic customers concerned about are in line with the actual situation. Different topics are independent of each other, at the same time, there is a strong correlation between topics and sub-topics. In the process of text mining, there are some cross-links in different word filtering methods. The LDA modeling results provide guidance for the design and optimization of automated QA systems for e-commerce websites. In addition, the paper puts forward a new avenue for web users to understand the topics of concern.
Natural Language Processing (NLP) is increasingly drawing researchers’ attention to mine for valuable knowledge in the massive literature arising in our technological era. Text mining is one of the NLP tools that can take the lead over manual curation of articles from different medical databases. It offers appreciable help by automating the whole information extraction process to get the required information saving effort and time. As Mobile Health (mHealth) for breast cancer patients is showing a clear positive impact on their treatment and recovery journey, text mining techniques can help in the continuous tracking of related research articles to figure out the drawbacks and highlight future directions. Therefore, this article proposes a model that initiate a deep search to curate articles dataset using National Center for Biotechnology Information (NCBI) database applying five different queries. The model preprocesses the curated dataset to remove outliers and extract the target articles only. In addition, it analyzes the articles’ titles to extract required information. Total number of retrieved articles was 610 that are further reduced to 13 after the applied filtration steps. Two of which are review articles that are concerned with effect of mobile apps on patients undergoing chemotherapy which is a cancer critical stage. The retrieval process took from few seconds to several minutes according to the number of articles related to the entered query, while the preprocessing stage is achieved in one second.
This paper was conducted in collaboration with the Office of Institutional Research at National Ilan University in Taiwan to analyze textual opinions found in teaching evaluation questionnaires and applied the analysis results to assisting the selection of outstanding teaching faculty members. The selection of outstanding teachers requires that selection committee members spend a large amount of time reviewing written data. Therefore, this paper develops a set of systems for the analysis of textual opinions in teaching evaluation questionnaires, providing reference materials for the selection committee. The teaching evaluation questionnaire is a form of educational data. This paper analyzes these data using educational data mining. In text mining, text sentiment analysis is a common textual data quantification method that can analyze the sentiment tendency of a text author. This paper uses a text sentiment analysis to quantify the students’ textual opinions and to provide the selection committee with the sentiment tendency of students’ comments on teaching faculty members. We analyze text sentiment separately for different classifiers by using the Chinese text sentiment analysis kit SnowNLP. We compare the efficacy of classifiers that do not take time series factors into consideration (naïve Bayes and fully connected neural network) to those that do [recurrent neural network (RNN), long short-term memory (LSTM) RNN, and attention RNN]. We found that classifiers that consider time series factors are more effective at analyzing text sentiment. Furthermore, adding LSTM cells and an attention mechanism to a traditional RNN classifier effectively improved its efficacy on long-sequence tasks. As a result, we chose the attention LSTM classifier—with a positive sentiment recognition rate of 97% and a negative sentiments recognition rate of 87%—as our preferred text sentiment classifier. Finally, we set up an analytics server that will be modularized to facilitate its integr... (Show More)
Biomed literature is abundant with wealth of information related to diabetic retinopathy (DR) risks and associated pathologies that can be extracted using text mining. The objective of the current study was to apply text mining methods to extract the information related to DR risk factor associations. We have used the R package pubmed.mineR to analyze the DR related literature available in the PubMed database. The search keywords included “diabetic retinopathy” AND “risk” in the title or abstract. The corpus resulted in 4614 abstracts. The texts were segregated into different groups on the basis of their annotations related to awareness, risk factors, and complications. Results indicated that most associated pathologies with DR are diabetes, cardiovascular (CVD) and nephropathy. There are fewer number of articles related to prevention in comparison to treatment. In conclusion, it is recommended that emphasis should be given on prevention of DR at primary and secondary care levels and on development of cost- effective based guidelines for management of DR.
Technology-enhanced learning (TEL) is now at the heart of teaching and learning process in many higher education institutions (HEIs). Today, educators are faced with the challenges of pedagogically specifying what tools, methods, and technologies are used to support the teachers and students, and to help maintain/sustain a continuous education and practices. This study shows that there is an opportunity in the use of (educational) datasets derived about the teaching and learning processes to provide insights for fostering the education process. To this effect, it analyzed the students’ evaluation of teaching (SET) dataset (n=471968 ) collected within a higher education setting to determine prominent factors that influences the students’ performance or the way (TEL-based) education is being delivered, including its didactical impact and implications for practice. Theoretically, the study employed a mixed methodology grounded on integration of the Data-structure approach and Descriptive decision theory to study the rationality behind the students’ evaluation of the teaching and performance. This was done through the Textual data quantification (qualitative) and Statistical (quantitative) analysis. Qualitatively, the study applied the Educational Process and Data Mining (EPDM) model (a text mining method) to extract the different sentiments and emotional valence expressed by the students in the SET, and how those characteristically differ based on the period and type of evaluation they have completed (between 2019 to 2021). For the quantitative analysis, the study used a multivariate analysis of covariance (MANCOVA) and multiple pairwise comparisons post-hoc tests to analyze the quantified information (average sentiment and emotional valence) extracted from the SET data to determine the marginal means of effect the different SET types and evaluation period have on the students’ learning outcomes/perception about the teaching-learning process. In addition, the stud... (Show More)
The increasing number of online transactions and other internet activities give rise to the proliferation of online scam. The Philippine National Police - Anti Cybercrime Group (PNP-ACG) reported an increasing number of complaints from a double digit figure in 2013 to a triple digit figure in 2017. The challenge of addressing this problem in the Philippines is shared by other developing countries in Southeast Asia and other parts of the world. Since 2013 when the PNP-ACG was established, cybercrime data continue to be accumulated but were not given much attention and significance in research. Previous studies highlight the importance of taking advantage of data mining. However, the absence of empirical studies on cybercrime analytics in the country connotes the lack of exploitation of data mining in facilitating cybercrime investigations. This study exploits Weka text mining tool in order to draw insights by classifying a given online scam dataset. Online scam unstructured data were considered as dataset containing a total of 14,098 mainly Filipino words. J48 Decision Tree, Naïve Bayes, and Sequential Minimal Optimization were used to build classification models. All these three classifiers or algorithms were compared in terms of performance and prediction accuracy. The results show that J48 achieves the highest accuracy and the lowest error rate, followed by Naïve Bayes and then the SMO classifier. Also, the responses during validation reveal that J48 is preferred over the other classifiers as it easy to understand and apply in cybercrime investigations. This demonstrates how text mining can assist the PNP-ACG in analyzing online scam criminal data as it also highlights the importance of employing data mining tools in the legal and criminal investigation domains in the Philippines. Further work can be carried out in the future using different and a more inclusive cybercrime datasets and other classification techniques in Weka or any other data mining tool, and othe... (Show More)
Sequential Pattern Mining which aims to discover all frequent sequences of itemsets (patterns) from a large data collection has been applied in the Text Mining domain such as Text Categorization and Pattern Identification. However, in the area of Document Summarization the effort is still considered as green and exploratory. In the real world, a sentence is more than just a collection of un-ordered sequence of words, where each sentence carries their own meaning. By discovering these textual patterns is essential since the patterns can describe the text, by preserving the sequential order of the words in the document. Thus, the motivation here is to investigate the feasibility to develop a Sequential Pattern-based Summarizer model near future in order to reduce redundancy information from multiple text resources; at same time preserving the meaning of the original text document using the Semantic similarity approach. This paper reviewed some of the existing techniques in the area of multiple document summarizations to better understand the gap and issues underlying this area. By incorporating the semantic knowledge of sentences in the multiple documents is hoped to assist and alleviate the long-winding process for non-subject expert researches in trying to find the similarities and correlation between text resources.
The paper aims to analyze the emotional tendencies of the massive tourism reviews released by the tourists through OTA online, combined with the models of the emotional analysis in the tourism field and the advanced technologies such as big data, artificial intelligence and so on. According to the unstructured text information of online reviews, this paper adopts the Expand Vector Space Model (EVSM for short) to represent text knowledge and Web Text Classification Algorithm to extract the emotional features of tourists effectively. The experimental results illustrate the algorithm is easy to implement with high performance and accuracy. Furthermore, an empirical study on tourism resources of Hainan Province is conducted in the final as the important references of tourism marketing strategies, which involves the emotional analysis on tourism web evaluation of the 5A and 4A scenic spots and data analysis of communication operator on the inherent rules of tourists such as interests, trails and consumption behaviors.
This paper summarises the results of an initial study examining the alignment of NATO Policy documents through text analysis. The paper describes a simple approach to abstracting content from policy and implementation documents through subject matter expert (SME)-guided dictionary creation, and we describe the implementation of novel methods to measure and visualize the alignment between pairs of documents. We describe how the process was implemented to measure the vertical and horizontal alignment of NATO policy and implementation documents to provide real-time support for the drafting of NATO documents. We discuss challenges faced in the implementation, and propose potential future solutions.
Failure mode and effects analysis (FMEA) has been widely used in product design process as a reliability analysis technique. Design FMEA (DFMEA), which is used in the product development phase to identify and mitigate product risk, is one of the three application scenarios of FMEA. During the DFMEA process, verification and validation (V&V) activities are proposed to mitigate the risk of the identified potential failure modes. The V&V activities can be further planned and implemented to improve the product reliability under development. However, the DFMEA report usually contains rich text descriptions of potential failure modes and causes, and it is difficult and nonintuitive to fully understand these information for design improvements. In addition, it is also very challenging to optimize the planning of V&V activities by selecting a set of V&V activities to achieve expected reliability improvement effectiveness with the minimum resource consumption requirement. To address these two challenges, this article first proposes a method of applying text mining to the DFMEA report to obtain two types of hidden reliability information, including the classifications of failure modes/causes and the correlation between keywords. Then, a mathematical model is proposed to optimize the product V&V planning by selecting an optimal set of V&V activities. The application of the above proposed methods is illustrated through the product development of a diesel engine power generation system.
A critical sum of critical data is often found, as it were, in the unstructured portion of patient accounts in Electronic Health Records (EHRs), making it hard to handle and use it for tasks such as evidence-based health care or medical research. The proposed method uses a novel tool for the annotation of medical text. This is based on IBM Watson Platform and uses the UMLS as a source of medical expressions and their classifications; the experiments are applied on 100 medical documents collected from the MIMIC data set. The results obtained are promising in this area of research as we obtained a precision and recall value 91 % for the tested data.
The aim of the study is to present an advanced overview and potential application of the innovative tools/software's/methods used for data visualization, text mining, scientific mapping, and bibliometric analysis. Text mining and data visualization has been a topic of research for several years for academic researchers and practitioners. With the advancement in technology and innovation in the data analysis techniques, there are many online and offline software tools available for text mining and visualisation. The purpose of this study is to present an advanced overview of latest, sophisticated, and innovative tools available for this purpose. The unique characteristic about this study is that it provides an overview with examples of the five most adopted software tools such as VOSviewer, Biblioshiny, Gephi, HistCite and CiteSpace in social science research. This study will contribute to the academic literature and will help the researchers and practitioners to apply these tools in future research to present their findings in a more scientific manner.
textual content Mining is the process of extracting meaningful information from large volumes of unstructured text. it's far a form of synthetic intelligence used to investigate and interpret files along with consumer feedback, emails, transcripts, net logs, surveys, and on line documents. Its intention is to discover hidden shape, patterns, institutions, and correlations in textual facts in order to draw inferences and broaden new insights. textual content Mining allows a deeper level of intelligence and selection-making by means of uncovering which means from unstructured textual facts. by means of utilising present day statistical strategies and herbal Language Processing (NLP), text Mining can produce automatic fashions that interpret and translate textual content into intelligent information which can be used to make correct predictions and provide actionable insights. textual content Mining is also being used to find out new trends, categorize client sentiment, locate correlations in purchaser comments, and section document collections. eventually, text Mining is being used to assist automate selection-making by automating workflows, processing massive volumes of text in a fragment of time, and providing deep insights into patron conversations.
Distributed Software development process has dramatically changed over the last decade due to the integration of social collaborative development environment. The pull-based software development methodology made its mark in the open source distributed development as it is a convenient and effective system to organise collaborative contribution. Code reviews for software projects have been a best practice in software engineering. With the emerge of pull-based software development methodology, code reviewers faced difficulty in reviewing the contributions because of the higher number of incoming pull requests. In order to address this problem, reviewer recommendation systems have been implemented. In these systems, textual data mining techniques have been applied. This paper focuses on identifying the different approaches in terms of textual data mining used in the domain of the reviewer recommendations in pull-based software development and identifies their drawbacks and room for improvement. This paper contains the initial part of ongoing research and in the future, we hope to use this knowledge to come up with a solution that addresses the identified drawbacks and the identified improvements.
Product reviews from online communities are good sources to innovative ideas. Making use of this unstructured data from these reviews is a complicated process. The major task of identifying good ideas is ensuring the correct reviews are picked up for consideration. We propose a method of mining ideas by exploring online reviews data with a particular interest in discovering helpful suggestions in innovation. We use a classifier model to categorize whether the reviews from an online community provide a suggestion. The non-suggestive texts were discarded from the further analysis. The texts are then processed further by a sentence similarity comparison module that uses the Bidirectional Encoder Representations from Transformers (BERT) model. We use the sentence similarity techniques to filter common suggestions that are attractive to many reviewers. We describe how to use BERT based model for Natural Language Tasks. We conclude that mining reviews can help extract ideas from online communities. This is a generic implementation and can be adopted in any domain. We also suggest a human intervention to assess the context of the generated ideas. This method can help researchers and professionals discover ideas buried in unstructured online reviews.
With the continuous development of natural language processing and its impact on artificial intelligence, various applications such as automated Q/A systems, Language translation, and Language generation and processing have garnered significant attention. One key aspect of NLP is text mining, encompassing technologies like text classification and NER (Named Entity Recognition). In real-world research and engineering projects, the availability of extensive and dependable text data sets is vital for ensuring the success of these tasks. However, in the IT, Higher Education, Legal, and Finance industries, the resumes flow in high volume but are very cluttered. Creating large-scale text data-sets of resume corpus has traditionally relied on manual sorting and labeling. This manual approach is resource-intensive and time- consuming, resulting in subjective variations due to diverse human perspectives. Consequently, these data sets may not fulfill the requirements of the actual industrial applications. To address these challenges, This paper presents a skillful approach for automatically annotating text data sets using NER (Named Entity Recognition) from the IT, Higher Education, Legal, and Finance sectors. This includes the combination of bi-LSTM and CRF, which can be a powerful assistant in labeling the data with multiple entities. We are also validating our generated corpus with CNN and RNN to ensure the quality of the corpus generated from the raw resumes.
Text mining is one of the most important tasks in Natural Language Processing (NLP). It is often very difficult to detect events of interest from vast unstructured or data that is not properly labeled. In this paper, we present a machine learning-based approach to discriminate particular event-related news from its text content. We focus here particularly on violence and related events. We have created a dataset from popular Bengali newspapers and used a keyword-based search method to select violence-related news articles. Based on the dataset, we first trained a supervised learning model. We have experimented with six classification algorithms where Logistic Regression outperformed each of the contemporary algorithms in both full and reduced feature set acquiring 75.7% and 83.4% accuracy respectively. Using the models, we then further analyzed violence- related news data to find out important insights from unlabeled data.
Lack of multi-application text corpus despite of the surging text data is a serious bottleneck in the text mining and natural language processing especially in Persian language. This paper presents a new corpus for NEWS articles analysis in Persian called Persica. NEWS analysis includes NEWS classification, topic discovery and classification, trend discovery, category classification and many more procedures. Dealing with NEWS has special requirements. First of all it needs a valid and NEWS-content-enriched corpus to perform the experiments. Our Approach is based on a modified category classification and data normalization over Persian NEWS articles which has led to creation of a multipurpose Persian corpus which shows reasonable results in text mining outcomes. In the literature, regarding to our knowledge there are few Persian corpuses but none of them have Persian NEWS time trend characteristics. Empirical results on our benchmark indicate that in addition to reducing the problem dimensions and useless content, Persica keeps admissible validity and reliability in comparison with standard corpuses in the literature.
Urinary tract infections (UTIs) are among the most common infectious diseases occurring in either the community or healthcare setting. Approximately one third of all women have had at least one physician-diagnosed uncomplicated UTI by the age of 26 years. A broad range of pathogens can cause complicated UTI. Escherichia coli remains the most common organism causing UTI. Antimicrobial resistance is one of the most common problems in complicated UTI which can cause treatment failure and serious complications. Antibiotic-resistance has been increasing in the past decades worldwide. Chinese herbal medicine (CHM) has been an integral part of Traditional Chinese Medicine (TCM) for thousands of years. Some Chinese herbs in the study were proved to have antibacterial activity, which prompted their compound prescription use in the management of UTI. Many herbal formulations have been developed and used in the treatment of UTI and were proved to be effective and safe. Yet the principles of treating UTI with CHMs are hard to manage due to the complexity of TCM theory. In this study, a novel text mining method was development based on a comprehensive collection of literatures in order to explore the treatment principles more intuitively. Networks of TCM patterns and CHMs which are most frequently used in UTI treatment are built-up and analyzed, two major principles are explored in treating UTI from 44,662 records of literature: Clearing the damp-heat with strengthening healthy qi. These findings might guide the clinicians in treatment of obesity.
Vast growth of biomedical databases has increased most of the researchers focus on the field of Text Mining. The documents appear in unstructured format. To process and discover knowledge from these data, the unstructured databases must be converted to structured format. For this task Text mining plays a vital role. Text preprocessing is an essential step in text mining. The common preprocessing tasks in text mining are Tokenizing, Removing Stop words and Stemming. In this paper we have discussed the implementation steps we have done on PubMed abstract using Rapidminer.
Challenges in text mining arise from multi-corpus and high dimensionality involving natural language. Features from datasets needs to be composed or articulated. This paper aims to develop a new approach to align text feature using ontology, which can form the base of text dimension reduction. Firstly, a novel text feature graph is defined, based on which we can do articulation. Secondly, an algebra system is proposed for text feature graph computing. Finally, an instance is demonstrated to show the efficiency and accuracy of the proposed approach.
The dimensionality of traditional text representation is large, but the underlying text data is sparse. This makes text clustering a very challenging task. Using language models and deep contextualized representations is promising in many Natural Language Processing (NLP) tasks. However, some task-specific guidance is necessary to adapt language models to a novel domain or to particular downstream tasks. We present an empirical study of a pipeline for semi-supervised text clustering tasks. Our proposed method utilizes a small number of labeled samples to fine-tune pre-trained language models. This fine-tuning step adapts the language models to produce task-specific contextualized representations, improving the performance of downstream text clustering tasks. We evaluate two clustering algorithms using the output of three different language models on six real-world text mining tasks to demonstrate to what extent this pipeline can improve text clustering accuracy and the amount of labeled samples needed for improvement. Our experiments show that for topic mining in novel domains or surfacing the intentions of abstracts, language models begin to produce better task-specific representations using a labeled subset as small as 0.5% of the task data. On the other hand, to find topics in domains that are overlapping with pre-training corpora, language models need labeled subsets closer to 1.0% of the task data to overcome the catastrophic forgetting problem. Further experiments show the downstream clustering accuracy gain begins to slow down or plateau if language models are fine-tuned with more than 5% of the task data. There is a trade-off between the desired downstream clustering quality and the cost for labeling and fine-tuning language models. Fine-tuned with 2.5% of the task data, our approach matches or exceeds the current state-of-the-art for several clustering tasks and provides baseline results for two novel clustering tasks. These results provide solid guidance t... (Show More)
There are several requirements to the preprocessing of the classified texts. Within the frame of this work importance of these requirements have been analysed.
High School teachers in Japan are sending very busy days on their daily works including teaching, support for the club activities and deskwork. Among them, they share a lot of time for managing the club actives of students compared with other countries. School Social Worker can coordinate the professionals out of school and can help teachers by decreasing their burden on that area. There are few related papers concerning the support of club activities by utilizing the professionals outside. In this paper, a questionnaire investigation is executed in order to clarify their current condition and their consciousness, and to seek the possibility of utilizing school social worker for their support. Fundamental statistical analysis and Text Mining Analysis are performed. Some interesting and instructive results were obtained.
The growing aviation industry in both number of passengers and aircraft movements in combination with the implementation of Just Culture facilitates a growing number of occurrence reports being submitted by service providers to their competent authority. Still, actually using the sheer amount of safety data being collected with these reports for safety initiatives has proven to be difficult as analysing the reports is a time consuming activity. Moreover, reports usually suffer from missing data and labelling. This paper demonstrates an automatic text classification method by means of machine learning. By automatically classifying occurrence reports analysts may be able to work more efficiently as conducting queries on reporting databases become more accurate thanks to the categorization of reports. In this paper, a text classifier is trained by means of a Random Forest algorithm on the ICAO Occurrence Category. The classifier is used to classify more than 45,000 reports while reaching accuracy between the 80–93 %.
Concept location is a key activity during software modernization since it allows maintainers to exactly determine what pieces of source code support a specific concept. When concepts supported by an information system are getting outdated or misaligned, concept location becomes a time-consuming and error-prone task. Moreover, information systems embed significant business knowledge over time that is neither present nor documented anywhere else. For this reason, this paper proposes an approach to locate concepts in execution traces, which contain the actual and current information derived from the execution of the existing code. This approach, based on text segmentation tools, first retrieves and depicts relevant information from the execution traces, and then, it provides a mapping between such information and particular pieces of source code. The main idea of the approach is to take the file of the trace as text where the content of each function (i.e., method) appearing in the trace is a sentence composing the text and then applying the C99 algorithm to segment the text to a thematically coherent segments. The C99 is the state of the art algorithm proposed in text mining community to segment natural text. Each produced segment is composed by a set of functions or methods and bears a concept. Also, under this idea all techniques, proposed by text mining community, for text segmentation will be accessible to the software evolution community. We evaluated our approach on different execution scenarios of JHotDraw and ArgoUML.
In our era, most of the communication between people is realized in the form of electronic messages and especially through smart mobile devices. As such, the written text exchanged suffers from bad use of punctuation, misspelling words, continuous chunk of several words without spaces, tables, internet addresses etc. which make traditional text analytics methods difficult or impossible to be applied without serious effort to clean the dataset. Our proposed method in this paper can work in massive noisy and scrambled texts with minimal preprocessing by removing special characters and spaces in order to create a continuous string and detect all the repeated patterns very efficiently using the Longest Expected Repeated Pattern Reduced Suffix Array (LERP-RSA) data structure and a variant of All Repeated Patterns Detection (ARPaD) algorithm. Meta-analyses of the results can further assist a digital forensics investigator to detect important information to the chunk of text analyzed.
Medical records of Traditional Chinese Medicine (TCM) are usually free text and unstructured data, how to extract medical terms from TCM medical records based on conditional random fields is an interesting problem. TCM medical records obtained from dermatology in Guangdong Provincial Hospital of Chinese Medicine are segmented to single words and labeled with grammatical properties of words by TCM expert, and divided into training sets and testing sets. Clinical terms of training sets are also labeled with clinical terms labeling. In order to evaluate the recognition ability of CRF, three indicators (recognition Precision (P), recognition Recall (R) and F-score (F)) are defined, and three comparisons are given: CRF compared with HMM, ME, and MEMM; Cross test with CRF; and recognition accuracies on different types of clinical terms with CRF. The result shows that, CRF performs a best recognition ability comparing with HMM, ME and MEMM in different groups with different levels of training sets. CRF is credible and stable for different training samples by ten cross test. With the same CRF model, terms of TCM diagnosis can be identified most easily, and terms of symptoms and signs are relatively difficult to identify. So CRF is a suitable model on mining clinical terms in free text of TCM medical records.
Scientific research projects play an important role in promoting the science and technology competitiveness of a country. Due to lack of information open and sharing, it is possible to approve similar or duplicated projects by different government departments. In some way, these similar projects are a waste of scientific resources. To avoid such a problem, this paper proposes a discriminant framework for detecting similar projects based on big data mining technologies, providing evidence-based decision making for government departments during the project approval process. Firstly, we construct a big data file associated with officially approved projects, including project titles, principal investigators, research organizations, keywords, and bibliographies of published scholar papers. Secondly, a discriminant framework is proposed to detect similar projects by mining information from the above big data file. Finally, we adopt the Hadoop architecture to speed up the data mining algorithm. To demonstrate the effectiveness and feasibility of the framework, we implement a prototype system for similar projects detection.
Positive “word of mouth” is the key to successful innovation diffusion. Innovation managers pay close attention to examine customer sentiment. Online reviews are often the most accessible sources of customer feedback. Online review ratings and online review volume, the two most common metrics to interpret customer sentiment from online reviews, have some critical limitations. Online review ratings are prone to extremity bias since extremely satisfied or dissatisfied customers are most likely to leave online reviews. Online review volume can increase for any reason, and it does not necessarily indicate positive or negative customer feedback. This article explores text mining methods and proposes some alternative metrics to interpret customer sentiment. The findings show that sentiment scores might be less prone to extremity bias compared to online review ratings. Sentiment scores tended to fit a normal distribution while online review ratings were skewed to extreme values. Sentiment scores and review lengths, when combined, can provide a new angle to observe enthusiasm.
Social Media houses a vast amount of data which can be utilized for data mining. It has become an inseparable source that has been influencing the lifestyle of millions of people. The main objective of this paper is to identify the behavior of youth to sociality using social media mining techniques. This paper demonstrate how we can exploit social and structural textual information of Tweets to identify the trending topics using frequent patterns and classify the sentiments of Tweets using Natural Language Processing. Using social media as a platform this work intends to gather data and to analyze it to draw the behavioral pattern of the current youth that helps us to identify their interests, likes and dislikes. The experimental result of this approach shows the opinion of youth/publics on various matters and persons in different areas and verified the effectiveness. This technique proposed may help people to make decision and performance improvement.
Social networking sites these days are great source of communication for internet users. So these are important source for understanding the emotions of people. In this paper, we use data mining techniques for the purpose of classification to perform sentiment analysis on the views people have shared in Twitter. We collect dataset, i.e. The tweets from twitter that are in natual language and apply text mining techniques -- tokenization, stemming etc to convert them into useful form and then use it for building sentiment classifier that is able to predict happy, sad and neutral sentiments for a particular tweet. Rapid Miner tool is being used, that helps in building the classifier as well as able to apply it to the testing dataset. We are using two different classifiers and also compare their results in order to find which one gives better results.
With the rapid rise of artificial intelligence technologies such as machine learning and the rapid development of the big data industry, more and more attention is paid to the use of data itself, especially the Chinese text data, which is more complex in expression and richer in the information. It is a necessary step to process the raw Chinese text data before it is used for specific application tasks. However, the current strategies for processing data are generally to deal with data in different fields and specific application tasks. In this paper, to further improve the quality of Chinese data processing and give play to the application value of Chinese data, we propose a general and feasible Chinese text preprocessing strategy, named the multi-level data preprocessing strategy (MLDPS). This strategy uses four effective links to process raw Chinese text data systematically. We believe that the proposed MLDPS has relatively strong practical significance, and provides a better idea for preprocessing Chinese text data.
Ontology is a knowledge representation model that used in the semantic web. To perform domain enrichment in the ontology needed a fast and efficient method, this can be achieved by using text mining approaches. This research was exacting text mining approach to enrich ontology in epidemiology domain, especially in tuberculosis. This study utilizes existing ontology, namely Epidemiology Ontology (EPO) and data from a variety of scientific documents about tuberculosis. The data obtained from scientific documents regarding tuberculosis (pulmonary TB) will be used to enrich Epidemiology Ontology (EPO). In this study enrichment is done semi-automatically, term and concept extraction is done automatically from a natural language corpus and validation is done manually by involving epidemiology experts.
Researches on Big data has been a heated discussion because the rapid development of Internet brings a large amount of valuable data. In this paper, we proposed an effective method to mining the textual data. To be more specific, a multi-label classification method based on textual data mining was designed. The design of this method is built on deep learning. It first adopts Bret pre-training model to express the text, and then be processed with text mining module. This module employs the self-attention mechanism to get the sentence embedding of the text. Meanwhile, convolution is adopted to gain the local semantic information. Then, keyless attention fusion, an effective method used in multi-modal fusion is utilized to aggerate the above two parts. Besides, the value of textual data for reviews about commodities engrossed our attention, as it can heavily help the buying decision of individuals. Thus, our research of textual data focused on the reviews of commodities. Each data has multiple attributes, and that is the reason we chose Multi-label classification in Natural Language Processing as the direction of our model design. We conducted an in-depth discussion of the combination of multi-label classification and the features of those textual data. Finally, we compare our model with other baseline models with evaluation metrics of precision@K, recall@K, F1_score@K and Ndcg@k. Our model gains the best performance in those metrics. With the help of our model, customers can gain precise and objective information about the goods and their purchase decision can be more sensible.
Large amount of categories with skewed category distribution over documents still not a closed question in the state-of-the-art technologies in automated text classification. In this paper we present a proof of concept for an automatic model of complaints screening, using text mining. Through a complaints link of the Office of the Comptroller General (CGU) site, citizens have access to a form to file their complaints. These complaints must be screened and delivered to one of 64 CGU's coordination by subject. Nowadays the complaints screening is done manually. Considering the complaints storage on the database now and the arrival of new complaints, combined with the time spent on manual sorting, the timely analysis of the reported occurrences it becomes increasingly difficult. We compare two approaches: supervised learning with classifiers algorithms and unsupervised leaning with topic modeling and text search. The best results were achieved using ranking based on the Huffman Tree algorithm. This proof of concept demonstrated the feasibility of automatic sorting without the loss of quality compared to manual sorting.
Social media registers millions of messages per second. This paper aims to develop a visual analysis that allows identifying events of particular interest and the time intervals in which they occur. This analysis uses text mining techniques and sentiment analysis.
We proposed and completed a real-life data mining problem, aka, text corpus classification. We extracted 15,500 medical documents relevant to ten different diseases and used the bag-of-words model to create a word occurrence vector for each document. The latent semantic analysis (LSA) method is then used to reduce the occurrence vector’s dimensionality to a feature vector of dimension 200. We selected a multilayer perceptron (MLP) neural network to do the final classification and report the performance comparison with the other six classifiers. We also completed the grid search for the best feature subspace dimensionality.
This study explores traffic crash narratives in an attempt to inform and enhance effective traffic safety policies using text-mining analytics. Text mining techniques are employed to unravel key themes and trends within the narratives, aiming to provide a deeper understanding of the factors contributing to traffic crashes. This study collected crash data from five major freeways in Jordan that cover narratives of 7,587 records from 2018-2022. An unsupervised learning method was adopted to learn the pattern from crash data. Various text mining techniques, such as topic modeling, keyword extraction, and Word Co-Occurrence Network, were also used to reveal the co-occurrence of crash patterns. Results show that text mining analytics is a promising method and underscore the multifactorial nature of traffic crashes, including intertwining human decisions and vehicular conditions. The recurrent themes across all analyses highlight the need for a balanced approach to road safety, merging both proactive and reactive measures. Emphasis on driver education and awareness around animal-related incidents is paramount.
Medical Knowledge Discovery and Data Mining (KDD) over text is a promising yet difficult technology for unlocking meaning and uncovering associations in vast clinical text repositories. We report our experience in developing a new text analytic system called MEDAT or Medical Exploratory Data Analysis over Text, which overcomes several problems in text mining. The MEDAT system employs an annotated semantic index with a large number of assertions (propositions). The semantic index is able to capture complex assertions which encapsulate conceptual relationships including their modifiers at a granular level. The index represents semantically equivalent sentences with the same symbols, a necessary component for KDD semantic queries, including semantic Boolean and correlation queries. The graphical user interface enables users to perform complex semantic analysis of the Roentgen corpus, consisting of 594,000 de-identified radiology reports with 4.3 million sentences, without having to learn a programming language. The MEDAT architecture offers a novel framework for text mining in other medical domains.
The population of tourists has grown rapidly with the development of social media. The rise of social media has changed the behaviors that passengers visit sightseeing spots. Online consumer reviews could be considered as the main channel for providing valuable information to consumers. Revisit intention could directly influence the future behavior of customers. It's also one of the crucial factors that enhance the income growth of tourism. However, relatively few researchers focused on why passengers don't revisit directly. Therefore, this study will focus on the topic that why passengers didn't revisit again. We'll use textual reviews of social media instead of questionnaire survey. And text mining and feature selection (Least Absolute Shrinkage and Selection Operator, LASSO) methods have been employed to identify the factors that affect passenger non-revisit intention. From the results, this study will provide some suggestions for the tourism industry to improve their service quality and increase their revisit intentions.
Text mining approaches are being used increasingly for business analytics. In particular, such approaches are now central to understanding users' feedback regarding systems delivered via online application distribution platforms such as Google Play. In such settings, large volumes of reviews of potentially numerous apps and systems means that it is infeasible to use manual mechanisms to extract insights and knowledge that could inform product improvement. In this context of identifying software system improvement options, text mining techniques are used to reveal the features that are mentioned most often as being in need of correction (e.g., GPS), and topics that are associated with features perceived as being defective (e.g., inaccuracy of GPS). Other approaches may supplement such techniques to provide further insights for online communities and solution providers. In this work we augment text mining approaches with social network analysis to demonstrate the utility of using multiple techniques. Our outcomes suggest that text mining approaches may indeed be supplemented with other methods to deliver a broader range of insights.
The exponential growth of the data may lead us to the information explosion era, an era where most of the data cannot be managed easily. Text mining study is believed to prevent the world from entering that era. One of the text mining studies that may prevent the explosion era is text classification. It is a way to classify articles into several predefined categories. In this research, the classifier implements TF-IDF algorithm. TF-IDF is an algorithm that counts the word weight by considering frequency of the word (TF) and in how many files the word can be found (IDF). Since the IDF could see the in how many files a term can be found, it can control the weight of each word. When a word can be found in so many files, it will be considered as an unimportant word. TF-IDF has been proven to create a classifier that could classify news articles in Bahasa Indonesia in a high accuracy; 98.3%.
Data mining technology is the key technology and core content of big data age. The undergraduate data mining course introduces the basic concepts, basic principles and application techniques of data mining, as well as the characteristics and new technologies of data mining under the background of big data. According to the characteristics of undergraduate students, the curriculum should weaken the theory and algorithm as much as possible, and emphasizing the application. Through analysis and experiment on various examples, to enable students to face the specific application problems, can use the SPSS Modeler to designing a data processing, select the appropriate data mining method, and finally get the ideal results of data mining.
Educational Data Mining is a prominent area to explore information in educational fields using data mining algorithms. In this paper we have used few learning algorithms to effectively rate the faculty belonging to an educational institute on the basis of feedback submitted by the students. Our proposed model uses sentimental analysis and machine learning classifier algorithms for capturing the emotions from the students feedback system. This model gives an accurate and efficient way to rate the faculty belonging to a particular educational institute. With this proposed model the faculty will be evaluated and rated with certain specified parameters which will help us to improve the academic and education standard.
Data mining has pulled in a fantastic arrangement of consideration inside the data business and in the public arena in general as of late, because of the wide accessibility of huge sums data and likewise the nearby at hand might want for moving such data into supportive data and knowledge. The data and knowledge picked up will be utilized for applications beginning from market research, fraud detection, and client retention, to production management and changed investigations. This paper is to inspect the part of data mining for data extraction in web page, structure and utilizations mining in current web models, and likewise the frameworks the strategy for separating designs from data. This paper furthermore blessing data processing natives, from that data processing question dialects will be composed. Issues concerning an approach to incorporate a data mining framework with a database or data distribution center are specified. Moreover to discovering a grouping of data mining frameworks, and its troublesome examination issues for building data processing tools of the long run.
With the evolution of web technology, there is a huge amount of data present in the web for the internet users. These users not only use the available resources in the web, but also give their feedback, thus generating additional useful information. Due to overwhelming amount of user's opinions, views, feedback and suggestions available through the web resources, it's very much essential to explore, analyze and organize their views for better decision making. Opinion Mining or Sentiment Analysis is a Natural Language Processing and Information Extraction task that identifies the user's views or opinions explained in the form of positive, negative or neutral comments and quotes underlying the text. Various supervised or data-driven techniques to Sentiment analysis like Naïve Byes, Maximum Entropy and SVM. For classification use support vector machine (SVM), it performs the sentiment classification task also consider sentiment classification accuracy.
In order to improve the level of power customer service, we need to start with natural language processing technology, and conduct in-depth text mining of power customer complaint work order. Combined with the format and characteristics of power customer service work order, this paper proposed a text mining strategy of power customer service work order based on natural language processing technology which includes the following processes: work order data cleaning, text segmentation, information characterization, model training and model evaluation is proposed. The TF-IDF algorithm and the model optimization method based on accuracy calculation are applied in this strategy, which greatly improves the level of power customer service.
Systematic Literature Review (SLR) is a structured way of conducting a review of existing research works produced by the earlier researchers. The application of right data analysis technique during the SLR evaluation stage would give an insight to the researcher in achieving the SLR objective. This paper presents how descriptive analysis and text analysis can be applied to achieve one of the common SLR objectives which is to study the progress of specific research domain. These techniques have been demonstrated to synthesis the progress of Master Data Management research domain. Using descriptive analysis technique, this study has identified a trend of related literary works distribution by years, sources, and publication types. Meanwhile, text analysis shows the common terms and interest topics in the Master Data Management research which are 1) master data, 2) data quality, 3) business intelligence, 4) business process, 5) data integration, 6) big data, 7) data governance, 8) information governance, 9) data management and 10) product data. It is hoped that other researchers would be able to replicate these analysis techniques in performing SLR for other research domains.
Technological innovation is becoming one of the critical factors in promoting social development all over the world. The vigorous development of patent applications in recent years provides an opportunity to reveal the inherent laws of innovation, but it also puts forward higher requirements for patent mining technology. An essential step in patent text mining is to build a technical portrait for each patent, that is, to identify the technical phrases involved, which can summarize and represent the patent from a technical perspective. Previous technical phrase extraction methods thoroughly used technical phrases' characteristics and the relationship between technical phrases. Regarding our observations, the relationship between patent texts and technical phrases is also essential. Specifically, critical technical phrases are more relevant to the patent text and can be discovered by the attention mechanism. Motivated by this, we propose an unsupervised technical phrase extraction method based on the attention mechanism named UTESC. Self-attention captures the importance of technical phrases in sentences, and cross-attention captures the relevance between technical phrases and patents. Extensive experiments and algorithm comparisons on patent datasets have proven the effectiveness of our algorithm.
A large amount of information is generated across the globe through various IT enabled services. Health care industry has turned out to be the most significant and information-rich area. The term Health care is gaining popularity due to the improvement in medical science and the prevalence of new and peculiar diseases. In recent years, there is increased usage of electronic personal health care devices and consulting physicians for various illnesses that are gaining insight. Medical records are generated every second from hospital visits, inpatient wards and health care devices. These records that turn to become big data require a lot of analyses and predictions. Text mining is a broad field that needs more mining in the medical records as there are structured, semi-structured and unstructured data. This paper depicts/overlays the study of algorithms, tools and techniques involved in text mining for health care data.
Several problems are rised in order to enhance the effectiveness of communication in online discussion. The similarity and repetition of comments in terms of questions in the sentences or text meanings as well as triggers the emerging of miscommunication amongst participants in a forum discussion are investigated. Moreover, some comments seems are ignored or not been touched by other participants and in advance the effective used of forum discussion as knowledge acquisition and sharing can not be achieved. This paper studies the application of Centroid Linkage Hierarchical Method (CLHM) Algorithm and Hill Climbing methods in findings the similarity value of participants comments and clustering based on it. The analysis follows the text mining process including text processing, text transformation, attribute selection and pattern discovery. In order to test the validity and accuracy of both application methods, confusion matrix in euclidean and consine similarity were calculated. As the results, from variety numbers of comments groups, including Bersosial.com in 17 comments, Indowebster.com in 27 comments and Teknojurnal.com in 51 data comments provided the value of well-separated clusters performed. This testing also defined that the alteration of threshold and altitude did not affect the clustering process. From the calculation of F-measure values in confusion matrix explained that consine similarity provided better result that euclidean distance where teknojurnal 0.89, indowebster,com 0.71 and bersosial.com 0.57. This showed that CLHM algorithm and Hill climbing methods are effective approaches and have been successfully applied in comments clustering of online discussion.
Artificial Intelligence (AI) and Natural Language Processing (NLP) emerged as the powerful tools in the healthcare sector offering potential to revolutionize patient care through efficient and accurate analysis of the unstructured text data, particularly the electronic health records (EHRs). Recognizing the critical role of adherence to the established standards in healthcare, the article introduces the Doctor Prescription Evaluation System. This software design aims to assess compliance of the doctors prescriptions with these standards, ultimately enhancing the patient care. The proposed model employs CountVectorizer and supervised machine learning algorithms to map the input text in the output labels. By providing a comprehensive overview of the software algorithm, the article meticulously analyzes each step and unravels the fundamental mechanisms driving its functionality. The findings of this study expose the system's ability to accurately evaluate and analyze doctor prescriptions, enabling efficient decision-making and ensuring high-quality patient care.
Sentiment analysis emerged as an important computational domain to gain insights from snippets of texts, as social media recently gained popularity. Text mining has long been a fundamental data analytic for sentiment analysis. One of the popular preprocessing approaches in text mining is transforming text strings to word vectors which form a high-dimensional sparse matrix. This sparse matrix poses challenges to induction of an accurate sentiment classification model. Feature selection is usually applied for finding a subset of features from all the original features from the sparse matrix, in order to enhance the accuracy of the classification model. In this paper, a new feature selection method called Optimized Swarm Search-based Feature Selection (OS-FS) is proposed. OS-FS is a swarm-type of searching function that selects an ideal subset of features for enhanced classification accuracy. The swarm search in OS-FS is optimized by a new feature evaluation technique called Clustering-by-Coefficient-of-Variation (CCV). The proposed scheme is verified via a mood classification scenario where 100 sample news are extracted from CNN.com. One of six human emotions (or sentiments) would have to be recognized from the news contents, by computer using text mining. The results show superiority of OS-FS over the traditional feature selection methods.
Approximately 1.6 million individuals use the e-commerce website “amazon” to buy things from a variety of categories, including food. Reviewing products by consumers who have already purchased them is beneficial to those who are considering doing so, however reviews can be either positive or negative. The buyer finds it difficult to read through such many evaluations before making a purchase, but machine learning ideas and training models make it possible. Our objective is to categorize the reviews based on the attributes that are present in the dataset in order to address issues like these. Redundancy is present in data when it is presented to us in its raw form. So, since evaluations with a score of 3 are regarded as impartial, we delete them along with redundancy. After that, we use the NLP tool kit (a column in the data set) to preprocess the text by removing any stop words (such as in, as, is, on, and punctuation), and we lowercase each letter. The suggested approach renders the text into machine-understandable language using word embedding techniques. Text processing is necessary because customer reviews written in language that is understood by humans cannot be read by machines. The data must be in a machine-readable language in order to apply any classification technique. We separate the data into train and test set after the preprocessing is complete. After the training is complete, we use this model on a test set of data to determine its accuracy. Next, we utilize classification methods like logistic regression and XG Boost to see how accurate our model is. This study’s conclusion involves using the model we developed to predict the review based on previous reviews. In this project, we build a model, feed it with existing reviews, apply it to upcoming reviews, and then forecast if the product is good or not. For this work we have taken the data set from Kaggle.
Teaching evaluation system is widely used to assess and investigate the education quality. Presently, sentiment analysis contributes for student sentiment polarity detection in teaching evaluation which collects the feedback messages. Text mining techniques are broadly extended to classify the effective improvement of the sentiment polarity analysis. Furthermore, the feedback messages from opened-end questions which stored in teaching evaluation system are selected for the classification. In addition, various methods used for classification in the experiment are Naïve Bayes, ID3, J48 Decision tree. In this paper, reducing the feature in data preprocessing stage and teaching sentiment analysis using voting ensemble method of machine learning are proposed and compared with existing typical machine learning for sentiment analysis. The experimental results show that the voting ensemble learning integrate with Chi-Square feature selection exhibits higher than typical classifiers.
In natural language processing, cleaning up a lot of scraped text data is an important step that involves getting rid of irrelevant and noisy data from the text corpus. Text data obtained from web scraping or other sources may contain various unwanted elements such as HTML tags, non-textual characters, and punctuation marks, which can negatively impact the accuracy and efficiency of NLP algorithms. The study simplifies obtaining text data from multiple websites by employing TxtPrePro, a simple pipeline for scraping and text preprocessing that can be used for topic modeling, text summarization, sentiment analysis and other purposes. The proposed method used web scraping to collect a large amount of text data then performed proper text preprocessing, obtained some information such as tables visualizations and obtained a cleaned corpus.
Companies operating patient support programs for chronic diseases have been dedicated to enhancing treatment adherence by utilizing data from various interventions of the programs. The purpose of this paper is to examine whether the textual patient notes recorded by program coordinators can be beneficial to predict non-adherence and provide useful insights. In this paper we show work in processing and analyzing over 20,000 patient notes corresponding to 1313 Psoriasis patients using statistical analysis and several NLP methods, such as term representation, sentiment analysis and topic modelling. To build predictive models, Support Vector Machine (SVM), Random Forest (RF) and Logistic Regression (LR) are tested with different feature subsets. The best performing model is SVM with 93% accuracy and 91% recall of non-adherent. Additionally, we also present patterns to differentiate non-adherent and adherent patients in terms of completion efficiency of call objectives and uncontactable problem. Accordingly, high-risk patients can be targeted to take interventions.
Artificial intelligence in education (AIEd) is one rapidly developing area. The significant challenge for researchers is determining the best machine learning approach to use when submitting a study to AIEd. The purpose of this research is to provide a comprehensive and scientific assessment of important AIEd studies across five domains. This study looked into surveys and analyses of 25 papers, the majority of which sought to identify the most effective approaches to using machine learning tools in education. These analyses were based on identified research gaps and suggested future directions. Researchers want to know which AI techniques are more beneficial and which educational data mining (EDM) dimensions are limited and prominent in AIEd research. From 2016 to 2023, this article includes significant research studies based on five critical EDM with AIEd dimensions: decision support system, performance evaluation, text mining, outlier detection, and opinion mining.
Sentiment analysis represents context-based text mining that classifies and abstracts particular information from the source data, and helps to interpret people's feelings. It adopts the NLP approach to classify people's opinions regarding goods or feedback. Sentiment analysis handles people's opinion and behavior in the context of feelings and attitudes concerning an episode or incidence. There are many domains that use opinion mining widely such as commercial goods review, social media study and film evaluation and so on. Semantic analysis provides major support in building recommendation frameworks. User provides textual feedback such as web-based reviews, remarks or responses on social networking and business sites. Machine learning is a common approach used for classification. This project devises a nascent methodology through the integration of dissimilar frameworks. This work applies Random Forest as a feature extraction method for taking out crucial features of the data suite. The K-mean clustering is enforced which can cluster similar type of features and in the last voting classifier is applied. Voting classifier consists of LR, KNN, SVM models. Python is the tool used in this work for the implementation of the devised methodology and evaluates new methodology with respect to some global performance indices.
With the comprehensive development of Internet technology, web data mining technology has achieved rapid development and is widely used in various industries. Cloud computing is a new method of sharing infrastructure, which is based on open standards and services and centered on the Internet, providing safe, fast and convenient data storage and network computing services. Cloud computing has strong information storage capacity, high security and high level of data processing and analysis. As an indispensable part of storage technology, the application level of cloud computing technology has been improved, effectively ensuring the quality and efficiency of data preservation. By analyzing the characteristics of Web data mining technology in cloud computing environment, we can realize the breakthrough of cloud computing in data storage and improve the storage capacity and security. This paper analyzes the Web data mining algorithm in the cloud computing environment, improves the accuracy and effectiveness of data mining, and uses the cloud computing environment to improve the Web data mining strategy, thus laying the foundation for the healthy development of China's Internet.
As warnings of diseases are easily neglected by people, the golden treatment time is missed. Osteoporosis is one of the examples that its symptoms, e.g. knee and back pain, are usually overlooked. In this paper, we implement an osteoporosis prediction system based on text cloud mining analysis to recognize potential osteoporosis patients. The prediction system recognize a potential osteoporosis patient according to his/her symptoms, status, and medical records. A library of use cases are firstly implemented according to collected osteoporosis patients. Similarity measure is calculated for a person and the cases in the library. A person with more similar to the cases of osteoporosis patients means he/she has a higher chance to suffer from osteoporosis.
The Frequent Pattern Mining (FPM) algorithm, which is commonly used to find frequent itemsets, does not scale well in today's Big Data, especially for dynamically incremental databases. As a result, the FIM algorithms must be developed to support incremental mining. The aim of the Incremental frequent mining is to discover interesting patterns which is used to support data analysis and for making decisions. Incremental mining aims to extract patterns from the dynamic databases which have applications in domains such as product recommendation, text mining, market basket analysis, and web click stream analysis. Several algorithms have been proposed to mine the frequent patterns from both the original and incremental databases. The field of incremental mining is becoming more vibrant and increasing as new algorithms are developed every day. This paper presents a detailed survey of incremental pattern mining, and its methods.
Collecting student feedback is commonplace in universities. These surveys usually include both open-ended questions and Likert-type scale questions but the answers to open questions tend not to be analysed further than simply reading them. Recent research has shown that text mining and machine learning methods can be utilized to extract useful topics from masses of open student feedback. However, to our knowledge, not many off-the-shelf applications exist for processing open-ended student feedback automatically. Additionally, the use of text mining tools may not be available to all educators, as they require in-depth knowledge of text-mining, data analysis, or programming tools. To address this gap the current study presents a tool (Palaute) for analyzing written student feedback using topic modeling and emotion analysis. The utility of this tool is demonstrated with two real-life use cases: First, we analyze student feedback data collected from courses in a software engineering degree programme, and then feedback from all courses organized in a university. In our experiments, the analysis of open-ended feedback revealed that on certain software engineering course modules the workload is perceived as heavy, and on some programming courses the automatic code grader could be improved. The university-wide analysis produced indicators of good teaching quality, such as interesting courses, but also some concrete improvement points like the time given to complete course assignments. Therefore, the use of the tool resulted in actionable improvement points, which could not have been identified using only numeric feedback metrics. Based on the demonstrated utility, this paper describes the design and implementation of our open-source tool.
Knowledge discovery is inseparable from data mining and data reconstruction, and the digital humanities application platform provides diversified text mining and visualization methods for further tapping inherent value of information resources. Based on characteristics of the construction of data resources under the environment of digital humanities, the thesis analyzes the copyright dilemma in digital resources development from the aspects of data development methods and data copyright ownership, and explores the two operating mechanisms of Docusky and Hathitrust data capsule, thus to optimize the digital humanities application platform reasonably for the purpose of the balanced interests in the development and utilization of digital resources.
Productivity of organic coffee plants in Indonesia is still lower if compared by productivity of coffee which use ordinary cultivation. One of the problems, which is faced by farmers to develop organic coffee is no certainty market. This could be because not all people of Indonesia are able to buy organic coffee products which quite expensive. Based on these, it is necessary to analyze public perception sentiment of organic coffee products, to identify potential and opportunities the development of organic coffee farming in Indonesia. This research uses a text mining approach to classify the public perception sentiment on organic coffee products based on tweet which posted in social media, i.e., twitter. Sentiment classification is performed by Naïve Bayes Classifier algorithm. The most of sentiment value formed in this research is positive sentiment. These results show that the public perception on organic coffee is in positive manner. So that the prospect of organic coffee plants development in Indonesia and the market opportunity of organic coffee products are predicted to rise as well.
Twitter is a microblogging social networking service that is widely leveraged by individuals to broadcast information. Twitter text messages are a popular medium for reporters on the field. Twitter messages about armed conflicts are rich in information for national security and defense communication strategies. In this paper, an approach is discussed to measure text similarity between pairs of Greek-language messages about the Russian-Ukrainian conflict. The goal is to give readers a fundamental grasp of how information is shared on Twitter in relation to military developments. According to evaluation studies of the preliminary results of the proposed approach, pairs of Twitter messages referring to the same incident are highly likely to produce a high similarity score. Observed deviations from this hypothesis are attributed to linguistic elements and the coverage of one aspect of events per message. The later illustrates the wealth of broadcasted information that can be explored by domain experts. In conclusion, text similarity can be used to derive first insights from messages that discuss the complex course of armed conflict developments.
Chronic conditions are long-term diseases that inflict limitations to patient’s life-style. These conditions can be controlled but not fully cured. Therefore, effective self-management and opting for a healthy life-style can make a meaningful change for patients who suffer from chronic illnesses. In this study, we have focused on type 2 diabetes, as it is one of the most expensive chronic conditions that covers a wide spectrum of complications that can be prevented by healthy eating, being physically active, monitoring of blood sugar, compliance with medication and healthy coping skills. In order to detect life-style indicators, we have reviewed discharge notes of more than 40,000 patients who have been hospitalized in the critical care units of the Beth Israel Medical Center between 2001 and 2012. We looked at the discharge notes, diagnosis (ICD-9) and all the tests results and samplings of patients in the MIMIC-III dataset. The objective is to find life-style indicators of patients who are not compliant with the dietary restrictions, medications and exercise schedules that need to be followed by a diabetic patient. In order to find such life-style indicators, text mining and natural language processing analysis were used. We found promising results based on the Body Mass Index, A1C levels and history of diabetes of each patient. Next, we looked at the most repeated key phrases in every category and came up with different distributions for every patient group. Automating ineffective self-management detection in diabetic patients will eventually be helpful to medical experts, caregivers, insurance companies, behavioral researchers and patients themselves.
Extraction is an essential part of processing a document to ensure the success of the text mining process. In this study, the example of the SRS document used is the Integrated Service Application (APTU) KPKNL Bandung, an application to manage the process of submitting service tickets at the State Property and Auction Service Office. There is a difference in interpreting the activities that exist in the Use Case Description artifact with a Sequence Diagram that provides an overview of the functionality of a process to show the involvement of an activity related to the Use Case Description. This study aims to perform step extraction on the Use Case description. The results of this extraction are compared for their suitability with the sequence diagram using the concept of text mining. There are core results from this research activity. First, the highest similarity between documents is in the SP01 and SD01 documents, with the similarity value being 0.69108792. Second, the highest similarity between words is found in words “list” and “menu,” with the similarity value being 0.9412. Third, the Kappa Score from Gwet's AC1 formula using the Python programming language is 0.12362, which means “Slight Agreement,” while the Kappa Score value using a questionnaire filled in by the expert is 0.97464, which means “Almost perfect.
There is a tremendous amount of attention being focused on improving human health these days. The World Health Organization (WHO) statistics show that disease and mortality rate greatly depend on access to proper healthcare, which is not available to a vast majority of the global population. This technical paper presents our vision of automating some of the healthcare functions such as monitoring and diagnosis for mass deployment. We explain our ideas on how machines can help in this essential life supporting activity. Diagnosis part of the problem has been researched for long, so we set out working on this first, while the remaining is still in idea stage. We give insights into our work on automating medical diagnosis using text mining techniques and include some initial results.
Technology roadmapping is a widely accepted method for offering industry foresight as it supports strategic innovation management and identifies the potential application of emerging technologies. While roadmapping applications have been implemented across different technologies and industries, prior studies have not addressed the potential application of emerging technologies in the retail industry. Furthermore, few studies have examined service-oriented technologies by a roadmapping method. Methodologically, there are limited roadmapping studies that implement both quantitative and qualitative approaches. Hence, this article aims to offer a foresight for future technologies in the retailing industry using an integrated roadmapping method. To achieve this, we used a sequential method that consisted of both text mining and an expert review process. Our results show clear directions for the future of emerging technologies as the industry moves toward unmanned retail operations. We generate eight clusters of technologies and integrate them into a roadmapping model, illustrating their links to the market and business requirements. Our study has a number of implications and identifies potential bottlenecks between the integration of front- and back-end solutions for the future of unmanned retailing.
A satisfactory cross-border logistics service (CBLS) can help promote business activities in cross-border e-commerce. Kansei engineering (KE) is an approach to design the elements which satisfy customers' affective and emotional perceptions into services and products. In this study, the KE approach is applied to derive ideas for the development of CBLS. For this purpose, Partial Least Squares (PLS) is used to analyze the relationships between the feelings of customers and service elements of CBLS. Moreover, this study demonstrates the applications of text mining techniques to analyze the online contents regarding CBLS. Online content mining assists in identifying the service elements and Kansei words for CBLS. Importantly, the relationship between the feelings of customers and service elements of CBLS obtained by online content mining provides complementary results for CBLS design.
This paper presents a new application of data mining techniques, particularly text mining, to analyze educational questions asked by teachers in classrooms. More specifically, it reports on the performance of four machine learning techniques and four feature selection approaches on the classification of teacher's questions into different cognitive levels identified in Bloom's taxonomy. In doing so, a dataset of questions has been collected and classified manually into Bloom's cognitive levels. Preprocessing steps have been applied to convert questions into a suitable representation. Using the dataset, the performance of machine learning techniques under feature selection approaches has been evaluated. The results show that Rocchio Algorithm performs the best regardless of the used feature selection approach. Moreover the best RA performance can be obtained when Information Gain is used for feature selection.
Emotion is defined as human beings' psychological state in which it is also used to establish a relationship and convey messages between the author and the readers. The lifestyle that has been heavily influenced by social media makes the delivery of information and ideas becoming unrestricted. Meanwhile, information and ideas always involve the authors' emotions, such as anger, fear, joy, or sadness. This research applied a text mining model to detect through the classification process. The dataset used was the best-worst scaling technique that has been classified by previous researchers. The challenge of this dataset is to classify sentences or texts into four emotional categories, namely anger, fear, joy, and sadness. This research employed Naïve Bayes classifiers as the method that was optimized by particle swarm optimization. From several previous types of research, this method was able to generate high accuracy. By applying it to the emotional dataset that has different characters from most datasets, this method was able to provide a quite good classification performance.
This paper presents a study aimed at investigating on whether the text mining technique using graphs can be used to analyze the relevance of the messages in online forums. Experiments were carried out with a program that calculates the thematic relevance of text contributions. In addition, the paper presents results found in the automatic analysis obtained by the application of the software.
Exploring university researchers with expertise related to various academic topics often takes a long time, and the use of different selection factors, such as the publication number or citations, causes inaccurate findings and tends to miss the targets that are set by the university or faculty's research authorities. This study aims to create a decision support application for exploring the expertise and research collaboration of researchers in Chiang Mai University by using the Spyder and Visual Studio Code from Anaconda to extract data from the Scopus database; create web application from the Python Flask Framework and HTML by working with MySQL database to store data according to Text Mining and Bootstrap. The web application development facilitates university executives or faculty's research department regarding the managerial decision in order to search for the expertise researchers according to the interested academic topics. The web results show the expertise of each researcher and the faculty's expertise; and the list of institutes that have collaborated with an individual researcher in Word Cloud format. Concerning the scoring criteria, different factors are used, such as the number of citations, the SJR values from the data sources published in the Scopus database, and the number of publications in each topic.
In the context of the rapid development of artificial intelligence technology, teachers' teaching methods, students' learning habits and behaviors are quietly changing. The application of artificial intelligence to the training of teaching skills for teachers has become a research topic in the training of teaching skills for teachers. one. This research is based on the teaching characteristics of the information technology subject, based on the ideas of grounded theory, combined with the training objectives, the nature of the information technology subject and the teaching rules, to construct a conceptual model of the video teaching behavior of normal students. The model is constructed on the basis of understanding the characteristics of the subject and the teaching rules, and can reflect the teacher's teaching philosophy and teaching implementation process. Based on this model, the teaching behavior tag database extracted from the simulated teaching videos of normal students has laid a certain foundation for subsequent research and AI applications. On this basis, this research uses text mining technology to construct four models: word vector model, behavior classification model, behavior effect classification model and overall effect prediction model to automatically evaluate the simulated teaching situation of normal students.
The proliferation of text data in traffic crash reports makes it imperative for utilization in effective ways. However, crash reports are often underutilized considered as there is a lack of effective text mining and analysis tools. This study proposes a framework integrating text mining and network topology theory to discover the latent hazardous patterns in crash reports. Based on text mining techniques, the most relevant keywords in police reports were identified. The association rule mining (ARM) is used to judge the strong correlation relationship between keywords. Additionally, the strong correlation relationship was combined with the network topology method to construct a thematic network of the text data. With the thematic network, the Louvain algorithm is applied to infer the latent hazardous patterns in crash reports. The approach was validated by the reports of severe truck-involved crashes in China from 2013 to 2020. Several latent patterns were identified from the crash reports. The modularity measure of the network with lift value outperforms that with traditional co-occurrence frequency, showing that the proposed method can effectively extract risk factors and hazardous patterns from crash reports. The framework can be incorporated into intelligent transportation systems to cope with increasing textual data.
Strategic alliances among organizations are one of the central drivers of innovation and economy and have raised strong interest among policymakers, strategists and economists. However, discovery of alliances has relied on pure manual search and has limited scope. This research addresses the limitations by proposing a text mining framework that automatically extracts alliances from news articles. The model not only integrates meta-search, entity extraction and shallow and deep linguistic parsing techniques, but also proposes an innovative ADT-based relation extraction method to deal with the extremely skewed and noisy news articles and AC Rank to further improve the precision using various linguistic features. Evaluation from an IBM alliances case study showed that ADT-based extraction achieved 78.1% in recall, 44.7% in precision and 0.569 in F-measure and eliminated over 99% of the noise in document collections. AC Rank further improved precision to 97% with the top-20% extracted alliance instances. Our case study also showed that the widely cited Thomson SDC database only covered less than 20% of total alliances while our automatic approach can covered 67%.
A growing body of literature in social science has been devoted to extracting new information from social media to assist authorities in manage crowd projects. In this paper geolocation (or spatial) based information provided in social media is investigated to utilize intelligent transportation services. Further, the general trend of travel activities during weekdays is studied. For this purpose, a dataset consisting of more than 40,000 tweets in south and west part of the Sydney metropolitan area is utilized. After a data processing effort, the tweets are clustered into seven main categories using text mining techniques, where each category represents a type of activity including shopping, recreation, and work. Unlike the previous studies in this area, the focus of this work is on the content of the tweets rather than only using geotagged data or sentiment analysis. Beside activity type, temporal and spatial distributions of activities are used in the classification exercise. Categories are mapped to the identified regions within the city of Sydney across four time slots (two peak periods and two off-peak periods). Each time slot is used to construct a network with nodes representing people, activities and locations and edges reflecting the association between the nodes. The constructed networks are used to study the trend of activities/locations in a typical working day.
The rapid advancement of technology has facilitated the integration of artificial intelligence(AI) into various domains, including the genetic medicine and healthcare industry. This integration has paved the way for AI to contribute significantly to research efforts focused on intellectual disability. AI has the potential to aid medical professionals in the early detection of genetic variations and the identification of underlying causes of diseases. The issue of intellectual disability becomes more pronounced when attempting to provide education to individuals with intellectual disabilities in the context of mainstream schooling. It is worth mentioning that AI and its approaches have the potential to diagnose and identify cognitive impairments and mental health conditions. This work aims to provide a systematic review of intellectual disability, encompassing its various varieties, and explore the utilization of AI approaches for diagnosing intellectual impairment through text-mining techniques. This article also examines the correlation between AI and intellectual disability while also acknowledging the potential for improved prospects for individuals with intellectual disabilities. This study also highlights the challenges and issues of AI in intellectual disabilities
Preprocessing is an important task and critical step in information retrieval and text mining. The objective of this study is to analyze the effect of preprocessing methods in text classification on Turkish texts. We compiled two large datasets from Turkish newspapers using a crawler. On these compiled data sets and using two additional datasets, we perform a detailed analysis of preprocessing methods such as stemming, stopword filtering and word weighting for Turkish text classification on several different Turkish datasets. We report the results of extensive experiments.
In many companies of the province of Tohoku (North-East Japan), stock prices were declining just after the East-Japan earthquake. However, on the other hand, some companies' stock prices rose in Tohoku. We have analyzed the reasons using text mining technologies on web data. In our studies, a combination of a stock price analysis and the text mining of the company can weave unexpected facts like a word associative game. First we surveyed stock prices of all listed companies that were headquartered in the damaged three prefectures in Tohoku. Then, we found the remarkable growth of YAMAYA which was an alcohol wholesaler and retailer company. The headquarters are located in Sendai-city, the capital and biggest city of Miyagi prefecture. On the other hand, we found YAMAZAWA's growth which was the same kind of an alcohol retailer company as YAMAYA. Although stock prices of almost all super markets in Tohoku intensely grew just after the disaster, the growth of YAMAYA and YAMAZAWA was distinguished. Therefore we conducted text mining to seek the reason of the company's stock price increase. As a result, we thought the alcohol consumption in Sendai-city caused the rapid stock price rises. In addition, from the comparison, we found that the recovery of YAMAZAWA's was quicker than that of YAMAYA's. We think that the reason for the YAMAYA's delay is that the earthquake damage in Sendai-city was larger than that in Yamagata. YAMAWA is headquartered in Sendai-city and YAMAZAWA is headquartered in Yamagata prefecture. The difference about locations would make the delay in the stock price recoveries. From the analysis, we inferred that an increase of alcohol consumption was one of the triggers for the economic reconstruction in Sendai-city. The paper discusses the economic reconstruction processes through increases of stock prices in the Tohoku province.
In Information Retrieval (IR), Text Mining (TM), and web search, Multi-label Text Classification (MTC) plays an essential role. A document can fall into more than one category in MTC. Text documents frequently include High Dimensional (HD) non-discriminative (noisy and irrelevant) phrases, resulting in high computing costs and impoverish learning performance of Text Classification (TC). The Feature Selection (FS) procedure is complicated by three issues caused by small samples and HD datasets. First, given limited samples and HD, FS is unstable. Second, with HD, FS takes longer. Third, a particular FS approach may not provide enough Classification Accuracy (CA). In this paper, we have developed a two-stage FS approach based Meta-heuristics Algorithm (MA) for MTC. The first stage work on the filter-based FS approach, while the second stage is based on the multi-objective Grey Wolf Optimization (GWO) algorithm. The first objective is to diminish the Hamming Loss (HL), and the second objective is to decrease the Selected Features (SF). We have used the Multi-Layer Perceptron (MLP) model for the classification task. The experimental findings show that the suggested FS scheme achieves superior HL with a less number of features.
Despite the importance of conducting systematic literature reviews (SLRs) for identifying the research gaps in software engineering (SE) research, SLRs are a complex, multi-stage, and time-consuming process if performed manually. Conducting an SLR in line with the guidelines and practice in the SE domain requires considerable effort and expertise. The objective of this SLR is to identify and classify text-mining techniques and tools that can help facilitate SLR activities. This study also investigates the adoption of text-mining (TM) techniques to support SLR in the SE domain. We performed a mixed search strategy to identify relevant studies published from January 1, 2004, to December 31, 2016. We shortlisted 32 papers into the final set of relevant studies published in the SE, medicine and social science disciplines. The majority of the text-mining techniques attempted to support the study selection stage. Only 12 out of the 14 studies in the SE domain applied text-mining techniques, focusing primarily on facilitating the search and study selection stages. By learning from the experience of applying TM techniques in clinical medicine and social science fields, we believe that SE researchers can adopt appropriate SLR automation strategies for use in the SE field.
The volume of digital information is growing considerably in the last two decades and there is currently a huge concern about obtaining this content quickly and effectively. In the health sector it is not different, to retrieve medical records that obtain relevant information about treatments and progresses of clinical conditions may speed up new patients' diagnosis. In this work it is described a framework proposed for automatically indexing information based on semantics and on text mining techniques. This task should work in parallel with the insertion of data into electronic records. The original contributions come down to search engine in texts organized so as to potentiate the amount of results obtained, as evidenced by the experiments carried out. The stored information is automatically fragmented into words, which have a semantic dictionary based on a framework that enables the information retrieval through semantics.
Text mining of consumer's dialogues regarding their service experiences provides a direct and unbiased feedback to service providers. This research proposes an analysis process to analyze unstructured input from consumer dialogues. The goal is to apply the critical incident and text mining methods to discover factors that contribute to customer satisfaction and dissatisfaction. The critical incident method is used to construct an open-ended questionnaire to collect customer's positive and negative opinions toward the service provided. Valid and reliable text mining techniques are used to cluster significant text to help analyze incidents that customers care about. A case study of consumers riding the Kaohsiung Mass Rapid Transit System (KMRT) was cased to evaluate the proposed analysis process. Based on dialogues collected from the open-ended questionnaires, the analysis process extracts key phrases related to consumer's best and worst service experiences, creates significant dialogue clusters, and derives meaningful trends, baselines, and interpretations of consumer satisfaction and dissatisfaction. The results of this case study can be used as a basis for building more complete analytical methods to understand consumer experiences and provide strategic feedback for service providers.
The Korean military's demand forecasting of spare parts is a major topic for logistics assistance, which considerably affects efficient management of the national defense budget. A time series forecasting method has previously been used, but the results lack accuracy and need improvement. In this study, we collected mass data including 17,451,247 structured and unstructured data on anti-aircraft missile data in the army's DELIIS. Then, we used an ensemble technique to reduce the model's variability and demonstrated increased accuracy for demand forecasting compared with the previous time series method.
We discuss the topic analysis of syllabi of Faculties of Engineering in Japanese national universities. To extract the important information (the course content, methods, etc.) from the syllabus document, we used the latent Dirichlet allocation (LDA). This work-in-progress paper describes that the topics of the syllabi of Faculties of Engineering from an analysis of actual data.
The ever increasing accessibility of the web for the crowd offered by various electronic devices such as smartphones has facilitated the communication of the needs, ideas, and wishes of millions of stakeholders. To cater for the scale of this input and reduce the overhead of manual elicitation methods, data mining and text mining techniques have been utilised to automatically capture and categorise this stream of feedback, which is also used, amongst other things, by stakeholders to communicate their requirements to software developers. Such techniques, however, fall short of identifying some of the peculiarities and idiosyncrasies of the natural language that people use colloquially. This paper proposes CRAFT, a technique that utilises the power of the crowd to support richer, more powerful text mining by enabling the crowd to categorise and annotate feedback through a context menu. This, in turn, helps requirements engineers to better identify user requirements within such feedback. This paper presents the theoretical foundations as well as the initial evaluation of this crowd-based feedback annotation technique for requirements identification.
Clinical pathway is a multi-disciplinary treatment plan and work mode, which is favorable for improving healthcare service quality and reducing medical costs. Most of references demonstrate that variance analysis and handling is the key to clinical pathway management. Thus, the clinical pathway variance has become the focus of scholars. This paper uses the text mining technique to present a literature review of 496 academic articles in the field of clinical pathway variance analysis and handling, which published between 1994 and 2018. Moreover, this paper conducts a bibliometric analysis to visualize the clinical pathway variance research. In variance analysis and handling, there are a lot of imprecise knowledge and fuzzy relations to be reasoned with knowledge of different domains. In this study, methods of clinical pathway variance analysis and handling are illustrated. In addition, this paper points out the limitations of each method. Based on the results, the future prospects of clinical pathway variance analysis and handling research is proposed.
Text categorization is the task of deciding whether a document belongs to a set of pre specified classes of documents. Categorization of documents is challenging, as the number of discriminating words can be huge. Many existing text classification algorithms simply do not work with these many number of words. Traditional text classification algorithm uses all training samples for classification, thereby increasing the storage requirements and calculation complexity as the number of features increase. Mining medical records for relationships between living factors and the symptoms of a disease is an important task, however there has been relatively little research into this area. The proposed work evolves a text classification algorithm where al l cluster centers are taken as training samples there by reducing the sample size and introduces a weight factor to indicate the different importance of each training sample. A similarity measure function is used to classify a new patient document, based on the measure. Experiments on real life data show that the proposed algorithm outperforms the state of art classification algorithms such as k-nearest neighbor.
With the rise of social networking, many consumers are willing to particulate in discussions on the social media, and to share and to express their comments for certain products. Enterprises can analyze the consumers' preferences and the strengths and weaknesses of diverse products on the market with a large number of online reviews. However, few studies have been made at applying deep learning to the sentiment analysis on word of mouth of smart bracelet in Chinese reviews. The purpose of this research is to explore the impact of online word of mouth on the consumers of smart bracelets and propose a long short-term memory (LSTM) recurrent neural network (RNN) deep learning model to the sentiment analysis on word of mouth of smart bracelets. The experiment results of sentiment analysis on word of mouth of smart bracelet show that the 89.92% accuracy based on the deep learning is significantly higher than the 70.67% on the Naïve Bayes algorithm and the 66.01% on the support vector machine. The contributions of this paper are two-fold. First, we have proposed an approach with deep learning to sentiment analysis on the word of mouth of smart bracelets. The experimental results show that the proposed deep learning approach can effectively enhance the accuracy of sentiment analysis. Second, through the text analysis, we have constructed a sentiment dictionary called iTSBSD for the word of mouth of smart bracelets in Chinese reviews on Taobao.
Text mining is thought to have a high commercial potential due to the significant amounts of unstructured text data produced on the Internet. The practice of obtaining previously undiscovered, comprehensible, potentially useful patterns or knowledge from a corpus of text data is known as text mining. In this study, we attempt to extract the structured information from the text and then use various machine-learning models to categorize the data. We then look for the model that provides the highest level of classification accuracy.
The successful development of an intelligent text mining application requires the collaboration of two main stakeholders: subject matter experts and text miners. In this paper, we describe a new methodology, agile text mining to improve that collaboration. Agile text mining is characterized by short development cycles, frequent tasks redefinition and continuous performance monitoring through integration tests. We introduce Sherlok, a system supporting the development of agile text mining applications and present an application to extract mention of neurons from a very large corpus of scientific articles. The resulting code and models are publicly available.
This research investigates how online shopping for clothing affects the user experience by evaluating user feedback using text mining. Results show that the seller can influence a user's emotions through various factors, such as product experience and value experience, in order to enhance user satisfaction. Moreover, the consumer's inner experience can be indirectly improved by four aspects: external experience, time experience, space experience, and environmental experience. Furthermore, product experience has an important impact on the consumer's inner experience.
Clustering is widely used in data mining and learning systems. It is not one specific algorithm, but a general task to be solved which can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. However the clustering is not easy task especially for the complex datasets like text mining where the information does not depends only on terms frequency. This paper presents an effective approach for dealing with similar problems. The proposed algorithm is a category dependent weighted seeds affinity clustering algorithm. The advantage of the proposed algorithm is that clusters can be easily modified according to the field of interest of the user. The superiority of the proposed algorithm is also validated by the simulation results comparison using Reuters-21578 dataset. Results shows improvement over k-means, Affinity and Seeds Affinity Algorithm.
The popularity of Android intelligent terminal has increased the need to evaluate Android operating system. However, traditional evaluation models largely aim at common software and overlook the characteristics of Android. Moreover, the selection of evaluation index is limited to expert's experience and questionnaire surveys. In this paper, we develop a text mining based model to evaluate Android system quality scientifically. We collected massive data from users' comments of mainstream Android forums and Android system evaluation reports. Further extracted evaluation index from users' comments through text mining methods and obtained index weight by counting the frequency of the index. Based on ISO/IEC9126, we classified the evaluation index and designed a multi tree based algorithm—Meval to calculate the evaluation result. Finally, we made a comparative analysis with the traditional evaluation model via huge actual test cases. Results show that our model can not only ensured the accuracy of the evaluation, but also optimize itself with the changing of users' quality needs.
In order to better support the text mining of historical texts, we propose a combination of complementary techniques from Geographical Information Systems, computational and corpus linguistics. In previous work, we have described this as `visual gisting' to extract important themes from text and locate those themes on a map representing geographical information contained in the text. Here, we describe the steps that were found necessary to apply standard analysis and resolution tools to identify place names in a specific corpus of historical texts. This task is seen as an initial and prerequisite step for further analysis and comparison by combining the information we extract from a corpus with information from other sources, including other text corpora. The process is intended to support close reading of historical texts on a much larger scale by highlighting using exploratory and data-driven approaches which parts of the corpus warrant further close analysis. Our case study presented here is from a corpus of Lake District travel literature. We discuss the customisations that we have to make to existing tools to extract placename information and visualise it on a map.
The rapid increase in the number of text documents available on the Internet has created pressure to use effective cleaning techniques. Cleaning techniques are needed for converting these documents to structured documents. Text cleaning techniques are one of the key mechanisms in typical text mining application frameworks. In this paper, we explore the role of text cleaning in the 20 newsgroups dataset, and report on experimental results.
For the last few years, text mining has been gaining significant importance. Since Knowledge is now available to users through variety of sources i.e. electronic media, digital media, print media, and many more. Due to huge availability of text in numerous forms, a lot of unstructured data has been recorded by research experts and have found numerous ways in literature to convert this scattered text into defined structured volume, commonly known as text classification. Focus on full text classification i.e. full news, huge documents, long length texts etc. is more prominent as compared to the short length text. In this paper, we have discussed text classification process, classifiers, and numerous feature extraction methodologies but all in context of short texts i.e. news classification based on their headlines. Existing classifiers and their working methodologies are being compared and results are presented effectively.
Nowadays the need of a scientific and rigorous tool of automatic authorship classification has become pretty important, especially for ancient documents authentication such as religious or historical books. Hence, in this paper, we conduct some experiments of authorship classification on the Quran and Hadith in order to see if they could have the same author or not (ie. Was the Quran written by the Prophet or only sent down to him, as claimed?). This task, which is commonly called authorship discrimination, represents an important authorship classification application. It consists in checking whether two texts are written by the same author or not by using some AI (Artificial Intelligence) and TM (Text mining) techniques. In our case, two main investigations are conducted and presented: in the first one, the two books are analyzed in a global form; in the second investigation, the two books are segmented into 25 different text segments: 14 segments are extracted from the Quran and 11 ones are extracted from the Hadith. The different segments have more or less the same size, with approximately 2080 tokens per text segment. Several classifiers are employed: SMO-based Support Vector Machines (SVM), Multi Layer Perceptron (MLP) and Linear Regression (LR). This research work has allowed getting extremely interesting information on the ancient books origins.
Nowadays, personalization e-learning becomes one of the interesting issues in e-learning research that continues to grow. Many studies proposed different approach to deliver adaptive learning content according to learner's characteristics, needs, and preferences. The outcomes from studies are various too. In this paper, in order to get best approach in task deliver learning content, preliminary study to analyze two major approaches that usually used, they are text mining and ontology approach has been conducted. Both approaches will be applied in a personalization e-learning system. The effectiveness and efficiency to obtain which approach is the best will be measured. In order to support the study, data such as learning material, participant, and learning activities have been collected and analyzed.
As we know, one of the essential social problems is unemployment. Hence, many job advertising agencies tend to consolidate various job offerings to assist the job seekers. As these job descriptions carry unstructured text, therefore, it makes the process of finding and grouping relevant jobs difficult. Hence, the aim of this paper is to automatically extract information from job descriptions. This involves application of text mining and information extraction approaches. To carry out this study, first various features are extracted and later Gradient Boosting classification algorithm is used to perform information extraction. Fields such as salary, required degree, required experience etc. are being extracted automatically. Results show that various features affect the overall result. This proposed model is currently evaluated with the help of precision, recall, receiver operating characteristics curve, and area under the curve (AUC). Experiments show that boosting classification tends to provide reasonable result with 77% of precision.
Publicly available pre-trained word embeddings are rich sources for turning critical high-dimensional representations of huge text data repositories into meaningful compact vectors essential for text mining applications. With many of such pre-trained embedding sources available, each faces limitations in the appropriateness of their language use for the downstream text-mining tasks. Meta-embeddings aim to tackle this ambiguity challenge by fusing multiple embedding sources into one feature space. However, current meta-embedding methods assume vocabularies across sources are similar or even identical; which unfortunately stands in sharp contrast to the fact that many sources barely overlap. Further, these methods encode a meta-embedding for each word by reconstructing its actual embedding values (word-encoder), while valuable information of relationships (distances) among words within each source are not directly considered. In this work, we instead propose a novel relation-encoder learning approach called Similarity-Preserving Meta-Embedding (SimME) that directly integrates word-pair relationships from partially overlapping embedding sources. SimME embeds words such that their similarities are learned from those observed in multiple pre-trained sources. To handle relations between words that are not present in all sources, we introduce maskout, a new loss term, that steers the learning selectively to the sources containing said relations. SimME consistently outperforms state-of-the-art methods by 10% on average and with up to 20% across several core metrics in 4 popular mining tasks on 23 datasets.
Emotion detection is a branch of sentiment analysis that focuses on the identification and study of emotions. Text mining, analysis of extracted information, and in-depth research are now at the centre of organisational success due to the development of web 2.0. It enables service providers to provide their clients with specialized services. Due to the simplicity of obtaining data and the enormous advantages its deliverables give, text-based extraction and analysis are now the topics of much research. The notion of ED from texts is examined in this article, which also highlights the major techniques that academics have taken to develop ED systems that employ texts. The proposals' key contributions, methods used, datasets consulted, outcomes attained, and strengths and limitations are described. Additionally, emotion-labelled data sources are offered to give beginners access to appropriate text datasets for ED. The essay concludes by outlining several unresolved problems and potential lines of inquiry for text-based ED. Due to its numerous potentials uses in business, cultural studies, cognitive science, human-computer interaction, cognitive computing, etc., emotion recognition in the text has recently gained popularity. The availability of a wealth of textual material, particularly opinionated and self-expressive writing, also considerably increased public awareness of this issue.
Crime analysis is a law enforcement function that involves the systematic analysis for identifying and analyzing patterns and trends in crime. Crime analysis also helps in devising solutions to crime problems and formulating crime prevention strategies. Crime mapping is used by the analyst to map, visualize and analyze crime incident patterns. Mapping crime allows crime analyst to identify crime hot spots, along with their trends and patterns. This paper is focused on analyzing and mapping crime through online newspaper using text mining techniques and discussed about the review of an existing system, find out the problem and solve them using the appropriate methods.
Now a day most of the information is available in digital form to get the proper data that is a challenging task. Most of the researchers focused on these problems and come up with the new model to retrieving the information from the digital system. In this paper, we learn performance of the different linguistic patterns and statistical scores considered is carefully studied and evaluated in order to design a method that maximizes the quality of the results. Our proposal is also evaluated for several well distinguish domain, offering in all cases, reliable taxonomies considering precision and recall along with F-measure. In this paper, we propose sequential pattern mining based pattern taxonomy relation, which discover pattern effectively, to achieve the goal we use some state of art data mining method and popular algorithms for evolution, for the experimental result we use Reuters (RCV1) dataset and the results show that we improve the discovering pattern as compared to previous text mining methods. The results of the experiment setup show that the keyword-based methods not give better performance than pattern-based method. The results also indicate that removal of meaningless patterns not only reduces the cost of computation but also improves the effectiveness of the system.
Information Extraction is an activity of examine text for information relevant to some interest. Information extraction needs depth analysis than simple key word searches. The information extraction system recognizes and extracts knowledge from a massive literature and extracted knowledge is accumulated in a knowledge base. Many conventional automatic information extraction approaches using Natural Language Processing and Text Mining technologies have been proposed to extract meaningful information automatically in biomedical realm. These conventional approaches have considerable pitfall that whenever a different extraction goal become visible or any component in system is upgraded, extraction has to be reapplied from beginning to the whole text collection although only a minor part of the text collection might be influenced. In this paper we have applied Stanford dependency grammar to furnish easy description of the grammatical relationships in a sentence. This work relates incremental information extraction approach in which extraction needs are exhibited in the form of database queries. This work aims that in the occasion of installation of a upgraded component, reduction in the processing time takes place as compared to a conventional approach.
To understand user behavior, representing the semantic knowledge of user-product interaction is essential. In this paper, we represent the interaction between user and product via large language model (LLM)-assisted knowledge graph construction. We capture users’ behavioral actions and static properties of the products from raw text data of “user review” and “product catalog”. Moreover, the information needed for updating the knowledge graph is captured by raw texts of “news related to the products”. The proposed methodology integrates them as a single knowledge graph to provide causal reasoning on user-product interaction. To alleviate the situation where a small quantity of annotated text exists in these data, we use LLM as a data annotator and augmentor.
Science has time and again proven to be one of the most powerful tools in finding solutions to the problems faced by the world. Let it be natural or man-made challenges, hard work put into finding efficient answers to tackle them has proven to safeguard the ecosystem. Sometimes the research community is put under pressure when humanity faces the challenge of survival like the Covid-19 pandemic. A great extent of published works needs to be studied to find an optimal solution to existing or new queries related to the virus. In this research work, we build an efficient data mining tool using the CORD-19 Dataset to help the community come up with answers to Covid-19 related questions. We use a combination of semantic and keyword search to reduce the solution space of our model. Our model makes use of parallelism, paraphrasing, and state-of-the-art natural language processing techniques which will serve as a time and energy-saving tool for the information need of all doctors and researchers who are trying to put an end to the pandemic and avoid future possible outbreaks.
This paper approaches the problem of processing medical records with the aim of data analysis from a larger set of domain specific medical data. We solve this problem in cooperation with doctors from Department of Cardiology of the East Slovak Institute of Cardiovascular Disease. For this purpose, we designed, implemented and tested first prototype of a software system able to read and process medical records about cardiological patients and provide anonymized data for further analysis.
Traumatic Brain Injury (TBI) is a leading cause of disability worldwide, there is one TBI case every 15 seconds and in every 5 minutes someone becomes permanently disabled due to it. Brain injuries lack of surgical or pharmacological therapies, therefore Cognitive Rehabilitation (CR) is the generally adopted treatment. Computerized CR tasks are increasingly replacing traditional "paper and pencil" approaches. Nevertheless, CR plans are manually designed by clinicians from scratch based on their own experience. There is very little research on the amount and type of practice that occurs during computerized CR treatments and its relationship to patients' outcomes. While task repetition is not the only important feature, it is becoming clear that neuroplastic change and functional improvement occur after specific tasks are performed, but do not occur with others. In this work we focus on the preprocessing, patterns and knowledge extraction phases of a Knowledge Discovery in Databases (KDD) framework. We propose considering CR programs as sequences of sessions and pattern searching (association rules, classification models, clustering and shallow neural models) to support clinicians in the selection of specific interventions (e.g. tasks assignations). The proposed framework is applied to 40000 tasks executions from real clinical setting. Results show different execution patterns on patients with positive and negative responses to treatment, predictive models outperform previous recent research, therapists are provided with new insights and tools for tasks selection criteria and design of CR programs.
Culinary tourism attracts a large number of international tourists to experience local delicacies. With the availability of internet services, tourists form an impression about the destination by accessing the reviews digitally. This paper aims to determine the culinary experience dimensions by evaluating the boundaries of digital evidences that are written online by the international tourists after experiencing local food at the attraction. The data was gathered from a popular review website TripAdvisor.com which is widely used in travel and tourism industry. A total of 267 reviews from international tourist about Delhi street food were gathered and qualitative analysis was carried out using bigram analysis in R software to identify the frequent phrases. Later, dimensions were classified, based on the frequent phrases. The findings suggested that identified dimensions of tourists experience on Delhi street food can enhance the destination image of the attraction in front of international tourist who is scrutinizing the reviews online. This paper used a novel dataset and the results can contribute to the culinary literature. Future studies can be planned using bigger respondent values at different geographical locations.
The technological progress in image acquisition, image processing, and improvements in the field of big data in the last decades opens a wide range of possible applications for image processing using analytical methods. The most popular concepts in this field of tension are computer vision and image mining. However, the issue is that there is no clear distinction between these two concepts in the current scientific discussion. To address this need, a structured literature review was carried out. Thereby, the key characteristics as well as the process of image mining could be identified and compared to computer vision. Thus, this research in progress study was a first step towards differentiating these two concepts. In conclusion, a hypothesis could be derived, which will be considered now in our ongoing research.
The text mining is technique that extract data from the semi structured or unstructured text and finds pattern it thought as knowledge discovery from the text. Deep learning is most powerful machine learning technique that is used to extract the feature of the text and from the feature which is identified gives the prediction for the opinion from the various text. The opinion which is mined that helps to decide the social people opinion and sentiments that will be used as an input to various business decision. This article provides overview of sentiment analysis and deep learning approach.
Skin cancer is dangerous since the beginning of its existence. Till now it is a big question mark to scientists that are trying hard to figure out the ultimate solution to discover skin cancer. Researches are made for fifty years but still, the experts are unable to detect skin cancer with a hundred percent accuracy in this modern era of technology. Thus, people's data from all around the world is gathered to cancer diagnosis centers. Back then, the data was recorded manually but due to the advancement of technology and the behavior of the computer systems, it is recorded digitally and it is manipulated in a way so that a conclusion could be achieved. This paper presents a study to compare skin cancer using data mining techniques and algorithms in the most accurate form. With the help of data mining, people's data from around the world is preprocessed in order to eliminate redundancy and missing values. Then clusterization is done on pre-processed data by clustering different algorithms to separate like and unlike data. These compared tables will help out for implementation of data mining algorithms for this disease.
The list of features and text representation model that is utilized are critical components that determine the performance and accuracy of text categorization. Feature selection is a technique for improving classification accuracy and processing efficiency by removing redundant or superfluous words (features) and keeping only those that give vital information that assists in categorization. This paper presents the recent researches that studied the impact of feature selection methods on Arabic text classification.
In this work sentiment analysis of annual budget for Financial year 2016-17 is done. Text mining is used to extract text data from the budget document and to compute the word association of significant words and their correlation in computed with the associated words. Word frequency and the corresponding word cloud is plotted. The analysis is done in R software. The corresponding sentiment score is computed and analyzed. This analysis is of significant importance keeping in mind the sentiment reflected about the budget in the official budget document.
On the movie market, the user-generated word-of-mouth (WOM) impacts on the box office revenue and the span of the life-cycle of a film production. This article raises the question to what extent text mining can be useful tool in the process of understanding the consumers' experience and the evaluation of movie WOM. The results of the presented empirical study suggest that a text mining analysis of film reviews can offer both valuable insight about the viewer's vocabulary which indicates how consumers think about the movie, and insight about the general WOM valence. The managerial implication of the study is related mostly to the film promotion. In this article, the empirical study is presented within the broader context of other possible methods aimed at evaluating movie WOM.
Text classification is one of the key methods used in text mining. Generally, traditional classification algorithms from machine learning field are used in text classification. These algorithms are primarily designed for structured data. In this paper, we propose a new classifier for textual data, called Supervised Meaning Classifier (SMC). The new SMC classifier uses meaning measure, which is based on Helmholtz principle from Gestalt Theory. In SMC, meaningfulness of terms in the context of classes are calculated and used for classification of a document. Experiment results show that new SMC classifier outperforms traditional classifiers of Multinomial Naïve Bayes (MNB) and Support Vector Machine (SVM) especially when the training data limited.
Today, the amount of data generating are very large. Big data are large and complex data sets with an alarming Velocity, Volume and Variety. Depending upon the variations of data, big data constitutes social Data, machine data and transaction based Data. Social data collected from Facebook, Twitter etc. Machine data are RFID chip reading, GPRS etc. Transaction based data includes retail website's data. Around the variations of different types of data major part is text data. Text data is structured data. Deriving of high quality structured data from unstructured text is text analytics. Converting unstructured data into meaningfull data is text analytics process. CV parsing is one of the text analytics technique. It is resume parsing or extraction of CV. CV parser integrate candidate's resume with recruitment work flow and automatically processes incoming CV's. This paper proposes CV parser model using text analytics. The proposed CV parser model extracts entities required in recruitment process in the companies.
Review topic mining involves extracting specific evaluation themes from user-generated reviews by identifying characteristic words, serving as the basis for fine-grained opinion analysis and sentiment recognition within review texts. The rapid identification of latent topics from vast collections of review data has remained a prominent and persistent concern within the field of natural language processing. This paper provides a comprehensive review of research progress in review topic mining, introducing the fundamental concepts of review topic mining. Three distinct approaches to review topic mining are discussed, including review topic mining based on word frequency statistics, review topic mining using topic models, and review topic mining that integrates deep learning models. Particular emphasis is placed on exploring cutting-edge methods within these review topic mining approaches. Finally, an analysis of the development trends of deep learning in the field of natural language processing is presented.
Automation in software testing is proliferating in a multitude of technical fields, predominantly in the automotive industry. With the escalation in features provided by the vehicle, the complexity of embedded software increases. It subsequently entails an increase in the software modules of the product. The software integration testing of the entire project has become a rigorous task since building multiple test-environments is required to test the interactions amongst all software modules. Analysis of test reports generated by these environments is performed manually to calculate the function coverage, call coverage, and detect dead code. The process of coverage calculation and error isolation is time-consuming. This paper presents a method of automatic analysis of the test reports generated after software integration testing by utilizing text mining techniques. Text mining is a data analysis technique employed to elicit high-quality information from textual sources like full-text documents, emails, and HTML files. The use of Python tool for text mining is explored in this paper. Software integration testing at Varroc is performed using VectorCAST tool that generates test reports in HTML format. A python script is developed that reads and scrutinizes all the reports and provides output in excel sheet.
Configuration bugs are one of the dominant causes of software failures. Previous studies show that a configuration bug could cause huge financial losses in a software system. The importance of configuration bugs has attracted various research studies, e.g., To detect, diagnose, and fix configuration bugs. Given a bug report, an approach that can identify whether the bug is a configuration bug could help developers reduce debugging effort. We refer to this problem as configuration bug reports prediction. To address this problem, we develop a new automated framework that applies text mining technologies on the natural-language description of bug reports to train a statistical model on historical bug reports with known labels (i.e., Configuration or non-configuration), and the statistical model is then used to predict a label for a new bug report. Developers could apply our model to automatically predict labels of bug reports to improve their productivity. Our tool first applies feature selection techniques (e.g., Information gain and Chi-square) to pre-process the textual information in bug reports, and then applies various text mining techniques (e.g., Naive Bayes, SVM, naive Bayes multinomial) to build statistical models. We evaluate our solution on 5 bug report datasets including accumulo, activemq, camel, flume, and wicket. We show that naive Bayes multinomial with information gain achieves the best performance. On average across the 5 projects, its accuracy, configuration F-measure and non-configuration F-measure are 0.811, 0.450, and 0.880, respectively. We also compare our solution with the method proposed by Arshad et al. The results show that our proposed approach that uses naive Bayes multinomial with information gain on average improves accuracy, configuration F-measure and non-configuration F-measure scores of Arshad et al.'s method by 8.34%, 103.7%, and 4.24%, respectively.
Summary form only given. The use of the Internet permeates more and more areas of our daily life. People share and use information in forums and social networks in ways unimaginable just a few years ago. This fantastic medium with global reach, easy access and fast information propagation is, unfortunately, also often a tool for illegal activities. Especially in areas like commercial fraud a huge increase of criminal acts can be observed. To meet these challenges, law enforcement authorities need to build and reinforce capabilities in the domain of OSINT. Characteristics of the Internet like the volume of available data, the plurality of languages and the speed of change make it difficult for public authorities to keep pace. The OPTIMA group of the Joint Research Centre (JRC) does research in the field of open source information extraction and text mining. As part of this research it develops tools which can be used in operational settings. As part of its mission to provide scientific and technical support to EU policies, these tools are provided to law enforcement authorities in Member States of the European Union. The first part of the talk will give an overview of our research in information extraction and text mining. Furthermore, our desktop text mining tool, EMM OSINT Suite, which is in use by law enforcement authorities in Europe, will be presented. Our "lessons learned" with relevance to the research community will be shared. The second part will discuss the impact of general trends in internet technology and research on our work now and in the future.
The purpose of this study is to clarify the differences of educational objectives between national technical colleges and national institutes of technology. This was achieved by using the text mining method. We collected text data from the educational objectives of all technical colleges and all institutes of technology websites. Words that appeared most frequently in the text data were listed for the both groups, technical-colleges group (TCG) and institutes of technology group (ITG). First, the appearance probability of each word was calculated. Only the word, “engineer”, had the probability gap of more than 1 %. The word was frequently appeared in TCG. Second, the Chi-square test was used to select the distinguishing words and 10 most distinguishing words were found. “Research / Graduation research”, “Progress / improvement / growth”, “Talent / human resource”, “Society / social environment”, “Contribution”, and ”Technology / science” were distinguishing words for ITG and “Engineer”, “Acquire”, “Practical / practical skills”, and “Information technology / information processing” were for TCG. Obviously, institutes of technology are aiming improvement and progress of society and science through their research. On the other hand, the purpose of technical colleges is to nurture the practical engineering for industry.
We introduce RLIMS-P version 2.0, an enhanced rule-based information extraction (IE) system for mining kinase, substrate, and phosphorylation site information from scientific literature. Consisting of natural language processing and IE modules, the system has integrated several new features, including the capability of processing full-text articles and generalizability towards different post-translational modifications (PTMs). To evaluate the system, sets of abstracts and full-text articles, containing a variety of textual expressions, were annotated. On the abstract corpus, the system achieved F-scores of 0.91, 0.92, and 0.95 for kinases, substrates, and sites, respectively. The corresponding scores on the full-text corpus were 0.88, 0.91, and 0.92. It was additionally evaluated on the corpus of the 2013 BioNLP-ST GE task, and achieved an F-score of 0.87 for the phosphorylation core task, improving upon the results previously reported on the corpus. Full-scale processing of all abstracts in MEDLINE and all articles in PubMed Central Open Access Subset has demonstrated scalability for mining rich information in literature, enabling its adoption for biocuration and for knowledge discovery. The new system is generalizable and it will be adapted to tackle other major PTM types. RLIMS-P 2.0 online system is available online (http://proteininformationresource.org/rlimsp/) and the developed corpora are available from iProLINK (http://proteininformationresource.org/iprolink/).
In the modern era, people are highly engaged in the virtual world to express their feelings and opinions. Social platforms and news portals are the precious media for expressing views on any sector nowadays. Thousands of information are being added to social media sites every second. During this research, some specks of data were collected and analyzed for extracting public sentiment. Sentiment Analysis is the process to identify computationally and categorize opinions expressed in a piece of text significantly to determine the author's attitude towards a specific topic. It is one of the most renowned research areas in Data Science. There is much research done on sentiment analysis in various sectors for the English language and increasing. Still, works for the Bengali language are limited to Bangla corpus and Bangla micro-blogging. So, this research has aimed to apply Sentiment Analysis to the Bengali language in which the public can express their opinions and emotions in their endemic words. However, it is not easy to apply it to the Bengali language because of the complex grammatical structure of Bangla. This paper represents the method of preprocessing the dataset using the NLP and applying Machine Learning approaches to classify test data. The process starts with removing unnecessary words and then using the TF-IDF vectorizer for feature extraction and applying cosine similarities, LSA and SVD for feature selection to form a meaningful and optimized feature vector to undertake the experiments. Then an analysis was performed to compare different machine learning approaches to the extracted information. Finally, the proposed model categorizes the document into five classes, namely negative, near negative, neutral, near positive, and positive, with profound accuracy, which might further be applied on similar types of datasets.
Vaccine is one of the interesting discussions that are often found in various timelines, both from social media and local scope. The covid vaccine is a topic that is often discussed because the virus is still not over until now. Many researches are discussing how to stop this virus and produce various types of vaccines that are claimed to be able to stop cases of Corona Virus infection. Astrazeneca and Sinovac are two of several types of vaccines circulating in several countries and have a good reputation even though it differ from one another. Indonesia as one of the countries with the highest death rate due to Covid-19 with a total daily death of ±2000 people make Indonesia vulnerable to the level of transmission. To overcome this problem, Indonesia uses Astrazeneca and Sinovac as an effort to prevent deaths from Covid-19. This study focuses on comparing public opinion responses to both types of Astrazeneca and Sinovac vaccines using data collected via Twitter with the text mining technique. The data obtained then is preprocessed so that the data can be used properly for further processing so as to produce the appropriate model. This study used the Convolutional Neural Network (CNN) method in the training process which has a good reputation in making deep learning models. From the process that has been passed with 2000 tweets from Indonesian users, 1347 tweets were ready to be used in the training process and resulted in 59.5% positive responses and 40.5% negative responses for the Astrazeneca vaccine. In addition, for the Sinovac vaccine, 53.3% of positive responses and 46.7% of negative responses were obtained.
Recently the Parkinson's disease (PD) is getting a lot of attention because the economic impact of this disease to the society and to the country is huge. As the population of the old age people is increasing at a higher rate and will also increase at a higher rate in the future the early prediction as well as early diagnosis of the Parkinson's disease become a do or die situation for many developed countries. With the advent of the wearable device and some intelligent data mining approach the research related to Parkinson's disease has increasing at a higher rate throughout the world. As these disease affect most of the human body parts it is difficult for so many clinicians which part of the body should analyzed to get early detection of the disease. Lot of research has been done in different context related to Parkinson's disease a summarization of this topic using manual approach text long time. So to get the important information with the short time, in this paper a text mining based approach has been implemented to a group of texts related to PD that includes only abstracts of papers published in the last three years (2014-2017). We have used some visualization approach to highlight the relationship between gait-Parkinson's disease from the PD based research articles. This analysis will help the clinicians to judge the bonding between them and put focus on them based on the bonding.
It is an interesting and meaningful thing to explore the associated networks between cold herb and hot herb within the framework of traditional Chinese medicine because they might be useful for both clinical practice and scientific research. In this study, focused on the fields of disease, pattern/syndrome, symptom, and decoction, we constructed the associated networks between cold herb and hot herb. By analyzing these networks, we found that there is no clear separation between cold herb and hot herb. What's more, they have mutual parts among their networks on disease, pattern/syndrome, symptom, and decoction.
This paper proposes a new approach to evaluate future technological value of patents using TRIZ evolution trends. Previous studies using TRIZ evolution trends have determined patents with high evolutionary potential as high value technology in the future without considering relative importance of each TRIZ trend in a specific technology domain. Thus, previous approaches have limitations in that the importance of TRIZ evolution trends can be different according to the technology domain and current stage of the technology cycle of the technology domain. To overcome such limitations, we propose a method which can consider the priority of importance of TRIZ evolution trends and the technology cycle. To this end, we adopted an SAO-based text mining approach and semantic similarity measurement method.
People can trace a course or derivation of subject from the tweet that the speaker is likely against the subject or target. The purpose of the task is to develop the automated systems to determine whether people can infer the twitter. Most of automatic stance classification systems generally need to recognize parts of related information that may not be present in the focus text. And it also needs to decide the tweet whether favor or against the topic depending on the condition if this tweet present the target or focus text. Name Entity Recognition (NER) is the one of the best method to classify the stance for neutral and non-neutral. If the stance is non-neural, the term frequency weighting feature is extracted to classify the tweet whether favors or against the target. The main purpose of this paper is to identify the tweet by using the combination of NLP and text mining based features. System Performance in SemEval 2016 Task A and Task B confirm the success of our approach to classifying the stance in Twitter. Our approach for stance classification is also suitable in other social data and opinion mining problems in social network.
Following the recent advances of artificial intelligence, financial text mining has gained new potential to benefit theoretical research with practice impacts. An essential research question for financial text mining is how to accurately identify the actual financial opinions (e.g., bullish or bearish) behind words in plain text. Traditional methods mainly consider this task as a text classification problem with solutions based on machine learning algorithms. However, most of them rely heavily on the hand-crafted features extracted from the text. Indeed, a critical issue along this line is that the latent global and local contexts of the financial opinions usually cannot be fully captured. To this end, we propose a context-aware deep embedding network for financial text mining, named CADEN, by jointly encoding the global and local contextual information. Especially, we capture and include an attitude-aware user embedding to enhance the performance of our model. We validate our method with extensive experiments based on a real-world dataset and several state-of-the-art baselines for investor sentiment recognition. Our results show a consistently superior performance of our approach for identifying the financial opinions from texts of different formats.
In this today's technology, many of digital documents are being generated and available each day. However, it would cost a vast amount of time and human efforts to classify them in reasonable categories like important and unimportant, spam or no-spam. The text document classification tasks pass under the Automatic Classification (also known as pattern Recognition) problem in Machine Learning and Text Mining. It is necessary to classify large text documents into specific classes, to make clear and search simply. Classified data are easy for users to browse. The importance of common text document placement is the representation of the unknown text for some pre-categories as representations for survival. The Combination of classifiers is fused together to increase the accuracy classification result in a single text document. The contemplate text document classification depend on different representation model and fusion based classifiers are explained in the paper. In order to examine different techniques, Enhanced Sentence Vector Space Model (ES-VSM) and a Bigram is used to match the layout of a problem document. The result completed by assessing different current classifiers by looking accuracy of their performance in advance. This will explain and promote a willingness of new research participants to respond to challenging situations and respond to similar responses.
Descriptive information of many accidents and accident symptoms is contained in voluntary reports on civil aviation safety. In order to solve the shortcomings of manual analysis of aviation unsafe information text at this stage, a multi-channel text convolutional neural network-bidirectional gated recurrent unit (TextCNN-BiGRU) analysis model based on attention mechanism is proposed. Firstly, the initial text is vectorized by word2vec, the window values are selected through experiments to form three channels. Then, the strong learning ability of CNN is used to extract local features, the bidirectional gated recurrent unit is used to extract contextual global information, and the attention layer and pooling layer are used to obtain And optimize the important features. Finally, using the softmax function is to minimize the error loss. The simulation experimental verification results show that the classification performance of the proposed model has a classification accuracy of 94%, and the loss function value is stable at about 0.22%.It has good generalization ability and can effectively solve the problem of incomplete information mining for a single model and effectively improve the classification effect.
Stack Overflow is a popular community-based Q&A website that caters to technical needs of software developers. As of February 2015 -- Stack Overflow has more than 3.9M registered users, 8.8M questions, and 41M comments. Stack Overflow provides explicit and detailed guidelines on how to post questions but, some questions are very poor in quality. Such questions are deleted by the experienced community members and moderators. Deleted questions increase maintenance cost and have an adverse impact on the user experience. Therefore, predicting deleted questions is an important task. In this study, we propose a two stage hybrid approach -- DelPredictor -- which combines text processing and classification techniques to predict deleted questions. In the first stage, DelPredictor converts text in the title, body, and tag fields of questions into numerical textual features via text processing and classification techniques. In the second stage, it extracts meta features that can be categorized into: profile, community, content, and syntactic features. Next, it learns and combines two independent classifiers built on the textual and meta features. We evaluate DelPredictor on 5 years (2008 -- 2013) of deleted questions from Stack Overflow. Our experimental results show that DelPredictor improves the F1-scores over baseline prediction, a prior approach [12] and a text-based approach by 29.50%, 9.34%, and 28.11%, respectively.
Facebook is the largest digital social network in the world, and is the most popular social network in Thailand. This paper proposes Thai text topic modeling system that turns Facebook posts into valuable users' group interests. Latent Dirichlet Allocation (LDA) for topic modeling, if applied directly on Thai text posts, does not capture well the group interests due to unique characteristics of the data like intentional typo. The main contributions of the paper include the integration of Thai slangs from posts for extracting Thai words, insertion and stop words removal, slang stemming, and applying LDA for seed word acquisition and topic modeling enhancement. The experiments performed on Thai Facebook posts of undergraduate student volunteers at Assumption University was used to demonstrate feature size reduction, model enhancement, and discovery of meaningful group interests.
With the increasing amount of warranty/performance data available to reliability professionals, we have a golden opportunity to learn much more about our designs/products and come up with better reliability/maintainability models. However, this additional info often comes in the form of unstructured text from technicians, researchers, and customers. Inability to find groupings and trends in this data can hide the real impacting issues that our traditional reliability data won't be able to uncover. This session will seek to show how modern analytic software can help provide a way to quickly and easily explore this unstructured text data to gain better insights into our true reliability. We will cover the basics of how to visualize, group, analyze, model and ultimately find trends using warranty/performance unstructured text data.
This study uses a Use Case Description (UCD) based approach to measure the similarity of software requirements in the development of learning applications for students of SMKN 7 Baleendah. The similarity was measured based on Functional Requirements and UCD. The use of the Use Case in this study is used predominantly as the basic model of Artifacts. In general, this study aims to determine the suitability of User Requirements with the software to be built through the similarity between the results of requirements elicitation activities and the software modeling. This similarity activity is carried out through a Text Mining approach in the form of Text Pre-Processing activities on elicitation results, which are then compared with Unified Modeling Language (UML) artifacts at UCD. The specific objectives of this research have the following activities: eliciting requirements, making Use Case Diagrams and Description, clarifying between UCD and SRS software models, determining the similarity of elicitation results, validating and reliability using Gwet AC1. The results of the similarity measurement get the highest value of 0.948683. The kappa index obtained is 0.8831398, where the proportion of agreements obtained is "Almost Perfect."
This research attempts to provide some insights on the application of text mining and Natural Language Processing (NLP). The application domain is consumer complaints about financial institutions in the USA. As an advanced analytics discipline embedded within the Big Data paradigm, the practice of text analytics contains elements of emergent knowledge processes. Since our experiment should be able to scale up we make use of a pipeline based on Spark-NLP. The usage scenario is adapting the model to a specific industrial context and using the dataset offered by the "Consumer Financial Protection Bureau" to illustrate the application.
In software projects, technical debt metaphor is used to describe the situation where developers and managers have to accept compromises in long-term software quality to achieve short-term goals. There are many types of technical debt, and self-admitted technical debt (SATD) was proposed recently to consider debt that is introduced intentionally (e.g., through temporary fix) and admitted by developers themselves. Previous work has shown that SATD can be successfully detected using source code comments. However, most current state-of-the-art approaches identify SATD comments through pattern matching, which achieve high precision but very low recall. That means they may miss many SATD comments and are not practical enough. In this paper, we propose SATD Detector, a tool that is able to (i) automatically detect SATD comments using text mining and (ii) highlight, list and manage detected comments in an integrated development environment (IDE). This tool consists of a Java library and an Eclipse plug-in. The Java library is the back-end, which provides command-line interfaces and Java APIs to re-train the text mining model using users' data and automatically detect SATD comments using either the build-in model or a user-specified model. The Eclipse plug-in, which is the front-end, first leverages our pre-trained composite classifier to detect SATD comments, and then highlights and marks these detected comments in the source code editor of Eclipse. In addition, the Eclipse plug-in provides a view in IDE which collects all detected comments for management.
A drug is an ingredient intended to be used in establishing the diagnosis, preventing, reducing, eliminating, and curing a disease or symptom of a disease. The magnitude of the effectiveness of the drug depends on the dosage and sensitivity of the organs of the body. Accuracy in the selection of drugs can be done in several ways, one of which is by conducting a condition analysis and drug review to find out the effectiveness of the drug to be used. Text Mining is one of the disciplines that can be used to extract information from a collection of documents under these conditions. In carrying out the text classification process there are several algorithms that can be used, one of which is the K-Nearest Neighbor (KNN) algorithm, this algorithm has the characteristic that is with an approach to finding cases by calculating the proximity of new cases to old cases. In this study, the dataset is divided into 2 parts, namely 70% training data, and 30% testing data. Based on the results of tests conducted in this study, the KNN algorithm produces an accuracy of 77.86%.The results of such accuracy are also influenced by the many training data used. The more data trained, the better the accuracy value.
With the advent of the Internet and wide-spread popularity of online technology-enhanced learning platforms, many pedagogical activities today involve learners in online discussions such as synchronous chat. In this study, we describe a text mining method used for analyzing teamwork from such chat dialogue of students. The steps in the text mining method such as pre-processing and classification are described and the results of our analysis are presented in this paper.
Strategy against the spread of the Covid-19 virus in Indonesia by enacting Large-Scale Social Restrictions. The implementation of the Scale Social Restrictions forced all universities in Indonesia to close their institutes and conduct lectures online. Online lectures are considered as a solution to continue the teaching process during a pandemic. However, the lack of adaptation and sudden changes caused various responses and public opinions on social media. For this reason, this study aims to conduct text mining on Twitter in order to analyze public sentiment on the topic of "online lectures" the data obtained are classified into 2 classes (positive and negative). The results of the accuracy of the nave Bayes test with the cross validation technique obtained a result of 81.57%. For class precision, positive predictions are 100%, while for negative predictions the results are 73.06% and recall from true positive is 63.13% for true negative is 100%. And for the accuracy of K-Nearest Neighbor 62.10%, for class precision positive prediction is 62.06% while for negative prediction results are 62.13% and recall from true positive is 62.24% for true negative is 61.95%
With the increase of adopting Electronic Health Records (EHR) in clinical practice, a promising venue where patient histories are analyzed to conduct clinical research has emerged. In the same time, the sheer number of biomedical literature introduced novel text mining methods that are able to analyze, structure, and summarize information and integrate it into various domain-specific databases and ontologies. However, novel approaches that complement and go beyond clinical and biomedical research are required in the domain of evidence-based medicine where the link between the two areas of research is established. In this research, we utilized NLP, text mining, and data mining methods to extract, analyze, and discover knowledge of clinical entities' associations from clinical notes in EHR and link them with the biomedical literature to optimize the evidence-based medicine utilization.
A corpus is a large collection of texts that can be automatically analyzed for linguistic patterns and structures using interactive tools. Corpus-based language learning has gained prominence in recent years thanks to the advances in computing technologies, such as text mining, searching, and natural language processing. The size and variety of corpora have also grown significantly in recent years. However, most of the corpus tools currently available are designed for linguistic research, not for classroom teaching and learning. In this paper, we present a new corpus tool called Text X-Ray that features visual text analysis and supports corpus-based language learning. This program helps students analyze their own writing and compare their writing with a corpus. This tool has been tested in real classroom settings and has received several positive feedback from a range of instructors and students.
Recommender system is something that every website or app that provides a reliable interface of user must have. It prevents user from wasting time in segregating what he needs and encourages efficient exploring. Various factors helps to acquire the data of user interest and utilize them in building a system which recommends them on the basis of profile of their activities and interest created. Semantic web helps to create protocols for ontology that defines the type and efficiency of the recommender system that we will use. Also various principles help to develop the same.
With the rapid growth of the number of electronic transactions conducted over the Internet, recommender systems have been proposed to provide consumers with personalized product recommendations. A hybrid symbolic and quantitative approach for recommender agent systems is promising because it can improve the recommender agents' prediction effectiveness, learning autonomy, and explanatory power. However, recommender agents must be empowered with sufficient domain-specific knowledge so as to reason about specific recommendation contexts to improve their prediction accuracy. This paper illustrates a novel text mining method which is applied to automatically extract domain-specific knowledge for context-aware recommendations. According to our preliminary experiments, recommender agents empowered by the text mining mechanism outperform the agents without text mining capabilities. To our best knowledge, this is the first study of integrating text mining method into a symbolic logical framework for the development of recommender agents.
Social media has become one of the most important sources of information dissemination during crisis and pandemics. The unknown nature of these disasters makes it hard to analyze the comprehensive situational awareness through different aspects and sentiments to support authorities. Current aspect detection and sentiment analysis system largely relies on labelled data and also categorize the aspects manually. So, in this research, we proposed a hybrid text analytical framework to do aspect level public sentiments analysis. Our approach consists of three layers, first we extracted and clustered the aspects from the data by utilizing the widely used Latent dirichlet allocation (LDA) topic modelling, then we extracted the sentiments and label the dataset by using the linguistic inquiry and word count (LIWC) lexicon, then in third layer of our framework we mapped the aspects into sentiments and sentiments are then classified with well-known machine learning classifiers. Experiments with real dataset gives us promising results as compared to existing aspect oriented sentiment analysis approaches and our method with different variant of classifiers outperforms existing methods with highest F1 scores of 91 %.
Hypertension becomes a major public health problem due to its serious morbidity and mortality and serious cardiovascular, cerebrovascular and kidney complications. The complementary therapies of Traditional Chinese Medicine (TCM) play important roles in hypertension treatment. Acupuncture, as an important component in TCM, is frequently used in treatment hypertension. In this paper, we mined the data set of literature downloaded from SinoMed on hypertension focused on its treatment with acupuncture technology in TCM common use. The text mining results showed that Shuigou, Zhongshu, Burong, Zusanli, and Taichong were the top 5 acupoints, while Governor Meridian, Bladder Meridian of Foot-Taiyang and Stomach Meridian of Foot-Yangming were the top 3 meridian used for hypertension treatment. The combinations of acupoints were demonstrated in network, which were widely used in the clinical practices of acupuncture. The results provided good references for both clinical practice and medical research.
In today's world, data volume is immense and is expanding at an exponential rate. A leading factor of this is the data shared over social media by millions of people daily in the form of text, audio, video, and images. Opinion mining is a branch of text mining that seeks to extract public opinion about an event or particular topic on any online platform like review sites, Twitter, Facebook, etc. Most of the work in opinion mining is done in the English language and very little is done in Roman Urdu, which is a dominant social media language in Pakistan and India. Inflation and politics are amongst the most talked-about and discussed topics on social media in Pakistan. In this study, an Inflation and Politics-based Roman Urdu dataset is prepared that is extracted from the Roman Urdu dataset available at Kaggle. Various operations (with text processing, without text preprocessing, with attribute selection, without attribute selection) were performed on the data over machine learning algorithms of Naive Bayes, Bayes Net, KStar, Decision Tree, and Random Forest and determined which algorithm gave the best accuracy on the training data. WEKA (Version 3.8.4) was used to generate opinion mining regarding politics and inflation in Pakistan using the Roman Urdu dataset.
In this paper we report on a study for discovering hidden patterns in commonly seen parasites by using abstracts from MEDLINE database. Parasites affect millions of people in the world and cause tremendous morbidity and mortality. Diagnosing parasites can be difficult because some symptoms and related to gene-proteins can be common to some of them. We utilize a web based biomedical text mining tool to find symptoms and gene-proteins. After selecting the most common symptoms and gene-proteins, we create two datasets with the frequencies of symptoms and gene-proteins for each parasite. For this work we selected the k-means algorithm for clustering analysis and apply it on the datasets. In addition, we compared different algorithms to observe the performance of k-means. Clustering analysis generated different types of groups of parasites. Although the results are not 100% certain, they can make positive contributions to medical researchers and experts for the diagnosis of parasites.
In this study, for multiple keywords, we collect text data on those keywords from cyberspace by crawling and scraping. Then, natural language processing is used to reduce the text data to word groups of nouns that are considered to be important, and the importance of each word is calculated from the transition probability between words, and a network is generated. In addition, we construct a system to support idea generation by drawing 3D directed graphs to check the strength of word associations from arbitrary viewpoints.
Digital transformation has reached out to every aspect of human lives, including personal and institutional provided healthcare. In order to inspect changes occurring in the healthcare domain due to digital transformation, a literature review is conducted. A systematic literature search in the Clarivate Analytics' Web of Science database was carried out, along with the text mining in order to analyze the papers. The following attributes are considered for the classification and analysis of the papers: time component; methodological approach; research areas, and; topic identification. Results revealed that the digital transformation of healthcare has just recently begun to induce more interest in the research community. Furthermore, a mixture of scholars' profiles investigates the topic, including engineering and information scientists, as well as medical and healthcare service researchers. Additionally, by using text mining, we have identified four discourses of topics that are addressed in the literature when considering the digital transformation of healthcare. The interdisciplinary nature of the topic calls for the inclusion of diverse experts from the medical, informatics and public administration areas in order to advise proper government actions and stimulate the research field.
There were several news categories that present editors with challenges. Some news categories, such megapolitan, national, celebrity, news, lifestyle, and economics, used vocabulary that was quite similar to that of the other categories. International required an editor to be familiar with the article's contents in order for it to be uploaded and placed in the proper category. We had to first digest the news before we could label it compared to other kinds of data. The text mining approach, which attempts to make text or documents may be processed so that it will aid in the process of news classification, will be used to categorize and determine the type of news in this context. The Light Gradient Boosted Machine (LightGBM) model was used in this study to increase the gradient point with a learning stage and obtain the optimal value. This model's training process was intended to be quick while consuming less storage space and processing information more accurately. The accuracy of the classifications made using a confusion matrix to quantify the findings of this investigation, which were news type classifications, was 86%.
Intelligent input and analysis system of pre-Qin literature based on intelligent text extraction and the analysis algorithm is studied in the paper. Compared with the traditional computing model, the active storage computing model distributes the computing tasks on the host node and the device node. If the computing task is allocated properly, the computing efficiency can be increased by about 65%. The character segmentation of Chinese characters is not suitable for the above method due to the complexity of the Chinese characters. The trough projection method is used to segment Chinese characters, that is, column scanning. Considering this, the novel intelligent text extraction and the analysis algorithm is designed considering SVM and PCA. The test is applied to the recognition tasks. Through the verification analysis on the results, the performance is shown.
This study was conducted to produce a supporting method for consulting service companies so that the companies can respond to client demands irrespective of the consultant expertise. Occurrence of future problems in client companies is predicted using text mining with data taken from a consulting company. To do so, correspondence analysis and Data Envelopment Analysis (DEA) discriminant analysis are used. As described herein, the proposed method is improved using Intermediate Distribution Frame (IDF) values and standardization for the application in a real problem. For computer experiments to verify the effectiveness of the proposed method, prediction is made about cancellation problems. The prediction is compared to predictions produced by the consultant.
As Internet usage has heavily increased within recent years, money launderers have started to take advantage of Online Financial Transaction (OFT) services to facilitate their money laundering activities. However, law enforcement has struggled to understand and detect OFT services that criminals use for money laundering. To assist law enforcement in its efforts to identify and monitor OFT services, we have designed the Online Financial Transaction Services Identification Tool (OFTSIT), which crawls the Internet and determines the probability that they are OFT services. OFTSIT analyzes a website's content and extracts textual features using latent semantic indexing (LSI). LSI is a text mining approach that can extract a small number (<; 10) of features from more than 40,000 possible words on a website. OFTSIT inputs the LSI discovered features into a generalized linear model to produce the probability that a website is an OFT service. Testing showed that OFTSIT outperforms current method of manual searching. This paper describes the system architecture, algorithms employed to classify OFT services from other websites, and performance testing to demonstrate OFTSIT's operational relevance.
This study aims to provide new insights into the development of integrated elderly care policy system in China over the years through text mining. 483 policy documents are collected and processed through natural language processing technology, and based on this, social network methods are used to analyze the results from the perspectives of policy themes, policy intensity, and policy subjects. The study shows that the integrated elderly care policy system in China has gone through three different stages, with significant changes in policy themes and implementation models, which have led to various policy influences on society. At the same time, the study summarizes the existing problems of the system at this stage and provides corresponding policy recommendations.
Summarization is a perfect example of one of the numerous text mining jobs that may be improved with concept maps. A broad idea map illuminates the connections between the numerous thoughts being mapped. This article describes an evaluation technique designed with tokenization, proposition extraction, and refining in mind. The Deep Tokenizer approach, which is based on deep learning principles, has a processing time of 5 milliseconds and a tokenization accuracy of 98%. The extraction accuracy with the given proposition extractor approach was 95%, and the processing time was 10 milliseconds. The program was modified specifically to extract propositions from tokenized text. The suggested proposition refiner technique was proven to have a refining accuracy of 94% and a processing time of 8 milliseconds. The goal of this strategy was to back up and enhance the ideas derived from the data. Bar charts were used to graphically show the performance statistics of the various algorithms. These findings indicate the techniques' usefulness and reliability in dealing with large text datasets, shedding light on possible applications in natural language processing.
The increasing incidents of collapses in urban old civil buildings in recent years highlight the urgent need for vulnerability analysis. To enhance the resilience and disaster response capabilities of these buildings, this paper adopts an Indicator-based methodology (IBM) to develop a comprehensive evaluation index system for assessing the physical vulnerability of urban old civil buildings. The text mining method is applied to extract core factors from 8 representative official investigation reports. Expert consultation is then used to complement and categorize these key factors. As a result, a comprehensive physical vulnerability index system comprising 16 influencing factors is constructed. The findings of this research reveal that the physical vulnerability of civil buildings encompasses two main aspects: structural vulnerability and external disturbance vulnerability. This index system can help operators effectively evaluate the physical vulnerability of old civil buildings and make decisions regarding reinforcement and risk mitigation strategies. Furthermore, the proposed index system can assist operators in establishing a foundational indicator library for evaluating the physical vulnerability of old civil buildings in urban areas.
As businesses become increasingly reliant on big data analytics, it becomes increasingly important to {\em test} the choices made within the data miners. This paper reports lessons learned from the {\em BigSE Lab}, an industrial/university collaboration that augments industrial activity with low-cost testing of data miners (by graduate students). BigSE is an experiment in academic/ industrial collaboration. Funded by a gift from LexisNexis, BigSE has no specific deliverables. Rather, it is fueled by a research question ``what can industry and academia learn from each other?''. Based on open source data and tools, the output of this work is (a) more exposure by commercial engineers to state-of-the-art methods and (b) more exposure by students to industrial text mining methods (plus research papers that comment on methods on how to improve those methods). The results so far are encouraging. Students at BigSE Lab have found numerous ``standard'' choices for text mining that could be replaced by simpler and less resource intensive methods. Further, that work also found additional text mining choices that could significantly improve the performance of industrial data miners.
This paper proposes a method for construction of classifiers for discharge summaries. First, morphological analysis is applied to a set of summaries and a term matrix is generated. Second, correspond analysis is applied to the classification labels and the term matrix and generates two dimensional coordinates. By measuring the distance between categories and the assigned points, ranking of key words will be generated. Then, keywords are selected as attributes according to the rank, and training example for classifiers will be generated. Finally learning methods are applied to the training examples. Experimental validation shows that random forest achieved the best performance and the second best was the deep learner with a small difference, but decision tree methods with many keywords performed only a little worse than neural network or deep learning methods.
Existing systems to support the daily decision taking process carried out by health professionals need to be used independently to perform different text mining subtasks. In practice, there are few systems that unify all the subtasks into an unique framework, easing therefore the clinical work by automating complex clinical tasks such as the detection of clinical alerts as well as clinical information coding. In this sense, the MiNerDoc system is proposed, whose main objective is to support clinical decision-taking process by analysing tons of textual clinical reports in an unified framework. MiNerDoc performs two basic functions that are of great importance in the medical field: detection of risk factors based on the recognition of five medical entities (Disease, Pharmacologic, Region/Part Body, Procedure/Test, Finding/Sign), and automatic prediction of standardized diagnostic codes (MeSH descriptors). A major feature of MiNerDoc is it includes external knowledge sources such as MetaMap and UMLS to terminologically and semantically enrich the interpretation of clinical texts. Some study cases are considered in this work to demonstrate the power of MiNerDoc.
Text feature selection plays an important role in text mining. Terms are the key players in document representation. The document representation can help application in following areas-indexing, summarization, classification, clustering and filtering. Text instances come with a challenge of high dimensional feature space and using such features can be extremely useful in text analysis. Hence it is important to extract important terms from a document. In this paper, we examine the impact of NLP features (stop words, stemmer and combination of both) on predictive performance of base classifiers and ensembles of Naive Bayesian category. We selected different category of base classifier like NB, SVM, KNN and J48 as these are frequently used by the researchers in text mining. IMBD movie review dataset is used as a standard dataset for experimental work. We prepared ensembles of Naive Bayesian with base classifiers and found ensemble gives better performance over the base classifiers with entire NLP categorical dataset. Ensemble of NB with SVM out performed among other ensembles with different categorical dataset.
Everyday large volume of data is gathered from different sources and are stored since they contain valuable piece of information. The storage of data must be done in efficient manner since it leads in difficulty during retrieval. Text data are available in the form of large documents. Understanding large text documents and extracting meaningful information out of it is time-consuming tasks. To overcome these challenges, information in the form of text are summarized in with an objective to get relevant knowledge with the help of text mining tools. Summarized text will have reduced size as compared to original one. In this paper, we have tried to highlight major techniques for extracting important information from a given text with the help of topic modeling, key phrase extraction and summary generation. For topic modelling LSI and NMF method is used, weighted TF-IDF method is used for key phrase extraction while text summary is generated by using LSA and Text Rank method.
To be competitive under the paradigm of Industry 4.0 (I4.0), companies must develop a Production Planning and Control (PPC) able to respond to disturbances. Notably, unexpected machine breaks leading to corrective maintenance actions drastically delay operations as they inhibit the machine production, affecting the production schedule. To create an adaptive PPC, data from the shop floor must be collected and analyzed. However, these data are often unstructured, as they come from either sensors or humans, which makes their use difficult. Notably, human based operations such as maintenance activities often produce textual data. Hence, the objective of this paper is twofold: firstly, it aims to propose a framework for a model capable of estimating the production inhibition time based on textual data. Secondly, it settles the basis for a Dynamic Production Schedule (DPS) model considering these estimations. To achieve this, an approach using Text Mining (TM) and Machine Learning (ML) techniques is proposed.
This paper deals with the problem of topic identification of Arabic noisy texts, which is an important research field, regarding the growing amount of shared textual information in the world. The dataset used in this survey is constructed by collecting several corrupted Arabic texts from different discussion forums related to six different topics. The proposed algorithms use the k-nearest neighbor classifier based on the Tf-Idf to identify the texts topics. Furthermore, two training schemes are proposed for the creation of the reference profiles. Moreover, several distance measures are proposed and employed to compute the similarity between texts/topics. Results show that the proposed distance measures are quite interesting in topic identification.
Appearing social networks these days, the capacity of produced information has an increasing growing. The usual learning techniques don't have an efficient performance and the need of utilizing increasing learning methods is seen as a necessary factor. In mining the text in social networks we can see that text mining and social analyzing in texts are new topics in data analyzing which are considered as important factors growing very fast. Developing Microblogging sites like Twitter leads to make opportunities to make and applying some theories and technologies leading to mine and research trends. In this article we will evaluate Twitter the social network its characteristics and introducing and comparing data mining algorithms to online investigation on texting data. Researches show that stochastic gradient descent superior than other online evaluating techniques in analyzing text.
Non-classroom courses in higher education significantly encourage creative thinking. Awarding credits for accomplishments through non-classroom courses encourage students for more participation and improve their problem-solving skills. Machine learning techniques help to automate the maintenance of such non-classroom courses. This paper proposes a process to introduce and automate such a non-classroom. Firstly, students will submit a softcopy of the certificates/proofs of their accomplishments such as winning/participating in a competition/event through a web-based user interface. Secondly, the authenticity of these proofs/certificates is tested using text mining, image processing, and web scraping techniques. Text is extracted from the submitted certificates using Tesseract Optical Character Recognition (OCR) Engine and stored as a MySQL database which is matched with the information provided by the student in the web interface and other sources such as flyer of the event. Further, the extracted text is used as search criteria for web scraping and matched with the scraped text. Further, the authentication test can be performed with these keywords and tags by search through social media. Facial Encodings and Recognition is done on the images submitted to authorize the user. Finally, the evaluation is performed and the final result indicating the credits/grade is revealed to the user.
Bugs are abundant in software systems. The impact that a bug has on operation of a product is defined by its severity. Bugs with high severity such as blocker or critical needs to be fixed instantly as compared to bugs with low severity. Thus predicting the severity of bugs is crucial. Therefore, text mining techniques are applied on the summary of bug reports and prediction is done using various machine learning techniques. In this study, a systematic review is done from 2008 to 2017, in which prediction of severity is done using text mining. 29 relevant research studies are found following a systematic review process. The study deduces that textual data can be used to predict the severity of bugs using various machine learning techniques. After the analysis of results, it was concluded that Bayesian learner techniques are commonly used machine learning technique and Information Gain as feature selection method used. The study also analyses the most common dataset and tools used to predict the severity of reported bugs. The performance of various machine learning techniques is evaluated and results can be generalized to other studies. This study will benefit the researchers and guide them in further research in the area of severity prediction using textual attributes of software bugs.
In this research, we propose the version of K Nearest Neighbor which considers similarity among attributes for computing the similarity between feature vectors. The text segmentation task is viewed into the binary classification where each pair of sentences or paragraphs is classified into whether we put the boundary or not, and the proposed version resulted in the successful results in previous works concerned with the text categorization and clustering. In this research, we define the similarity measure based on both attributes and values, modify the KNN using it, and apply the modified version into the text segmentation task. We may expect more compact representation of data items and improved performance in the text segmentation task as well as other tasks of text mining. Therefore, the goal of this research is to implement the text segmentation system which provides the benefits.
Based on text mining, this paper prevents and controls financial risks by quantifying the logical relationship of financial risks, and at the same time deals with the problem of unreliable word frequency quantification of financial events. This paper proposes a quantitative analysis method of financial risk logical relationship based on BERT and mutual information combined with domain knowledge and quantifies the relationship in the general data set COPA and the financial domain data set. Based on BERT and mutual information, it can effectively solve the problem of unreliable word frequency quantification. The accuracy rate of this method in the quantification of financial risk logical relationship reaches 80.1%, which is 3.09~37.39% higher than the benchmark model. Only the corpus in the financial field is considered, and the effect on other non-financial corpus is pending inspection. This method can reveal the evolution path of financial risk events and improve the effect of quantifying the logical relationship of financial risk.
This paper introduces a workbench called SINDI-WALKS capable of extracting inherent valuable information from scientific literatures such as articles and patents. The system identifies PLOTs, Person, Location, Organization and Terms and extracts the semantic relations between them. Also, the workbench provides two test beds for monitoring the run-time behavior of the core engines, SINDI-CORE (PLOT recognition engine) and SINDI-LINK (relation extraction engine). As an important auxiliary tool, an intuitive annotation system is additionally furnished to construct training or evaluation collections for the engines above. Various administrative functions are also provided to visually manage the extracted semantic triples in an efficient manner. The proposed toolsets in this paper will facilitate the performance optimization of the information extraction as well as the efficient management of the extracted semantic triples.
Customer satisfaction plays an important factor for the business' success, particularly in aviation industries. One way to measure customer satisfaction level is using customer reviews. This study evaluates and analyzes customer reviews of services and facilities of Soekarno-Hatta Airport as the largest airport in Indonesia using text mining approach of sentimental analysis. Support vector machine and Naïve Bayes classifier are classification techniques used to identify positive or negative sentiments contained in review sentence. The results of classification techniques for sentiment analysis in this study indicate that support vector machine has higher accuracy value than Naïve Bayes Classifier in analyzing sentiments. The output of this study is evaluation in improving the quality of airport services and facilities, identification of service aspect and airport facility which become the strength and weakness as well as improvement prioritization of aspects that still become weakness in achieving desired level of customer satisfaction.
Due to the exponential growth of the number of texts available online, automatic text mining techniques have been receiving much attention recently. Pattern-based text mining techniques have been specially studied because they allow users to extract points of interest from text automatically. However, the quality of obtained results is highly dependent on the quality of the rules that are used in the mining process. Currently, the rules that are more effective are the ones that are defined manually, but the creation of such rules is resource-consuming as it requires experts to define them. In this paper, we develop a method of automatic rule definition for pattern-based text mining. Our method is investigated in terms of aspect classification and sentiment analysis. We also examine combining pattern-based text mining with machine learning techniques. The evaluations were performed using two text data, namely, those mentioning API from Stack Overflow and those of Amazon review on PC peripherals. Evaluation results showed that meaningful rules were defined automatically by our method. We also show that our methodology can be used together with manually defined rules, resulting in outperforming other methods.
Most of the Islamic documents have been documented and published to Muslims to be studied and applied in daily activities. One of these documents is a Hadith. Along with the development of hadith technology not only recorded on sheets of paper but also stored on digital data storage media in the form of databases. Document grouping allows users to get better information than using documents in the same group. Fuzzy techniques allow different membership levels. The purpose of grouping documents is to get different groups. This is needed for better document search. The document used in this study is an Indonesian translation of the Sunnan An Nasa'i Hadith tradition with a grouping technique using a combination of Text Mining and Fuzzy C-Means algorithm as a method of grouping it. By comparing the results of manual calculations against the results of calculations using RStudio software, the accuracy obtained is 80%.
The proliferation of social networks, IoT, cloud computing platforms has enabled the collection and storage of massive amount of data feasible. A large portion of the gathered data is available in text form leading to requirement of efficient text mining techniques. One of the basic tasks in text mining is text classification. The paper presents a detailed comparative performance analysis of popular machine learning based classifiers for text classification. Even though support vector machine classifier outperforms others in many cases, none of the classifier can classify all datasets. In order to gain more insights, the probability distributions of features across datasets are examined and the emergence of power law probability distributions is confirmed in many. This finding provides a rationale for the effectiveness of maximum Tsallis entropy classifier recently proposed in the literature. These results will help practitioners in choosing appropriate text classification method.
The sentiment behind financial text has been observed to have correlations with stock market trend. Though widely discussed, the study on this topic faces the challenge coming from the lack of open dataset and labeled financial text. In this work, we collected a large amount of Chinese financial text from financial news, research report, stock BBS and corporate announcements. It contains 3 million articles about 128 stocks from 2010 to 2018. And then we proposed a model mapping from the text and latent sentiment to the abnormal market trend. It combines the posting amount, daily market index with the RBM-embedded document vector, and extracts the abnormal features via LSTM. After that a neural net is employed to identify the abnormal trend. The experimental results on our dataset show the effectiveness of our approach comparing to baseline methods.
Wikipedia is an online encyclopedia which contains millions of articles related to different subject domains. Wikipedia also has a search page itself to display the links corresponding to Wikipedia articles for a given user query input. This search result page displays the search results according to the relevance order, without any content based grouping. This paper presents an experimental deduction of a search result clustering methodology to group the links, returned by the search result page for a particular keyword, based on the contents of the HTML documents, represented by the links and label these resulted groups meaningfully. The proposed methodology is based on the concepts and theories of Text Mining. Grouping of search results makes easy and efficient for the user in finding the desired Wikipedia document. It is also possible to view the different applications and usages of a given keyword very quickly. This work identifies the best clustering algorithm for document clustering and investigates the ways to determine optimum number of clusters to have a better grouping and label the groups. We evaluate our proposed method by conducting several experiments and the results indicate that our method has a higher precision and recall.
This work aims to design a proposed decision support system (DSS) for helping investors in making investment decision by using rule text-mining based algorithm to analyze news headline and implement analyzing pro-gram based on a manual analyzed headlines. The news analysis program (NAP) was used as an important stage in making investment decision on sample of the Gulf Cooperation Council (GCC) stock markets using Alarabia.net and Reuters.com which treated as a source of media noise that has an influence on the value of stock quoted stock market. The second kind of data that proposed to use in this system is the financial data of GCC stock market. The resulted data can be used in further steps to make better understanding of stock market companies behavior such as the statistical, data mining calculation for choosing the best period of time that give the best reaction of stock market ratios to the news indicators and using the vector measure construction method (VMCM) for classifying companies according to their response to the news.
News has been an important source for many financial time series predictions based on fundamental analysis. However, digesting a massive amount of news and data published on the Internet to predict a market can be burdensome. This paper introduces a topic model based on latent Dirichlet allocation (LDA) to discover features from a combination of text, especially news articles and financial time series, denoted as Financial LDA (FinLDA). The features from FinLDA are served as additional input features for any machine learning algorithm to improve the prediction of the financial time series. We provide posterior distributions used in Gibbs sampling for two variants of the FinLDA and propose a framework for applying the FinLDA in a text and data mining for financial time series prediction. The experimental results show that the features from the FinLDA empirically add value to the prediction and give better results than the comparative features including topic distributions from the common LDA.
Universities collect qualitative and quantitative feedback from students upon course completion in order to improve course quality and students' learning experience. Combining program-wide and module-specific questions, universities collect feedback from students on three main aspects of a course namely, teaching style, content, and learning experience. The feedback is collected through both qualitative comments and quantitative scores. Current methods for analyzing the student course evaluations are manual and majorly focus on quantitative feedback and fall short of an in-depth exploration of qualitative feedback. In this paper, we develop student feedback mining system (SFMS) which applies text analytics and opinion mining approach to provide instructors a quantified and exhaustive analysis of the qualitative feedback from students and avail insights on their teaching practices and this in turn will lead to improved student learning.
PDCA cycle of the environmental management accounting under the background of data mining and intelligent systems is studied in this article. Initially, the data mining has been expanded to text mining, image mining and other fields, becoming a standard term, including text mining, image mining, are used to optimize the traditional algorithm. The visualization technology is a necessary means of the data mining to analyze and also knowledge discovery serves as an important tool to promote knowledge dissemination and transformation. The important technical foundation of the PDCA is to be analyzed for the better presentations. Finally, the application scenarios are tested. The experimental results have reflected the robustness.
The medical texts contain rich information about the health status of patients, including personal demographic details, examination items, diagnostic narratives, and corresponding outcomes. There is often an underlying dependency between a patient and multiple medical texts. Such dependency has a substantial influence on the accuracy and reliability of the patient's comprehensive diagnostic outcomes, thus necessitating the modeling of dependency. Most of the existing studies on medical text classification focused only on textual content and often overlooked the inherent dependency between the patient and medical texts, leading to the loss of relevant contextual information. To address the above problems, we propose an approach for medical text classification using heterogeneous Graph-based DEpendency MOdeling with Hierarchical ATtention (GDEMO-HAT), which accounts for patient-notes dependency and retains the contextual information. We model the dependencies between the patients and their medical texts via heterogeneous graphs. To facilitate medical text mining, we incorporate domain knowledge into the construction of a Patient-Notes-Physician (PNP) graph. Based on the constructed PNP graph, we introduce the hierarchical attention mechanism for classification that assigns different levels of importance to nodes based on distinct semantic contexts and thereby effectively captures the intricate interdependencies among these nodes. The representations of dependencies are integrated at the semantic level, thereby enabling a more comprehensive understanding of textual data. To demonstrate the effectiveness of the proposed approach, we conduct a classification task of the diagnosis of cardiovascular and cerebrovascular diseases based on medical text. The classification accuracy rate achieves 94%, which is significantly higher than the performances of the other compared models, such as Bert.
In this research, we propose a particular version of KNN (K Nearest Neighbor) where the similarity between feature vectors is computed considering the similarity among attributes or features as well as one among values. The task of text summarization is viewed into the binary classification task where each paragraph or sentence is classified into the essence or non-essence, and in previous works, improved results are obtained by the proposed version in the text classification and clustering. In this research, we define the similarity which considers both attributes and attribute values, modify the KNN into the version based on the similarity, and use the modified version as the approach to the text summarization task. As the benefits from this research, we may expect the more compact representation of data items and the better performance. Therefore, the goal of this research is to implement the text summarization algorithm which represents data items more compactly and provides the more reliability.
The increase in the number of online published research papers can be attributed to the recent developments of the internet and web technologies. However, researchers and online users have a difficult time getting relevant and accurate information due to information explosion on the internet. In this paper, we seek to establish which algorithms and similarity metric combinations can be used to optimise the search and recommendation of articles in a research paper recommender systems. Our investigation utilised non-linear classification algorithms with text similarity measures. An offline evaluation approach is utilised to determine the models accuracy and performance, while various similarity metrics are assessed using available datasets. We will utilise the Recursive PARTitioning (rpart), Random Forest and Boosted machine learning algorithms on research paper similarity evaluation datasets. The rpart algorithm generally performed well when compared to the Boosted and the Random Forest algorithms by getting an average accuracy and time efficiency of 80.73 and 2.354628 seconds respectively. The cosine similarity performed best when compared with the other similarity metrics. New similarity metrics and measures are going to be proposed. It has been established in this paper that there are better combinations of metrics and algorithms when attempting to develop models that can be used for research paper similarity evaluation and recommendation. Further challenges and open issues are identified.
As the "bag of words" approaches cut down the linkage between the words, they are hardly to be applied to explore the causal relations between the terms described from the text corpuses. To discover the networked causal knowledge from the corpus, we (1) propose the algorithm pv-swapping and the PV-parse-tree to adjust the term orders for the sentences by observing the relationship between the grammatical voice and causal relation, provide the NLP approaches to transform the corpus to the sets of ordered terms sequences (OTS) by preserving the semantic orders of the phrases in the sentences, (2) formalize the causal network extracting problem in the corpus and show that it is a NP-hard problem, (3) propose the algorithms NE-IC and heuristic-majority-vote to extract the causal network of the terms based on the OTS sequences. We provide sufficient experiments on the real data sets. The experimental results show that our methods are both effective and efficient to discover the causal network of the terms, and the resulted causal networks of heuristic-majority-vote with less conflict causal relations or cycles than the results of NE-IC. At the last, we also provide experiments on several causal knowledge discovering tasks based on the resulted causal networks to show their interesting applications.
With the development of science and technology, the intelligent distribution network is making rapid progress and the demand for power supply is also increasing. The classification and analysis of the power outage causes in distribution network is helpful to the prediction of power failure. There is a large gap in the proportion of categories of the sample data of power grid outage causes, and serious category imbalance will lead to the deviation of classification results. In order to classify the original data more reasonably, the sample value added technology based on generative adversary network is used to expand the category data with less data in this paper. In order to realize automatic classification of power grid outage causes, the method of text data preprocessing and the text classification model based on deep learning are studied. Finally, the experimental results show the influence of sample capacity and classification accuracy.
Emergence of Web 2.0, internet users can share their contents with other users using social networks. In this paper microbloggers' contents are evaluated with respect to how they reflect their categories. Migrobloggers' category information, which is one of the four categories that are economy sport, entertainment or technology, is taken from wefollow.com application. 2105 RSS news feeds, whose category labels are same with microbloggers' contributions, are used as training data for classification. In this study two types of users' contributions are taken as test data. These users are normal micro loggers and bots. Classification results show that bots provide more categorical content than normal users.
Mobile robots are at the forefront of studies in the field of robotics. Research topics such as path planning and task planning take place importantly in the field of mobile robots. In this study, papers in the field of mobile robots in the last 20 years are discussed. The abstracts of 13,404 papers in the Scopus database were analyzed in two 10-year periods between 2002-2011 and 2012-2021, before and after 2011, when the Industry 4.0 Revolution emerged. Analysis results were created by text mining method. According to the results of the analysis, the research topics named Path Planning Algorithm and Automated Guided Vehicles in Industry were among the topics that attracted attention. As a result of these analyses, changes in research areas with developing technologies were shared with both academic and industry stakeholders for future study.
The critical step for developing countries is policy formation/reforms. The process of policy formation is focused on enhancing the quality of its citizen“s life or society and transforming the economy of the country. There will always be a positive/negative impact on the industries/companies within the policy implementation domain. Such conditions lead firms realizing the business advancement in planned policy initiates to shape or support it by helping the government. Contrary, firms sensing adverse effect on them due to proposed new policy, tries to resist or delay the deployment of policy. This article presents a case of an Indian Government“s proposed `Electric Vehicle (EV) Policy' and the reactions received to it from various stakeholders viz. firms under policy domain, consumers and different associations. The feedback from different stakeholders is examined in this study to determine how the transition took place in proposed new policy using Text Mining approach. The analysis consists of reactions of stakeholders to the new policy in terms of Shape, support, oppose and delay, and Government“s change in its stand due to the industry”s reaction.
The massive growth of biomedical text makes it very challenging for researchers to review all relevant work and generate all possible hypotheses in a reasonable amount of time. Many text mining methods have been developed to simplify this process and quickly present the researcher with a learned set of biomedical hypotheses that could be potentially validated. Previously, we have focused on the task of identifying genes that are linked with a given disease by text mining the PubMed abstracts. We applied a word-based concept profile similarity to learn patterns between disease and gene entities and hence identify links between them. In this work, we study an alternative approach based on topic modelling to learn different patterns between the disease and the gene entities and measure how well this affects the identified links. We investigated multiple input corpuses, word representations, topic parameters, and similarity measures. On one hand, our results show that when we (1) learn the topics from an input set of gene-clustered set of abstracts, and (2) apply the dot-product similarity measure, we succeed to improve our original methods and identify more correct disease-gene links. On the other hand, the results also show that the learned topics remain limited to the diseases existing in our vocabulary such that scaling the methodology to new disease queries becomes non trivial.
Online news provides a convenient way for users to read novel news. Building online news corpus is important to many text mining and data mining issues. The creation of web news data required to construct a set of HTML parsing rules to identify content text. When a website rapidly change the layout style, the parsing rules (wrapper) should be reconstructed. In this paper, we address this issue and propose a news content recognition algorithm that is portable to different language and various domains. Our method first scans the entire HTML document and detects a set of candidate blocks. Second, the proposed weighted scoring function that combines stop word language models and HTML penalty functions is used to rank the importance of each candidate. We then check the block which obtains the highest score and a predefined threshold value. To validate the approach, we conduct experiments by using 533 online news HTML files from 24 web sites. The empirical study shows that our method achieves ~95% macro F-measure rate in recognizing news content.
Information technology is advancing faster than anticipated. The amount of data captured and stored in electronic form by far exceeds the capabilities available for comprehensive analysis and effective knowledge discovery. There is always a need for new sophisticated techniques that could extract more of the knowledge hidden in the raw data collected continuously in huge repositories. Biomedicine and computational biology is one of the domains overwhelmed with huge amounts of data that should be carefully analyzed for valuable knowledge that may help uncovering many of the still unknown information related to various diseases threatening the human body. Biomarker detection is one of the areas which have received considerable attention in the research community. There are two sources of data that could be analyzed for biomarker detection, namely gene expression data and the rich literature related to the domain. Our research group has reported achievements analyzing both domains. In this paper, we concentrate on the latter domain by describing a powerful tool which is capable of extracting from the content of a repository (like PubMed) the parts related to a given specific domain like cancer, analyze the retrieved text to extract the key terms with high frequency, present the extracted terms to domain experts for selecting those most relevant to the investigated domain, retrieve from the analyzed text molecules related to the domain by considering the relevant terms, derive the network which will be analyzed to identify potential biomarkers. For the work described in this paper, we considered PubMed and extracted abstracts related to prostate and breast cancer. The reported results are promising; they demonstrate the effectiveness and applicability of the proposed approach.
Since the internet's emergence, numerous facets of daily life have evolved, including online investment opportunities. Cryptocurrency investment has gained popularity in this digital era. Prospective investors now have various cryptocurrencies to choose from, tailored to their preferences. Due to the volatility of cryptocurrency market, investors need to have careful consideration of which cryptocurrency to be chosen including its related decision to be made during their investment. One crucial consideration in this selection process is assessing fellow investors' opinions, often found on social media. This study employs text mining, using the k-means clustering algorithm to explore prevailing sentiments among cryptocurrency investors. The data source consists of Twitter comments on three prominent cryptocurrencies, Bitcoin, Ethereum, and Binance totaling 55,651 tweets. The findings predominantly reveal neutral sentiments towards these cryptocurrencies. Notable positive sentiment topics for Bitcoin include “worth” and “new,” while negative sentiments revolve around “firm” and “bank,” and neutral sentiments are linked to “digital” and “year.” Ethereum exhibits positive sentiments like “good” and “defi,” negative sentiments such as “time” and “long,” and neutral sentiments around “price” and “ethic”. Binance's positive sentiments include “live” and “kind,” a negative sentiment related to “case,” and neutral sentiments encompassing “learn” and “check”. Moreover, the Davies Bouldin Index evaluation for the coin clusters yielded scores of 0.7996 for Bitcoin, 0.7820 for Ethereum, and 0.7149 for Binance. These indices, falling between 0 and 1, signify well-structured clustering.
With the rapid development of the Internet, network interactions become more frequent. How to use network big data to obtain user needs in a timely, fast, and accurate manner is the only way to improve the competitiveness of enterprises in the market. This article focuses on product-related online review texts, and proposes a new method for obtaining user needs based on Internet information, which is, using the TF-IDF function to extract product feature words, then using a method of similarity forest calculation based on path and depth to classify the review text, and using the sentiment dictionary to calculate the sentiment tendency of the review text under each type of feature set. So, a product feature evaluation model based on text mining can be constructed. Then, through the automatically generated survey questionnaire to simulate the shopping decision behavior of the user, a large amount of real user demand information is quickly obtained. This method also provides convenience for users' shopping decisions and has verified its feasibility and effectiveness by taking Jingdong Mall's top ten mobile phone brands as an example.
Various computational algorithms have been developed for constructing the genetic networks and by using text mining. The rapid growth in the biomedical literature, however, still encourages the importance of improving these algorithms for a better understanding of the protein/gene biochemical interactions. This paper proposes a text mining system that constructs a gene-gene-interaction network for the yeast genome by identifying the co-occurrence frequency of the genes in the biomedical text. The system determines connected genes based on their appearance in several levels of the text (i.e. abstract and sentence). This paper highlights the importance of recognizing the sparsity of biomedical data when designing a text mining prediction system. It does so by employing a rare event classification model that reflects the population using small samples of data. The results show that this method has the potential of improving the prediction accuracy for gene-gene-interactions.
To solve the problem of single system education mode in network distance education, an intelligent service model of distance education based on Web mining is proposed. This paper first provides the general process of data processing in distance education system, including data preparation and selection, data preprocessing, mining algorithm selection and implementation. Then, through the improvement of Apriori algorithm, we extract part of the experimental data to construct the mining model. Through the application analysis of the model, we acquire the rule set to provide personalized guidance for students. According to the mining results, the system can dynamically generate a set of pages that users may be interested. The practice has proved that the application of Web Mining Technology in the distance learning system improves the personalized service level of the learning system, and provides an intelligent auxiliary means for the decision analysis.
Twitter is one of the online media that produces a number of information data on significant disaster situations, namely tweets, some of which have tweets that have valuable information to take humanitarian assistance measures. The purpose of this study is to train models based on information sourced from Twitter by classifying tweets that are informative and not informative by using algorithms for classification. Based on previous research algorithms that are considered appropriate in dealing with this problem are Support Vector Machine algorithms based on that, in research developing research models by adding features of the Smote UpSampling imbalance and Gini Index and adding a Naïve Bayes algorithm to compare the accuracy of the classification algorithm. The data used is the data of tweets related to earthquake disaster events that occurred in Indonesia that were collected using the Rapid Miner application and using GataFramework in text preprocessing. Based on the method proposed in this study the Support Vector Machine algorithm produces 81.03% accuracy superior to the Naïve Bayes algorithm which produces an accuracy of 80.30%, based on the results of accuracy both enter into a good classification algorithm of the resulting accuracy.
Social media has given new way of communication technology for people to share their opinions, interest, sentiments. Huge amount of unstructured data is generated from social media like Facebook, twitter, LinkedIn, which is repository of useful insights. Analytics can be applied to extract various useful insights form this. Main objective of this paper is to extract the knowledge from large social media data, identify the people sentiments and behavior to make cognizant decisions. Effective sentiment analysis of people emotions is necessary for complex topics like politics, government, present trends and healthcare. These objective are achieved by real time retrieval of twitter data and perform sentiment analysis. It helps to classify emotions of the people are positive, negative, happy, sad or neutral towards national parties of India (BJP and INC). Considering people emotions, parties can identify their weakness and look after improvement to fulfill society needs and work towards satisfying people requirements. Using text mining and unsupervised lexical method classified tweets related to these parties to identify people emotions for the parties.
Many studies have been carried out on the impact of Electronic Health Records (EHR) on doctor-patient communication. In this study, we emphasize on verbal communication and conduct an exploratory analysis to understand how the presence of double screen EHR impacts doctor-patient verbal communication across different races. We use sentiment as a measure of double screen EHR sharing, where positive sentiment indicates preference towards EHR sharing using a double screen and negative sentiment indicates otherwise. Our study indicates that the doctor's gender and patient's race have a noticeable impact on EHR screen sharing adoption and sentiment.
Text categorisation discipline has recently attracted many scholars because of the large number of documents on the World Wide Web (WWW) that contain hidden useful information which can be utilised by organisational's managers for decision making. However, the majority of research conducted in text categorisation is related to English data collections while there is limited research attempts conducted on mining corpuses in Arabic. This paper investigates the problem of Arabic text categorisation in order to measure the performance of different rule based classification data mining techniques. Precisely, four different rule based classification approaches: C4.5, RIPPER, PART, and OneRule are compared against the known CCA Arabic text data set. Experiments are carried out using a modified version of WEKA business intelligence tool, and the results determine that the least suitable classification algorithms for classifying Arabic texts is OneRule whereas RIPPER, C4.5 and PART have similar performance with respect to error rate.
Tibetan text representation, which has great influence on Tibetan text Categorization and Cluster, is the groundwork in Tibetan text mining. Tibetan microblog is one of the most popular Tibetan network media. Researches on Tibetan microblog are now increasing. However, because of the special features of microblog text and the features of Tibetan language, traditional Tibetan text representation method cannot satisfy the need. This paper proposes a Tibetan microblog text representation method that is based on shallow parsing and takes the Tibetan micro-blog sentiment analysis experiment. First, for Tibetan micro-blog text, the syntactic structure is generated by using syntactic tree. Second, the semantic feature space is built based on syntactic structures semantic features. Then, the semantic Cluster centroid is formed with the K-means method in the feature space. Last, the TF-IDF value based on cluster is calculated. The experiment shows, the method of this paper is compared with the SVM+TF-IDF and Naive Bayes+ the Maximum Entropy method, the F-measure is as high as 91.4%.
The purpose of this study is to evaluate the service quality of mobile banking apps of Korean banks and derive improvement priorities by conducting Importance-Performance Analysis from user reviews through text mining techniques. Furthermore, the Importance-Performance Competitor Analysis (IPCA) is conducted to analyze the internal and external strengths and weaknesses of mobile banking apps compared to other banks for each bank in various ways. A total of three Korean banks were selected for analysis: A commercial bank (Kookmin Bank), a Local bank(Gyeongnam Bank), and an Internet Bank(Kakao Bank).
Text data mining is the process of extracting and analyzing valuable information from text. A text data mining process generally consists of lexical and syntax analysis of input text data, the removal of non-informative linguistic features and the representation of text data in appropriate formats, and eventually analysis and interpretation of the output. Text categorization, text clustering, sentiment analysis, and document summarization are some of the important applications of text mining. In this study, we analyze and compare the performance of text categorization by using different single classifiers, an ensemble of classifiers, a neural probabilistic representation model called word2vec on English texts. The neural probabilistic based model namely, word2vec, enables the representation of terms of a text in a new and smaller space with word embedding vectors instead of using original terms. After the representation of text data in new feature space, the training procedure is carried out with the well-known classification algorithms, namely multivariate Bernoulli naïve Bayes, support vector machines and decision trees and an ensemble algorithm such as bagging, random subspace and random forest. A wide range of comparative experiments are conducted on English texts to analyze the effectiveness of word embeddings on text classification. The evaluation of experimental results demonstrates that an ensemble of algorithms models with word embeddings performs better than other classification algorithms that uses traditional methods on English texts.
In this paper, we provide an approach to extract biomedical information related to breast cancer using text mining technology. We first extract entities related to breast cancer, then find these relationships, and visualize these biomedical information. Finally, these extracted biomedical information are annotated in the original experimental dataset which offer researchers as breast cancer biomedical information corpus. the approach provide an new way to obtain biomedical information of breast cancer. Moreover, it is promising for development of biomedical text mining.
In modern society, some famous news websites such as Sina and Times server to provide information every day for millions of users. But with the continuous development of information technology, the amount of disorder data is increasing. How to organize the text and make automatically text classification has become a challenge. The traditional manual classification of news text not only consumes a lot of human and financial resources, but also hardly achieved classification task quickly. In this paper, the paper mainly makes a research about the news text classification. It proposes a news text classification model based on Latent Dirichlet Allocation (LDA). Due to the dimension of the news texts is too high, this model uses topic model to make text dimension reduced and get features. At the same time, the paper also makes a research on Softmax regression algorithm to solve multi-class of text problems in our life and make it as model's classifier. The paper evaluates proposed model on a real news dataset and the result of the experiment shows the improved model performs relatively well. The model can effectively reduce the features dimension of the news text and get good classification results.
Examining data for similar items is one of the fundamental data-mining problems. Application of methods for similarity search could be useful for plagiarism or near-duplicate web page detection. The computerized methods developed during last years are mainly focused on English language. However, Slovak language has several specific attributes and using these methods may not be precise enough. Our objective of this research paper is to develop a Proof-of-Concept for document similarity estimation process devoted to Slovak Language. The complexity of Slovak language gives us an opportunity to analyze and adjust the methods parameters and thus achieve higher accuracy. Text mining process suggested in this article utilizes stop words list and shingles to accurately measure documents similarity. Results are further validated using date constraint.
Power companies have accumulated a large amount of maintenance data of power equipment in text format. To extract valuable information, text mining is expected. Text similarity is an important method for text mining; however, the feature dimensions increase for long texts. In this paper, the SimHash algorithm is used to map the original text into a 64-bit binary fingerprint, and the similarity between texts is then determined with Hamming distance. With this method, the recommendation model of maintenance measures is established. The verification results show that the recommendation model has good predictions based on the SimHash and Hamming distance algorithm, and is potential to apply in the practical field.
Massive text stream is a very important source of information. There are rich graph structures contained in massive texts, which can be used as an effective method to mine trends embodied in the contents in text streams. This paper formally defines the event graph model based on systems science theory, and discusses its properties. This model aims to extract the potential events and relationships between them from text streams.
A good text classifier is a classifier that efficiently categorizes large sets of text documents in a reasonable time frame and with an acceptable accuracy. Most of the text classification approaches are based on the statistical analysis of a term, either a word or a phrase. Though statistical term analysis shows the importance of the term, it is tedious to analyze when more than one term has the same frequency level but one may contribute more meaning than the other. When analyzing by concept based mining it is easy to identify the most contributable term of the document. The performance of the categorizer is mostly depends on how well the system is trained for different categories. This paper introduces a novel approach of self appreciating model in which each of the positive testing is redirected to the training system to make the training stronger and stronger at all possible test events.
The main purpose of text mining techniques is to identify common patterns through the observation of vectors of features and then to use such patterns to make predictions. Vectors of features are usually made up of weighted words, as well as those used in the text retrieval field, which are obtained thanks to the assumption that considers a document as a "bag of words". However, in this paper we demonstrate that, to obtain more accuracy in the analysis and revelation of common patterns, we could employ (observe) more complex features than simple weighted words. The proposed vector of features considers a hierarchical structure, named a mixed Graph of Terms, composed of a directed and an undirected sub-graph of words, that can be automatically constructed from a small set of documents through the probabilistic Topic Model. The graph has demonstrated its efficiency in a classic "ad-hoc" text retrieval problem. Here we consider expanding the initial query with this new structured vector of features.
Manufacturing and production systems have become increasingly complex in the past decade to meet the competitive demand in a growing industry. As these systems grow in complexity and flexibility, there is a need for efficient management and analysis of these systems. Model-based systems engineering (MBSE) addresses the complexity inherent with systems development with a model-centric approach that supported tailored modeling languages, methods and tools. This paper identifies the thematic evolution and trends and relationships found in the use and application of MBSE specifically in the manufacturing and production engineering domain. A collection of 471 published article from Institute of Electrical and Electronics Engineers (IEEE) and Science Direct over the past decade were used for the analysis using text mining techniques. Due to the limitation on the access to full text information of all the articles identified, only abstracts were considered for analysis. This effort helps the researchers across the domain to explore the reason behind and understand the change of the thematic perspectives of MBSE application over the last decade. In addition, the finding of the growing interest in addressing the aspects of complexity and systems requirements, and on the aspects of the use of MBSE for identifying and addressing the challenges related to Cyber Physical Systems help in paving a path for future research.
“Routine to Research” (R2R) is well known for Thai research related to the development of routine works of medical and public health practitioners. R2R research contains useful practical knowledge beneficial to the health of Thai people. However, this knowledge cannot be shared easily because it is unstructured and not classified text, moreover, no tool for R2R Thai knowledge sharing yet. In this research, we attempt to use text mining techniques to get insights of R2R research data and the K4ThaiHealth is first implemented as a prototype for basic R2R knowledge sharing. A set of basic medical corpus are developed using Thai medical International Statistical Classification of Diseases and Related Health Problems (ICD10TM) and several resources. They are used for R2R Thai medical text classification and key terms relationship extraction. The results are classified into diseases, organs, symptoms, and others. K4ThaiHealth is then used as a knowledge sharing prototype to offer health and medical practice knowledge extracted from R2R data sharing to Thai people. R2R WordCloud and R2R WordNet are used to display the diseases knowledge extracted from R2R research data and their relationships to diseases, organs, symptoms and others are visualized.
Clinical decision-making can be improved by analyzing patient characteristics and results. However, most medical data is only presented in narratively written reports, making efficient data collection and analysis impossible. Manual medical record evaluation requires a significant investment of time and labor, and clinical reviewers' differences might result in inconsistent data gathering, leading to biased study results and incorrect assumptions. Using natural language processing (NLP), medical data can automatically and deterministically be extracted from free-text clinical reports. Furthermore, it can potentially increase the pace and scope of clinical research. Comparing machine learning versus non-machine learning approaches, machine learning has proven more effective in solving many issues in various medical fields. This review explains how clinical note databases have been used to apply machine learning-based "Natural Language Processing (ML-NLP)". Examples of optimization algorithms are given to illustrate how straightforward and useful they are in the context of clinical note databases. A secondary data collection method is used for this research to gather relevant data and statistical data related to the research topic.
The paper provides a detailed description of the algorithms developed for linguistic purposes. We used the elements of supervised machine learning to identify and categorize new elements (scientific terms) in specialized texts in English. The algorithms are developed in the form of a modified version of the classical Rosenblatt's perceptron and presented in the article as the consequence of stages corresponding to the terms identification procedure, one-, two-, and three-word terms identification, the generalized and reduced schemes of one-, two-, and three-word terms categorization.
With the rapid development of Internet technology, the statistical analysis of online recruitment information attracts more and more people's attention. Online recruitment information is often unstructured and has a large amount of data. Traditional statistics and data mining methods have become invalid. In this paper, we adopts crawler and text mining technology to carry out data collection and statistical analysis of online recruitment information, which overcomes the shortcomings of traditional analysis methods. Firstly, crawler technology is used to collect data, adopting the technical route of requests-Beautiful soup-Re. Secondly, on the basis of exploratory analysis, text mining technology is used to carry out the text clustering analysis of job requirements in the recruitment information. After the text is normalized, Word2vec model based on neural network is constructed for text vectoization to simplify the calculation. Elbow rule and contour coefficient method are used to cluster the sample data into 8 categories and 2 categories respectively. The experimental results show that the contour coefficient method has better clustering effect, and the data analysis positions are divided into two categories: technical type and business type, which has certain reference value for job seekers and educational institutions.
The advent of the era of big data has brought unprecedented opportunities and challenges to all walks of life. For Chinese medicine researchers, the problem that they need to consider now is to integrate Chinese medicine into the environment for developing big data and make use of big data technology to carry forward the theory of Chinese medicine to the world. The subjects of this study are coronary heart disease and apoplexy, two same clinically syndrome, which are complex diseases with a large number of documents in the medical database. In this paper, a big data mining model based on the two lines of "macro to micro" and "micro to macro" is used to construct, divide and compute the complex networks, and then the possible common biology based research methods are proposed by studying the same "traditional Chinese medicine syndrome" and the combination of big data mining and complex network analysis of two diseases for the study of objectivity of Chinese medicine in big data environment to provide a new way of thinking and methods.
Information on market size is fundamental for marketing and business decision-making. However, market size information is mostly available at the level of industry or broad categories, which include a large number of product groups. Since many companies require market size information at the segmented product level, especially for their own products, a methodology to estimate market size by specified product group (i.e., item) is required. This study proposes a market size estimation model based on the measurement of string similarity between product names. The market size of the target items is estimated by calculating the sales of individual products matched to the target item according to the measured string similarity. We tested the model by estimating the market size of 1,417 items from 165,056 individual enterprises’ product sales data. Eight types of string similarity measuring algorithms were applied, and among them, Cosine, Jaccard, and Jaro-Winkler algorithms showed the highest accuracy in matching the individual product names to the target item names. The market sizes estimated from the model showed a high correlation (Pearson correlation coefficient of 0.93) with those officially surveyed. Our approach can be utilized for intelligent market information systems to dynamically estimate market sizes requested by information users for diverse target products without pre-labeling or predetermined taxonomy on the products.
Online media (online newspaper) is the mass media that is growing very rapidly. Newspaper online is simple and easy to use. Related to online Newspapers, many news articles that are created by the Office of the daily news. So, this requires speed to create a news article. The structure of writing a news article that consists of a title, summary, and the body of the news. The opening sentence that contains a summary of the news called Terrace news or lead. The application is made is the application summarization text Indonesia news articles based on the frequency of the term method. Summarize the process consists of case folding, tokenizing, filtering, frequency and number of terms. The results of this application allows users to conduct among Indonesia news articles summarization 1-2 sentence. The application had conducted trials against some articles and data managed to summarize and compare the results with the champion and an expert on language, with a success rate of 87.5%.
In the Bangla Language Processing system, stop word is the least contributed and resourceful domain than any other language. Stop word is used for sentence formation purposes without holding any significant value in expressing the semantic meaning of a sentence. The detection and removal of Bengali stop words is an essential preprocessing task in some fields such as sentiment analysis, text mining, information retrieval, text categorization, search index and retrieval system, text summarization. But there is no standard Bengali stop word list available, also no standard dataset is publicly available with the solution of stop word-related special case problem where sometimes stop word act as a content word contextwise and viceversa. So, in this paper, a standard dataset consists of 20k sentences that can solve the stop word-related special case problem, are introduced and made publicly available on GitHub that can motivate the researchers to contribute in this field and enrich the resource of the Bangla Language Processing system. This is the first publicly available Bengali stop word standard dataset, also the dataset is experimented with by applying different approaches and machine learning classifier algorithms, and the achieved accuracy is commendable.
Data mining is the way toward finding designs in substantial data sets including strategies at the crossing point of machine learning, insights, and database systems. Data mining is an interdisciplinary subfield of software engineering and measurements with a general objective to extricate data (with smart techniques) from a dataset and change the data into a fathomable structure for further use. The paper overviews diverse parts of data mining research. The paper also talks about the algorithm selection issue in data mining.
The rapid development of technology accelerates the process of distributing the song in Indonesia. But that songs have not been followed by the age label of the listener, such as the age label on the movie. It certainly makes people difficult to choose songs that appropriate to their age, so people are free to enjoy a variety of songs and probably will earn the bad effect to the mental development. Therefore, there need to classify the lyrics of Indonesian language songs according to the age of the community. Several types of research classify song by its genre like pop, rock, jazz, i.e., classify by emotion of the song, and region of the song. In this study, develops a system that can categorize the lyrics into four age groups of the listener, namely children, adolescents, adults and all ages using naïve bayes classifier (NBC) algorithm with the number of datasets are 400 titles obtained through crawling on lirik.kapanlagi.com website. The testing algorithm using training data of 60, 70, 80, and 90 percent from the overall dataset with equitable distribution resulted accuracies are 62.5%, 65%, 67.5%, and 67.5% respectively.
Finding out new potential users for specific products are always the needs of the marketing department in industries. While Traditional ways like RFM model perform poorly in exploring new users. While the popularity of social media like Twitter and Facebook provides advertisers a new way to find, understand and target their users. In this paper, we propose a new method to find out and rank high-value target audience for a specific brand by utilizing machine learning and text mining approach. Overall tweets from 10 accounts in Twitter are collected to build the target and non-target dataset. In order to solve the data imbalance problem, five data resampling methods are assessed. Ensemble learning approaches include Bagging and Boosting algorithm are used to build the classifier. The results show that SMOTE outperforms other resampling method and AdaBoosting algorithm outperform other single classifier and Bagging model. We also find out the existence of marking accounts exists so that a threshold is set to filter these accounts which are not real users. We believe that our approach could be used in industry for identifying high-value users for online marketing purpose.
In order for a text or collection to be understood, it is very important to understand the terms contained in it. In this study, it is aimed to detect terms in a domain-specific (Cyber Security) corpus. A two-layer method is suggested for the determination of the terms used in single words or phrases. Term candidate words are determined by statistical methods in the first layer. In the second layer, the possibility of using these words in phrases with semantic approaches is checked. In the study, Word2Vec approach was used to determine semantic affinity and 3 different datasets were used. The results show that the terms used in singular or binary patterns were successfully determined using the proposed method.
The availability of code in many online repositories and collaborating platforms has posed new challenges in source code attribution not only for plagiarism detection but also in other settings such as in the use of insecure copied code in commercial application, etc. The sophistication of different type of attacks in the code sequence used especially by the students requires more effective code similarity detection algorithms. In this paper, a novel source code detection method is proposed that can identify programmers' social network based on advanced pattern detection text mining techniques. The proposed methodology has significant advantages against existing methods since ARPaD algorithm can detect all common patterns between all possible code sequences in one run. Therefore, the computational time is massively reduced to 0(mn log n). In order to assess the performance of the methodology, a new dataset was created by assigning to 46 students a code project with specific instructions. The assessment results have been visualized, producing the social network graphs of possible collaboration teams.
Breast cancer (BC) affects women of all ages after puberty around the world, but the rate spontaneously rises as the women get older. Saudi Arabia is home to a large number of BC patients. Many women are diagnosed with BC and are frequently admitted to the hospital, resulting in a massive amount of clinical data that can be analyzed and used for medical research. However, proper text mining algorithms must be implemented in order to extract valuable insights and knowledge discovery from these medical notes. In this study, 7 years of BC patients' medical notes data (between 2012-2018) were officially obtained from the hospital and experimented to diagnose patient issues using three machine learning (ML) algorithms. The dataset contained 62,845 clinical text records on "X-ray screening results" recorded by the doctors after seeing the mammogram for patients who had visited the hospital through emergency and regular appointments. Natural language processing (NLP) models were employed to exploit the text data and converted into symbols. Furthermore, the text clustering algorithms K-means, K-medoids, and agglomerative clustering were carefully experimented. The results indicated that K-means outperforms the other two algorithms, with seven identified clusters. K-means clustering also yielded 0.198 work saved over sampling (WSS) and 5.5 database performance (DB) values and was identified as the best unsupervised learning algorithm for medical text clustering.
According to credit reports of small micro-enterprise in the bank, this paper verifies the authenticity of credit indexes based on text mining. And credit indexes are selected with PC A. In order to compare, credit evaluation models are built respectively with indexes corrected and non-corrected. The result shows that credit evaluation model built on authenticity corrected with text mining could use fewer indexes to evaluate small micro-enterprise credit effectively. And the evaluation model could guide the bank to establish different competition strategies of loan price. There are more qualitative indexes than financial indexes in small micro-enterprise credit evaluation model It suggests that more attention should be paid to development prospect of the small micro-enterprise when its credit is evaluated.
Text mining has been widely used in economic analysis. Besides extracting individual opinion from a single text, it is more important to accurately aggregate group opinions from mass text files. This research proposed a text mining framework for constructing policy impact indicators to monitoring the public opinions on real estate market policies. A real estate domain dictionary has been developed based on the released policy announcements in Chinese real estate government websites. And corresponding public opinion index are constructed to evaluate the impact of real estate policies on the target market. In addition, we have improved the handling of emotional words in the public opinion dictionary. Word2Vec and So-PMI models are used to identify the polarity of trend words, and the score of each sentiment words is calculated based on its co-occurrences with the trends words using a modified label propagation algorithm. Beijing real estate market is taken as the test bed. Empirical study show that the policy public opinion index explains a significant part of Beijing housing price changes. In-sample and out-of-sample forecasts of Beijing housing prices indicate that the policy public opinion index is capable of increase the prediction accuracy.
The aim of this systematic review is to determine the current state of the art in the real-time classification of user-generated content from social media. Focus is on the identification of the main characteristics of data used for training and testing, the types of text processing and normalization that are required, the machine learning methods used most commonly, and how these methods compare to one another in terms of classification performance. Relevant studies were selected from subscription-based digital libraries, free-to-access bibliographies, and self-curated repositories and then screened for relevance with key information extracted and structured against the following facets: natural language processing (NLP) methods, data characteristics, classification methods, and evaluation results. A total of 25 studies published between 2014 and 2018 covering 15 types of classification algorithms were included in this review. Support vector machines (SVMs), Bayesian classifiers, and decision trees were the most commonly employed algorithms with recent emergence of neural network approaches. Domain-specific, application programming interface (API)-driven collection is the most prevalent origin of datasets. The reuse of previously published datasets as a means of benchmarking algorithms against other studies is also prevalent. In conclusion, there are consistent approaches taken when normalizing social media data for text mining and traditional text mining techniques are suited to the task of real-time analysis of social media.
The majority of the data is preserved as text (about 75%), hence It is believed that text mining has a significant commercial potential. Unstructured texts continue to be the most readily available source of knowledge, despite the fact that knowledge may be accessed in many other places. Text classification that assigns documents to predetermined categories. Machine learning approaches can categories texts more accurately. The goal of this work is to introduce text classification, give a description of the text classification technique, a general review of the classifiers, and a comparison of a few of the current classifiers. It is based on performance, time complexity, and other factors. On the basis of speed, accuracy, benefits, and drawbacks of existing classification methods such as Decision Tree, Naive Bayes, Support Vector Machine, and k-Nearest Neighbours are compared.
Trends analysis of bibliographic libraries in the context of text mining has gained popularity during the recent era. Many fields have been explored to find the trends of research during different periods. The purpose of this study is to provide the essential key phrases in the field of Artificial Intelligence over a selected time spans to show the evolution of researchers' interest in this specific area of computer science over a time period. More precisely, for analyzing the trends of the esteemed field, Artificial Intelligence, we scrapped the metadata of research articles published by IEEE between 2010 and 2019 through IEEE Xplore API and created a corpus. The preprocessing is performed, and after that, a phrase mining technique is applied to this data to extract quality key phrases for research trend analysis of different time spans. The results are visualized for readers' better understanding. Also, the similarity between our extracted phrases and author given keywords was calculated by us.
Crowdfunding has become one of the important channels of getting fund for many start-ups. But, the low success rate has been a critical issue. Therefore, how to increase the success rate of fundraising projects is one of the main concerns of all fundraising activities. This work aims to study the effect of sentiment of reviews for the success of crowdfunding projects. We will use data mining and text mining to analyze the collected data. Least absolute shrinkage and selection operator (LASSO) and back-propagation networks (BPN) based feature selection will be employed to find the important factors for the success of crowdfunding projects. Next, support vector machines (SVM) will be employed to evaluate the performance of selected factors set. Experiment results can help fundraisers to increase the success rate of crowdfunding projects.
In the past three to four years, COVID-19, a highly contagious disease caused by a novel coronavirus, has resulted in a significant number of fatalities. Numerous interventions have been carried out in an effort to preserve human life. Despite the current decline in the COVID-19 outbreak, the virus responsible for the disease has undergone a significant mutation, resulting in modifications to treatment protocols. As viruses endure mutations, disease severity tends to increase. Therefore, prior expertise treating patients is beneficial for diagnosing the disease and determining an effective treatment plan. This is the objective of this study, which proposes a text mining approach for identifying relevant information regarding COVID-19 treatment modalities in clinical trials. Constraint-based k-means clustering is the principal algorithm utilized in the proposed method. The datasets utilized in this study are PubMed abstracts related to COVID-19 in clinical trials. Three medical professionals will provide a ground truth for the dataset used in the study. Upon conducting an evaluation of the outcomes using the recall score, it is possible to conclude that the proposed methodology produced results that are deemed satisfactory.
The Vector Space Model (VSM) is a common document representation model that is widely used in data mining and information retrieval (IR) systems. However, this technique poses some challenges such as high dimensional space and semantic loss representation. Therefore, the latent semantic indexing (LSI) is proposed to reduce the feature dimensions and to generate semantic rich features that represent conceptual term-document associations. In particular, LSI has been successfully implemented in search engines and text classification tasks. In this paper, we propose a novel approach to enhance the quality of the retrieved documents in search engines for Arabic language. That is, we propose to use a new extension of the LSI technique instead of just using the standard LSI technique. The LSI method proposed is based on employing the word co-occurrences to form a term-by-document matrix. The proposed method is to be based on the documents evaluating cosine similarity measures for term-by-document matrix. We will empirically evaluate the performance using an Arabic data collection that contains no less than 500 documents with no less than 30,000 unique words. A testing set contains keywords from a specific domain will be used to evaluate the quality of the top 20-30 retrieved documents using different singular values (i.e. different number of dimensions). The results will be judged on the performance of the proposed method as it is compared to the standard LSI.
It is a big challenge to develop effective methods that can discover high quality and useful features in text documents. Most existing information retrieval and text mining methods focuses on term-based approach that often suffers from the problems of term variation and noise. This paper illustrates an innovative approach that discovers relevant knowledge to precisely describe text features for retrieving web information. In particular, it extracts precise text patterns by considering both relevant and irrelevant documents. Then, the discovered patterns are used to find accurate relevant features in a training set. The proposed approach has been evaluated through the implementation of a novel information filtering model and a comparative evaluation is conducted by invoking state-of-the-art models. The experimental results obtained based on the Reuters Corpus Volume 1 and TREC topics show that the proposed approach significantly outperforms the best baseline method.
The paper is devoted to the issues of automated categorization of textual information which can be applied in the systems intended to block inappropriate content. The approach used for feature selection and construction is proposed. The text mining methods used for research (Decision Tree classifiers) are analyzed. Besides that, the techniques of Web sites analysis that provide information in different languages are suggested. The aspects of collection and analysis of text features required for classification in certain categories are investigated. Results of experiments on analysis of text correspondence to different categories are given. The classification quality is evaluated. The text classification component, developed as a result of this paper, is intended for realization in F-Secure systems aiming to block inappropriate web content.
Introduction to Data, Text, and Web Mining for Business Analytics Mini-track.
In this work an automatic classifier of undergraduate final projects based on text mining is presented. The dataset, comprising documents from four professional categories, was represented by means the vector space model with different index metrics. Also, a number of techniques for reduction dimensionality were applied over the word space. In order to construct the classification model the K-nearest neighbor algorithm was applied. Using 10-fold cross-validations we could obtain 82% of predictive accuracy. However, we achieved an accuracy of 95% with a recommendation of up to two categories taking into account the interdisciplinary in documents. This classifier was integrated into an application for automatic assignment of reviewers, which performs this assignation from teachers who belong to the areas recommended.
By mining MOOC learners' online review text, the key factors affecting MOOC learners' loyalty are extracted to help course organizers improve the teaching content, optimize the design, implementation, and management of online courses, and improve the utilization efficiency of the MOOC platform. This paper collects the learners' review texts on MOOC's online excellent course platform in China, preprocesses the review texts, uses the TF-IDF algorithm to calculate the weight of text features, and extracts the topic of texts through the LDA topic model for analysis. From the perspective of learners, this paper points out that learning outcome, media technology, learning effect, course practicability, course interest, course learnability, teaching method, teaching content, learning experience, discussion and communication, and teachers' level are important factors affecting online learners' loyalty.
Much research in machine understanding aims to construct an intelligence of machine. Certain concepts are necessary to allow the machine understand knowledges of human. However, there is a difficulty to allow the machine makes some decision on imprecise denotations in many domains such as a legal interpretation. Once, the machine will deal with vague information while pretending to be human-like, a fuzziness in knowledges must be detected before being constructed. This paper presents a methodology to classify a fuzziness in passages from four Thai codes of law, the four pillars of Thai law namely 1) The Criminal Code 2) The Criminal Procedure Code 3) The Civil and Commercial Code and 4) The Civil Procedure Code. We proposed three sources of a fuzziness in Thai law, i.e. 1) judge's opinion 2) proof of evidences and 3) relative section or laws. The experiments were designed to measure and compare the accuracy performance among five well-known text classification methods namely 1) Logistic Regression 2) Decision Tree 3) Random Forest 4) Support Vector Machine, and 5) Convolutional Neural Network. The experimental results show that a convolutional neural network significantly outperforms them with 96.49% accuracy.
An increasing number of patients and family members interact online and exchange their experiences with diseases and therapies. The huge amount of online health data represents a rich source of knowledge pharmaceutical companies. The analysis of this data enables the identification of strengths and weaknesses of their drugs. An approach is presented which allows the extraction and analysis of patient experiences with drugs expressed in online reviews by combining methods coming from text mining and data mining. The approach is exemplarily applied to a data set comprising patients' experiences with smoking deterrents.
Emotions are at the peculiar element for thoughtfulness and perception ourselves and others, and the computerized annotation and recognition of emotion could enhance our knowledge and understanding with technologies. Nowadays, mobile messaging application as well as other social applications has become a dominant part of an individual's social interaction and relationships. The aura around these applications where people share their expression, feelings and perception are emotionally contented. The proposed framework for differentiating emotional interactions messaging application, and then to distinguish relationships as friends, family, and professional colleagues. The aim is to mine the emotional content of textual expression in the mobile messaging system. The framework also includes the generation of special dictionaries for endless short form languages, for emotags, interjections and for frequently used foreign language words.This Mobile message emotion extraction model consist of two modules, one for relationship classifier and Stress analyzer and reliever, which includes different levels for mining the emotional data, i.e. text gathering, database tables, text processing and emotional text mining then through an unsupervised technique which uses the kmeans clustering algorithm to determine individuality of texts and predict relationships.
The advent of social media has allowed channeling of the voice of sports fans that have essentially lead to gathering and storing fan-generated, large-scale opinions about sports match and team performance. Although research utilizing social media data for the purposes of supporting consumer market research has been increasing throughout the recent decade, there is a lack of studies using social media mining approach to improve team performance. In this paper, an opportunity mining approach is proposed to identify opportunities to improve team performance based on text mining and cluster analysis. A case study of the 2018 Fédération Internationale de Football Association (FIFA) World Cup final qualification of Korea, Korea versus Uzbekistan, was conducted to explain how the proposed method works. Fan comment data collected in the study revealed 16 different opportunities that would satisfy fans with regard to the team performance, and of those, two main extreme opportunities were identified.
As an essential way to ensure success of construction projects, on-site inspection involves intensive paperwork, while generating large amounts of textual data. Lack of understanding of information hidden in text-based inspection records always leads to overlooking of important issues and deferred decisions. Therefore, a novel text mining approach based on keyword extraction and topic modeling is introduced to identify key concerns and their dynamics of on-site issues for better decision-making process. Then, the proposed approach was demonstrated in a real world project and tested with 7250 issue records. Results showed that the proposed method could successfully extract key concerns hidden in texts and identify their changes with time, thereby enabling a more efficient on-site inspection and data-centric decision-making process. This research contributes: (1) to the body of knowledge a new framework for discovering key concerns and their changes with time in texts, and (2) to the state of practice by providing insights on hot topics and their changes with time to reduce on-site issues and make decisions efficiently.
Nowadays, most of the data in an organization is unstructured in nature. This unstructured data consists of huge amount of information that businesses can use it to understand about customers, products and market. Any organization that provides business to customers is required to deliver faster and better quality service. The analysis of this data is necessary and can help highlight issues that customers are facing in regard to products that employees can respond to and correct, improve customer satisfaction. We analyze the unstructured data using natural language processing techniques in order to obtain data in structured form. This work makes use of unstructured customer error logs that are related to server failure. Manual analysis of these error logs is tedious and time consuming. To automate the analysis and to extract relevant information, natural language processing and rule engine consisting of set of rules are being used. The aim of this work is to analyze the logs by the above mentioned techniques which help us to predict the causes of the server failure like the type of case it belongs, commodity, and symptoms related to the failure. In this way this work can help us to handle the customer issues efficiently and help determine the cause in an easier and faster way.
Text Mining is a challenging task due to the lack of a naturally structured representation and the high dimensionality induced by the feature extraction techniques commonly used. Different feature extractions can lead to multiple views that can capture different aspects of the text documents being analyzed. The combination of these features can lead to a better accuracy in classification tasks but, also, an undesirable increase in the number of features. In this work, we investigate the use of a feature extraction technique called DCDistance used as a multiple feature extraction for text documents combined with a Genetic Algorithm based feature selection, hereby called MVDCD. The results show that the main advantage of MVDCD is that the dimensionality is reduced by more than 90% while significantly increasing the classification accuracy when compared to vanilla DCDistance and other feature selections techniques. A side effect of the use of DCDistance and MVDCD is the possibility of model interpretability, as the extracted features are explicit.
User generated content (UGC) provides abundant tourist information regarding destinations. The textual digital traces bring great opportunity along with great challenges. Text mining approaches including sentiment analysis, multiclass text classification, and network analysis are suitable for extracting the buried pattern under piles of unstructured data. We processed 18.721 reviews from worldwide tourists about Bali’s 15 topmost tourist attractions. This study uncovers the tourist perception through textual data using sentiment analysis to extract the positive and negative perceptions, and multiclass classification to extract the tourist cognitive concern for each destination. We discover the tourist visiting patterns deeper by combining perception tone and cognitive concern results using network analysis to map out the destinations’ popularity, interconnectivity, and major cognitive perception. Most of the tourists disclose positive expressions and give their concerns about Bali’s natural attractions. They feel best for the social setting and environment aspect, and worst for the accessibility. Sacred Monkey Forest Sanctuary is the most favorite destination and a potential point of a visit to other destinations. This research provides insight into the global perception of Bali’s topmost destinations for government and other tourism stakeholders to support the development and improvement of Bali’s tourism.
The aim of this article is to determine whether a compartmentalized curriculum at undergraduate lead to similar silos appearing in postgraduate research outputs. For this, data in the form of subject contents of undergraduate studies were obtained from the prospectus of various academic departments in the faculty. Additionally, data in the form of abstracts of research articles published by the postgraduate researchers in these departments were obtained. A total of 118 articles published between January 2016 and May 2020 was extracted from Scopus database. K-means algorithm was used on the corpus consisting of abstract dataset to obtain the clusters. Topic modelling using Latent Dirichlet Allocation (LDA) was then applied to obtain the main topics of the clusters. It was observed that three clusters were adequate in explaining a high percentage of variance in the data and there exists a substantial overlap in the main topics of the three clusters.
A huge number of text documents are divided into a limited number of groups using the unsupervised learning technique known as text clustering. While the clusters contain different text documents, each cluster contains similar documents. Swarm intelligence (SI) optimization methods have been successfully used to resolve a variety of optimization issues, including difficulties with text document grouping. However, the traditional SI algorithms have many drawbacks including the local optima, low convergence rate, and low accuracy. Hence, the present research work proposes a novel text clustering approach based on bacterial colony optimization (BCO) for separating text documents based on similarity. Three separate text document datasets are used for the experiments, and performance is assessed using three different performance measures. The novel BCO approach produces high accuracy and a quick convergence rate, according to the analysis of the results. The suggested BCO text clustering strategy compares several text clustering methods to analyze strength and robustness.
In recent years, microblogging as an important social way, and gradually integrated into life, users can express personal feelings anytime and anywhere, sharing information, etc on the platform. Weibo brings convenience of message for people at the same time, also brings a lot of criminals using weibo for fraud. The fraud groups use weibo to set the language trap, in order to cheat money of others, taking the interests of others. This study tried to use the method of social network analysis and data mining technology, analyze the fraud group, thus define the organizational structure, the characteristics of the group, in order to develop potential fraud group in weibo, help users identify fraud, avoid being deceived.
Requirements elicitation is the activity of identifying facts that compose the system requirements. One of the steps of this activity is the identification of information sources, which is a time-consuming task. Text documents are typically an important and abundant information source. However, their analysis to gather useful information is also time consuming and hard to automate. Because of its characteristics, the identification of information sources and analysis of text documents are critical in time-constrained projects, which are typically addressed through agile approaches. This paper presents a strategy for time-constrained elicitation, which is based on mining GitHub content. The strategy aims the identification of information sources (similar projects) and the automatic analysis of textual documents (projects content) through text mining techniques. Furthermore, it maintains the traceability between the data mined and its sources, boosting the reuse of existing information. A tool is being created to support the strategy.
Because of its real-time nature Twitter is a powerful source of information for detecting real time event happening in the surrounding. People make tweets on whatever events they are seen in their day to day life. In this paper, we take advantage of this for detecting road traffic by using tweets related to traffic and accidents. The goal of this system is to assign an appropriate class label by using classification techniques. For classification of tweets, we employed logistic regression and SVM classifier. We also used some text mining, machine learning, statistics and natural language processing techniques to extract meaningful information from raw tweets. As there are 6,000 tweets are twitted per second on twitter. So Apache spark is used to handle such large amount of data.
We study the literature in major journals and conferences on the usage of shallow learning and deep learning methods for text classification. Shallow learning techniques such as Naive Bayes, Support Vector Machine, Random Forests were initially widely used to solve problems in text classification. however, these techniques generally require the presence of a precise feature extraction model, which is often very complex to produce precise accuracy. For this reason, researchers continue to try to find other learning techniques that are more efficient and provide a significant increase in accuracy. So currently deep learning methods such as Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) are more widely used to solve text classification cases. From 2016 up to the present, this literature study aimed to recognize and assess research methods and datasets utilized in text classification studies. Seventy-three text classification research articles posted from January 2016 until July 2021 were retained and chosen to be explored further based on the established inclusion and exclusion criteria. This literature review was conducted in a methodical manner. A systematic literature review is defined as a method for recognizing, evaluating, and interpreting all available study materials for the purpose of answer certain research questions. The following diagram depicts the overall distribution of text classification methods. Furthermore, public datasets were used in 85 percent of the research projects, whereas private datasets were used in 15 percent of the research studies. Twenty different strategies have been used. Eight of the most commonly used approaches in text classification were identified from the twenty methods. Researchers recommended integrating various machine learning methods, employing an increased algorithm, appending feature selection, and applying parameter optimization for some classifiers to improve the accuracy of machine learning classi... (Show More)
One way to counter the increased competitive pressure in globalised markets is to increase a company's own innovation capacity. In most cases, innovations are a particular result of the environmental influences on the product. Text mining aids the systematic analysis of the various external influence dimensions with their large amount of data. The developed method developed in the context of this research identifies innovation potentials in the product environment and showcases their influence on the existing product. First, text data is retrieved from external influence dimensions. Through specific preprocessing, this data is transformed into a document term matrix, which is the basis for subsequent topic modelling. Finally, a semantic network shows the interconnection of both, the existing product and the analysis results.
The main purpose of this article is to model the topic of OVO Fintech user comments using LDA. This research is a word processing research conducted using text mining. The statistical population is the comments of OVO e-wallet Fintech users in 2021 who take data randomly. LDA (Latent Dirichlet Allocation) and Python programming language were applied to analyze data and implement text mining algorithms from topic modeling. Findings are the most important keywords in Fintech user comments. Also, there are 6 important topics identified in the comments of Fintech users by applying a topic modeling algorithm.Text mining and Latent Dirichlet Allocation were applied to analyze the e-wallet. OVO is a hot topic for Fintech users. Finally, in addition to a retrospective approach to data collection and analysis, the results can be utilized with a prospective approach to strategic planning and policy making.
Nowadays, The world is experiencing a huge growth in the volume of exchanged texts, which makes some of it untapped. Text Mining is the set of techniques that analyze these large masses of information, extract relations that can be unknown beforehand and provide solutions that help decision making. In this sense, stemming is a common requirement of these techniques. It includes reducing different grammatical forms of a word and bringing them to a common base form. In what follows, we will discuss these treatment methods for arabic text, show their limits and provide new algorithm to improve them.
The purpose of this paper is to present comparison dimension methodology and the results of a comparison study of three data mining tools (WEKA, RapidMiner and IBM SPSS). Today, a vast amount of data mining tools supports different algorithms and business objectives with free or paid. This paper attempt to compare three selected data mining tools and the regression algorithms to provide the review information by considering different dimensions like data management tasks, functionality and model type, process-dependent features and system requirements and vendor information for SMEs to choose effective and efficient data mining tools.
Prescription opioids are powerful pain-reducing medications. Thousands of articles that focus on prescription opioid use (POU) and its associated medical disorders have been published. However, it is time-consuming and labor-intensive to extract and understand the information of all POU-related published articles. In this study, we applied the well-adapted topic modeling method, Latent Dirichlet Allocation (LDA), to perform text mining on POU-related literature. We have collected six large academic abstract datasets by searching PubMed using the Medical Subject Headings (MeSH): prescription opioid, codeine, morphine, hydrocodone, oxycodone, and methadone. We then applied topic modeling to identify topics and analyze topic similarities/differences in these six datasets. Word clouds and histograms were used to depict the distribution of vocabularies over each topic in which the most prevalent words conveyed a topic’s meaning. TreeMap and trend analysis were performed to fingerprint abstracts and explore the prevalent topic dynamics in the POU-related literature. Results showed the ability of topic modeling as a computational tool to segregate a vast quantity of articles into different themes that provide a systematic literature overview. The LDA topics recaptured the search keywords in PubMed and revealed further relevant themes by comparison analysis between different datasets.
Learning analytics is the list of methods allowing the measurement, collection, analysis and use of relevant data for the purpose of understanding the behavior of learners and improving learning materials. Generally, learning analytics methods focus on data analysis about the performance of learners and the interaction with the learning environment. A key characteristic that may impact negatively or positively the learner performance is their emotional state. Considering the learner's emotions during the learning process may enhance the learning experience by improving the motivation and the engagement of the learners. Furthermore, depending on the sociocultural context to which they belong and through which they have evolved, learners transmit, control, and regulate their emotions in different ways. These cultural variations need also to be considered when dealing with emotions. In this paper, we will first explore the idea of emotion and its relations with learning, collaborative learning, and culture. Second, we will present a set of selected existing works which have used learning analytics and especially text mining when mining emotion. Then, we will introduce our approach called EmoMining which is ontology-based learning analytic approach for emotion detection taking into account cultural differences. Finally, we present the developed Moodle plugin evaluate and validate the proposed approach.
Students' mobility and credit validation has been a concern for several years among higher education institutions in Ecuador, this process involves a huge amount of manual work due to the absence of an automatic system to measure the similarity between different course contents. In order to tackle this problem, we propose an approach to semantically compare the syllabi contents through text similarity methods. Such methods have been widely used in different domains, in this work we take the higher education institutions syllabi to the Text mining world and develop a method to compare their semantic contents. We propose an approach that uses pre-processing techniques, Latent Semantic Analysis for dimensionality reduction, text enrichment through the Wikipedia API and Google Engine, Support Vector Machine as classifier, and cosine similarity as similarity metric. Our results show that our method successfully measures similarity among higher education institutions syllabi and can be generalized to most Ecuadorian institutions.
Knowledge has played a significant role in every sphere of human life. To acquire knowledge we have to analyze the unlimited data that is available to us in various formats in the form of databases. We can analyze this data and find hidden information with the support of data mining. Data mining refers to the process or method that extracts interesting knowledge from large amounts of data. Data mining have number of applications and these applications have enhanced the various fields of human life including business, education, social media medical, scientific etc. The field of data mining has seen enormous success from the inception, in terms of wide-ranging application achievements and in terms of scientific advancement and understanding. The key objective of this paper is to provide an overview of evolution of data mining from its beginning to the present stage of development.
Data mining is also known as Knowledge Discovery in Database (KDD). It is also defined as the process which includes extracting the interesting, interpretable and useful information from the raw data. There are different sources that generate raw data in very large amount. This is the main reason the applications of data mining are increasing rapidly. This paper reviews data mining techniques and its applications such as educational data mining (EDM), finance, commerce, life sciences and medical etc. We group existing approaches to determine how the data mining can be used in different fields. Our categorization specifically focuses on the research that has been published over the period 2007–2017. With this categorization, we present an easy and concise view of different models adapted in the data mining.
A global niche top (GNT) company has globally high market share in a specific field. While specializing in the limited field and aiming at a high share, a scale is secured aimed at the world market. This strategy attracts attention for small and medium-sized enterprises whose management resources are restricted. However, since those companies have various target fields, understanding the common success factor only by analyzing each example is difficult. Moreover, it is difficult for a selection-type questionnaire to understand the success factor in various fields. In this study, we attempted to clarify success factors by analyzing the free description data written about GNT companies using text mining. As a result, information related to success factors of various types of industries was acquired.
Text can be thought as the combination of characters. In the environment where the size of unstructured text data is hugely more, to process such data by computers is a challenging task. To extract meaningful and useful patterns from the text, some pre-processing methods and algorithms are required. Feature selection or Reduct generation intends to determine a smallest attributes subset which can represent the same knowledge as the original features(attributes) represented it. Rough set theory (RST) is such a mathematical tool, which can be used with tremendous success. Here, In the paper, we proposed a Rough set based approach for feature selection in the Text data set, which fulfil the aim of Text mining. We have taken different sample Text case documents (like biography text data, sample research articles of various domains, news articles from some sources) as input, these files can be in the form of .txt, .pdf etc. or any other format. We have also presented complexity analysis of our proposed algorithm and experimental results on a sample text data sets.
Term-based approaches can extract many features in text documents, but most include noise. Many popular text-mining techniques have been adapted to reduce noisy information from extracted features but still contains some noises features. However, the noise features are extracted from the same training documents that good features extracted from. Therefore, the main problem is that some training documents contain large a mount of noises data. If we can reduce the noises data in the training documents that would help to reduce noises in extracted features. Moreover, we believe that remove some of training documents (documents that contains noises data more than useful data) can help to improve the effectiveness of the classifier. Using the advantages of clustering method can help to reduce the affect of noises data. The main problem of clustering is defined to be that of finding groups of similar projects in the data. In this paper we introduce the methodology that using clustering algorithm to group training data before use it. Also we tested our theory that not all training documents are useful to train the classifier.
Our study presents a comprehensive analysis of news articles from FlightGlobal website during the first half of 2020. Our analyses reveal useful insights on themes and trends concerning the aviation industry during the COVID-19 period. We applied text mining and NLP techniques to analyse the articles for extracting the aviation themes and article sentiments (positive and negative). Our results show that there is a variation in the sentiment trends for themes aligned with the real-world developments of the pandemic. The article sentiment analysis can offer industry players a quick sense of the nature of developments in the industry. Our article theme analysis adds further value by summarizing the common key topics within the positive and negative corpora, allowing stakeholders in the aviation industry to gain more insights on areas of concerns or aspects that are affected by the pandemic.
Information safety is significant for state security, especially for intelligence service. OSIA (open source intelligence analyzing) system based on cloud computing and domestic platform is designed and implemented in this paper. For the sake of the security and utility of OSIA, all of the middleware and involved OS are compatible with domestic software. OSIA system concentrates on analyzing open source text intelligence and adopts self-designed distributed crawler system so that a closed circle is formed from intelligence acquisition to analysis process and push service. This paper also illustrates some typical applications of anti-terrorist, such as the "organizational member discovery" based on Stanford parser and cluster algorithm, the "member relation exhibition" based on paralleled PageRank algorithm and the like. The results of experiences show that the OSIA system is suitable for large scale textual intelligence analysis.
In order to explore the semantic clues and online shopping consumer behavior rules contain online shopping commentary, this paper takes 55560 after-sales evaluation as the research object to put forward the netizens related hypothesis evaluation of social network evolution, and evaluation of innovation of Internet users social network evolution diagram visualization description and empirical analyses. The study found that, the commodity “quality”, “practical”, “the seller” “attitude” attribute has a high center, and consumers rely on degree bigger to merchandise the four attributes; the highest correlation between utility and quality of goods in the commodity attribute of CO word matrix; the whole network density of consumer evaluation is relatively large, suggesting that it on the online shopping attitude and behavior will have a great impact. Therefore, this study finally reveals the social network online shopping reviews the evolution process and the consumer behavior evolution characteristics.
In success or failure of software project, software requirements plays a critical role. Requirement engineering is a human intensive process so the chances of errors, and failure to elicit and specify the right requirements, are more. The software practitioners have to be cautious about what is actually required by the stakeholders and elicit only those features and functionalities rather than some features that may look good but are impractical, and may lead to project failure or timeline delays, without adding much value to the project. In this paper, we introduced a data mining approach to capture user’s requirement for a generic software project, which in our case, was a campus management system (CMS). Initially, we started with a set of Software Requirement Specification (SRS) documents for similar type of projects done before and we extracted the most frequently appearing requirements which would potentially be the ‘Needs’ of the stakeholders, using text mining technique. After that, the low frequency requirements were extracted which could potentially be the ‘Wants’ of some particular stakeholder. Later on, we applied association rule mining to find out links between multiple requirements that led us to the requirement that frequently appeared together, as such links can be useful in identifying requirement dependencies and specification of further requirements. In order to validate the extracted requirements, elicitation sessions in the form of interviews and questionnaires were held with the potential stakeholders of the project and the results of the comparison between the extracted and elicited requirements are discussed at the end of this paper.
Text data constitutes the bulk of all enterprise data. Text repositories are not only tacit store-houses of knowledge about its people, projects and processes but also contain invaluable information about its customers, competitors, suppliers, partners and all other stakeholders. Mining this data can provide interesting and valuable insights provided it is appropriately integrated with other enterprise data. In this paper we propose a framework for text-driven analysis of multi-structured data.
Online marketing is one of the most effective marketing strategies used in a business due to its cost effectiveness and broader range. However, some online marketing strategies have been changed over the time. Furthermore, new laws concerning data privacy and security were introduced and implemented. In order to predict which marketing strategies would be popular, the examining of research trends of online marketing in a time period could be an answer. This paper presents the key research themes in online marketing using data mining techniques. The methods were applied to 506 academic papers selected from three different famous online publication databases. The acquired papers were published between 2013 - 2017. The results show some declined research trends, along with steadily popular techniques including engagement, hashtags, and click.
As semantic information is often missing in text representation, this paper proposes semantic graph structure to represent text and optimize graph structure by semantic similarity matrix. Then calculate the similarity of semantic graph structure by using the maximum common sub-graph of graph theory. Finally, K-means algorithm will be applied to expand Chinese text clustering to improve text clustering effect. Experimental results show that the proposed algorithm based on semantic graph structure is more conducive for the representation of semantic information. It significantly improves the accuracy of text similarity in the text similarity measure and will be applied to text clustering to further improve the effect of text clustering by adjusting the corresponding parameters.
Manufacturing processes, products or services can be represented digitally by Digital Twins (DT) aiming to monitor, simulate, study, analyze and control the process, to improve the product quality, to reduce the process setting time, to improve supply chain, risk and maintenance management, the production process flexibility and reconfigurability, to shorten the product design and testing stages, to optimize the total process real-time monitoring, performance and management. In this paper an approach for making an overview of available specialized DT software by implementation of text mining tools is proposed and implemented.
Sentiment analysis is the most vital research field in the area of natural language processing and text mining. It is a powerful tool for entrepreneurs that introduces products upon online selling websites where customers can post their reviews. In this paper, we have taken the state-of-the-art research papers which are highly cited and analyse them to understand the different techniques and methods to perform sentiment analysis. As sentiment analysis is related to natural language processing it is having several issues of computation. We have discussed many different techniques and tried to suggest the best way to work with them. We have also discussed the different applications of sentiment analysis.
Bloom's Taxonomy is a unity of three domains, which are divided into lower orders and high orders based on the Bloom Taxonomic Cognitive Domain, the level is used to classify learning objectives and serve as benchmarks for evaluating student achievement. Basically, an evaluation of student achievement can be done by giving questions on exam activities. The questions given are then classified according to the level in the Cognitive Domain. However, because the number of questions is too many and the classification is still manual, it causes the classification results are not accurate and inconsistent. Therefore, the employing of the Naive Bayes Classifier in classifying exam questions based on levels in the Cognitive Domain can be a solution. This study uses real-world dataset collected from mid-terms and final exams questions taken from Department of Information Systems, Telkom University from the academic year 2012/2013 to 2018/2019. In particular, we examined Words, Characters, and N-gram as indexing terms. The results showed that the classification using Naïve Bayes and TF-IDF with N-gram as indexing terms achieved precision of 85% and recall of 80%.
Rail accidents represent an important safety concern for the transportation industry in many countries. In the 11 years from 2001 to 2012, the U.S. had more than 40 000 rail accidents that cost more than 45million.Whilemostoftheaccidentsduringthisperiodhadverylittlecost,about5200haddamagesinexcessof141 500. To better understand the contributors to these extreme accidents, the Federal Railroad Administration has required the railroads involved in accidents to submit reports that contain both fixed field entries and narratives that describe the characteristics of the accident. While a number of studies have looked at the fixed fields, none have done an extensive analysis of the narratives. This paper describes the use of text mining with a combination of techniques to automatically discover accident characteristics that can inform a better understanding of the contributors to the accidents. The study evaluates the efficacy of text mining of accident narratives by assessing predictive performance for the costs of extreme accidents. The results show that predictive accuracy for accident costs significantly improves through the use of features found by text mining and predictive accuracy further improves through the use of modern ensemble methods. Importantly, this study also shows through case examples how the findings from text mining of the narratives can improve understanding of the contributors to rail accidents in ways not possible through only fixed field analysis of the accident reports.
Mining traces of Human Computer Interaction (HCI) in Computing Environment for Human Learning allows identification and detection of the learner's activities and motivation. Motivation is a key factor in all areas of life; it is a concept that has received much attention in psychology learning. The learner's motivation is a dynamic process that evolves constantly over time, depending on his personality and his traits. The higher learner's motivation is, the higher his ability will be. In this paper, we combine web mining types and fuzzification in order to supply the decision process about the learner's motivation. Web mining is a valuable source of information that depict the leaner's behavior; the proposed approach thereby, suggests an extraction model of motivation indicator during the learning session through learner's browsing trace. However, motivation is very vague notion; we consider fuzziness as a concept in which the scale of motivation construction is based.
Many techniques in Relation Extraction (RE) require an understanding of the concept and association of relations between entities. This paper presents advanced approaches that combined many methods to extract simple and complex relations between different entities from the biomedical literature. The literature comparative study includes a wide variety of techniques for RE all of which fall into one of the following approaches such as supervised, semi-supervised, higher order relation and self-supervised approaches. Supervised approach needs very detailed data, while in semi-supervised approach only a small knowledge base to automatically annotates features. This approach does not need an initial set of labeled pages for extraction rules. Self-supervised is an integration of these two approaches. On the other hand, the term higher order relation indicates a relation that connects different biomedical relations together. The F-score performance differs from relation to relation when it is extracted. Finally, an extensive performance evaluations with real datasets for both supervised, semi-supervised, higher order relation and self-supervised approaches are described for entity - RE tasks.
The use of smart wearable devices in health care has proliferated rapidly in the past few years in diagnosis, prevention, prediction and treatment. This has been made possible through the continuous technological advances like artificial intelligence, internet of things that allows continuous 24×7 monitoring of health, activity, mobility, and mental status. In spite of their benefits there are still challenges related to their adoption. If the challenges and barriers in adoption are identified it may help to expand their increased adoption. The current study adopts text mining approach through R to retrieve information from articles published in Pubmed and Google scholar. Based on the occurrence frequency of the words six major themes were identified as notable challenges and barriers. They are related to sensor, devices, data, monitoring and system. The other challenges identified were related to security, lack of regulation, risk of data loss, accuracy and ethics. The results prove that it is possible to extract information through textmining.
Fraudulent financial reporting has become a global phenomenon which is adversely affecting the economic and social growth of an organization and the nation they belong to. The financial statements published periodically by the companies contain quantitative and qualitative information. The quantitative information includes numbers, metrices, ratios etc. and qualitative information comprises of comments/notes given by the auditors, disclosures by the management in textual format. A number of analytical models have already been applied for finding the possible solution for financial statement fraud by using quantitative information. A little or no work has been done for detecting fraud by analyzing the qualitative information present in financial statements. In this paper, we propose a text mining framework for detecting financial statement fraud by identifying and analyzing the linguistic data available in the financial reports.
Text mining applications using natural language processing are often confronted with long and complicated sentences. This is observed particularly in the abstracts of scientific articles where authors summarize, in few sentences, the various facts described throughout the manuscript. Being rich in novel and important information, the abstract has been the primary target of biomedicai text mining applications. In this work, we aim to simplify complex sentences in abstracts of biomedicai text so that they can be readily processed by text mining applications. We focus on syntactic constructs that are frequently encountered in the biomedicai literature, such as coordinations, relative clauses, and appositions, with emphasis on their boundary detection. Our approach yielded good detection performance (average F-measure between 86.5% and 92.7%), and aided in improving biomedicai text mining applications, RLIMS-P and Rank Pref.
Text classification is the important part of information retrieved and text mining, in text classification process, the traditional KNN classification algorithm's calculation volume is huge and KNN classification of precision will fall when between the category have more common, in this basics, the improved KNN method is proposed, first the most likely k0 candidate category are got through Rocchio classification method, and then the part of representative sample are extracted in the k0 category training document. This method solves the above two problems to a certain extent, and has good results in the classification, improving classification performance.
In the present work, we propose a new model of radical descriptors in Arabic Text Mining. This model will be based on the addition of lexical information contained in the morphological pattern of the Arabic word. We developed a statistical model by Hidden Markov Chain to disambiguate the morphological analysis of corpora, and we propose a new method to measure the relationship between descriptors based on a notion of “fuzzy measure of the presence” and we adapt the traditional statistical measures to this context, and we outline the key measures of similarity and distances used in Text Mining.
In recent years, vast amount of biomedical literature is produced and published. Recent developments in biomedical text mining shows potential for supporting scientists in understanding new information from the existing biomedical literature because volume of electronically available biomedical literature are increasing massively. Automated literature mining offers one opportunity to discover different entities from literature. Web Technologies allow these entities to be stores and publish in the form to the further reuse by the researchers. The approach presented here includes text mining methodologies to automatically extract different entities from biomedical text. For this purpose biomedical articles based on Traditional Chinese medicine are extracted from Bio Med Central and Pub Med Central and used as corpus. Using text mining techniques of tokenization, splitting, stemming, lemmatization, parsing, named entity recognition are used for preprocessing of corpus. Candidate terms are identified by applying C-Value algorithm. These candidate terms and existing Seed/Ontological Terms are tagged in corpus. Using lexical and contextual profiles comparison between candidate terms and already existed Seed/Ontological Terms, we have identified new concepts. Identified concepts are evaluated.
Crash data is the foundation of traffic safety analysis. In China, the accident reporting form (ARF) only allows reporting one cause for each crash based on the prespecified crash cause code, which may lead to inaccuracy in recording crash data, especially for state-related crashes. This study investigated the directly contributory factors behind the state-related crashes through the development of natural language processing (NLP) and deep learning models based on 1,625 state-related crash narratives. Based on the directly causal factors described in the crash narratives, the state-related crashes were labeled by speed-related, turning-related and other causes. Then the crash narratives were vectorized for model training and frequent analysis. The text-CNN, LSTM, and GRU, and SVM models were applied to reclass the vectorized crash. The results showed the text-CNN model demonstrated the best performance in text classification, with an AUC value of 0.90 for micro-average curves. The results from this study can engage the usage of crash narratives and help identify the actual causative reason behind some inaccurate crash value designation.
With the rapid development of the information technology, information overload becomes a serious problem during the information acquisition process. To relieve this difficulty, knowledge map is a systematic approach to reveal the underlying relationships between abundant knowledge sources. However, few studies focused on how to optimize the coordinates of knowledge items in the map to help users easily understand complicated relatedness among knowledge topics. To bridge this gap, this paper proposes a novel knowledge map construction approach using text mining and artificial bee colony (ABC) algorithm. First, the textural documents related to a certain domain are represented as a term vector in m-dimensional space with the term frequency-inverse document frequency (TF-IDF) analysis. Second, hierarchical clustering is applied to identify important topics. Third, high-dimensional relationships among knowledge items are transformed into a 2-dimensional space optimized by the ABC algorithm. A set of experiments shows that setting appropriate number of clusters is important for visual perception. In addition, a practical example in topic trend analysis using the proposed approach is demonstrated at the end of this paper.
This work introduces the Cassiopeia model, which allows for knowledge discovery in textual bases, used for the purposes of text mining in distinct and/or antagonistic domains. The most relevant contributions include the use of summarized texts as an entrance in pre-processing stage of clusterization, language independence with the use of stop words and the treatment of high dimensionality, a problem that is inherent to Text Mining. In the knowledge extraction, the texts are clustered and reclustered according to a similarity criterion. With the results obtained, the study hopes to show the impact of including summarization in the process of text clusterization. The experiments conducted in this study indicate that text clusterization using summaries is in fact much more effective than direct clusterization of texts in their entirety, as measured by internal and external measures traditionally employed in the field of text clusterization. Finally, the post-processing stage creates clusters of summarized texts with a high degree of informativity, a quality that is inherent to summarization. The clusters are highly esteemed with the indexed words. This fact is due to the process proposed by the Cassiopeia model, which allows for strong similarity among the clustered texts. In the future, this similarity will allow for the creation of categories based on the word indices of each cluster.
Current research and development investments and innovation developments show that there are more failed innovation attempts than ever and there are numerous occasions where companies need to be highly selective by assessing customers’ needs or preferences. Crowdfunding is a great approach for this purpose, where investments are made if there are funders who are interested in the proposed innovations. As crowdfunding platforms have proved to assist in the introduction of innovations, the details of these projects can be a great inspiration for others. The aim of this study is to integrate crowdfunding projects into innovation and product developments as a source of potential ideas and trends. We use a text mining approach to analyze 8021 crowdfunding projects from the period 2009–2018. In this study, we cluster these projects into nine innovation areas desired by consumers. Through our methodological approach, we examine the linkage between associated text, features of projects, and funding of projects to uncover emerging product features that illustrate the desires of consumers. Our proposed model offers theoretical contributions to the innovation processes, especially the fuzzy front end with an open innovation approach. We provide practical contributions by revealing crowdfunding platforms as a means to gain insights into product elements for the design of new products and services.
E-government complaint service has been frequently used by many governments around the world, but less attention have been given to big data analysis especially for data mining. There are many techniques in data mining that is frequently used by developer for business, healthcare, technology, and also government. However, the existing research has shown that selecting suitable data mining techniques for complaint service can provide useful information to government. This literature review aims to investigate data mining technique that suitable to e-government complaint service according to desired outcome and characteristic of data obtained. There are 5 determination factor in this study to determine the suitable data mining techniques such as the desired outcome, characteristic of the data, and etc. The techniques itself have been found 6 such as classification, regression, clustering, summarization, change and deviation detection, and dependency modelling. The result will benefits to government in selecting data mining techniques for complaint service to analyse a large sum of complaint service data.
With the deepening of the research of artificial intelligence theory, the research of pattern recognition has made further development, and the applicable fields have also been continuously expanded. Pattern recognition can be used in text and speech recognition, remote sensing nuclear medicine diagnosis. This article optimizes and analyzes intelligent manufacturing based on pattern recognition, and uses improved association rule data mining algorithms to improve it from many aspects, Association rules are not limited by the number of dependent variables and can discover associations between data in large databases. Data analysis is carried out based on factors in different production stages of intelligent manufacturing to demonstrate the feasibility of the improved algorithm of association rule data mining. The experimental simulation results show that the application of the improved pattern recognition model in this research has made the estimated value of the remanufactured product remanufactured to a maximum of 4.05, and the production effect has also been greatly improved, the floating range is between 6.13 and 7.90, and the overall evaluation can reach 9.06. The experimental results show that the effect of the improved pattern recognition model is greatly improved.
Most information is stored as text, managing a vast amount of documents in digital forms is vital in text mining applications. Text Mining is a field that extracts hidden, useful information from the text document according to user's query. Text Categorization is the most important part of Text Mining. Text Categorization, also known as Text Classification or topic spotting, is defined as a classification of text documents under predefined categories. Although Punjabi text categorization is a promising field, not much work has been done as compared to English text categorization. This paper proposes a method which uses a categorized Punjabi documents corpus, and then the weights of the tested document's words are calculated to determine the document keywords which will be compared with the keywords of the corpus to determine the tested document's best category.
Text mining is usually used to detect document similarities and plagiarism. The field of education is one area that is prone to plagiarism. Plagiarism can kill someone's creativity because this action does not require energy and does not have to think hard. Therefore, the act of plagiarism must be prevented from causing harm to various parties. By using matching strings on documents, it can be used to detect plagiarism. One method that can be used is Rabin-Karp Algorithm, but in several studies that have been done the researchers did not test the k-gram value and database value, in theory, this would affect the performance of the Rabin-Karp Algorithm. Therefore in this study, the selection of k-gram values and prime bases was conducted to determine the effect on the performance of the Rabin-Karp Algorithm. The results showed that the selection of gram values and prime bases affected the processing time in testing the data and the similarity values of the documents being tested. In this study the value of k = 5 on k-gram has the fastest time for the testing process, both testing with multiple data 25 and testing the data for all amounts of data the number is 300.
VOCs (Voice of Customers) coming through call centers involve a variety of contents such as complaints about products and services, requests for product information and questions about maintenance services, among others. By now, it has been widely recognized that VOCs constitute a valuable source of information for enhancing customer services. Since receiving more than 100,000 VOCs per month is not rare, however, it is virtually impossible to read all of them. Accordingly, it is important to mechanize a procedure for distinguishing VOCs of potential business importance from others, so that valuable information can be extracted from the selected VOCs with speed and cost efficiency. For this purpose, this paper develops a segmentation algorithm for identifying VOCs with Sales Potential and those with Negative Attitude based on a neural network approach combined with text mining.
In most software development, a Bug Tracking System is used to improve software quality. Based on bug reports managed by the bug tracking system, triagers who assign a bug to fixers and fixers need to pinpoint buggy files that should be fixed. However if triagers do not know the details of the buggy file, it is difficult to select an appropriate fixer. If fixers can identify the buggy files, they can fix the bug in a short time. In this paper, we propose a method to quickly locate the buggy file in a source code repository using 3 approaches, text mining, code mining, and change history mining to rank files that may be causing bugs. (1) The text mining approach ranks files based on the textual similarity between a bug report and source code. (2) The code mining approach ranks files based on prediction of the fault-prone module using source code product metrics. (3) The change history mining approach ranks files based on prediction of the fault-prone module using change process metrics. Using Eclipse platform project data, our proposed model gains around 20% in TOP1 prediction. This result means that the buggy files are ranked first in 20% of bug reports. Furthermore, bug reports that consist of a short description and many specific words easily identify and locate the buggy file.
The main objective of this article is to compare user comments from 4 Fintech applications in Indonesia using the Latent Dirichlet Allocation (LDA) algorithm. This research is word processing research carried out using text mining. The statistical population is comments from Fintech users Ovo, Dana, LinkAja, and Doku, with data taken from May 2022-May 2023. LDA and the Python programming language are applied to analyze the data and implement text mining algorithms from topic modeling. Each application group has different topic coherence, each application group is analyzed and 4 large categories are found, namely system convenience, system problems, customer service (CS) problems, and service ease. The results of text mining and processing using Coherence and Latent Dirichlet allocation can be used to identify hidden topics and be used as input for system improvements
In recent years, the research on text classification algorithm is still a hot topic in text mining. The KNN is a classic text classification algorithm. The rule of finding the nearest neighbors directly affects the performance and precision of categorization. In this paper, we mainly focus on distance measure and similarity. We propose a new text classification algorithm which combines KNN and Choquet integral. Choquet integral provides a new way to find the k-nearest neighbors. The result of experiment shows that the performance of this method is better than the classical distance measure or similarity measure.
Data compression plays an important role in data mining in assessing the minability of data and a modality of evaluating similarities between complex objects. We focus on compressibility of strings of symbols and on using compression in computing similarity in text corpora; also we propose a novel approach for assessing the quality of text summarization.
With the rapid development of the Internet, network public opinion forms more rapidly and spreads more widely, and has a great impact on the society. In the context of social network media, network public opinion has largely influenced and eventually changed people’s attitude towards the important topics such as government and national policies. For this reason, in this study, the paper analyzes how netizens are positive and negative to their countries in Australia, UK and Canada. The paper designs an efficient crawling system by embedding the two text corpora, and extracts the positive and negative words as well as the positive sentences by text mining. The results indicate the average of positive words in Australia is the largest, and the average of negative words in Canada is the smallest. Australia has the highest ratio of the positive words to negative words among the three countries. The average of positive sentences per website in Australia is 0.357, which is largest. The results reveal Australians are more positive to their country than British and Canadians. Finally, the suggestions to improve netizens positive to their countries are discussed.
The implementation of rural revitalization strategy in China makes “rural tourism” become one of the key industries of rural development. A comprehensive mining of the characteristics of various villages is helpful to promote the overall development of rural economy and realize the modernization of agriculture and rural areas. Based on big data and relevant policy texts, this paper develops a labelling framework, and proposes a deep mining of the rural themes and labels by integrating the deep learning-based text classification and LDA topic mining. In the study, a knowledge graphs of 4,814 entities and 7,369 relationships including villages, scenic spots and featured products is constructed using multi-source heterogeneous data, and we also realizes label fusion and system construction. Through the visualization and analysis of the rural panoramic feature persona, the development modes and the features of villages are described in different aspects. By the construction of rural personas, the study presents the typical features of key villages in rural tourism in China from multiple perspectives, which provides a strong data support for building the recommendation system of online tourism websites and is of great value for promoting the rural tourism.
This paper proposes the method to classify news articles by combining tf-idf and n-gram. This method extracts characteristic words from each news article and classify news based on these words. Numerical experiment results show the relationship among the news articles and visualize the similar articles with networks. Additionally, the authors compare proposal method with only tf-idf in order to verify the effectiveness of this method.
Personality assessments are at present nearly entirely dependent on self-reports, and machine learning methods have been rarely applied to this field. This study used machine learning to predict people’s self-reported proactive personalities. Based on a sample of 901 participants that used Weibo text and short answer text, the authors used five machine learning algorithms for classification: Support Vector Machine (SVM), XGboost, k-nearest neighbor (KNN), naïve Bayes, and logistic regression. Seven different indicators – Accuracy (ACC), F1-score(F1), Sensitivity(SEN), Specificity (SPE), Positive Predictive Value (PPV), Negative Predictive Value (NPV) and Area under Curve (AUC) – combined with hierarchical cross-validation were also used to make the comprehensive evaluation of models. Based on this, we proposed a method to classify people’s proactive personalities based on text mining technology. The results showed that the SVM and naïve Bayes outperformed the other methods on short texts (i.e., short answer texts) and mixed long texts (i.e., short answer & Weibo text). In the context of the texts used, mixed long text (i.e., short answer & Weibo text) improved and stabilized the indices, and this combination was the best choice of text for predicting proactive personality. In addition, the SVM was the most stable classifier in most situations, even on Weibo text that was not suitable for analysis as long text, and it also recorded good results in terms of accuracy, the F1-score, and the AUC.
Patent text mining is an important task that requires domain knowledge. The patent text is sometimes not clear and contains many ambiguous and technical words. Traditional text mining approaches are not satisfactory enough for patent text mining. In this paper, we consider various embedding techniques for patent documents and try to find how to represent the patent text for other downstream tasks such as patent classification, patent recommendation, finding similar patents, knowledge mining, etc. We compare several embedding approaches with the patent classification task. The experimental results demonstrate that using contextual word embeddings can perform better than the conventional static word embedding approaches.
Crime analysis is a technique for discerning and breaking down instances and sequences or patterns in crime. Our approach can anticipate locals which have high possibilities for crime and we are able to imagine crime induces zones. The broaden development of this mechanized technique, crime information savants (police departments, CBI, CID etc.) can help the Law imposition officers to expedites the pave the way for comprehending infringements. Using text mining, the already opaque can be segregated from the useful data in unstructured information. The technique, we have between software engineering and criminal equity for constructing an information mining system that can help unravel the crime in India. This approach will help solve the crime faster and efficiently.
Chiang Mai, as the second largest city of Thailand, attracts a large number of tourists to come. Chinese tourists, which are the main tourist group, are very important for Chiang Mai tourism industry. To help the Chiang Mai Tourism Administration to better improve tourism facilities, a tourist density map which shows the density of tourists in different time periods and different places is required. The objective of this study is to design a tourist density map of Chinese tourists. Data collection of this research is from a Chinese tourism website called Mafengwo, by using text mining as the main tool to capture Chinese tourist's comments under tourist behavior theory. Based on behavior analysis, then a tourist density map is designed to understand the differences in the density distribution of Chinese tourists in various attractions. This article is divided into four parts, firstly is the introduction part with research background and problem identification, and second part gives an over view of theories and methods to conduct the research: then research methodologies is delivered in details in third part; and last part is research result and conclusion.
Stemming is a process of reducing inflected words to their stem or root from a generally written word form. This process is used in many text mining application as a feature selection technique. Moreover, Arabic text summarization has increasingly become an important task in natural language processing area (NLP). Therefore, the aim of this paper is to evaluate the impact of three different Arabic stemmers (i.e. Khoja, Larekey and Alkhalil's stemmer) on the text summarization performance for Arabic language. The evaluation of the proposed system, with the three different stemmers and without stemming, on the dataset used shows that the best performance was achieved by Khoja stemmer in term of recall, precision and F1-measure. The evaluation also shows that the performances of the proposed system are significantly improved by applying the stemming process in the pre-processing stage.
Phrase identification using frequencies is employed in text mining studies. In this work, we propose that all phrases occur in a document are not equal as the semantic relevance between the phrases and text varies. We proposed to identify the relevance of phrases in natural language texts using phrase weights. The pilot study results are worth to investigate further.
Sentiment analysis or opinion mining consist of many different fields like natural language processing, text mining, decision making and linguistics. It is a type of text analysis that classifies the text and makes decision by extracting and analyzing the text. Opinions can be categorized as positive and negative and measures the degree of positive or negative associated with that event (people, organization, social issues). So, it's basically people's opinion study, study of emotions and appraisals in the direction of any social issue, people or entity. Recently most of the researches have been done on the sentiment analysis of products and services. The analysis of events and issues, data is retrieved from social media like twitter etc.
The extraction of meaning or at least inter-dependencies using data and text mining methods is well understood. Numerous approaches have been taken to select relevant information from often very large data sets. The discarding of items that are not relevant to a parameterized retrieval is usually based on an 'include or do not include' decision imbedded in some kind of branch-and-bound algorithm, made to a varying extent sophisticated by the use of machine learning techniques. This paper addresses the discarding process as noise elimination within the context of well-established signal processing methods. It proposes an entropy-based approach using a value-weighted matrix for word relevance matching, where whole text is partitioned according to whether there is a direct relevance of word pairs to the declared meaning being sought, which is expressed as a set of parameters and the noise is considered as errors in the data stream. The resulting non-noisy data is depicted as a text meaning vector, where terms of direct relevance to the initial parameter values are stored.
Political-related news is one of the most popular topics in various media platforms. When news is produced through a process of selection and rephrase by reporters and media firms, reporters' personal political leaning and personal opinions may influence the process, important messages may inevitably loss. In Taiwan, the Parliamentary Library of Legislative Yuan website provides detailed contents about activities happening in the Legislative Yuan, including such contents as transcripts and video recordings of interpellation, conference speech, interim and legislation proposals. Although there is a complete record of information provided online, but the quantity of the legislative documents are far too much for citizens to make sense of. It is imperative that better organized information released to the public would facilitate readers to reduce the cognitive loads in understanding what issues have been discussed by legislators and reported by the media. To minimize the gap between legislative documents and the general public, this study proposes a text mining mechanism to automatically cluster legislative and news documents to identify media frames, and then represents the proportion of each frame corresponding to information sources. The automatic clustering system can determine media frames with the minimum amount of human interference. The results of interviews show that the information system proposed in this study is able to provide political domain experts hard evidences of media framing, and assist the public to discover media framing phenomenon, which are the major contributions of this research.
The professional integration of young university graduates remains the main concern of all universities. As a result, these universities make enormous efforts to prepare laureates adapted to the job market. Therefore, it is considered appropriate to know the profiles of students employable in the labour market. Several models have been implemented to predict profiles acceptable by employers. In this paper, we present a Systematic Literature Review (SLR), from 2005 to 2019, about data mining (DM) techniques used to analyze and predict the professional integration of students and young graduates. We used several scientific databases and repositories: Scopus, WoS, IEEE, Springer, ScienceDirect and ACM to extract and analyze 157 references, with Zotero and NVIVO. After a meta-analyse, we focus on works that use and compare DM techniques. As a result, we found that the most famous DM techniques in our context are: SVM, Naïve Bayes, PCA, Logistic Regression, k-means algorithm, KNN, decision tree, Neural networks, text mining and Item Response Theory. From this SLR and according to the accuracy, we conclude that logistic regression, decision trees, ANN, Random Forest, Text-IRT are the best DM techniques for employability studies.
Now days customer expect the company's business how to listen, respond and suggest services through social network. Based on the customer engagement of social channels is approximately nine times faster than social network. The customer will expect banks to make use of social channels. Approximately two billion people using social media around world, and engage the customer on the social channels in banks. In order to improve the product and service development, marketing, business performance and risk management, banks can take out key insights that will enable on the social media. By analyzing the huge volumes of information available on social media is to deliver quicker and more efficient customer service and customized financial advice. Bank needs the customer, to build the social media strategies, in order to drive reliability, revenue and success is all about the customer experience.
Text Mining refers to an extraction of certain nontrivial, hidden and interesting knowledge from an unstructured textual data. In this paper, efforts are directed to interpret text mining queries in the healthcare domain. To do so, the dataset is taken from the 1mg-company that has emerged during 2015 to provide transparent, authentic and accessible healthcare information for the millions of people while guiding customers with the quality care that too at affordable prices. The different text mining algorithms are compared to generate knowledge extraction of keyterms while linking the personalized search concepts with respect to the healthcare domain, and for the better search recommendations. The algorithms are: basic TF-IDF, SGRank with IDF, TextRank, and modified TF-IDF. The best results are obtained with the modified TF-IDF with the Shingle analyzer where post-release overall is reduced.
Recent advances in the metaverse have revolutionized the way services are experienced, creating a virtual world that seamlessly blends real-life and digital experiences. While research on metaverse services has traditionally focused on technological advancements, recent efforts emphasize the need for a customer-oriented approach to evaluating service quality. However, few studies have explored this customer-oriented approach. To address this gap, this paper identifies and prioritizes nine service features that significantly influence customer satisfaction in metaverse services from a customer-oriented perspective. In particular, this study analyzed 437,527 online customer reviews of Roblox, Bitmoji, and VRchat by employing text mining and machine learning algorithms, such as topic modeling, sentiment analysis, and logistic regression. As a result, the ‘co-experience’ feature emerges as a crucial factor, closely aligned with user objectives when engaging with metaverse services. These findings provide valuable insights for service managers to enhance their offerings effectively, positioning them favorably in the evolving metaverse landscape.
Text documents are often high dimensional and sparse, it is a great challenge to discover the clusters among the unlabelled text data, because there are no obvious clusters by common distance measure. In this paper we present a latent subspace clustering method to find text clusters. In our algorithm, we use latent factors extracted by probability latent semantic analysis (PLSA) to generate latent clustering subspaces, and then use the distance between sample and each latent clustering subspace as similarity for text clustering. On some text document datasets our method shows effective implementation for text clustering.
Hierarchical fusion based data mining is a novel approach to analyzing high density data traffic, where multiple sources of data are combined in a hierarchical manner to develop an effective representation for the data. This is typically done by creating a hierarchical structure in the form of a tree, consisting of multiple methods for feature extraction, clustering and data classification. These hierarchical structures can be used to identify patterns in the data that are not easily discernible otherwise. The hierarchical fusion based data mining has been applied to a variety of domains such as image analysis, text mining, and web mining. It has been shown to be effective in finding patterns in high density data traffic, which otherwise may be difficult to detect due to the sheer volume of data. Additionally, the hierarchical structure allows for better scalability when the data sets grow with more data.
Since October 17, 2019, Lebanon has experienced unprecedented popular protests, demanding the departure of the entire political class, accused of being gangrened by corruption. Country paralyzed, institutions closed for more than two weeks, the eyes are turned to universities that have closed their doors but whose community (teachers and students) actively participate in the national jump. In this work we perform an exploratory analysis of social media usage by universities in Lebanon during the national revolution using social big data technology on Twitter in comparison to the national usage of twitter. Important information was collected, analyzed and visualized using the R language.
Identity theft, fraud, and abuse are problems affecting all market sectors in society. Identity theft is often a "gateway" crime, as criminals use stolen or fraudulent identities to steal money, claim eligibility for services, hack into networks without authorization, and so on. The available data describing identity crimes and their aftermath is often in the form of recorded stories and reports by the news press, fraud examiners, and law enforcement. All of these sources are unstructured. Hence, in order to analyze identity theft data, this research proposes an approach which involves the collection of online news stories and reports on the topic of identity theft. Our approach preprocesses the raw text and extracts semi-structured information automatically, using text mining techniques. This paper presents statistical analysis of behavioral patterns and resources used by thieves and fraudsters to commit identity theft, including the identity attributes commonly linked to identity crimes, resources thieves employ to conduct identity crimes, and temporal patterns of criminal behavior. Analyses of these results increase empirical understanding of identity threat behaviors, offer early warning signs of identity theft, and thwart future identity theft crimes.
It is an important task for data mining and summarizing to extracting features of data. The task of extracting text feature is to extract useful information from texts with identifying and exploring interested patterns. We propose a strategy to extracting feature based on joint conditional entropy and genetic algorithm. Joint conditional entropy is the uncertainty measure of a set of variables given conditions. It is used to get the feature words which represent texts. Genetic algorithm has been applied successfully in many fields. The algorithm is useful for obtaining solutions of optimizing search problems. In this paper, we firstly preprocess texts in order to get the words, then, present the joint conditional entropy which can be applied to define the fitness function of genetic algorithm for discovering proper words which can represent texts. Finally, experimental result shows that this approach is suitable for extracting ideal features of text.
Preprocessing is an essential and primary step in generating taxonomy automatically for text documents because text data is unstructured; and more inconsistent and noisy than structured data. Different combinations of preprocessing techniques have been applied in generating taxonomy to amplify pertinent information for further analysis and processing. This research investigates the impact of various preprocessing techniques on the quality of the generated taxonomy. Various combinations of preprocessing techniques have been applied in taxonomy generation on two text data sets, selected from different domains. The experimental results revealed that selecting a suitable combination of preprocessing techniques can improve the quality of automated taxonomy. However applying all preprocessing techniques in the generation does not guarantee high quality.
The society has important roles for ITE implementation that were regulated in several articles. With the explosion of social media network enabling several governments across the world become closer to their citizen to achieve transparency and engagement. This opportunity has been changing the way of government in order to understand about facts and realities that evolving in the society, particularly involving public policy implementation. Moreover, in the world of big data, the feedback from social media users can be profitably used to extract useful information to support actions that policymakers take throughout the policy cycle. This study work used text mining to explore the twitter data to help the government see what urgent issues, challenges, and benefits arise from the implementation of policies, especially the implementation of ITE which is attracting the attention of “netizens” today. The overall process of the method used are text extracting, text cleaning, and text representation. This study resulted that it is clearly stated that the users talked about the elastic interpretation of UU ITE article which can potentially be abused by the certain person. The original tweets attracted user's attention to re-post that message repeatedly
Software products must show high-quality levels to succeed in a competitive market. Usually, products reliability is assured by testing activities. However, SW testing is sometimes neglected by Companies due to its high costs - particularly when manually executed. In this light, this work investigates intelligent methods for SW testing automation, focusing on the software products review process. We propose a new process for test plan creation based on the inspection of SW documents (in particular, Release Notes) using text mining techniques. The implemented prototype, the SWAT Plan tool (SPt), automatically extracts from Release Notes relevant areas of the SW to be examined by exploratory tests teams. SPt was tested using real-world data from Motorola Mobility, our partner Company. The experiments compared the current manual process with the automated process using SPt, accessing time spent and relevant areas identified in both methods. The obtained results were very encouraging.
This article is a review on news retrieval and mining research areas in recent years based on a qualitative approach. It addresses news retrieval and mining in four main categories of News Retrieval and Extraction, News Content Analysis, News Propagation Analysis, and News Visualization. Each indicated category entails various research areas that have been investigated through several studies. This study depicts the immense extent of news retrieval and mining, the interconnected methods, tools, and theoretical foundations as well as the evaluation methods and the results. The study helps to gain a better understanding of news mining research areas.
In this modern era of communication, people are always connected to the internet. Hence, everyone tends to express their opinions on social media or e-commerce websites about commercial products, movies, sports, social and geopolitical matters and even on government policies. These opinions reflect the corresponding person's view or sentiment about that particular matter, which ultimately leads to the formation of opinion polarity regarding a specific issue. In today's context, Twitter, Facebook or other social platforms often witness a lot of opinion waves regarding some of the today's most hot topics, and one of them surely is the introduction of Goods and Services Tax or GST in India. The rising sentiment analysis and opinion mining regarding this issue is helping researchers to understand the insight of public emotion. It can be also implemented to gain an idea of allover opinion polarity of people in this matter. GST was one of the most rending topic on social network platforms during Jun-July 2017, so in this paper, we present a simple and robust work to gather, analyze and graphically represent people's opinion about India's new taxation system using Naive Bayes algorithm.
Machine Learning has been used to automatically generate a probabilistic food-web from Farm Scale Evaluation (FSE) data. The initial food web proposed by machine learning has been examined by domain experts and comparison with the literature shows that many of the links are corroborated. The FSE data were collected using two different sampling techniques, namely Vortis and pitfall. The corroboration of the initial Vortis food web, generated by machine learning, was performed manually by the domain experts. However, manual corroboration of hypothetical trophic links is difficult and requires significant amounts of time. In this paper we review the method and the main results on machine learning of trophic links. We study common trophic links from Vortis and pitfall data. We also describe a new method and present initial results on automatic corroboration of trophic links using text mining.
Customer emotional satisfaction (known as Kansei) and retention is deemed to be vital for all service businesses including for the high-tech services such as Internet Service Providers (ISPs). Functionality and usability attributes such as network quality and connection speed are regarded as basic requirement. However, there has been little attention and academic research to evaluate the importance of perceived ISPs' service quality on customer emotional need and satisfaction (Kansei). Hence, more Kansei-based offerings are required as more service performance and delivery to obtain customer emotional satisfaction. Moreover, due to customer dynamics, more representative Kansei and robust solution are expected. This study discusses how Kansei-based mining design is proposed and enhanced by Taguchi methodology for robustness of improvement strategy. Both theoretical and practical implications are discussed.
With the rapid development of Mobile Internet, user-generated contents have become important user data sources. It has been well recognized that users' attitude and purchase decisions may be influenced by others. However, it has not been fully revealed that how such influence will be formed, and how users' attitudes and requirements will be changing under the user influence. To tackle the problems, a data-driven approach was proposed to track the user influence on their product/service requirements, and then construct a user influence network to show the specific connections. To illustrate the proposed approach, a case study on RED (a social media APP) was performed. More than 500 fans of electric car were investigated. Based on their online behaviors, such viewing, reposting, or liking, a time-series user social network was constructed. The results show that the main EVs features that attract users remain similar, but the specific focus, such as the fashion color, grade, is changing under user influence. It can help enterprises to deeply understand the essential product features from user perspective, and the dynamic user expectations.
This paper presents a comparison of automatic review summarization with and without stop words. An extractive, unsupervised graph-based ranking model TextRank is employed to highlight the differences between both approaches. Experimental results on 50 sample reviews have shown that the usage of stop words removal can be impactful in determining the result of review summarization, which suggests that depending on the user requirements, it should be considered whether stop words removal needs to be performed or not.
Many text mining tools cannot be applied directly to documents available on web pages. There are tools for fetching and preprocessing of textual data, but combining them in one working tool chain can be time consuming. The preprocessing task is even more labor-intensive if documents are located on multiple remote sources with different storage formats. In this paper we propose the simplification of data preparation process for cases when data come from wide range of web resources. We developed an open-sourced tool, called Kayur, that greatly minimizes time and effort required for routine data preprocessing steps, allowing to quickly proceed to the main task of data analysis. The datasets generated by the tool are ready to be loaded into a data mining workbench, such as WEKA or Carrot2, to perform classification, feature prediction, and other data mining tasks.
The huge amount of text documents has made the manual organization of text data a tedious task. Automatic text classification helps to easily handle the large number of documents by organising them automatically into predefined classes. The effectiveness and efficiency of automatic text classification largely depends on the way text documents are represented. A text document is usually viewed as a bag of terms (or words) and represented as a vector using the vector space model where terms are assumed unordered and independent and term frequencies (or weights) are used in the representation. Graphs are another text representation scheme that considers the structure of terms in the text document which is important for natural language. Terms weighted on the basis of graph representation increase the performance of text classification. In this paper, we present a novel approach for graph-based supervised term weighting which considers information relevant for the classification task using node centrality in the co-occurrence graphs built from the labelled training documents. Our experimental evaluation of the proposed term weighting scheme on four benchmark datasets shows the scheme has consistently superior performance over the state-of-the-art term weighting methods for text classification.
An understanding of success factor relationships in the context of business-to-business where Inter-organizational Relationship (IORs) between organizations is crucial for effective strategic management to accomplish marketing goals. Several studies regarding those success factors and their influences have been conducted and published as articles. We apply the technique of Named Entity Recognition and find a suitable model for extracting the entities of success factors. The appropriate model needs only 60 research abstracts and the performance as high as 0.9 is achievable. Furthermore, we find that there is no significant improvement affected by applying sentence normalization.
Personalized e-learning systems are mainly structured based on two basic models: domain model and user model. When considering personalization in a collaborative environment with user-generated content via Web 2.0 technologies, the task of generating the models is very challenging. This paper presents an approach to extract information related to the domain model and user model for the purpose of personalization. The extraction is using chat conversations using different social media tools during collaborative learning as sources for the text analysis. Text mining technique has been incorporated to clean the collected data and extract the required information.
This paper presents text and data mining in tandem to detect the phishing email. The study employs Multilayer Perceptron (MLP), Decision Trees (DT), Support Vector Machine (SVM), Group Method of Data Handling (GMDH), Probabilistic Neural Net (PNN), Genetic Programming (GP) and Logistic Regression (LR) for classification. A dataset of 2500 phishing and non phishing emails is analyzed after extracting 23 keywords from the email bodies using text mining from the original dataset. Further, we selected 12 most important features using t-statistic based feature selection. Here, we did not find statistically significant difference in sensitivity as indicated by t-test at 1% level of significance, both with and without feature selection across all techniques except PNN. Since, the GP and DT are not statistically significantly different either with or without feature selection at 1% level of significance, DT should be preferred because it yields `if-then' rules, thereby increasing the comprehensibility of the system.
Now-a-days due to increase in the availability of computing facilities, large amount of data in electronic form is been generated. The data generated is to be analyzed in order to maximize the benefit of intelligent decision making. Text categorization is an important and extensively studied problem in machine learning. The basic phases in the text categorization include preprocessing features like removing stop words from documents and applying TF-IDF is used which results into increase efficiency and deletion of irrelevant data from huge dataset. This paper discusses the implication of Information Retrieval system for text-based data using different clustering approaches. Applying TF-IDF algorithm on dataset gives weight for each word which summarized by Weight matrix.
This study intends to help data mining developers to get better performance when obtaining the most frequent N-grams in Text Mining projects. The process of building new variables is one of the oldest and still challenging problems in Data Mining projects. The most frequent N-grams are commonly used as input variables in Text Mining projects. The N-grams represent the occurrence of N items in sequence in a given text. The items can be letters or words. This paper presents a performance comparison between the two main approaches of data storage, relational and NoSQL databases in the task of obtaining the most frequent N-grams. Validation of the study was executed using a database from a known benchmark from an international competition organized by PAN@CLEF 2013. The one-tailed paired t-test showed that NoSQL approach is statistically superior to the relational approach with a confidence level of 95%.
Sharing short reviews on Social Networking Sites has contributed to rising textual data. For this short review text, needed a text classification model to identify short texts, organize them in structured form to define predefined classes efficiently and accurately. In this paper, Imdb movie review text classification model is designed. The Proposed paper is categorized into three phases: - Pre-processing of textual data, Weighting of text, Development of classifier model. For words weighting process, we considered term frequency-inverse document frequency (Tfidf), N-grams (Bi-grams) and term frequency-inverse document frequency (Tf-idf) with N-grams (Bi-grams). After the phases, evaluation of the classification model is done by comparing different algorithms of classification like Naive Bayes, Support vector machine (SVM), Logistic regression, k-nearest neighbours (k-NN) and Decision tree on statistical parameters like accuracy, f1 score, precision, confusion matrix, recall. Finally, the liberation of each weight processing with the classifier is discussed. Combination approach Tf-idf with Bi-grams perform least in every data split scenario and with every classifier. Combinational approach gives negative results as compared to the other two weighting processes. However, the highest accuracy is Logistic Regression with Ngrams (Bi-grams) and Support Vector Machine (SVM) with term frequency-inverse document frequency (Tf-idf) in an 80:20 data split scenario with every classifier.
Uploading and sharing video has become a populartrend on internet recently. However, it is very difficult to understand which parts of video gain more attentions than other parts. Comment-popping video sharing websites, such as Niconico, is a new kind of emerging video sharing websites. Such websites allows users to input timestamp comments within shared video. This paper proposes two video summarization systems for comment-popping video websites based on the concept of folksonomy. The timestamp comments are analyzed and mined, and then the significant clips are chosen based on the mined information and the frequency of comments or keywords. Hereafter, a summary video is merged from these significant clips.
BioWorld is a computer-based learning environment that was designed to support novices in diagnosing medical diseases. In this study, we examine case summaries written in BioWorld. We explore the use of text classification techniques to mine case summaries written in BioWorld. In particular, we evaluate the accuracy of several text classification algorithms in Diagnosis Correctness and Novice-Expert Overlay Model (i.e., recognizing case summaries written by novice and expert physicians). Experimental results suggest that text classification is a promising approach for mining case summaries.
This paper proposes unsupervised methods for generating a vector representation of unknown words that do not appear in a corpus. Word embeddings such as Word2Vec, FastText, and Glove have been used in a wide variety of text mining applications. Different from the one-hot representation of words, word embeddings can consider the similarity between related words. Additional compositionality is another interesting characteristic that shows promise for semantic calculations. As word embeddings are applied as preprocessing in many text mining applications, the words for which vectors are not obtained from a corpus are not considered in subsequent processes. To solve this problem, this paper proposes generating vectors of unknown words without any additional corpus. The proposed approach obtains word vectors with arithmetic operations on the vectors of known words. The key idea is to use word categories and relationships among them, which include syntactic and semantic relationships. Three kinds of generation methods are proposed and evaluated in combination with the CBoW model of Word2Vec. Two kinds of experiments are conducted: one to evaluate the quality of generated word vectors and another to investigate the impact of generated word vectors on a document classification task. The results show that, in the best case, about 84% of the generated vectors are ranked in the top 10 most similar to actual vectors. It is also shown that the generated vectors affect the performance of the document classifier, but the effect is not negative.
Feature dimensionality has always been one of the key challenges in text mining as it increases complexity when mining documents with high dimensionality. High dimensionality introduces sparseness, noise, and boosts the computational and space complexities. Dimensionality reduction is usually addressed by implementing either feature reduction or feature selection techniques. In this work, the problem of dimensionality reduction is addressed by achieving feature reduction through the use of a novel membership function. For feature selection, Singular value decomposition and Information gain approaches are adopted through retaining top-k features. The approach of feature reduction is compared to feature selection techniques and results prove the dimensionality reduction achieved through proposed approach is better.
The extraction of the main content of a web page is a major issue in text mining. It provides less noisy input content prior to fine grained natural language processing methods. We present an unsupervised learning method to extract the main textual content of a web page. It relies on three stages: a clustering step of text blocks within a single web page, a phase of selection of the clusters associated with the main content, and a classification phase carried out on the data labeled by the two previous steps. The overall method allows to extract the main content at scale since it is fully unsupervised and the complexity at prediction time is low. Experiments are conducted to validate the generalization of the classifier and the quality of the obtained results.
The problem in all online learning is that all assessment such as final project is uploaded in it and lecture must evaluate all final project in a specific course that has different topics and subjects so it makes difficult for the lecture. This research built an application that makes a classification of final project documents from the student based on the same subjects and topics. The application takes data from database online learning in a specific course that the database of the final project has a different scope and broad topic. Classification is carried out based on the similarity of topics from the final project document for certain subjects. The document is in the form of text, so a text-mining algorithm is needed to determine some of the topics contained in the final project document. Determination of the final project document according to a particular topic requires a similarity algorithm. This research takes the final project file from Google Drive and the online learning database and implements it in a mobile application. The average result of testing is that the accuracy is 72.49%.
Being able to quickly and accurately capture requirements is crucial when using agile methodologies. Requirements, to that end, are often captured in an as-needed and informal manner, with continuous stakeholder interaction. Techniques such as interviews, user stories, rapid feedback, and text mining are commonly used in the industry to gather such informal requirements, which are often congruent with the concept of 'just-in-time' (JIT)-informally represented and continuously refined-requirements. Oral communication is an integral part of most of these techniques, and the loss or misinterpretation of verbally communicated requirements is a daunting problem. To address this issue, we propose a novel framework to assist developers in capturing verbal requirements in an accurate manner. In particular, our framework automatically captures, transcribes, and mines verbal communication, which in turn produces a set of loosely formulated candidate requirements for further elaboration. We also conducted a survey among practitioners concerning requirements in agile methodologies and our framework. The results of this survey provide positive feedback on the prospects of the framework and also indicate the prevalence of JIT requirements in closed source agile projects.
The distributed data mining (DDM) become one of the major research topics in information systems over the few past years, after the growth of the networks connectivity, and the data volume as well. At the same time the cloud computing made its appearance and proving that it's the future of: the modern computerization and the new promising solution for resource distribution. The last two points fits the requirements of this new technology to be adapted for the resolution of the DDM problems. In this paper we will present our new approach for the distributed data mining with using of the technologies of the multi-agent systems and the cloud computing. We propose an agent based system that work under a cloud service (SaaS) architecture as a modular, extensible, easy to develop and use DDM system.
A large number of conventional financial institutions, especially banks are moving to an Islamic financial model that's comply with the Shari'a Law with little change to current conventional practices (reverse eningineer current business processes) to accommodate the market for Islamic finance. This study is an attempt to design and develop the business processes for the Islamic financial institutions' (IFIs) products by investigating and collecting information through Islamic literature, surveys and interviews of experts in Islamic jurisprudence, regulators, academic and Islamic finance and banking practitioners. Then the findings will be assessed and evaluated using a Qur'anic Financial Corpus and use computational and analytical approaches to mine the Qur'an and the Hadith to find hidden knowledge on Islamic financial business processes. The knowledge acquired from this investigation will be translated into an Islamic financial process model to be adapted by Islamic and non-Islamic financial institutions. The outcome of this research will influence the future development, growth and diversification of Islamic financial services worldwide.
In this survey, we collect the related information that demonstrate the importance of data mining in healthcare. As the amount of collected health data is increasing significantly every day, it is believed that a strong analysis tool that is capable of handling and analyzing large health data is essential. Analyzing the health datasets gathered by electronic health record (EHR) systems, insurance claims, health surveys, and other sources, using data mining techniques is very complex and is faced with very specific challenges, including data quality and privacy issues. However, the applications of data mining in healthcare, advantages of data mining techniques over traditional methods, special characteristics of health data, and new health condition mysteries have made data mining very necessary for health data analysis.
In this paper, the changes due to the 4th Industrial Revolution will be more important in institutional and policy innovations than technological innovations. It is a little bit different from existing changes, economic growth and productivity increase because these innovations are becoming the core. When the importance of institutional and policy innovation increases, the role of public institutions will become more important. In such situation that the role of the public institution becomes the core and be more important, the effort is required to provide more effective and better service as well as information to customers by understanding how much the customers are satisfied with the service and information provided by each public institution. This study aims to suggest a new method for analyzing customer satisfaction through social text mining - a method that can be used together with supplementing the concern of customer satisfaction survey of public institutions which is currently being surveyed with questionnaire. By executing a kind of text mining emotional analysis of social text data left on SNS and homepage by people who used the services of each institution, it analyzes whether customers think of each institution is overall positive or negative.
To automatically and accurately identify electrical fault categories by means of data-driven approaches has attracted much attention in recent years. However, the research on fault diagnosis by mining unstructured data such as text data is far from being perfect. One problem lies in that a large number of human-labeled samples is often costly and difficult to obtain in real applications. In this study, we present a novel Label Embedding joint with Weakly-supervised Classification model (LEMWEC) for predicting fault category given the fault-descriptive corpus. Our model leverages a few labeled documents and a good number of pseudo-labeled documents to learn both sentence embeddings and the multi-label classifier. Then, the model will be iteratively refined by the newly generated pseudo documents. Experiments on a real-life dataset demonstrate the LEMWEC can improve the accuracy of electrical fault classification remarkably compared with a comprehensive set of baselines.
Authorship Attribution is a research area concerned with the automatic classification of text documents based on their authors. The principal objective of this investigation area is to discover the author of a given text is. This task becomes very tough as far as old text documents are concerned. In this paper, we attempted to broach the Authorship Attribution problem as it applies to old text documents. For this purpose, several experiments are conducted and their results are commented. In order to validate the performances of our system, we constructed a special dataset that we called ''10AAP '' (10 Ancient Arabic Philosophers), by quoting texts from the works of 10 ancient Arabic philosophers, where the topic of the different texts is the same. Moreover, the genre of the authors is also the same.
This paper presents a study on transforming social media posts into Volunteered Geographic Information (VGI). Social media posts are user-generated data that can be a valuable source of data, but contain unstructured text and need to be processed to be used efficiently. Volunteered Geographic Information refers to user-generated information with some degree of structure, specifically geographic metadata. We describe the process of transforming social media data into valuable geospatial information using text mining and geocoding methods. We analysed about 5000 posts about wildfires from a fan page with about 90,000 members, mostly firefighters or interested volunteers. This data was georeferenced using two systems, ESRI and Nominatim. We also combine social media with other external data sources (interviews with experts) to establish geographic relationships between wildfire phenomena and social media messages. This process demonstrates a smooth conversion of data from the text of published posts on social media, from fire posts to georeferenced data ready for further geospatial analysis. We show that converting unstructured data into VGI can help experts identify areas where emergency situations have occurred without the need for further content analysis. In this paper, we present the information retrieval process where existing geocoding batch methods could assist Smart Enviroment.
The rationale for a software system captures the designers' and developers' intent behind the decisions made during its development. This information has many potential uses but is typically not captured explicitly. This paper describes an initial investigation into the use of text mining and parsing techniques for identifying rationale from existing documents. Initial results indicate that the use of linguistic features results in better precision but significantly lower recall than using text mining.
In this work-in-progress research we exploited and investigated a virtual reality (VR) based, flexibility learning environment (FLE) in which adolescents with autism use, customize, and design an assortment of simulation games that represent and exemplify the application of forces and Newton's laws of motion. The simulation/game modeling and making tasks acted as the primers of practicing and assessing representational flexibility in solving the engineering design problems with computational thinking. The participants' participation behaviors and verbal utterances during the intervention sessions have been archived via screen and webcam recordings. The current study findings indicated that two approaches of speech or text data mining, multi-label classification and similarity index, can act as the in-situ performance assessment methods to evaluate the representational flexibility development for engineering design and computational thinking of a heterogeneous learner group.
Graphs and graph databases are applicable over a wide range of domains, including text mining and web mining. Using graphs to represent relationships between entities provides enriched models for emerging tasks of web search and information retrieval. Natural language processing algorithms use graphs to model structural relationships of texts efficiently, resulting in improved performance. However, the need to increase the accuracy of graph construction and weight allocation remains a fundamental challenge. Existing methods for these tasks provide limited efficiency and lack scalability for large graphs. In this study, we propose a novel graph-based method for text modeling and running a query to evaluate the similarity of text segments. In this method, the graph corresponding to the text is first created by modeling words and named entities by the state-of-the-art pre-trained BERT model. Graph nodes are then weighted in two stages. In the first stage, the nodes with more generalization obtain higher weights. The second weighting stage is done by the graph obtained from the query text. In this weighting step, nodes are considered important if they are specifically related to the query text. After determining the important nodes in the graph, the semantic similarity between the query text and the texts in the database is measured. The whole process of this framework uses a natural language processing pipeline in Apache Spark scalable platform. The efficiency of the model was evaluated for both distributed and non-distributed configuration and its scalability on a Spark cluster. Evaluation of the accuracy using the Pearson correlation coefficient shows that the proposed method performs higher performance than its competitors.
We study text analysis algorithms that use global optimization methods to compute local characteristics that are consistent with properties of the entire corpus rather than computed locally based on exogenous parameters. In the iterative implementations that we consider, each step both reads and updates a database of parameter values. Motivated by a need for rapid analysis of large corpora, we have developed methods for efficient access to such databases on parallel computers. These methods combine Bloom filters, in-memory caches, and an HBase cluster to reduce communication costs greatly relative to simpler approaches that either fully distribute or fully replicate the database. We also describe how this method can be incorporated into the MapReduce programming model, and illustrate its use within phrase segmentation programs. Our design can achieve considerable run time, latency and storage space improvements relative to other methods. In one phrase segmentation application, we improve performance by a factor of six relative to an HBase-based implementation.
Learning and getting more information from past accident records to understand the accidents deeply are important to prevent future accidents. Most Chinese railway accidents are recorded in the form of text reports and the information about text reports is often underutilized due to the lack of effective mining and analysis tools. In this study, text mining and natural language process (NLP) techniques were used to analyze railway accident reports. More specifically, the multichannel convolutional neural network (M-CNN) and conditional random field (CRF) model were designed to extract accident risk factors. The experimental results shows that our system achieves good performance and can effectively extract risk factors from the accident reports. At the same time, the main risk factors leading to accidents are summarized from four aspects. The system can be used to solve problem areas and strengthen the safety management of the railway industry.
Without a precise specification, an IT project might not remain on time and on budget constraints, or it might lead to a different outcome than desired. A number of established standards define how requirements must be written to avoid such issues. This paper describes our ongoing work to derive a comprehensive set of standardized criteria that IT-requirements must meet in accordance with IEEE 1233-1996 and ISO/IEC/IEEE 29148-2011. We also use a text-mining approach to identify IT-requirements that violate these standards. Our preliminary results are promising: In our biased dataset, we can use text features that are easy to compute, to filter out requirements that do not comply with the standards. Our beneficiaries are auditors, developers, Scrum teams, customers and other stakeholders whose projects are highly dependent on extensive IT-requirements specification.
Sentiment analysis on the YouTube video comments is a process of understanding, extracting, and processing textual data automatically to obtain sentiment information contained in one sentence of YouTube video comment. Text mining approach becomes the best alternative to interpret the meaning of each comment. The classification of positive and negative content becomes very important for the YouTube user to assess how meaningful the content that has been published is based on user opinion. Naïve Bayes and Support Vector Machine is extensively used as a basic line in tasks related to texts but the performance varies significantly in all variants, features, and numbers of data collection. Naïve Bayes is very good in classifying texts with the small number of data and document snippets while Support Vector is very good in classifying texts with relatively many numbers of data or full-length document. The combination of Naïve Bayes and Support Vector Machine produces better accuracy level and stronger performance with the use of a 7:3 scale of data that is 70% training data and 30% testing data. By producing the highest performance test values, namely precision of 91%, recall of 83% and flscore of 87%.
Social media has played a critical role in disaster management. Citizens and officials use social media to access emergency information, send warnings, and communicate with the affected community. On the other hand, the massive data generated by users can be analyzed to extract useful information such as the location and needs of the most affected areas,, as well as social effects of the disaster on the community. In this paper, we study data on two social media, Twitter and Telegram, during the widespread flood in Iran in 2019. The flood affected 80 percent of the country, caused around 2.5 billion dollars worth of damage and resulted in more than 220,000 people being forced into emergency shelters. We focus on the Telegram instant messaging service, as the most popular messaging platform in Iran. We collected all highly viewed flood-related posts from Telegram channels and Twitter posts in a course of 16 days. We analyzed these messages by the area they address, the topic they are assigned to and change in sentiment over time. Our analyses suggest that valuable information can be obtained by looking into data. Such information can guide policy makers in fast and effective decision making during and after a disaster.
Abstracts of research papers are meant to provide a brief condensed overview of respective research topics. This includes a glimpse of the new idea that the paper proposes. The aim of the research presented here is to investigate the feasibility of the effect of text position in the idea identification. The abstracts are structured in the form of introduction, body, and conclusion. It is hypothesized that research ideas tend to be phrased in conclusion section of paper abstracts. 25 abstracts of the scientific papers were used to automatically identify the position of ideas within abstract sections. The results support the notion that the conclusion of the abstracts significantly represents the ideas.
Research on Smart Cities tackles the challenges related to the rapid urban population growth combined with resources' scarcity. A key function of any Smart City initiative is to be able to continuously monitor and track a city's environment and resources so as to convert the data into intelligence for streamlining the city's operations. Social media has become one of the most popular means to allow users to communicate and share information, opinions, and sentiments about events and incidents occurring in a city. With the rapid growth and proliferation of social media platforms, there is a vast amount of user-generated content that can be used as source of information about cities. In this work, we propose the use of text mining and classification techniques to extract the intelligence needed from Arabic social media feeds, for effective incident and emergency management in smart cities. In our system, the information collected from social media feeds is processed to generate incident intelligence reports, including information such as: the event type; the event stage, the impact level, the environmental conditions on the incident scene; and the number of people impacted. Such real-time generated reports can be used by rescue teams for fast assessment and effective response to incidents and emergencies occurring in the city. The proposed algorithm was implemented and tested using datasets collected from Arabic Twitter feeds, and the obtained results are very promising.
Extracting knowledge from text data and taking its full advantage has been an important way to reduce its computation and accelerate processing, especially for large amounts of data. Thus, different approaches and methodologies for modeling and representing textual data have been proposed. In this paper, a graph-based approach for automatic indexing of unstructured data from an Arabic corpus has been proposed. First, each document in the collection is represented by a graph. After the generation of document graph, term weighting is computed to estimate the relevance of a term to the document. The graph representation offers the advantage that it allows for a much more expressive document modeling than the standard bag of words approach, and consequently, it improves classification performance. Experimental results show that the graph based indexing method is a promising approach for semantic and contextual indexation, and outperforms statistical based method (TFIDF) by 12% in F-measure.
A significant proportion of web content and its usage is due to the discussion-of and research-into consumer products. Currently however no benchmark dataset exists to evaluate the performance of text mining systems that can accurately identify and disambiguate product entities within a large product catalog. This paper presents an overview of the CPROD1 text mining contest which ran from July 2nd to Sept. 24th 2012 as part of the 21st International Data Mining Conference (ICDM-2012) that addressed this gap.
Majority of the existing text classification algorithms are based on the “bag of words” (BOW) approach, in which the documents are represented as weighted occurrence frequencies of individual terms. However, semantic relations between terms are ignored in this representation. There are several studies which address this problem by integrating background knowledge such as WordNet, ODP or Wikipedia as a semantic source. However, vast majority of these studies are applied to English texts and to the date there are no similar studies on classification of Turkish documents. We empirically analyze the effect of using Turkish Wikipedia (Vikipedi) as a semantic resource in classification of Turkish documents. Our results demonstrate that performance of classification algorithms can be improved by exploiting Vikipedi concepts. Additionally, we show that Vikipedi concepts have surprisingly large coverage in our datasets which mostly consist of Turkish newspaper articles.
Exploring potentially useful information from huge amount of textual data produced by microblogging services has attracted much attention in recent years. An important preprocessing step of microblog text mining is to convert natural language texts into proper numerical representations. Due to the short-length characteristics of microblog texts, using term frequency vectors to represent microblog texts will cause “sparse data” problem. Finding proper representations of microblog texts is a challenging issue. In this paper, we apply deep networks to map the high-dimensional representations of microblog texts to low-dimensional representations. To improve the result of dimensionality reduction, we take advantage of the semantic similarity derived from two types of microblogspecific information, namely the retweet relationship and hashtags. Two types of approaches, including modifying training data and modifying the training objective of deep networks, are proposed to make use of microblog-specific information. Experiment results show that the deep models perform better than traditional dimensionality reduction methods such as latent semantic analysis and latent Dirichlet allocation topic model, and the use of microblog-specific information can help to learn better representations.
The world wide web is expanding, everyday huge amount of data is added to the web. Finding relevant information in the web is becoming a difficult task. Web Mining is the process of analysing and mining the web to find useful information. By web mining we extract information that are implicitly present in the web. Web Mining is classified into Web Content Mining (WCM), Web Structure Mining (WSM), Web Usage Mining (WUM) based on the type of data mined. Web Structure Mining analyses the structure of the web considering it as a graph. WSM can be used to rank pages present in the web, to improve the efficiency of search engines. This paper discusses about Web Mining, its types, and various ranking algorithms used in Web structure Mining.
Data-driven intelligence can play a pivotal role in enhancing the effectiveness and efficiency of police service provision. Despite of police organizations being a rich source of qualitative data (present in less formally structured formats, such as the text logs), little work has been done in automating steps to allow this data to feed into intelligence-led policing tasks, such as demand analysis/prediction. This paper examines the use of police incident logs to better estimate the demand of officers across all incidents, with particular respect to the cases where mental-ill health played a primary part. Persons suffering from mental-ill health are significantly more likely to come into contact with the police, but statistics relating to how much actual police time is spent dealing with this type of incident are highly variable and often subjective. We present a novel deep learning based text mining approach, which allows accurate extraction of mental-ill health related incidents from police incident logs. The data gained from these automated analyses can enable both strategic and operational planning within police forces, allowing policy makers to develop long term strategies to tackle this issue, and to better plan for day-today demand on services. The proposed model has demonstrated the cross-validated classification accuracy of 89.5% on the real dataset.
Dyslexia is a specific learning disorder that can have a negative influence, especially on children’s learning success. Early detection and continuous therapy can help children achieve better success in school. Children with Dyslexia need several evaluations and therapy during the rehabilitation programs from therapists and psychologists. The Assessment was carried out manually and recorded using paper documents until recently. Besides, the children must meet the therapist physically. A computerized method for assessing Dyslexia children by using text processing is a new approach to better record the rehabilitation progress and provide a home evaluation application so that some social distancing can also be done, especially in the era of the Pandemic. In this study, the data was taken from 24 Dyslexia children during their three-month therapy program. The assessment program is done after the children follow the rehabilitation program from the therapist. During those three months, there were two times scheduled for rehabilitation and two times for Assessment. The Assessment uses three criteria, namely Writing Letter Series (WLS), Arranging Sentences (AS), and Copying Sentences (CS), that each Dyslexia child must do. Computer application for testing is developed with a scoring system implemented using text processing technique. Hamming distance and Text similarity system were applied to score the test result and then be compared with the manual scoring method performed by the therapists. In this study, The Hamming distance was done for the criteria of the WLS, reaching an accuracy of S3% compared to manual Assessment. For the criteria of AS, the Text Similarity technique achieves 96% accuracy compared to the manual Assessment, and in CS reaches 88Xs%. The results showed the high potential of applying text processing techniques in assessing Dyslexia children.
This paper intends to create an application that performs the Data Mining with textual information linked to geolocation data. The structure of the information is distributed in heterogeneous and complex scenario that presents a Social Network. The purpose is that on the final extraction of information, the results are worked for Social Recommendation. However, the recommender systems present some failures in the filtering of the results and the way they are suggested to users. Then, the article presents a methodology of social recommendation to Location-Based Social Network with text mining techniques, and expose issues that still need to research more effective and consolidated results.
With the advent of the Internet era, people can express their likes and feelings without leaving home. A large amount of text data has also emerged, such as microblog comments, movie reviews, news reports and other data. Accurate tagging of text is a key step in quickly finding the text data that they are interested in. Since the frequency of words in the text message varies, the more frequency words may not be suitable for the text label, but fewer frequency words are better suited to the content of the text. This work considers how to avoid the interference of common words and extract tags more accurately. A new solution method called Term Frequency-Inverse Document Frequency and K-Nearest Neighbor combined to extract multi-text tags are proposed to solve it. The feasibility and effectiveness of this method are verified by an example - Douban film review.
Rapid growth of the use of internet has generated the enhancement of customer satisfaction as a fundamental issue in the modern era. Customer reviews can provide resourceful information for monitoring as well as enhancing customer satisfaction levels and thus business organizations can improve customer and product services to their utmost level. Sentiment analysis can manipulate these customer reviews by extracting product aspects and identifying the sentiments as positive or negative and additionally, text mining is used to switch unstructured text into structured form for this purpose. The principal goal of this paper is to perform opinion classification. Bag of Words (BOW) and Term Frequency-Inverse Document Frequency (TF-IDF) feature extraction techniques are used along with N-gram and Support Vector Machine (SVM) is regarded for review classification. Customer reviews from different restaurants of Dhaka were collected to accomplish the task. We believe that the result from the experiment can point out the effectiveness of the implemented technique of customer review analysis by saving the time as well as minimizing the difficulties in measurement of customer satisfaction.
Aware on the benefits of social media as the networking platform, the extremist organization is utilized social media to spread the ideology, recruit new member and guided a suicide bomber alike. There are opportunities to analyze the content of document texts in social media including the terrorism detection and intention by extracting the content evident in their post, comment etc. The objective of this research is to analyze content posted in Twitter and to review whether post and conversation on Twitter will be highly related to terrorism intention or another way around. This study deployed Naïve Bayes classification technique which identified Twitter contents in Indonesian national language. The method has been processed text pre-processing, and dataset divided with hold out technique. Result of F-measure value indicates that 76% and 77% of texts are associated with the accuracy level of terrorism based on macro-averaging and micro-averaging indicators. The finding is contributed to the scanty literature on the early warning detection method in Indonesian language and assist the government to target the extremists' organizations.
Social websites are the main part of the 21st century. Also, most of the generation is dependent on the social websites for communication. Social media is the stages where everyone feels free to express their emotions. Content can be in form of image, text, video etc. In addition to this, these uploaded multimedia contents can also contain the abusive words or some abusive images that are not suitable in public interest. So this paper has presented various text classification algorithms.
One important pedagogical approach is to motivate students actively learn and discover new knowledge by using contemporary instructional design methods. This is in sharp contrast to another approach that forces students to take a passive rote-learning strategy. To promote active learning, CityU Hong Kong adopted the so-called discovery-enriched curriculum (DEC) pedagogical approach to enhance student learning at the tertiary education setting. While various curriculum design and instructional methods have been explored in recent years, an effective way to assess students' novel discoveries and creativity remains a great challenge. This paper investigates into a relatively new text analytics computational approach to facilitate instructors of identifying and assessing the concrete evidences of students' achievements under a DEC-based pedagogical approach. In particular, we illustrate a topic modeling-based text mining method which can extract the hidden intents and creative ideas of students embedded in their project work. According to our best knowledge, this is one of the few successful research work of applying a topic modeling method to enhance the assessment of students' achievements under a DEC pedagogical approach. The practical application and implications of our work is that educational practitioners can apply the proposed computational method to facilitate the assessment of students' novel discoveries.
Indonesia will benefit from a demographic boom in 2030 with a higher labor supply than in earlier decades. Then in industrial revolution 4.0 robotics and artificial intelligence will take the place of low-skilled or menial employment that don't require specialized expertise (AI). To aim research is Telematics Work Field Review Text Classification Using the Naïve Bayes Method. The method using Multinomial Naïve Bayes model which is trained to learn from patterns in training data set without being programmed explicitly. Then, based on the Term Frequency - Inverse Document Frequency, consider the weighting of the word used (TF-IDF). The text classification stage is then carried out using the multinominal nave bayes classification method with evaluation using the confusion matrix, following the acquisition of the TF-IDF value. In the study it took data with web crawling techniques on social media sites twitter. The data collected was 936 data consisting of 7,8% negative sentiments, 26,4% positive sentiments, and 65,8% neutrals. The results of accuracy testing using the Confusion Matrix. And from the results of such tests resulted in an accuracy of 66%, precision 73%, and recall 85%.
Knowledge is becoming the core of innovation, knowledge development, knowledge economy, innovation is the main source of inspiration. Knowledge assets and knowledge resources of the competition, have penetrated into natural resources, competition among the means of production, and how the effective management of knowledge has increasingly become a strategic issue to be faced with important practical significance. This paper attempts to build a practical business-oriented text classification based on the knowledge management system starting the empirical research of its related technologies and applications.
Data mining refers to extraction of information from huge chunks of the dataset. It's also called information mining. It is exercised in numerous fields like medicine, environment, education, crime, etc. In this research work crash investigation and analysis of the flights are done. Flight crashes may be caused due to pilot error, mechanical failure, bad weather, sabotages or human error. This research paper investigates international flight crashes since 1908 to 2009 through K-Means clustering data mining technique and cosine similarity. Clustering helps to put objects into the same group. Cosine similarity measure helps in finding similarity among different texts. The research work is done for identifying aboard/ground fatality rate with operators and location as well as to find similarity among the plane crashes.
With the world becoming more data-driven, enormous amounts of data are being added into databases, documents, libraries, and papers. The whole data would be useless if it is not utilized properly to get useful information out of it. And this useful information is never directly available for us as the data is mostly unstructured, confusing, and open-ended. The data can be well utilized if it is properly classified and could be categorized to have separate study on each category of data. In the era of technology, computer science, and mathematics is continuously working to resolve problems like these. With a huge amount of data being available it is not an easy task to analyze the data. A number of studies have been conducted to extract features from the data. Text Mining, a sub-field of machine learning that deals with such unauthorized and unorganized text to get meaningful data that can be analyzed and used. It is intensely being used to analyze the unstructured text, get structured features from the text and to find patterns. So, unsupervised learning is aiding the supervised algorithms to work on such kind of text. One of the growing text mining algorithms is the topic modeling, which distributes the documents into topics and topics into words. This algorithm is working better than other algorithms like tf-idfasit considers the actual scenario where the topic can be related to multiple words, so LDA is working closer to the real-world topics and its distributions.
Being in a fast-changing and highly competitive environment, fashion companies must constantly adapt to the changing tastes and needs of their consumers. In fact, fashion market trends are among the most important factors influencing consumers’ tastes. Therefore, this paper proposes a fashion trends detection system based on an experts’ approach. Instead of manually scouring tons of fashion magazines and weblogs, this system gathers web text data from such sources and applies text mining tools to detect the trendiest fashion topics discussed in a certain period of time. By defining a list of French fashion-related words and categorizing them into different fashion design elements - namely a garment’s type, color, pattern, material, and style - we came up with a French fashion dictionary used, along with Natural Language Processing techniques, to extract fashion-related words and expressions from the collected blogs. By analyzing the frequency of occurrence and co-occurrence of the extracted words, the proposed framework was able to detect and visualize trends, as well as identify trends’ evolution over time as a first step toward trends prediction.
As the amount of scientific literature continues to grow, it becomes increasingly difficult for scientists to stay up to date within their own domains. Text mining has been suggested as a solution to this problem but has hardly established itself as an alternative to traditional literature reviews in management studies due to the requirement for coding knowledge and familiarity with algorithms. To address this issue, the authors of this paper introduce a no-code solution to text mining based on hierarchical clustering and cosine distance for the clustering of scientific abstracts and titles to create a fine-granular thematic clustering of a research field of choice. To demonstrate the approach, the authors applied it to 2,386 social entrepreneurship abstracts and titles, clustering them into 346 thematic clusters and further categorizing them into 17 different groups. These groups reflect different focus points of the literature, such as sustainability or diversity and inclusion. The authors believe that this approach is valuable both for early-stage researchers, as well as for experienced researchers, as it saves resources and is user-friendly, helping them tackle the ever-increasing amount of literature.
Educational data mining (EDM) is an up-coming interdisciplinary research field, in which data mining (DM) techniques applying in educational data. Its objective is to better understand how students gain knowledge and recognize the settings in which they learn to improve educational outcomes. Educational systems can store a huge amount of data that coming from multiple sources in different formats. Each particular educational problem requires different types of the mining techniques because traditional DM techniques cannot be applied directly to these types of data and problems. There are many general data mining tools are available but these are not designed for deal with educational data and an educator cannot use these tools without knowledge of data mining concepts. To overcome from this problem many authors provided educational data mining tools with different data mining techniques for different purposes. This paper surveys the EDM Tools from 2001 to 2016 and presents the consolidated list of tools in one place to help the EDM researchers. Then categorizes the tools based on data mining methods and explains the usage of EDM Tools.
The classification of consumable media by mining relevant text for their identifying features is a subjective process. Previous attempts to perform this type of feature mining have generally been limited in scope due to having limited access to user data. Many of these studies used human domain knowledge to evaluate the accuracy of features extracted using these methods. In this paper, we mine book review text to identify nontrivial features of a set of similar books. We make comparisons between books by looking for books that share characteristics, ultimately performing clustering on the books in our data set. We use the same mining process to identify a corresponding set of characteristics in users. Finally, we evaluate the quality of our methods by examining the correlation between our similarity metric, and user ratings.
The exponential growth of unstructured messages generated by the computer systems and applications in modern computing environment poses a significant challenge in managing and using the information contained in the messages. Although these data contain a wealth of information that is useful for advanced threat detection, the sheer volume, variety, and complexity of data make it difficult to analyze them even by well-trained security analysts. While conventional Security Information and Event Management (SIEM) systems provide some capability to collect, correlate, and detect certain events from structured messages, their rule-based correlation and detection algorithms fall short in utilizing the information within the unstructured messages. Our study explores the possibility of utilizing the techniques for data mining, text classification, natural language processing, and machine learning to detect security threats by extracting relevant information from various unstructured log messages collected from distributed non-homogeneous systems. The extracted features are used to run a number of experiments on the Packet Clearing House SKAION 2006 IARPA Dataset, and their prediction capability is evaluated. In comparison with the base case without feature extraction, an average of 16.73% performance gain and 84% time reduction was achieved using extracted features only, and a 23.48% performance gain was attained using both unstructured free-text messages and extracted features. The results also show a strong potential for further increase in performance by increasing size of training datasets and extracting more features from the unstructured log messages.
It is difficult to compare the effect of all the medicines with the same effect. This is because there is little data in which various medicines carried out the direct comparison mutually, so far. The ethical pharmaceutical has presented various experimental results in order to obtain approval of the Ministry of Health, Labor and Welfare. The information is described in the ethical pharmaceutical package insert. Ethical pharmaceutical package insert is published and anyone can obtain them. It is possible to make a quantitative evaluation of ethical pharmaceutical with ethical pharmaceutical package insert, if we analyze them with text mining method. Therefore, we make an intensive research to estimate the effect of medicine with text mining as an example of analgesics in this paper. As a result, we are succeeding in extracting the difference in the effect of medicines in the form of magnification. However, the fluctuation range of magnification of effect among the extracted medicines was large. Then, this is because the size of this fluctuation range comes from the experimental configuration, we analyzed the effect of magnification of each experimental configuration. As a result, it was suggested that the fluctuation width of an effect decreases. Furthermore, the same result with anti-histamines medicine was also obtained. From this result, it became clear that the experimental configuration has contributed to the effect of the medicine.
In the Paper, the communication contents of China Li Ning on Weibo and bilibili were taken as the main research object. In addition, text mining technology was used to analyze and study the methods for the brand to mine and build relationship value from the perspective of communication. From such two paths of user-user relationship and brand-user relationship, China Li Ning's application of relationship value in brand communication was studied by using keyword extraction, semantic network analysis and emotion analysis. In this study, the user-user relationship was constructed by the creation of communication topics and contexts and the embedment of them into existing user relationship networks; while the brand-user relationship was consolidated through the path of “recycling of interest-creation-interaction-feedback”. New ideas for the brand communication of national trendy brands in the digital era can be provided according to the results of the study.
Machine understanding research aims to build machine intelligences. To make a machine understand, precise concepts are necessary. Numerous domains contain vague meanings when making decisions, such as a diagnosis or a legal interpretation. Once an artificial intelligence pretends to be human while dealing with imprecise data, a fuzziness in knowledges must be detected before constructing. This paper presents the methodology to detect a fuzziness in Thai law texts using a deep learning method. The experiments are designed to compare the performances of four well-known text classification methods, namely Decision Tree, Random Forest, Support Vector Machine, and Convolutional Neural Network. The fuzziness in this study refers to an imprecise meaning in law texts which may be ambiguous when interpreted by a machine. We built a labelled corpus from four Thai Law codes namely 1) The Criminal Code 2) The Criminal Procedure Code 3) The Civil and Commercial Code and 4) The Civil Procedure Code. We proposed three conditions to identify the fuzziness, i.e. 1) a decision depends on a judge's opinion 2) a decision that requires the production of evidence and 3) a decision which refers to other sections. The results of the experiment show that a Convolutional Neural Network significantly outperforms the others with 97.54% accuracy in comparison of all the dataset.
Interests in manufacturing process management and analysis are increasing, but it is difficult to conduct process analysis due to the increase of manufacturing data. Therefore, we suggest a manufacturing data analysis system that collects event logs from so-called big data and analyzes the collected logs with process mining. There are two kinds of big data generated from manufacturing processes, structured data and unstructured data. Usually, manufacturing process analysis is conducted by using only structured data, however the proposed system uses both structured and unstructured data for enhancing the process analysis results. The system automatically discovers a process model and conducts various performance analysis on the manufacturing processes.
Frances is an advanced cloud-based text mining digital platform that leverages information extraction, knowledge graphs, natural language processing (NLP), deep learning, and parallel processing techniques. It has been specifically designed to unlock the full potential of historical digital textual collections, such as those from the National Library of Scotland, offering cloud-based capabilities and extended support for complex NLP analyses and data visualizations. frances enables realtime recurrent operational text mining and provides robust capabilities for temporal analysis, accompanied by automatic visualizations for easy result inspection. In this paper, we present the motivation behind the development of frances, emphasizing its innovative design and novel implementation aspects. We also outline future development directions, and we evaluate the platform through two comprehensive case studies in history and publishing history.
With increasing outsourcing of project activities to contractors in process industries, ensuring contractor safety is becoming one of the top priorities of organizations. This study aims at identifying the major Hazardous Elements (HE) and Initiating Events (IE) of incidents occurring at a steel plant in India with respect to activities and safety incidents involving contractors. Accordingly, a supervised text classification model is used. First a training dataset is manually annotated to identify the HEs and IEs and then HE and IE are annotated for a test document based on cosine similarity values calculated between the test document and the training documents. Accordingly, 11 unique HEs and IEs have been identified in contractor safety dataset. This study also attempts to identify the optimal way to use cosine similarity to classify a test dataset. First, for each test document, top N training documents in decreasing order of cosine similarity values are considered for classification and the HE and IE as displayed by majority of those training documents are taken as the HE and IE of the test document. The percentage of accurately classified test documents is then calculated. In the next step, the study monitors the change in accuracy percentage as N is stepwise increased. The results show that on increasing N, the accuracy increases but the marginal increase starts diminishing after a threshold.
Web Data Mining is used to find valuable commercial information from large Database under Internet environment in e-commerce. Combining the practical condition of application of e-commerce in tourism industry, this paper does some research on the application of Web Data Mining Technology in tourism e-commerce website design, customer relationship management and network marketing and so on, and puts forward some corresponding implementation methods.
Workload of cybersecurity administrators has significantly increased with the proliferation of the internet and the accompanied cyberattacks. In order to help firms to identify most recent and emerging cyberattacks in a timely manner, this research applies machine learning methods to detect cybersecurity trends. As the rich, multifaceted, and updated online cybersecurity news serve as key information sources for cybersecurity administrators, this research utilizes the wealth of online cybersecurity news as the data source and develops a system to automatically collect multiple online cybersecurity news outlets, analyze collected news to detect emergence of cybersecurity events and present trend of cybersecurity news. This research can facilitate cybersecurity administrators in saving their time to read through multiple cybersecurity news websites and organize events from their memories or other records, thus enhance firms' capacity to actively protect against potential cyberattacks.
Malware is a major threat to the computer security. There are so many variants of malware evolving each day which results the loss in economy as well as data and personal information is precious than economy. Malware prevention can be achieved by carefully analyzing the content of the EULA. As EULA analysis deals with analyzing the unstructured text documents, text mining concept plays an important role. This paper analyzes the detection mechanism for malware detection, using text mining over an ample set of existing EULA. The comparison presented in this paper highlights some malware analysis and text mining techniques. Although there are several malware detection systems exist, evolution and revolution is required to fill the gap in malware evolution and detection. In order to achieve the goal or to verify the authenticity of an executable, this paper provides an alternate approach to detect the malware before execution. EULA analysis which acts as gateway can be included in other behavioral or data mining based malware detection system to achieve both external and internal security of the computing system including mobile devices.
Text processing and mining gained a lot of traction recently due to rising interest in integration of Natural Language Processing with data analytics algorithms, in particular Deep Learning Models. In this study, newspaper columnists are classified according to vector models created by their posts. Hence, we may not only be able to determine an unclassified post's author, but also author profiles can be formed by grouping similar styles together. DeepLearning4J Java library and Doc2Vec class are mainly the preferred deep learning solutions for text mining. The vector models of 5, 10, 15, and 20 authors were created from 20k corner posts. Two particular implementations, Distributed Memory (PV-DM) and Distributed Bag of Words (PV-DBOW) models were adapted and their performances are compared. According to the results, it is seen that some authors are clearly distinguished from other authors. Such a model can be used for author profile extraction, plagiarism detection and identifying which author styles are similar.
Textual analysis is traditionally used by literary critics as a central methodology to interpret creative writings, however this method is significantly affected by human-error, which cause the failing to offer one correct interpretation of the text. As an example, the Vietnamese-American writer Viet Thanh Nguyen’s short story collection The Refugees (2017) has received opposing critical receptions: Whereas some critics applaud the stories for their truthful representations of the two countries, others criticize them for their biased depictions. This study aims to demonstrate how text mining can offer a more objective analysis of the representation of the two main countries in the selected stories. We propose a Big Data Analytics lifecycle that consider two empirical methods. The first method used N-grams, while the second method propose sentiment analysis using lexicon dictionary. The study revealed that text mining is useful in discovering the hidden pattern of textual data and resolving the problem of human error that occurs in performing the analytics manually.
Software Engineering is an application of engineering to the design, development, functioning and maintenance of software in a systematic manner. This paper deals with a model for the extraction of documents based on key topic search by automatically parsing the software engineering papers that are published so far. The performance of the model is evaluated based on the key topic search in the software engineering field. Using text mining technique such as Natural Language processing (NLP, the semantics of the text from which the importance of a term in a sentence of document can be captured. The statistical approaches of NLP, Log Likelihood and RV Coefficient are used for extraction of documents based on the key features. The publications are extracted and clustered with similar papers From the experimental analysis given its clear that the proposed technology using Log Likelihood comparison and RV coefficient is efficient in finding out the most important topics rather than by using the existing technologies such as TF-IDF.
This paper describes a service architecture for a financial market monitoring and surveillance system in which different components interact in coordination with internal and external service providers to produce proactive alarms for potential fraud cases. The proposed service system is demonstrated through an exemplar case study of text mining and data mining to analyze the impact of 'stock-touting' spam e-mails and misleading press releases on trading data. It also shows how an independent service provider could have helped by raising the alarm about a potential on-going 'pump and dump' scheme. The proposed service architecture extends the Market Monitoring Framework (MMF) [6], by incorporating automated linguistics-based text mining techniques to extract the key concepts of spam e-mails and press releases, relate them with other available information and highlight any signs that they might be part of 'stock touting campaigns'. The analysis of emails and press releases through text mining components could help to raise proactive alarms that would not have been possible otherwise. Evaluation of the proposed service system is carried out through a case study that relates to a real case from the over-the-counter (OTC) market, and which was prosecuted by the SEC. Through this case, the paper goes on to explain how the proposed approach could be used within the existing fraud analysis process, the extent to which the process could be automated, the relationships with other types of analysis and the role that fraud analysts play.
In recent years, text categorization based on machine learning is a widely used technology in the field of natural language processing and text mining and has gained many advances. Feature selection is one of the key problems in text categorization. The chief obstacles to feature selection are noise and sparseness. In this paper, we propose an approach of Chinese text feature selection based on CV (contribution value), POS (part of speech) filter and synonym merge. We carry out experiments over corpus-TanCorpV1.0 and find that the proposed method performs better than traditional ones.
Wikipedia is considered to be one of the most famous online encyclopedias. We study the issues related to trending articles on Arabic Wikipedia and how it is influenced by certain external stimulants: for example, breaking news, celebrities' tweets, special events from the past, instant messages on any social media application or any other reasons that could affect the Arabic Wikipedia articles in terms of the number of visitors, which we named the popularity level. By using a data- and text- mining techniques, and the software platform Rapidminer, we developed two models that enabled us to predict the popularity level of Arabic articles on Wikipedia, depending on the features of their stimulants.
The online mass media plays a critical role in influencing the public opinion about controversial political events. Bias in press reports and articles to some ideological or political sides is common and opposites the neutrality nature of press and media. Bias can take different aspects and ways. One of the main aspects of press bias is using mislead terms and vocabularies. In summer 2014, Western media, news and press agencies covered Israeli war on Gaza. In general, Palestinian people complain that there is a notable bias in western media with the Israeli story and opinion and vice versa. In this research paper we report a text mining experimental study, that's have conducted on western media analysis to identify patterns in the press orientation and further in the media bias towards side to another. We have followed the text mining techniques and machine learning in an effort to detect the bias in news agencies. We have crawled news articles form seven major outlets in the western media. Then we have made preprocessing to convert them into useful structured form, building sentiment classifiers that be able to predict articles bias. In addition, we have compared three of supervised machine learning algorithms used in sentiment classification associated with different number of grams, where we have found that SVM with bio-gram gave the better outperformed outputs, with performance metrics are 91.76% accuracy, 88.33% recall and f- measure 91.46%.
Identifying drug-drug interactions is an important and challenging problem in computational biology and healthcare research. There are accurate, structured but limited domain knowledge and noisy, unstructured but abundant textual information available for building predictive models. The difficulty lies in mining the true patterns embedded in text data and developing efficient and effective ways to combine heterogenous types of information. We demonstrate a novel approach of leveraging augmented text-mining features to build a logistic regression model with improved prediction performance (in terms of discrimination and calibration). Our model based on synthesized features significantly outperforms the model trained with only structured features (AUC: 96% vs. 91%, Sensitivity: 90% vs. 82% and Specificity: 88% vs. 81%). Along with the quantitative results, we also show learned “latent topics”, an intermediary result of our text mining module, and discuss their implications.
The government work report of the State Council is a kind of comprehensive policy text. This paper uses text mining technology to carry out a comprehensive multi-granularity, multi-level quantitative analysis of the government work reports, which has a great practical and instructive significance for relevant personnels to understand the evolution of domain knowledge in a short time. Firstly, a series of text preprocessing is done by using the Chinese word segmentation tool combined with three kind of dictionary built by authors, i.e., the domain word dictionary, the domain synonym dictionary and the domain stopword dictionary. Then, according to the co-occurrence information of words in the government work reports, we attempt to conduct topic modeling on the corpus consisted of all the government work reports and single government work report respectively, Finally, we find 12 latent topics for the corpus, such as the "Economic reform", "Agriculture", "Government construction", "Defense military" and so on. Based on the 12 topics, we conduct the topic modeling on every single government work report, with which topic evolution analysis is carried out over the whole period of all government work reports.
In the realm of natural language processing (NLP), paraphrase detection is a highly common and significant activity. Because it is involved in a lot of complicated and complex NLP applications like information retrieval, text mining, and plagiarism detection. The proposed model finds the best combination of the three types of similarity techniques that are string similarity, semantic similarity and embedding similarity. Then, inputs these similarity scores that range from 0 to 1, to the machine learning classifiers. This proposed model will be benchmarked on “the Microsoft research paraphrase corpus” dataset (MSRP) and from this approach for paraphrase detection problem, the accuracy acquired is 75.78% and F1-Score of 83.01 %.
With the rapid growth of unstructured data accessible via web, managing these data and finding undiscovered information in huge dataset become a necessary task. Consequently text mining, which can be defined as gleaning important information from natural language text, has emerged. In this study, in order to facilitate information management for aspect based sentiment analysis studies, a Turkish sentiment corpus, which is comprised of user reviews and is annotated semi-automatically, is constructed. In the constructed corpus, the root form of the words, the usage (aspect/multi-aspect/seedsentiment/absent) of these words, Part of Speech (POS) tags and their polarities are defined. Turkish hotel review dataset which contains 1000 reviews and 5364 sentences for this study was crawled from a web source. The system takes reviews, aspect and seedsentiment lists and returns JSON data structures of the annotated corpus. In this paper, both we provide a ready to use dataset for developing aspect based sentiment analysis applications and we make this dataset easy to use for Java applications by creating JSON data.
When multiple terms in the query point to a single concept, the solution is easy to map. But, when many morphologically similar terms refer to separate concepts (showing fuzzy behavior), then arriving at a solution becomes difficult. Before applying any knowledge generation or representation techniques to such polysemic words, word sense disambiguation becomes imperative. Unfortunately, with an exponential increase in data, the process of information extraction becomes difficult. For text data this information is represented in form of context vectors. But, the generation of context vectors is limited by the memory heap and RAM of traditional systems. The aim of this study is to examine and propose a framework for computing context vectors of large dimensions over Big Data, trying to overcome the bottleneck of traditional systems. The proposed framework is based on set of mappers and reducers, implemented on Apache Hadoop. With increase in the size of the input dataset, the dimensions of the related concepts (in form of resultant matrix) increases beyond the capacity of a single system. This bottleneck of handling large dimensions is resolved by clustering. As observed from the study, transition from a single system to a distributed system ensures that the process of information extraction runs smoothly, even with an increase in data.
A critical item of a bug report is the so-called "severity", i.e. the impact the bug has on the successful execution of the software system. Consequently, tool support for the person reporting the bug in the form of a recommender or verification system is desirable. In previous work we made a first step towards such a tool: we demonstrated that text mining can predict the severity of a given bug report with a reasonable accuracy given a training set of sufficient size. In this paper we report on a follow-up study where we compare four well-known text mining algorithms (namely, Naive Bayes, Naive Bayes Multinomial, K-Nearest Neighbor and Support Vector Machines) with respect to accuracy and training set size. We discovered that for the cases under investigation (two open source systems: Eclipse and GNOME) Naive Bayes Multinomial performs superior compared to the other proposed algorithms.
As large amounts of finance-related microblogs, posted by both professional and amateur investors, are mingling in online social networks that could bring impact to real financial markets, it's an essential task to analyze financial texts posted by different users. Besides that, as barely nobody could be an all-round expert in various financial products such as stocks, futures and bonds, topic preferences of different users should also be considered in these mining tasks. That is to say, financial text contents should be analyzed in both user and topic level. In recent advances of deep learning technology, learning the "embedding" representation of contents is a state-of-art method in financial text mining over hand-crafted feature extraction techniques. Along this line, in this paper we propose a hybrid embedding method that both the "embedding" representation of users and the "embedding" representation of financial topics are learned from the posted finance-related contents in online social networks. Following the commonly used methods in embedding-related researches, we apply extensive experiments with real-world datasets to show the effectiveness of our hybrid user and topic embedding (UTE) approach, in both intrinsic and extrinsic ways. It's shown that our approach can intrinsically distinguish social network users with their learned embeddings, so they can be grouped into human-explainable clusters. Moreover, we also propose a case study on sentiment analysis by applying our user and topic hybrid embedding with a deep contextual neural network architecture. The results prove that our approach outperforms other baseline methods in financial-related sentiment analysis, and hopefully for other downstream text mining tasks as well.
Communication among millions of internet users has become very popular in sharing opinions on different aspects of life every day. The stake holders of education institutions are interested in the online profile and visibility of the institutions by exploiting the communication opportunities offered by the Internet. This paper aims to analyze the role of Online Social Networks (OSNs) in education institutions data analysis. The most popular micro-blogging platform twitter is used for data gathering and statistical software R for data analysis. A survey was conducted to collect the tweets for one month and around 110 tweets were examined based on the expectations, requirements and needs of the stake holders. An algorithm is developed to create and analyze the corpus using text mining; and explained the discovered phenomena. The most frequent words and the significance of those words in the events happened over a period of time in education institutional area is identified.
The present world gives much significance to the images rather than text data. Since the image data is more flexible and understandable than text data it is mostly preferred for the data analysis and storage. The major issues caused in the images are getting erased and duplication. For the erased images, there are many solutions to retrieve the deleted images or erased images. Image mining technique plays a vital role in today's retrieval process. Image mining is the process of extracting the images from the databases. It may also recollect the deleted images. It is easy to gain the image data but it cannot easily change or update the data the existing approaches in image mining process are image retrieval, neural networks, object recognition. Image mining process undergoes on the basis of many algorithms such as classification, clustering etc. The main objective is to discuss about the various existing image retrieval process and the challenges faced.
The advent of social media and the rapid development of mobile communication technologies have dramatically changed the way to express the feeling, attitude, mood, passion etc. People often express their reactions, fancies and predilections through social media by means of short texts of epigrammatic nature rather than writing long text. Many micro blogging services like Twitter enable people to share and discuss their thoughts and views in the form of short texts without being constrained by space and time. Millions of tweets are generated each day on multifarious issues. Sentiments or opinions for diverse issues have been observed as an important dimension which characterizes human behaviour. Public frequently articulate their opinions towards various issues. In an effort to gain insights from people's point of views, this paper applies text mining on tweets generated on Twitter sites for two famous Indian political diplomats: Arvind Kejriwal and Narendra Modi. The results reveal value of this competitive study and how these diplomats could deal with their political affairs in a better way and identify areas where they need to take a better step into. This study could really help these diplomats to improve their political strategies.
In this paper the authors investigate the authorship of several short historical texts that are written by ten ancient Arabic travelers: this Arabic dataset, which was collected by the authors in 2011, is called AAAT dataset. Several experiments of authorship attribution are conducted on these Arabic texts, by using different lexical features such as words, word-big rams, word-trig rams, word-tetra grams and rare words. Furthermore, seven different classifiers are employed, namely: Manhattan distance, Cosine distance, Stamatatos distance, Camberra distance, Multi Layer Perceptron (MLP), Sequential Minimal Optimization based Support Vector Machine (SMO-SVM) and Linear Regression. For the evaluation task, several experiments of authorship attribution are conducted on the AAAT dataset by using the different quoted features and classifiers. Results show good attribution performances with an optimal score of 80% of good authorship attribution. Moreover, this investigation has revealed interesting results concerning the Arabic language and more particularly for the short texts.
Text clustering is an unsupervised process forming its basis solely on finding the similarity relationship between documents with the output as a set of clusters [14]. In this research, a commonality measure is defined to find commonality between two text files which is used as a similarity measure. The main idea is to apply any existing frequent item finding algorithm such as apriori or fp-tree to the initial set of text files to reduce the dimension of the input text files. A document feature vector is formed for all the documents. Then a vector is formed for all the static text input files. The algorithm outputs a set of clusters from the initial input of text files considered.
The recent progress in computing has made iteasier to collect and store huge amounts of information in a text. The growing size of text datasets in text mining and the high dimensionality associated with knowledge discovery is a great challenge that makes it difficult to classify documents into various categories and sub- categories. This paper focuses on how text can be mined from social networks and then categorized using n-grams to determine specific trends and patterns. The main aim of Knowledge Discovery is to extract knowledge from data in the context of large databases. The volume of information that is available is increasing every day. This data ranges from that used in business transactions to scientific data, sensor data, pictures, videos, etc. There is, therefore, a need for a system capable of extracting the core of available information and automatically generating reports, opinions, or summaries of data to aid organizations in better decision-making. Knowledge Discovery is a repetitive process where evaluation measures are often enhanced, mining done on data can be refined, there is an integration of new data, and the data is transformed to get accurate and more appropriate results. The data collected from social networks need to be filtered to capture specific text that will be useful to a PR brand following what clients say about their products online. There is a need for a technique that will provide a quick and precise way of fetching specific text from huge amounts of data on social networks to help analyze the feedback. This research analyzes the use of n-grams to fetch specific text from near-real-time customer feedback that is in the form of large data on Twitter to help Public Relations agencies determine the trends and patterns that will help them align their brands with customer preferences.
Text data understanding on social networking systems has become an important source of data for companies to understand their stakeholders better. The shift from pattern mining of structured database to non-structured text data has alerted companies to have a stronger presence in the new social media world. This research uses text miner module in the Statistical Analysis System (SAS) to analyze conversational data compiled from Yammer enterprise microblogging system. These inputs are used to study the topics discussed among employees. It is also used to examine the knowledge sharing activity among employees in the case company. This can be accomplished by analyzing the topics maps produced SAS software system. One is able to analyze the topic of discussion and the frequency of each topic on microblogging system platforms to observe the knowledge sharing and knowledge creation activity among employees. The case study company in this research project is a knowledge centric organization involves in knowledge sharing activity using a server-based Knowledge Management System (KMS). This research chooses employees that are involved in an active project. They will use Yammer instead of the current KMS system. The topic and text analysis diagrams are used to identify the patterns of discussion and the topics exchanged between employees. SAS (Statistical Analysis System) text mining tool is used to carry out the text mining analysis works where a number of visual representation graph were developed to study the communication patterns among employees. The results of this research had shown that text mining is able to surface employees' frequency of communication and topics of conversation through posting activities using Yammer in this research.
During the past few years, the construction of digitalized content is rapidly increasing, raising the demand of information retrieval, data mining and automatic data tagging applications. There are few researches in this field for Arabic data due to the complex nature of Arabic language and the lack of standard corpora. In addition, most work focuses on improving Arabic stemming algorithms, or topic identification and classification methods and experiments. No work has been conducted to include an efficient stemming method within the classification algorithm, which would lead to more efficient outcome. In this paper, we propose a new approach to identify significant keywords for Arabic corpora. That is done by implementing advanced stemming and root extraction algorithm, as well as Term Frequency/Inverse Document Frequency (TFIDF) topic identification method. Our results show that combining advanced stemming, root extraction and TFIDF techniques, lead to extracting a highly significant terms represented by Arabic roots. These roots weights higher TFIDF values than terms extracted without the use of advanced stemming and root extraction methods. Decreasing the size of indexed words and improving the feature selection process.
Given the increasing importance of sustainability in the business world, there has been growing interest in using topic modeling approaches to identify reported topics in corporate sustainability reports (CSR). Due to the inconsistent legal foundation and different sustainability standards, the content of individual reports can vary greatly. In this paper, the corporate social responsibility reports of DAX 40 companies from 2017 to 2021 are therefore analyzed using Latent Dirichlet Allocation (LDA). In particular, we attempt to identify topics that are suggested by the Global Reporting Initiative (GRI) Sustainability Standard for large public companies. In addition, a comparison is made throughout the years. The study shows that specific guidelines of the GRI can only be identified to a certain degree using LDA. Although some topics that partly reflect the content of the GRI can be found by the model, the overall structure of the GRI can’t be replicated. Overall, this study shows that an evaluation of the content of sustainability reports can be successful in terms of the relevance of the reported topics, although the results depend heavily on the respective pre-processing steps.
This short paper discusses the “Integrating Data Mining and Data Management Technologies for Scholarly Inquiry” project. In this “Round Two” Digging Into Data Challenge award, we explored uses and approaches for large-scale data analysis and processing for the Humanities and Social Sciences through the integration of several infrastructure frameworks: Cheshire, iRODS, and Amazon Web Services (EC2 computing and S3 storage). Our “big data” consisted of the entire texts collection of the Internet Archive (approximately 3.6 million volumes) and the entire JSTOR database. We performed surface-level natural language processing on this data to identify noun phrases and further refinements to identify personal, corporate, and geographic names. We then used resources including library and archival authority records to identify variants and merge names. The goal is to create an integrated index of persons, places, and organizations referenced in our collections.
The Brazilian public legal sector is typically marked by the disproportionate ratio between the number of cases and the available number of specialized employees. However, many of these processes have minor importance for the State, which promotes overload for highly specialized and expensive employees with non-important activities that could be performed automatically or redistributed to lower level professionals. The present work proposes a decision support approach for analyzing inheritance processes in a state attorney's office. The objective is to develop a system for collecting, classifying, analyzing and ranking processes using text mining and machine learning in order to automate the document analysis procedure in a semantic manner. The put forward approach offers a novel way for objectively distribute processes according to their monetary importance and the hierarchical level of the involved professionals. The approach also proposes a tunable semantic analysis, which takes into account the user experience for the automatic document ranking, which can be easily adapted for other analytical activities in attorney's offices.
Text categorization is a task for text mining that involves pattern classification and is essential for the effective management of textual information systems (TIS). Each document is automatically assigned one or more categories from a set of predetermined categories as part of TIS. There is increased interest in creating tools to aid people in more effectively finding, filtering, and managing existing digital resources as the amount of information is rising. Those developed tools are involved in the administrative sectors and social benefits. This paper provides a general overview of text categorization models using various supervised machine learning techniques, including Logistic Regression (LR), Naive Bayes (NB), Random Forest (RF), Support Vector Machine (SVM), and AdaBoost, and compares their performance on various metrics, including accuracy and precision. The comparative analysis shows that the most effective algorithm for text classification is NVM with the highest accuracy of 96.86%, furthermore, AdaBoost is the worst in this case study with its minimum accuracy of 74.49%. We have also shown a comparison of other supervised machine-learning algorithms in this paper.
Advancement in digital technology has led to an increase in the text data exponentially. A field called `text mining' turns the massive amount of text data into high quality or actionable knowledge so that it can help in making the optimal decision, reduces the time and human effort to analyze it. We can perform several tasks on text data including part of speech tagging, parsing text, extract the relevant information, classify the text data, clustering, etc. Text representation is a necessary step to do all these tasks and its effect, especially on the end results of text classification or clustering is highly considerable. The aim of this paper is to highlight the prerequisite procedures to represent text data, different text representation methods, the role of dimensionality reduction, different proximity measures and their evaluation methods to assess results of text clustering or classification.
E-commerce with the immersive scene of live streaming gives users a new interaction and consumption experience, extending a new path for traditional marketing. Live barrage, being the primary means of users’ social engagement in live e-commerce, encapsulates their inclination to purchase products, commonly referred to as consumption intention. The mining of live barrage enables the exploration of latent user behavior and consumption intention. The current methods employed for mining consumption intention in e-commerce live barrage encounter challenges including insufficient corpus, excessively brief text, and significant semantic deficiencies. Therefore, this study proposes a paradigm of consumption intention recognition by combining the text features of barrage: by constructing a dataset of Live barrage, cleaning the original dataset, and then using a fused BERT’s feature network model BERT-BiLSTM to identify the consumption intention in live e-commerce Live barrage. The experimental results demonstrate the efficacy of the proposed method for recognizing consumption intention. Finally, this study conducted text mining on live barrage containing consumption intent, including word cloud visualization, topic extraction, and co-occurrence analysis, further investigating consumers’ purchasing needs and intentions.
Along with globalization's positive effects, there are many negative consequences. Cybercrime, cyber-war, and cyberterrorism have entered social media platforms like Twitter. Therefore, identifying opinion patterns in social media is an important research task to understand what types of conversations about terrorism case patterns occur. This study will add to the knowledge of terrorism concerns in Indonesian social media. Then we might be willing to seek guidance way earlier and avoid any conflict. We attempt to analyze Twitter data with the keyword “terrorism” based on the existing problems. We want to prove that using the FP-Growth algorithm can generate rules from terrorism-related tweets. The FP-Growth algorithm generates 75 association rules from Tweet data which was collected on November 20, 2021, with minimum support of 0.05 and minimum confidence of 0.9.
This tutorial will introduce attendees to the HathiTrust Research Center's Extracted Features Dataset, and demo new data fields and functionality introduced in the latest version, 2.0. Generated from the over 17 million volumes in the HathiTrust Digital Library, the EF 2.0 Dataset supports text and data mining methods while still adhering to a public domain, restriction-free data model. This tutorial will introduce the EF 2.0 Dataset, the key concepts behind its creation, and hands-on research use cases for the dataset using IPython notebooks.
The Market-driven requirements engineering (RE) is different from traditional requirements engineering and poses a number of challenges. Due to rapidly changing technology; demands for improvements in market driven requirements management process has increased. This paper is an effort to carry out the analysis of the existing practices and challenges in the market-driven RE. We have studied various case studies of market driven products and analyzed the problems faced in handling continuously emerging requirements. This paper gives an insight into the key issues involved in requirements management of market driven products. These issues are identified after content analysis of existing literature and case studies. To further elaborate the most important issues, we performed Text Mining on literature collected by us and compared the results of test mining with content analysis. The most important issues that we identified are related to RE process and product.
Prime intends of web mining is to mine valuable information and knowledge from web. Social network analysis has become a very well-liked field of research as it is functional for many applications. In this study we will examine the existing soft computing techniques in the area of web mining. We develop efficient methods and algorithms using soft computing approaches. Our Framework will base on Hybrid soft computing Model. Validation is verified only by numerical simulations and MATLAB simulations.
The identification of communities in social networks is a common problem that researchers have been dealing using network analysis properties. However, in environments where community members are connected by digital documents, most researchers have either emphasize to solve the community discovery problem computing structural properties of networks, ignoring the underlying semantic information from digital documents. In this paper, we propose a novel approach to combine traditional network analysis methods for community detection with text mining techniques. This way, extracted communities can be labeled according to latent semantic information within documents, called topics. Our proposal was evaluated in Plexilandia, a virtual community of practice with more than 2,500 members and 9 years of commentaries.
People's reliance on computers and the computing power they provide is growing by the minute. An ever-increasing amount of data is being created each day, and the power to analyze this data requires the use of cluster computers to process and calculate data. It has been discovered that data clustering is a beneficial data mining approach. There have been a number of recent attempts to cluster data mining methods. Using a Raspberry Pi cluster, this study employs the Apriori algorithm, which is the most generally used algorithm, to extract frequent itemsets from large data sets. The fundamental aim is to build a cluster and provide data analysis capabilities based on an examination of the major clustering phases in order to illustrate the power of cluster computing and the applications of data analytics. Each Raspberry Pi uses the MPI standard and Python multiprocessing to share a large task and then coordinate their findings among a group of four or more MPICH systems at the conclusion of the processing. At the data partitioning stage, the issue of load balancing must be taken into account. According to our testing results, clustering accelerates sequential classification by a factor of 10. There is a noticeable increase in performance when there are additional processors installed. Additionally, we discovered that item count had a bigger effect on clustering performance than transaction count.
Due to the rapid advancement in social media and technology, it generates a large amount of data in different areas of applications. Social media analysis and text mining are all about collecting the most valuable data and drawing actionable conclusions. Text mining also referred to as data mining it is which contains various nodes in the form of data which is often linked together to form a pattern. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. In this study we have analyzed and mounted social media data from Twitter, new articles, and Reddit which suggest that domestic abuse is acting as an opportunistic infection, flourishing in the condition created by the pandemic. The computing tweet sentiments of domestic violence amongst various social media platforms is a major factor of concern. We have used several topic modeling techniques such as Latent Semantic Analysis (LSA) uses a bag of words model, Hierarchical Dirichlet Process (HDP) is a nonparametric Bayesian model for clustering problems, and Latent Dirichlet Allocation (LDA) is a generative probabilistic model for collections of discrete data. Therefore, in this project, we tend to propose a deeper insight into the rise in domestic violence on social media and to provide a holistic approach to tackle this situation.
The enormous amount of information stored in text form cannot simply be used for developing applications in various domains. This is so because applications usually handle text as simple sequence of characters only. Therefore, specific (pre) processing techniques and algorithms are needed in order to extract interesting facts from the text. In sense, text mining can be defined as an automated process to extract key facts and information from available text in an effective and efficient manner, so that, extracted facts can become platform for various applications. In depth analysis of various text mining techniques, their working, complexity, merits and demerits have been presented in simple yet effective manner.
In this paper, we briefly introduce the social sensing and sentiment analysis paradigms. The first one regards the general framework in which information coming from social media, and in particular from On Line Social Networks (OSNs), may be used for mining useful knowledge to be exploited in several real-world applications. Indeed, all the elements that people share on OSNs (texts, links, positions, images and so on) may be considered as the informative content of social or human sensors. This content may be used in several contexts such as real-time monitoring, prediction and identification of events, and for studying opinions, sentiments, moods and emotions that people share in the texts published on OSNs. The more specific framework, in which information coming from social networks are adopted for detecting the polarity (e.g., positive, neutral, or negative) of the sentiment associated with a text, is labelled as sentiment analysis. In this work, we also show two real-world applications of both social sensing and sentiment analysis.
Pre-processing of data in the abstract techniques are by far important and the duration process involved in the flow of information discovery. The convolution pre-processing is determined by the source of data from which we are retrieving. The aim of the paper is to find out how important is pre-processing methods to discover the interesting pattern of user session data and also how to use pre-processing method on mining the textual data. We work on transactional and sequence model for weblog files in text format, text representation and sequence rule analysis is been considered as work model. We make an analysis by comparing four datasets which comprises of different data quality of text and it will be pre-processed in different measures like data will be determined with paragraph sequence, data is determined with sentence sequence, data is determined with paragraph sequence excluding stop words. We utilise the impact of these advanced techniques of pre-processing on both quality and quantity of data measurable in case of paragraph sequence identification.
Crime against women in India has become an eminent topic of discussion in recent years and the issue has been brought to the foreground for concern due to the increasing trends in crimes performed against women. Most of the crimes get reported and a massive dataset is being generated every year. Analysing the crime reports can help the law enforcement section to take preventive measures for reducing the crime, but processing this voluminous data is strenuous and error-prone. So application of various text mining techniques can be of great help for visualising the crime trend. The present work proposes an effective methodology for analysing crime against women in India. The work involves extracting crime reports from online newspaper articles and the documents consisting crime reports of various states and union territories of India are made to undergo several preprocessing techniques. Each document is treated as bag-of-words and finally an exhaustive list of words have been prepared. Similarity has been measured among the words for selecting relevant features for crime trend analysis. Based on the selected crime features, community detection algorithm has been applied for partitioning the states based on crime against women in India. It is a graph based clustering approach and all the states of India have been considered as nodes of the graph. Each community is a group of states which are similar based on crime trends. The work also gives a new direction of application of the algorithm with certain limitations for incremental datasets where data are being generated regularly. The present methodology compares with some existing feature selection methods and clusters obtained in each method using same algorithm are evaluated which shows that the proposed method is more accurate than the existing methods.
Storytelling is a fun activity, especially for telling a story to children which can affect the character of our children based on the story. Story from the internet resource can be a long story or a short story. For the former, it needs to make a summary to become shorter. When it is a long one, it is more difficult to tell our kids and it is more relevant to make a summary. For creating a summary, it will be easier to proceed with a computer, consequently, it needs a computer application that automatically provides a short overview of the folklore. The Indonesian folklore is summarized by several processes in a web application with a python programming language as a development tool. The process of summarization begins with processing the text that is entered by the user and it is eliminated in all of the special characters. Then the result is processed in the form of tokenizing to get a set of words. From this process, all of the set’s elements that are belong to the stop word list are removed and then the rest of them are counted each for the word frequency that they appear. After finding the word frequency, the sentences are scored by the accumulation of word frequency that they have and are sorted based on this score. Lastly, the sentences are sorted based on each of them appear from the original version and make them be a single paragraph. The result of this research is that text summarization can be carried out with the computer application and python programming languages in the form of a website. The number of sentences that are determined can be selected by the user in the program. Several Indonesian folklores are applied to the application and rated by the reader. Moreover, the application delivers 87% of reader acceptance and it is almost similar to several existing online websites that generate text summary.
The emergence of Academic Social Networks (ASNs) has provided opportunities for researchers to upload their publication work, discuss issues with an author, ask questions and build a connection between the researcher communities. With this large information available online, there is need of analyzing the information propagation in ASNs. In this paper, we analyzed the topic propagation in ASNs. We have used the popular academic search engine, Google Scholar for data collection and statistical software `R' for data analysis. We used a scrapper collect the data set from Google Scholar and around 500 papers were examined to find the propagation of the current trend of topics in the specific domain. The result helps the novice researchers to choose the current trending topics and work on the issues in this topic.
ASEAN Free Trade Area (AFTA) is an agreement from ASEAN countries to form a free trade area which is used to increase the economic competitiveness in ASEAN regional area. Some governmental agencies and national companies have to prepare to face AFTA and build their strategies to survive from trade area by understanding what Indonesian people think about that. In this paper we present a system to analyze the opinion of Indonesian people about AFTA and extract a useful information of the sentiment analysis from their opinion. Our system consists of 5 computational steps: (1) Data Acquisition, (2) Text Mining, (3) Sentiment Analysis Engine, (4) Temporal Projection Engine, and (5) Context Processing Unit. We apply a new approach to analyze opinions about AFTA by opinion mining based on temporal sentiment analysis. This approach extracts and process textual data automatically to obtain sentiment information in the opinion sentences. The results show effectiveness of the simple approach for gaining the sentiment information from people's opinions and some stakeholders and students who are concerned in AFTA have assessed this solution.
Text Analytics is the process of parsing textual content from massive non-structured sources like different blogs, emails, tweets, HTML files, full-text documents, articles and extracting relevant facts. Text Analytics in big data is a multidisciplinary domain. Information Extraction, named entity extraction, is a subdivision of text analytics. Natural language Processing (NLP) is an extensive area for linguistic analysis of the text by using segmentation, tokenization, POS tagging, and syntactic parsing. Big data along with NLP will overcome major challenges in different domains of analytics. In this paper, we have presented some common and basic approaches like the BOW model, TF-IDF Model, Vector Space Model, and Word2Vec model along with their advantages and limitations and made a comparative analysis over enormous documents for finding accurate content, and semantic relationship between words and resolving ambiguities issues. We will also propose an NLP-based information extraction framework for processing and analysis of text documents in a Hadoop-based big data environment.
The role of small-group discussion in disaster risk management is to create concrete and practical plans that include regional information. A facilitator is required to encourage residents to reveal such regional information through discussion. However, it is unclear how the facilitator contributes to a fruitful discussion. This study is designed to identify the topics discussed by residents in small-group discussions and, using text mining methodology, to evaluate the regionality of these topics. Our method is useful for evaluating whether a facilitator guided participants to discuss topics with high regionality and generality during the phases of developing and summarizing the discussion, respectively.
There have been many major changes in the education sector due to the closure of educational institutions to suppress the spread of the Covid 19 pandemic, one of which is teaching methods in Indonesia. Problems arise when schools have to use e-learning for students and online assessments have to be done. It is known that difficulties are experienced when taking semester exams or when teachers give assignments online. This study applies text mining techniques using the Ratcliff/ Obershelp algorithm to determine the similarity value between two strings, namely between student answers and the teacher's answer key. A collection of answer data obtained from one of the 6th grade teachers at elementary school and applied to the Examz web application. This application is built to find out the status of answers based on the error tolerance that has been predetermined by the teacher. The results of this study indicate that the Ratcliff/ Obershelp algorithm has achieved a correction accuracy rate of 90% in Natural Science subjects, 93% in social science, 83% in Civil Education, 91% in SBK, and 97 in Religion. The final result for the average accuracy of the application is 91.00% in determining the status of student answers.
The paper addresses some roles of concept-based representations in document clustering to support knowledge discovery. Computational Intelligence algorithms can benefit from semantic networks in the definition of similarity between pairs of documents. After analyzing the tuning of semantic networks in a systematic fashion, the research defines and evaluates a novel semantic-based metrics, which integrates both classical and style-related features of texts. Experimental results confirm the effectiveness of the approach, showing that applying a refined semantic representation into a clustering engine yields consistent structures for information retrieval and knowledge acquisition.
Extracting informative contents of clinical importance from gene expression is challenging in bioinformatics. However, reviewing existing literature reviews found that almost all the frequently adopted techniques for analyzing gene expression data are associated with problems. Therefore, the proposed system offers a cost-effective framework that contributes to a simplified text mining approach with a novel design of gene-network analysis followed by optimization to incorporate novelty in this field.
This study aimed to analyze the learning objectives of the higher education textbooks, according to verbs of intended learning outcomes which were set by the Quality Management of Institute of Statistical Studies and Research (ISSR), Cairo University, Egypt. This study provides a set of recommendations that will help to choose the best chapters of a textbook which is investigating the largest amount of targeted intended learning outcomes that helps explore the key performance indicators for organizations. A dataset of learning objectives for each chapter and intended learning outcomes has been collected. Each learning objective from chapters and the Intended learning outcomes were processed through the removal of punctuation and stop words, tokenization, stemming and term weighting.
The sustainable development of new energy is the key to the low-carbon transformation of China's energy system, and the analysis and interpretation of new energy policies are the basis for the scientific layout and steady progress of the new energy business of enterprises. Based on this, this paper constructs a new energy policy text classification recognition model based on the CNN (convolutional neural networks) model, combs the policies released in the new energy field in China in the past 15 years, and uses expert consultation method and manual pre-processing method to extract, clean and classify the policy keywords. Among them, different from the previous policy classification methods, this paper classifies the policies into three types of policies, namely, guiding, incentive, and regulatory, according to the policy characteristics. Based on this, the constructed policy features are input into the CNN model for energy classification and identification. The results show that the accuracy of the model recognition is about 65.96%, which indicates that the CNN model applied in this paper can classify the new energy policies effectively.
Consecutive example mining is a critical subfield in information mining and it's a troublesome issue since mining may require assembling or looking at expansive number of subsequence blends. Instructions to viably and productively mine interim based successions is testing issue. Three sort of interim based examples which utilize Greedy calculation, to find three kind of interim based successive examples. Time is inescapable of reality, and various social database approaches have been made to adjust to it. In realistic applications, facts can go over a couple times, and simply the general time span containing each one of the emphases may be known (consider, e.g., On January, John went to five social affairs of the Bioinformatics wander). While some brief social databases have defied, substances repeated at (known) incidental time, or single truths happened at fleetingly unverifiable time.
Natural Language Processing (NLP) is a prominent subject which includes various subcategories such as text classification, error correction, machine translation, etc. Unlike other languages, there are limited number of Turkish NLP studies in literature. In this study, we apply text classification on Turkish documents by using n-gram features. Our algorithm applies different preprocessing techniques, namely, n-gram choice (character level or word level, bigram or trigram models), stemming, and use of punctuation, and then determines the Turkish document's author and genre, and the gender of the author. For this purpose, Naive Bayes, Support Vector Machines and Random Forest are used as classification techniques. Finally, we discuss the effects of above mentioned preprocessing techniques to the performance of Turkish text classification.
SemEval (Semantic Evaluation) is an annual workshop where attendees participate in a series of evaluations (competitions) of computational semantic analysis for natural language processing (NLP). The series evaluations include 10-20 tasks each year. In this paper we present our entry to the SemEval-2014 Task 7 on the Analysis of Clinical Text evaluation. The main aim of this task is to analyze large amounts of clinical data and to find the mentions of clinical disorders. This task consists of two sub tasks: a) named entity recognition i.e, identifying disorder concepts that belong to Unified Medical Language System (UMLS) semantic group; and b) normalization, i.e, mapping mentions of disorders to UMLS Concept Unique Identifier (CUI). In this paper, we present a supervised machine learning system for prediction of disorder named entities based on the conditional random field model. The data set provided by the Task 7 organizer was used to evaluate our model.
Finding closely matching source codes are important in software development. By finding them, software architects will be able to identify similar implementation of classes, libraries etc. However, this is not an easy task, since there can be a large number of source code files. Manually matching each and every document may be difficult, if there is high number of documents. This research is to build a mechanism using term text mining methodology to find out similar documents from the given set of documents.
The revealed analysis studies on pair programming so far indicate that pair programming has produced affirmative effects on some aspects of students” performance. In the academic field, the usual practice of pair programming would be pairing the students in line with the programming skills of the students by the respective lecturers. This means, compatibility of the students in terms of their programming skills is the main focus when the pairing was done by the lecturers. Yet, research on elements that the students are looking into when they are given the liberty to decide on their partner in pair programming is lacking. In this study, a multi-layer perceptron (MLP) is developed to predict the preference of opting pair programming over solo programming. The Bayesian Information Criterion was used to select the best features in the prediction. The potential of unstructured text entered by the participants as comments in the questionnaire is incorporated in the MLP model to verify its capabilities towards prediction accuracy, i.e., to verify whether their comments are connected to their preference for pair programming versus solo programming. It was found that, when the students are given the freedom to choose their partner in pair programming, in the context of Malaysia, the students would pay attention to the ethnic criterion. This also suggests that the unstructured texts in the form of comments submitted by the participants in the questionnaire did not contribute to their choices on whether to undertake solo or pair programming.
With the prevalence of mobile devices such as smartphones and tablets, the ways people access to the Internet have changed enormously. In addition to the information that can be recorded by traditional Web-based e-commerce like frequent online shopping stores and browsing histories, mobile devices are capable of tracking sophisticated browsing behavior. The aim of this study is to utilize users' browsing behavior of reading hotel reviews on mobile devices and subsequently apply text-mining techniques to construct user interest profiles to make personalized hotel recommendations. Specifically, we design and implement an app where the user can search hotels and browse hotel reviews, and every gesture the user has performed on the touch screen when reading the hotel reviews is recorded. We then identify the paragraphs of hotel reviews that a user has shown interests based on the gestures the user has performed. Text mining techniques are applied to construct the interest profile of the user according to the review content the user has seriously read. We collect more than 5,000 reviews of hotels in Taipei, the largest metropolitan area of Taiwan, and recruit 18 users to participate in the experiment. Experimental results demonstrate that the recommendations made by our system better match the user's hotel selections than previous approaches.
Financial Documents reveal important financial information about a company's financial performance which plays a vital role not only to the stakeholders but also to the public. Therefore, many researchers utilize dynamic Text mining methods in financial document to identify, analyze, predict or evaluate a company's future financial value. In order to find deeply the relationship between companies and the stakeholders, provide a simplified method for them to identify the future financial performance of the corporation. In this paper, we present a Chinese Information Extraction System (CFIES) for Financial Knowledge Graph (FinKG). The major findings of the research show an increased importance of the key audit matters in finance. The major research contribution of this paper is that we have developed CFIES which can extract the tuples from the financial reports. The adoption of the information system can assist the development of a knowledge graph that can discover deep financial knowledge in the finance domain. The managerial implication is that building CFIES can efficiently enable us to clarify the complicated relationship between the corporations, board of directors, investors, and especially the asset, assisting the stakeholders to discover a new financial knowledge representation and to make a financial decision.
The identification of the potential standards essential patents (SEPs) can make great contributions to not only the technology management theory, but also to the real practices of establishment and development of enterprise competitiveness and standardization strategy. However, despite the importance of identifying potential SEPs, the approaches to identify potential SEPs lack of adequate mining of existing technical standard text and validation of identification results based on standard updates. Therefore, in this paper, we contribute to resolving this issue by proposing a research model that integrates text mining and the generative topographic mapping (GTM) to effectively identify and verity the potential SEPs based on existing and updated standards. The universal terrestrial radio access (UTRA) technology is selected as a case study. In this case, the TF-IDF algorithm and the Latent Dirichlet Allocation (LDA) method are applied to analyze the keywords and technical theme of standard and patent documents, and GTM is used to construct standard and patent map, then the two maps are mapped by the improved similarity algorithm we proposed. Finally, 39 potential SEPs of the technology are identified, 24 of which have been verified, and the other 15 are likely to be included in subsequent standard version. This paper will contribute to the identification of SEPs methodology, and will be of interest to UTRA technology research and development experts.
Business analysis is performed to determine the business that are popular, in Indonesia with text mining can take data from several news portal in Indonesia. Text preprocessing is used to change the text title and tags on the news to be converted into weights. The weight of the data will be processed using the K-Means algorithm to be grouped into clusters and each cluster will be visualized using Word Cloud so that words that often appear as popular word identification are known. Testing uses the Silhouette Coefficient to calculate the quality of each member against the cluster. Furthermore, each member will be interpreted according to the test results. Analysis is carried out every month in 2018 with a total of 995 data with a monthly average of 6 clusters, in January were the most popular business according to the number of members from 64 data formed 6 clusters, the most member clusters were cluster 1 the Silhouette Coefficient test results are strong 0.00%, medium 65.22%, weak 30.43%, not substantial 4.35%, Word Cloud formed was a leather bag business.
Visualization techniques have proved to be valuable tools to support textual data exploration. Dimensionality reduction techniques have been widely used to produce visual representation of document collections. Focusing on multidimensional projection techniques, good visual results are produced depending on how representative terms to discriminate the documents are chosen to compose the vector space model (VSM). To define a good VSM it is necessary to apply filters during the preprocessing in order to eliminate terms using their frequency. For that, the user must evaluate the term frequency histogram based on his/her expertise in the text subject and decide the threshold value for frequency cut. Usually it is a trial and error approach that requires the user to verify the quality of visual representation after each trial. In this paper, we propose an automatic approach that applies the Otsu's Threshold Selection Method for computing a threshold using a term frequency histogram. We conducted experiments that have shown our approach generates visual representations as good as those generated with a threshold obtained by trial and error approach. The contribution of our approach is that users with non expertise are able to generate good visual representations and the time to get a good threshold is decreased.
This paper presents the new facilities provided in defoe, a parallel toolbox for querying a wealth of digitised newspapers and books at scale. defoe has been extended to work with further Natural Language Processing () tools such as the Edinburgh Geoparser, to store the preprocessed text in several storage facilities and to support different types of queries and analyses. We have also extended the collection of XML schemas supported by defoe, increasing the versatility of the tool for the analysis of digital historical textual data at scale. Finally, we have conducted several studies in which we worked with humanities and social science researchers who posed complex and interested questions to large-scale digital collections. Results shows that defoe allows researchers to conduct their studies and obtain results faster, while all the large-scale text mining complexity is automatically handled by defoe.
Social Network Analysis (SNA) and Web Mining (WM) techniques are being applied to study the structures of social networks in order to manage their dynamics and predict their evolution. This paper describes how we used Semantically-Interlinked Online Communities (SIOC) ontology to represent the (latent) semantic relationships between the members of a large community forum (about 2,500), Plexilandia. We extended SIOC, taking advantage of topic-based text mining and developed data mining algorithms that used our SIOC extensions to provide a better understanding of the social dynamics of the members of the Plexilandia community. This new understanding helped us to detect and discover the key members of Plexilandia successfully.
The Helix project is a multiyear longitudinal research study focus on understanding what makes systems engineers effective. The Helix team gathered data through interviews, site visits, and surveys with individuals who are doing systems engineering (SE) work on programs, supervising systems engineers, using products from systems engineers, and are knowledgeable about SE. The responses provided by the participants were documented and analyzed using text mining techniques, such as topic modeling and word similarity. To interpret the results, the team developed several visualizations from three scenarios, full corpus, by organization type, and by domain. The visuals show the common language among systems engineers within topics and the relationship between words from the three scenarios mentioned above. Also, the article provides insights into what enables systems engineers to be proficient or effective in their organizations.
The book discusses the history of the Islamic inventors have been widely published, but the history books that have been known so far are textual-based so the search process for the aspect of the place and time aspects of historical events are sequential. Surely it makes the reader difficulties when trying to examine the history based on place and time-based historical events. This paper proposed new model to learning the history of Islam inventor during the Islamic civilization. This model using spatio-temporal to present Historical information which are displayed based on the time as year timeline and the location on map. This paper also provides a search feature to searching the highest similarity of historical information, using text-mining and clustering methods. This makes it easier for users to learning historical event. We compare result of our idea into several device and several keyword to searching history. The experimental result show the effectiveness of our idea to solve this learning. Tests on some users, showed that these applications can be used as a medium for teaching history of Islamic inventor during the Islamic civilization.
In recent years, rapid increase in use of Blogs has provided excellent collaborative opportunity for knowledge sharing and forming Communities. This facilitates critical reflection and chat among users with similar interest by engendering a virtual community in blogosphere. The issue is, formation of such community mostly relies on mutual awareness or appropriate link communication. While, automatically determining interest from users group, becomes significant to contact users with related interest to facilitate community sustainability. Hence, determining the potential of Semantic Web, this paper presents an architecture called Autorec, which will use Intelligent Agent and Text Mining technique to process bloggers content to identify their interest. Our approach is purely heuristic. It also uses Graph API to search for user with similar interest users on Social Networking Sites - SNS (Facebook) as to recommend them to create Virtual settlement and aids to sustain the Virtual Community.
With the rapid development of web 2.0, increased users of online social media show keen interest to express their opinion and reviews on numerous aspects such as microblogs, various products, hotels, movies and political issues. At present, text mining plays a vital role in various application domains such as online media, healthcare, security applications, business, marketing and industrial applications. In text mining, sentiment analysis or opinion mining is a task carried over to extract or classify the information. This paper presents an exhaustive study on the performance factors highlighting the current state-of art techniques and the open issues on various machine learning based text mining approaches.
The companies in India are mandated to perform social and environmental practices. Due to the mandate the companies are increasing using annual reports and specifically corporate social responsibility (CSR) and sustainability reports to their stakeholders and public about their CSR and environmental practices. The authors use company's annual reports of various Indian companies to find the common topics and thus the most common CSR tasks and practices described in these reports. This paper applies the natural language processing (NLP) as a technique towards CSR and sustainability research. Analysis of these reports led to 5 primary areas of common concern. The text classification models based on machine learning is used to identify the topics of primary focus in the text. The text data is classified with the help of the classifier label which enables us to create a supervised machine learning model The objective of the research is to find the efficient text classification model to effectively identify theme of the given text data.
Structured data, typically, is predefined data. Semi-structured and unstructured data are not predefined data that includes documents, emails, social media posts, images, videos, etc. In this research, a novel process is presented to extract structured data from emails about a domain such as on a project or product. This process consists of three phases: data cleaning, data extraction, and data consolidation. Data cleaning is done by validating the format for each email. Data extraction consists of keyword extraction, sentiment analysis, regular expression, entity extraction and summary extraction. Data consolidation is used to combine the extracted data to obtain structured data from emails. This will make the knowledge extraction process easy to manage and analyze. In large industries, it is better to consolidate all the emails regarding a project/product as one document using this process for later use. This solution will facilitate better decision-making.
Agile software development (ASD) is a popular research area that attracts the attention of the software development industry as well. Many studies have been conducted to explore the issues related to the ASD domain. Research is still very vigorous in this domain as there is continuous interest from companies to adopt ASD in their software development processes. Moreover, with the remote work setting that the pandemic forces, software companies search for new methods and approaches to manage their projects effectively and successfully. Hence, this study explores the research conducted in the last two decades in the ASD domain through text mining to reveal the trending topics that get attention from scholars. According to the results of this study, eight topics are identified for the research in ASD domain. Among those, topics of Team Communication, Agile Development, and Software Development Process are the most popular, the most funded, and the most cited topics respectively.
The NSFC is the largest government funding agency in China, with the primary aim to fund and manage basic research. The agency is made up of seven scientific departments, four bureaus, one general office, and three associated units. The scientific departments are the decision-making units responsible for funding recommendations and management of funded projects. Selection of research projects is an important and recurring activity in many organizations such as government research funding agencies. Current method of grouping proposals are based on manual matching of similar research discipline areas but it fails to be accurate. Text clustering methods those are not having semantic approach provide less accuracy. A novel ontology based text mining approach to cluster proposals is proposed. Research project selection is an important task for government and private research funding agencies. When a large number of research proposals are received, it is common to group them according to their similarities in research disciplines. The grouped proposals are then assigned to the appropriate experts for peer review. The review results are collected, and the proposals are then ranked based on the aggregation of the experts' review results.
This study combines text mining and content analysis to examine social media content from the top 20 engineers on Twitter, as well as frequent engineering communication topics on Quora and Reddit, to explore the following: (1) What are important communication skills on social media? (2) What rhetorical strategies do they use communicate information on social media, given the media's limitations? (3) What topics do engineers most commonly use to engage the public? (4) What topics are associated with “reach” (or distribution and reception of messages) on social media? Findings suggest that appeals and topics that engineers use on social media index personal, technical and professional themes, from technology info and link citations to art, culture, politics, and other modes. Social media blur public, private, and professional spheres of exchange. Public engagement creates heterogeneous networks of topics of interest and practice with diverse vectors of engagement. Interactions between soft skills such as reading, writing, listening and speaking characterize engineering's multidisciplinary workplaces. Additional important domains (e.g., communication environments, visuals literacy, and interpersonal ethos) emerged. Recommendations for practice and teaching include extending coverage of topics in engineering communication education, providing workplace training on social media, and revising communication policies on the job.
The article considers the process of digital transformation of education and the existing issues in this field. Digital technologies improve the efficiency of backend processes taking place within the unified learning platforms. The article offers possible solutions to the issue of one topic objects search in the learning repositories based on the case study of the Moscow Electronic School - a massive database of learning objects. The solution suggested in the article includes a special text mining model developed to analyze lesson script titles and a programmed algorithm aimed at extracting one topic lesson scripts and visualizing them in two-dimensional space. The efficiency of the developed model is supported by the visualization of 36,644 lesson scripts stored in the E-Library of the Moscow Electronic School.
The purpose of this study is to analyze the direction of the Artificial Intelligence (AI) education policy announced by the South Korean government and how key tasks are reflected in detailed policies to find implications. The study first identified three major policy directions as found in the South Korean government’s publication on ‘Education Policy Directions and Core Tasks in the Era of AI’ and conducted a text mining analysis on government policy materials, press releases, and reports related to AI education from 2020 to 2022 by extracting keywords. According to our topic analysis results, various AI education contents were being developed both before and after the national curriculum revision. Moreover, our research indicates that the advancement and activation of AI education policies will take place to smoothly implement the curriculum in classrooms, and additional policies and challenges will emerge when the new curriculum comes into effect in 2025.
Enormous amount of unstructured electronic health record are invaluable for the medical research in finding the relationship between the patient's disease and the final diagnosis. How to use computer automatically dig up these information has long been a hot spot. To get the relationship between the clinical outcomes and free text writing by nurse, we developed an automatic categorization system process natural language nursing record Based on vector space model. 210 cases of electronic nursing records, which were diagnosed as pancreatitis, were induced in this study. We filtered the restricted corpus for acute pancreatitis classification by information gain (information gain. IG), and construct a text classification system Based on Partial least squares discrimination algorithm (PLS-DA) and vector support machine (VSM). PLS loading value analysis found that there are 20 terms can be used to classify medical record text. Our innovative machine-learning algorithm effectively classified free texts of nurse care records associated with normal and acute pancreatitis diagnoses, after training on pre-classified test sets by PLS. This automatic identification technology focus in large-scale medical document may provide important clues to study the acute pancreatitis and other important common disease.
For the purposes of technology planning and research and development strategy development, we present a semi-automated method that extracts text information from patent data, uses natural language processing to extract the key technical information of the patent, and then visualizes this information in a matrix form. We tried to support qualitative analysis of patent contents by extracting functions, components, and contexts, which are the most important information about inventions. We validated the method by applying it to patent data related to nanosensors. The matrix can emphasize technical information that have not been exploited in patents, and thereby identify development opportunities.
This paper raises a Microblog topic detection method based on text clustering and topic model analysis. It solves the problem that the traditional topic detection method is mainly applicable for traditional media text, which is not very effective in handling sparse Micro blog short texts. In consequence of the structural data of the Microblog, which exists rich inter-textual contextual information such as retweets, comments, user hash tag, embedded link URL, we first put forward a feature weight pre-processing method. We also use a clustering algorithm based on word vectors to enrich the feature information of the data. On this basis, we extend the conventional LDA (Latent Dirichlet allocation) topic model to extract the hot topics in the Micro blog data. Compared with the traditional methods, the method raised in this paper is much more effective in the collected text corpus in Sina Microblog.
Risk Assessment Matrix(RAM) is a traditional method used for risk analysis of space engineering. The essential aspect in RAM is to identity the risk event and classify the risk level. Normally, this work is absolutely dependent on judgement of experts. But in the past 50 years, there are huge risk text in RAM structure which contain valuable knowledge about risk analysis. And with the arising era of big data, the risk analysis certainly will step into an intelligentized age. In this paper, we proposed a text mining system for risk event recognition and risk level classification intelligently. Our system has two process. The first one identifies the features of risk event based on a BIO format. The second one relies on machine learning techniques to map the relationship between features using a composite kernel-based method which consists of a shallow linguistic kernel and an extended dependency tree kernel function. A Supported Vector Machines is used in data train and test. The obtained results are encouraging after comparing the system with other methods, such as KNN, Naive Bayes.
Lexical relations, or semantic relations of words, are useful knowledge fundamental to all applications since they help to capture inherent semantic variations of vocabulary in human languages. Discovering such knowledge in a robust way from arbitrary text data is a significant challenge in big text data mining. In this paper, we propose a novel general probabilistic approach based on random walks on word adjacency graphs to systematically mine two fundamental and complementary lexical relations, i.e., paradigmatic and syntagmatic relations between words from arbitrary text data. We show that representing text data as an adjacency graph opens up many opportunities to define interesting random walks for mining lexical relation patterns, and propose specific random walk algorithms for mining paradigmatic and syntagmatic relations. Evaluation results on multiple corpora show that the proposed random walk-based algorithms can discover meaningful paradigmatic and syntagmatic relations of words from text data.
Citizen science has emerged in many countries to contribute to the prompt resolution of individual field problems and has been shifted toward Information System (IS) research. In the domain of IS, a citizen report mechanism has been introduced in many local governments to understand regional problems based on the public participation. The rising of social media enforces many organizations including the local governments to utilize any information from the citizens including texts. Text mining has been utilized in various types of analyses such as sentiment analysis. However, it shows many challenges when it comes to the local context. The local context of words could cause various conflation errors that highly affect the learning task such as classification methods. This study aims to propose a context-based text processing and feed the proposed approach into a machine learning framework to classify the data of citizen reports-. The context-based text preprocessing utilized statistical- and semantic-based measurements to extract the local context and elaborate domain expertise to verify the misinterpretation for further text processing such as feature extractions. Subsequently, the n-gram language models together with the Term Frequency and Inverse Document Frequency schemes were performed to build the features. The result showed that the context-based text preprocessing improved the classification performance in majority classifiers in about 3% with the combinations of n-gram features.
K _means algorithm is one of the typical clustering algorithms in text mining tasks. K_means algorithm is widely used in many areas because of its easy to implement and ability to handle large datasets with better scalability. However, the random selection of initial cluster centroid in traditional K_means algorithm for text clustering easily leads to local optimization and instability of clustering results. Therefore, in order to overcome this shortcoming, this paper propose an improved K_means algorithm for document clustering which based on following two points: (i)we used concept distance to optimize the choice of the initial cluster centroid, which can avoid the drawbacks caused by random selection; (ii)we adopted knowledge graphs to improve traditional k_means text clustering algorithm by optimizing the calculation of text similarity. Theoretical analysis and experimental results show that the improved algorithm could optimize the accuracy of text clustering effectively.
The technological intelligence tools that companies need to achieve sustainable development in today's digital age are extremely limited. To fill this gap in the literature, a novel technology intelligence tool is developed in this article. The developed technology intelligence tool uses patent analysis and utility mining. It is based on the HUP-growth mining algorithm, which is one of the utility mining algorithms, to find the relationships between technologies. The developed technology intelligence tool has three contributions to the literature. First, the importance level of the technologies and the repetitions of each technology in a patent have been taken into consideration. Second, the existing studies in the literature cannot use a technology intelligence tool to explore the relationships between technologies. Third, the developed technology intelligence tool is more successful in finding hidden patterns among patent documents. A real-life case study of business method patents about electronic shopping is conducted to show how the developed technology intelligence tool works in practice. The Apriori algorithm is also used to compare the results obtained from on the HUP-growth mining algorithm. The results of this study show that companies can quickly and effectively find association among technologies under concern using the developed technology intelligence tool.
The CiteSeerx project at the University of Arkansas uses a browsing interface is based on the Association for Computing Machinery's Computing Classification System (ACM CCS). CCS contains just 369 categories whereas the CiteSeerx database contains over 2 million documents. This results in more than 6500 documents per category, far too many to browse. To address this problem, we are exploring ways to automatically expand the CCS ontology. Previous work has focused on using clustering to automatically identify the new classes. This work focuses on how to label the subclasses in a semantically meaningful way to that they can support user browsing. We develop methods based on text mining from the subclass members to extract class labels. We evaluate three methods by comparing the suggested labels with human-assigned labels for existing categories.
Huge amount of data in today's world are stored in the form of electronic documents. Text mining is the process of extracting the information out of those textual documents. Text classification is the process of classifying text documents into fixed number of predefined classes. The application of text classification includes spam filtering, email routing, sentiment analysis, language identification etc. This paper discusses a detailed survey on the text classification process and various algorithms used in this field.
Educational data mining(or EDM) is an emerging interdisciplinary research field concerned with developing methods for exploring the specific and diverse data encountered in the field of education. One of the most valuable data sources in the educational domain are repositories of exam queries, which are usually designed for evaluating how efficient the learning process was in transferring knowledge about certain taught concepts, but which commonly do not contain any additional information about concepts they are related to beyond the text of the query and offered answers. In this paper we present our novel approach of using text mining methods to automatically annotate pre-existing exam queries with information about concepts they relate to. This enables automatic categorization of exam queries as well as easier reporting of learning concept adoption after these queries are used in an exam. We apply this approach to real-life exam questions from a high education university course and show validation of our results performed in consultation with experts from the educational domain.
In the field of information retrieval text categorization is the key research area in present. The text categorization selects entries from set of pre built categories and allots those to a document. Learning with high dimensional data space is challenging in a text categorization method. Learning with high-dimensional features may prompt a heavy calculation overhead and may affect the classification performance of classifiers because of unrelated and repetitive features. To improve the “scourge of dimensionality” issue and to accelerate the learning procedure of classifiers, it is important to perform feature reduction to reduce the size of features. This paper introduces a Bayesian arrangement approach and J48 classifier for auto text categorization using class-specific features. For text classification, the proposed strategy selects a specific feature subset for every class. In contribution J48 classifier combining with term weighting concept as weighted j48 classifier is used for classification. These methods increase the accuracy of classification and feature selection process and improve the system performance. The detectable importance of this methodology is that many feature selection criteria can be easily used. The weighted J48 classifier saves both the time and memory. The proposed system also uses Term weighting concept for preprocessing. These methods increase the accuracy of classification and feature selection process and improve the system performance.
The consumption of dietary supplements has started to become a common habit for Malaysians. People consume dietary supplements to improve their health, prevent or cure diseases, or overcome nutrient deficiency. Despite the benefits of dietary supplements, they could carry various risks. Besides that, selecting dietary supplements could be a challenging process as it needs to tailor the consumer’s dietary profile and health condition. To solve these problems, a system has been proposed to provide various information about the ingredients detected in the dietary supplements’ descriptions. It also provides recommendation functions based on the user’s health condition and dietary profile. Furthermore, it helps to perform diagnosis when the user suffers adverse effects after consuming a dietary supplement. To accomplish these functions, the system utilized text mining techniques like named entity recognition for extracting ingredient names from the description of dietary supplements. Meanwhile, expert system techniques like forward chaining and backward chaining were also implemented for the recommendation and diagnosis function. The system aims to act as an assistive tool that helps the user detect unaware risks in the dietary supplements and assist the user in selecting dietary supplements that best suit their health condition.
Single Chinese herbal medicine is the basic element in traditional Chinese medicine's clinical treatment against disease. Then, it is an interesting and meaningful task to acquire knowledge of a single Chinese herbal medicine within the framework of traditional Chinese medicine (TCM). In this paper, based on the knowledge of TCM, we explored the association rules of Danggui (Angelica sinensis in Latin). These associated rules include Danggui-TCM syndrome/pattern, Danggui-disease, Danggui-symptom, Danggui-TCM herbal formula, and Danggui-Chinese herbal medicines. Through text mining, these rules can be demonstrated in different networks. These associated networks represent a variety of knowledge points and most of them can be validated in textbooks of traditional Chinese medicines. Thus, these results might be useful for both clinical practice and medical research, and the approach provides a novel way in exploring associated rules of single Chinese herbal medicine within the framework of traditional Chinese medicine.
The amount of information available on the internet became enormously high because of the rapid development in the electronic medium. Now days, managing and retrieving useful information from the web becomes the very tedious job. Web document clustering is a methodology which tries to group the available web documents in a meaningful way. There are lot of issues are identified in this area like, heterogeneous sources of information, no universal model, dynamicity of information, the huge volume of data, etc. most of the existing document clustering models follows the “bag-of-words” document representation. The proposed model presents an incremental analysis of web documents based on semantic suffix tree which analyses web documents by means of concepts to improve the cluster quality. The concepts are evaluated at three levels, a sentence level analysis, a document level analysis and domain level analysis. The proposed model also uses self organizing feature map (SOFM) to group similar documents together in a cluster and organize similar clusters close together unlike most other clustering methods. A set of experiments using the proposed model on different data sets have been conducted. The experiment result shows a significant improvement in the cluster quality.
The paper describes different cases of combination of statistical, rule-based and pattern-based approaches for text classification, entity extraction and discernment of entity properties applied to medical domain.
Text feature is usually expressed as a matrix of huge dimensionality in text mining, and common clustering algorithm are not stable and cannot obtain clustering solution efficiently. Latent Semantic Analysis can reduce dimensionality effectively, and emerges the semantic relations between texts and terms. Clustering ensemble can get better clustering solution than single clustering method. A text clustering ensemble based on genetic algorithms is presented, which combines Latent Semantic Analysis and Clustering ensemble based on genetic algorithms. Experiments have demonstrated that text clustering ensemble based on genetic algorithms can effectively improve the clustering performance.
The analysis of crimes represents a great challenge to law enforcement agencies and organizations that collect information to analyze accurately and effectively the distribution of the behavior of criminal events, considering that the sources to be used in the process of generating intelligence are diverse in content and/or structure. Tools that have contributed to the analysis of fields in which massive amounts of data are available is "data mining", a powerful tool that can be used effectively in the analysis of large amounts of data and in the subsequent derivation of important analytical results. This paper presents a methodology of analysis of crime facts from online newspapers, identifying the different communes where the greatest number of criminal events occur, which gives an idea of potentially more dangerous places, through the detection and geographical mapping of critical points, or the analysis of the nature of the crime through the extraction of entities. Statistics that measure the predictive capacity of the model indicate that the methodology is robust to recognize crime events within the body of the news.
Modelling learning objects (LO) within their context enables the learner to advance from a basic, remembering-level, learning objective to a higher-order one, i.e., a level with an application- and analysis objective. While hierarchical data models are commonly used in digital learning platforms, using graph-based models enables representing the context of LOs in those platforms. This leads to a foundation for personalized recommendations of learning paths. In this paper, the transformation of hierarchical data models into knowledge graph (KG) models of LOs using text mining is introduced and evaluated. We utilize custom text mining pipelines to mine semantic relations between elements of an expert-curated hierarchical model. We evaluate the KG structure and relation extraction using graph quality-control metrics and the comparison of algorithmic semantic-similarities to expert-defined ones. The results show that the relations in the KG are semantically comparable to those defined by domain experts, and that the proposed KG improves representing and linking the contexts of LOs through increasing graph communities and betweenness centrality.
E-government is a government administration that utilizes information and communication technology to provide services to its users. Civil Servants who are one of the E-government users are obliged to be an example in the implementation and use of its. One application of E-government for civil servants is a mobile presence application called Face Biometric Location Authentication (FABIOLA). It is used by employees of Agency for the Assessment and Application of Technology. The application had been downloaded more than 5000 times with a 4-star rating on Google Playstore. Although it has received a fairly good rating, this cannot be used as a reference because there are still many negative reviews. To identify the problem, a text mining methodology and association rules are used. From the results of text mining analysis, several aspects of problems related to applications are aplikasi, foto, gagal, android, and absen. From these 5 aspects, association rules were carried out to facilitate fishbone analysis. From the results of the fishbone analysis, several solutions to the problem are proposed: application reengineering, the use of a stable internet connection, following the appropriate usage steps, and fulfilling the application preinstallation requirements.
In this paper, we propose a novel text representation paradigm and a set of follow-up text representation models based on cognitive psychology theories. The intuition of our study is that the knowledge implied in a large collection of documents may improve the understanding of single documents. Based on cognitive psychology theories, we propose a general text enrichment framework, study the key factors to enable activation of implicit information, and develop new text representation methods to enrich text with the implicit information. Our study aims to mimic some aspects of human cognitive procedure in which given stimulant words serve to activate understanding implicit concepts. By incorporating human cognition into text representation, the proposed models advance existing studies by mining implicit information from given text and coordinating with most existing text representation approaches at the same time, which essentially bridges the gap between explicit and implicit information. Experiments on multiple tasks show that the implicit information activated by our proposed models matches human intuition and significantly improves the performance of the text mining tasks as well.
Pattern mining has been widely studied in the last decade given its great interest for research and its numerous applications in the real world. In this paper the definition of query and non-query based systems is proposed, highlighting the needs of non-query based systems in the era of Big Data. For this, we propose a new approach of a non-query based system that combines association rules, generalized rules and sentiment analysis in order to catalogue and discover opinion patterns in the social network Twitter. Association rules have been previously applied for sentiment analysis, but in most cases, they are used once the process of sentiment analysis is finished to see which tokens appear commonly related to a certain sentiment. On the other hand, they have also been used to discover patterns between sentiments. Our work differs from these in that it proposes a non-query based system which combines both techniques, in a mixed proposal of sentiment analysis and association rules to discover patterns and sentiment patterns in microblogging texts. The obtained rules generalize and summarize the sentiments obtained from a group of tweets about any character, brand or product mentioned in them. To study the performance of the proposed system, an initial set of 1.7 million tweets have been employed to analyse the most salient sentiments during the American pre-election campaign. The analysis of the obtained results supports the capability of the system of obtaining association rules and patterns with great descriptive value in this use case. Parallelisms can be established in these patterns that match perfectly with real life events.
With the emergence of Web 2.0 applications that bestow rich user experience and convenience without time and geographical restrictions, web usage logs became a goldmine to researchers across the globe. User behaviour analysis in different domains based on web logs has its utility for enterprises to have strategic decision making. Business growth of enterprises depends on customer-centric approaches that need to know the knowledge of customer behaviour to succeed. The rationale behind this is that customers have alternatives and there is intense competition. Therefore business community needs business intelligence to have expert decisions besides focusing customer relationship management. Many researchers contributed towards this end. However, the need for a comprehensive framework that caters to the needs of businesses to ascertain real needs of web users. This paper presents a framework named eXtensible Web Usage Mining Framework (XWUMF) for discovering actionable knowledge from web log data. The framework employs a hybrid approach that exploits fuzzy clustering methods and methods for user behaviour analysis. Moreover the framework is extensible as it can accommodate new algorithms for fuzzy clustering and user behaviour analysis. We proposed an algorithm known as Sequential Web Usage Miner (SWUM) for efficient mining of web usage patterns from different datasets. We built a prototype application to validate our framework. Our empirical results revealed that the framework helps in discovering actionable knowledge.
A significant portion of the world population does not have access to proper healthcare. The key factor for healthcare's success is the physician's expertise. In this paper, we examine if that expertise can be modeled as an information corpus, a flavor of Big Data and extracted using text mining techniques, particularly using the Vector Space Model, to perform diagnosis. Using cloud and mobile technologies, medical diagnosis can then be made available everywhere there is Internet connectivity, reducing costs, increasing coverage and improving quality of life. The key to the possibility of performing medical diagnosis using an information retrieval approach is the data. This paper therefore focuses on the suitability of the dataset for automating diagnosis using text mining. We use various text mining tools relevant to the Vector Space Model to perform operations on the data to see if meaningful conclusions can be drawn from it. We present some of our observations from the experiments conducted and conclude with future directions.
Summary form only given.Businesses of all sizes have now realized that the Web is an invaluable resource for competitive intelligence, and consequently business decision making. But many have trouble collecting targeted & useful information, and are often further overwhelmed by the time required for analysis & monitoring. On another hand, text mining techniques have become widely used for information analysis in the scientific community in general, and are now ubiquitous in most Web Intelligence fields. With the availability of services such as Google Prediction API, or mature open source software such as GATE, RapidMiner or NLTK, one can expect a much wider adoption of text mining and associated machine learning techniques by expert developers. But how can these techniques benefit to the daily life of a wider business audience? As competitive intelligence is often focused on products, people, customers and competitors, there is an added value for systems providing analytics on these entities, whose recognition is fundamental to text mining and semantic analysis, and consequently is still under active scientific investigation. In this talk we will tour some of the specific requirements and options for building an efficient Web based competitive intelligence system with named entity analytics. We will see how some savvy simplifications can help to overcome common issues such as Web scale and Web content noise, and finally deliver acceptable usability and value for non-specialists, business users.
Recruiting a right candidate for a particular job is a tedious process as shortlisting few applications among thousands involves huge man power. Once the candidates are short listed then candidates have to go through different levels of interviews. Now a day's companies relay on third party for conducting the initial round of interviews. This paper aims to automate such interview process to help in recruitment with the help of robotic process automation and machine learning techniques. RPA bot monitors the mail and the file storage, if any new mail or file are received, the attachment with respect to the mails are downloaded and document classification is done on the attachments to find if they are resumes, text is extracted from the attachments, Name Entity Recognition (NER) is applied to get the important details in the attachments and the resumes are short listed based on the skills, experience and the education background. The precision value for text classification done with Naive Bayes Classifier is 92.08%, for document classification with 150k Bag of Words on BERT is 91.36%, for custom trained NER is 86.2% and finally for web scranning is 91.56%.
A semantic network is a graphical notation, for representing knowledge in form of interconnected nodes and arcs. In this paper we propose a novel approach to construct a semantic graph from a text document. Our approach considers all the nouns of a document and builds a semantic graph, such that it represents entire document. We think that our graph captures many properties of the text documents and can be used for different application in the field of text mining and NLP, such as keyword extraction and to know the nature of the document. Our approach to construct a semantic graph is independent of any language. We performed an experimental analysis to validate our results to extract keywords of document and to derive nature of graph. We present the experimental result on construction of graph on FIRE data set and present its application for keyword extraction and commenting on the nature of document.
Instagram is one of the popular social media applications used by a wide range of people around the world. The significant growth of active Instagram users affects the size of Instagram data. The more number of users, the larger and more various Instagram data is posted. In line with its popularity, in recent years many researchers begin to study and analyze it for various purposes, such as detecting event photos based on location, clustering the photo content, advertising strategies based on user types, and so on. As of now there are three types of data available in Instagram which are text, image, and video. In this paper we propose Term-Frequency and Inverse Document Frequency (TF-IDF) method to rank keywords of top twenty most followed Instagram users based on image captions of Instagram. The objective of this research is to automatically know the main idea of Instagram users based on 50 recent image captions posted. In our experiments, TF-IDF has been successfully implemented to reveal a set of keywords with its ranking. The highest ranking of keyword is indeed the main topic of a user, indicated by the value of TF-IDF. The result of study indicates that TF-IDF method is very useful to find and rank the keywords of Instagram users image captions. In the future research, the ranking keywords are needed in solving classification and clustering tasks as feature extractions.
This paper describes an approach to constructing sentiment lexicons in the financial domain. The approach takes advantages of news headlines and a given financial variable, such as stock prices, so as to generate candidates of sentiment expressions by fusing the two data resources. The candidates are then filtered based on their co-occurrences with financial seed words and are subsequently expanded by analogical reasoning by using distributed representation of words. Evaluative experiments on real-world news and stock price data show that the resulting lexicons are mostly reasonable and capture the characteristics of the target financial variables.
The method of extracting information from a text is called text mining. In contrast, sentiment analysis involves examining text data to determine whether a statement expresses good or negative conversations. In this study, a system was developed that categorizes tweets about West Bandung Regency tourism spots that contain both favorable and negative remarks. The Cross Industry Standard Process for Data Mining (CRISP-DM) technique is used to carry out the classification process utilizing the Naive Bayes algorithm. You will learn how locals feel about West Bandung Regency’s tourist attractions through this method. Six scenarios were developed using the categorization findings from 250 pieces of data that included both positive and negative sentiment. The scenario has got the greatest f1-score of 82.8%. These findings demonstrate how effectively the Naive Bayes algorithm performs sentiment analysis.
Graph-based text representation is one of the important preprocessing steps in data and text mining, Natural Language Processing (NLP), and information retrieval approaches. The graph-based methods focus on how to represent text documents in the shape of a graph to exploit the best features of their characteristics. This study reviews and lists the advantages and disadvantages of such methods employed or developed in graph-based text representations. The literature shows that some of the proposed graph-based methods suffer from a lack of representing texts in certain situations. Currently, several techniques are commonly used in graph-based text representation. However, there are still some weaknesses and shortages in these techniques and tools that significantly affect the success of graph representation and graph matching. In this review, we conduct an inclusive survey of the state of the art in graph-based text representation and learning. We provide a formal description of the problem of graph-based text representation and introduce some basic concepts. More significantly, this study proposes a new taxonomy of graph-based text representation, categorizing the existing studies based on representation characteristics and scheme techniques. In terms of the representation scheme taxonomy, we introduce four main types of conceptual graph schemes and summarize the challenges faced in each scheme. The main issues of graph representation, such as research topics and the sub-taxonomy of graph models for web documents, are introduced and categorized. This research also covers some tasks of understanding natural language processing (NLP) that depend on different types of graph structures. In addition, the graph matching taxonomy implements three main categories based on the matching approach, including structural-, semantic-, and similarity-based approaches. Moreover, a deep comparison of these approaches is discussed and reported in terms of methods and tools, the concept... (Show More)
Sentence embeddings have become an essential part of today's natural language processing (NLP) systems, especially together advanced deep learning methods. Although pre-trained sentence encoders are available in the general domain, none exists for biomedical texts to date. In this work, we introduce BioSentVec: the first open set of sentence embeddings trained with over 30 million documents from both scholarly articles in PubMed and clinical notes in the MIMICIII Clinical Database. We evaluate BioSentVec embeddings in two sentence pair similarity tasks in different biomedical text genres. Our benchmarking results demonstrate that the BioSentVec embeddings can better capture sentence semantics compared to the other competitive alternatives and achieve state-of-the-art performance in both tasks. We expect BioSentVec to facilitate the research and development in biomedical text mining and to complement the existing resources in biomedical word embeddings. The embeddings are publicly available at https://github.com/ncbi-nlp/BioSentVec.
The bootstrapping method is known as an application of the Page-rank technique for documents and words. The technique calculates the score of the words by mutually propagating the score of the words and the documents. However, sometimes the result is far away from the initial query word. The problem is known as 'topic drift'. This paper proposes to restrict the words to be to the top t words in the process of bootstrapping. The method is simpler than the technique known so far. The method is applied for the real bankruptcy information documents to extract the bankruptcy causes strongly related to the query. It is confirmed that the method prevents the topic drift.
Text mining is one of the main and typical tasks of machine learning (ML). Authorship identification (AI) is a standard research subject in text mining and natural language processing (NLP) that has undergone a remarkable evolution these last years. We need to identify/determine the actual author of anonymous texts given on the basis of a set of writing samples. Standard text classification often focuses on many handcrafted features such as dictionaries, knowledge bases, and different stylometric characteristics, which often leads to remarkable dimensionality. Unlike traditional approaches, this paper suggests an authorship identification approach based on automatic feature engineering using word2vec word embeddings, taking into account each author's writing style. This system includes two learning phases, the first stage aims to generate the semantic representation of each author by using word2vec to learn and extract the most relevant characteristics of the raw document. The second stage is to apply the multilayer perceptron (MLP) classifier to fix the classification rules using the backpropagation learning algorithm. Experiments show that MLP classifier with word2vec model earns an accuracy of 95.83% for an English corpus, suggesting that the word2vec word embedding model can evidently enhance the identification accuracy compared to other classical models such as n-gram frequencies and bag of words.
Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.
This study analyzes affective expression in dream log by text mining, guide participants focusing on the affective words in their dream log to release their emotions. This study provided a new method for exploring the correlation between dream and stress in psychology research area, and improved the application of knowledge management by text mining for dream log. The results show that teacher or counselor can improve their consultation by feeling empathy with the affective words in the dream log those emotions be ignored in previously consultation but picked from dream log by artificial intelligence.
Recent years have witnessed the ability to gather an enormous amount of data in a large number of domains. Also in the field of business process management, there exists an urgent need to beneficially use these data to retrieve actionable knowledge about the actual way of working in the context of a certain business process. The research field concerned is process mining, which can be defined as a whole family of analysis techniques for extracting knowledge from information system event logs. In this paper, we present a solution strategy to leverage traditional process discovery techniques in the flexible environment of incident management processes. In such environments, it is typically observed that single model discovery techniques are incapable of dealing with the large number of different types of execution traces. Accordingly, we propose a combination of trace clustering and text mining to enhance process discovery techniques with the purpose of retrieving more useful insights from process data.
Literature information is an important content implicit in network. Focus on the lack of necessary ability of traditional search methods in literature information retrieval on the semantic comprehension, this paper combined with the existing web text mining, proposed a new information retrieval method using ontology concept for vector space model to integrate literature documents semantic information, and an appropriate evaluation methods is given. Application demonstrates the feasibility and effectiveness of this algorithm.
In the process of analyzing the document images, the text separation from the background plays a vital role. If the quality of document image is good, then text can be separated very easily by applying simple techniques of thresholding. Whereas, in noisy images it requires very accurate analysis. Various thresholding techniques that are available are efficient and produce better results, but they are very slow in that they require threshold calculation for each and every pixel. In this paper, we overcome this problem by proposing an Otsu's method in which it follows a recursive method for segmenting an image. The experimental results show that the proposed Otsu's approach is able to extract the text very effectively from its background even if the input is a noisy image.
Taxonomy mining plays an important role for organizing and structuring of data in Content Management Systems (CMS). In this paper, we propose a novel approach that leverages multidimensional knowledge representation (MKR) for taxonomy mining from text documents and enriching the extracted information via Large Language Model (LLM). The data originates from a Smart City project in Germany, which addresses housing, care and health for elderly people. The applied method involves the extraction of relevant keywords from text and the utilization of the MKR framework to analyze and represent the information. Results are provided for a context builder that utilizes GPT-4 to enrich the taxonomy. The enriched taxonomy is then used in a WordPress CMS for information search, structuring and tagging of the blog entries accordingly.
Developing a secure software is time consuming and a complex activity. The main source of insecurity is vulnerabilities in the software. So the prediction of software vulnerability plays important role in software engineering, especially in web application development. A software vulnerability prediction model forecasts whether a software component is vulnerable or not. This paper describes various software vulnerability prediction models. Mainly two types of software vulnerability models are used to predict the vulnerability component in software. In software metrics based prediction model, different software metrics are used as an indicator of software vulnerability. In text analysis based method, source code of the software is used as input to the prediction model. Source code is converted into tokens and frequencies. These are used to predict the vulnerability.
This paper presents classification of DRDO tender documents into predefined categories. Since there is a consistent growth in the volume of digital documents, both on the internet and within organizations, the need to classify them into categories is obvious. In this paper we used `bag-of-words' technique to represent the tender documents. The dataset was prepared and fed into Weka toolkit. Classification was implemented by Naïve Bayes classifier using 10-folds cross validation technique. The machine resulted in classifying the tender documents with an accuracy of 77.36% by technology category and 67.2% by lab's name.
In this paper we design a system that adopts a novel approach for emotional classification from human dialogue based on text and speech context. Our main objective is to boost the accuracy of speech emotional classification by accounting for the features extracted from the spoken text. The proposed system concatenates text and speech features and feeds them as one input to the classifier. The work builds on past research on music mood classification based on the combination of lyrics and audio features. The innovation in our approach is in the specific application of text and speech fusion for emotion classification and in the choice of features, Furthermore, in the absence of benchmark data, a dataset of movie quotes was developed for testing of emotional classification and future benchmarking. The comparison of the results obtained in each case shows that the hybrid text-speech approach achieves better accuracy than speech or text mining alone.
This paper describes a solution for the improvement and support of decision-making processes in a reasonable time. It proposes and implements a suitable procedure based on appropriate methods and technologies to create a Web content mining application focused on the trend analysis. The following text describes used architecture, methods and data structures in the context of Web mining and the decision-making approaches. The resulted code runs as a timed script that handles different analyses over the selected Web sites - e.g. the most covered topics in the mass media servers, contexts of the top words based on the distance, saving outputs into the graph, etc. Authors are able to compare options and make decisions very fast, without reading the full contents of the sources. Object-oriented approach is used for the code writing so the code can be easily extended to handle number of different tasks (financial market decisions, crisis management, customer segmentation, social media monitoring, etc.). Real-time analysis of data from different Internet sources and a graphical visualization of results belong to the most important contributions of the proposed application.
Crime against women in India is on increase over the past few years and enormous crime reports are being generated everyday. But it is difficult to manually access the crime reports to derive useful information that can provide insights to the law enforcement officers for analysing the crime trends. The present work emphasizes on a simple yet efficient two stage approach for analysing crime against women in India. Initially, the proposed framework extracts crime reports from online newspaper articles. Once the data is collected, the first stage approach provides an interesting aspect by identifying named entities like name of states, cities, person etc. from the dataset and a collection of top ten entities of various categories is ranked according to their frequency of occurrence. The preliminary assessment shows feasible results which are also compared with crime records drawn from National Crime Records Bureau. However, the identified subtypes of entities are mostly ignored whereas dealing only with the basic entities fails to provide in-depth recognition of crime trends. So considering the subtypes can really provide the prerequisites for finer distinction in the field of crime data mining. The second stage approach in the present work considers the sub-types of named entities as ‘Modus Operandi’ features (mode of operation) of the crime that caters exquisite perception of the crime performed against women in India. Though lot of research exists on crime analysis, considering modus operandi features is very less. The present work demonstrates the effectiveness of the method with high recall and precision for the identified named entities.
In many governments and private institutions, one of the major tasks is to select the best project proposals for allocating the fund. These funding organizations select the proposals by submitting them to the reviewers for review. Manual process is too difficult when the number of projects is more. The earlier models introduced ontology based Text mining methods to cluster the proposals of any language without considering the reviewer's expertise with respect to their domain. The proposed method identifies the main topic of project in a hasty manner by using ontology based topic identification algorithm. It uses EM algorithm to group the proposals based on their domain, issues and technology for selecting the expertise in the domain for review. This approach gives better performance by allocating proposal to the appropriate reviewers.
Data mining techniques have been applied to various sectors in other to get information and knowledge to support major decision-making. With the emergence of new technologies that can harness data associated with education and the rapid growth in educational data, Data mining can be applied to a wide variety of applications in the educational sector for the purpose of improving the performance of students as well as the status of the educational institutions. Educational Data Mining (EDM) is a major research field that uses data mining techniques, machine learning algorithms and statistical techniques to basically helps the tutors modify their teaching strategies if the results with the current teaching model are not satisfactory. The potential influence of data mining analytics on the students' learning processes and outcomes has been realized Hence, a comprehensive review of educational data mining (EDM) and learning analytics (LA) in education was conducted. In this paper, we conducted an in-depth literature review on the use of EDM for analyzing the performance of students.
This research study ontology development by analyzing 6 months of customer comments on Dianping.com. The text analysis results are to get 1028 customer comments about independent coffee shops in Kunming. Among these, there are 183 customer comments about service quality. Based on these comments about service quality, there are 45 keywords from the analysis. Then the sentiment analysis is performed on the customer comments. The calculated sentiment scores of customer comments are 64 and −162, and the number of negative customer comments is 103, with negative sentiment accounting for 56.28% of the total 183 comments. It defines whether the customer comments are positive or negative. Furthermore, it shows that customers are dissatisfied with the service of independent coffee shops in Kunming. Therefore, this research finds that independent coffee shops in Kunming lack good service quality. In order to improve this situation, this research extends the ontology related to service quality using an ontological model to obtain an ontological model combining domain ontology and affective orientation related to service quality information in coffee shops based on the keywords and affective orientation in the comments. It allows managers to understand more intuitively and the relationship between each knowledge related to service quality in independent coffee shops from the models and find the critical points of service quality problems and improve them.
This paper proposes a method to find association rules for infrequent items. Despite the long history of association rule mining, infrequent items have been usually ignored. Recently, owing to the online nature of most systems, tackling infrequent items has become increasingly important to find emerging information. The proposed method not only has a sound theoretical background but is an exact solution of error minimization. Although highly similar to the standard method, Apriori, the solution uses a different formula than Apriori. Moreover, it consistently outperforms Apriori.
The main stakeholders in oral health management process are living people (seikatsu-sya) and dentists, the mutual awareness and understanding of them are supposed to be the indispensable basements to improve the quality of oral health treatments for both of living people and dentists. This study aims to examine the similarity and difference of a view of oral health and oral risk cognition between the main stakeholders with interview data in order to obtain a clue for generating their mutual awareness and understanding. Focusing on the method of narrative analysis, the authors discuss on the specifically process, the result, and the significance in this study. It was conducted interviews with five living people and five dentists analyzed their narratives by Key Graph which is a text mining tool. And then we arranged key concepts by KJ method, or an Affinity Diagram, which is a brainstorming tool which organizes a large number of ideas into their natural relationships. Through this analysis, the authors arranged the similarity and difference between Japanese living people's view of oral health and oral risk cognition, and those of dentists in a table. It is observed from the table that oral risk cognitions between Japanese living people and dentists have some difference, on the other side Japanese living people's super ordinate view of oral health and that of dentists are almost the same. This study revealed that it is desirable to apply the collaboration of Key Graph and KJ Method, such as the mechanical processing by the text-mining and the brainstorming.
In this paper, we propose a knowledge acquisition method for non-task-oriented dialogue systems. Such dialogue systems need a wide variety of knowledge for generating appropriate and sophisticated responses. However, constructing such knowledge is costly. To solve this problem, we focus on a relation about each tweet and the posted time. First, we extract event words, such as verbs, from tweets. Second, we generate frequency distribution for five different time divisions: e.g., a monthly basis. Then, we remove burst words on the basis of variance for obtaining refined distributions. We checked high ranked words in each time division. As a result, we obtained not only common sense things such as “sleep” in night but also interesting activities such as “recruit” in April and May (April is the beginning of the recruitment process for the new year in Japan.) and “raise the spirits/plow into” around 9 AM for inspiring oneself at the beginning of his/her work of the day. In addition, the knowledge that our method extracts probably contributes to not only dialogue systems but also text mining and behavior analysis of data on social media and so on.
Competence analysis of service providers is of great importance for a knowledge-intensive crowdsourcing platform to examine service provider'(SPs) effectiveness and contribution and therefore to improve its management and operation efficiency and accuracy. As the information highway highly-developed, there are a huge amount of online crowdsourcing communities where talent workers can exchange experience and ideas, which enables competence analysis using text mining. In this paper, we developed a competence identification framework to analyze and recognize SPs' competence in online knowledge-intensive crowdsourcing (KIC) context. We firstly crawled the experience sharing articles, which contained excellent SPs' experiences and ideas about the efforts that should be made to be successful, from several Chinese online crowdsourcing communities and then text mining techniques were applied to analyze these unstructured texts. Each sentence in the corpora was tokenized into several words, after which the words were clustered as different topics using Latent Dirichlet Allocation (LDA) model based on their underlying semantics. Furthermore, based on the LDA outputs, we identified six clusters of crowdsourcing SPs' competence and thus constructed the competence system on the basis of Spencer's competence dictionary and human intervention. Finally, the descriptions of the competence system were presented.
Lyrics in songs have important roles to give identity and storyline of songs. Lyrics are also considered as one of influence factor for popularity of the songs. However, it is difficult to manually assess the topic from numerous songs especially Indonesian's songs which have high poetic level in the lyrics. Knowing the intrinsic value of lyrics from numerous songs becomes a challenge, especially the lyrics written in complicated and complex language like Bahasa. This paper aims to know and interpret the topics from Indonesian's song lyrics. Indonesian's songs were obtained from daily TOP 200 Spotify in January 2017 – January 2018 with 193 different songs using Bahasa in the lyrics. Latent Dirichlet Allocation (LDA) for the topic modeling was used in this paper. Using 10 topics based on perplexity results, LDA has proper way to interpret the topics in numerous songs by giving information about top words in every topic and topic probabilities for each document or song.
The Automatic Text Summarization is most discussed area in Text Mining; there are various techniques available in text mining for text summarization. The two type of summarization are the extractive and abstractive text summarization. The main aim of text summarization is to obtain the concise meaningful text from the original text document. Keywords plays an important role in building a summarization text, there are several keyword extraction algorithms were proposed. In this paper, we implemented most popular keyword extraction algorithms the TF-IDF(a baseline algorithm), TextRank and RAKE algorithm. These keywords extraction algorithms were tested their effectiveness in finding important keywords from single document; the retrieved keywords are compared with the manually selected keywords. The comparison is performed to check the performance of each implemented algorithms with each other and with manually selected keywords.
Name entity recognition is the fundamental task in text mining area. This work focuses on the problems of multi-word and nested entity names. A combined approach is proposed for identifying multi-word and nested bio-entity names, which achieve an F-measure of 80.8% in extracting the total of bio-entity names and an F-measure of 82.2% aiming at nested entities. Experimental results show the combined approach is promising for developing text mining technology.