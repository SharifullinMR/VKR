This paper applies a fuzzy ontology framework to information retrieval system for enterprise management information system. The framework includes three parts: concepts, properties of concepts and values of properties, in which property's value can be either standard data type or linguistic values, i. e. fuzzy concepts. The fuzzy semantic query expansion is constructed by order relation, equivalence relation, inclusion relation and complement relation between fuzzy concepts defined in fuzzy linguistic variable ontology. The application to retrieve project information shows that the framework can overcome the localization of other fuzzy ontology's models, and this research facilitates the semantic retrieval of enterprise management information through fuzzy concepts on the Semantic Web.
The traditional information retrieval (IR) method mainly relies on the manual work of librarians, but with the development of computer technology and intelligent technology, the traditional IR method is gradually replaced by the inference mechanism of artificial intelligence (AI) by computers. Based on the analysis of the application of AI in IR system and the characteristics of the current era, this paper puts forward an Internet IR model based on deep learning. The simulation results show that the draw precision rate of this algorithm is 96.48% and the draw recall is 98.45%. The time consumption of this algorithm is lower than that of the two compared algorithms, which verifies the superiority of the proposed method in efficiency. Whether Internet IR can be fast and efficient depends on the intelligence of retrieval technology, and the retrieval function of Internet information can be greatly improved by using data mining technology. Driven by big data, the scientific classification of network information can be effectively realized by applying AI technology to network IR, and the corresponding information classification framework can be set by combining users’ network habits through keyword search.
Query Expansion is an important component for information retrieval systems. It makes possible the reformulation of the initial user query by adding new terms. In this paper, we propose a new approach for term selection in the relevance feedback process. This approach, based on Rocchio formula, is an adaptation to the XML information retrieval context. It can resolve two major problems specific to the XML information retrieval: the overlapping problem in the list of retrieved elements; and the problem of inclusion of irrelevant elements in the selection of new query terms.
In this paper, we design and implement a cross-media information retrieval system based on the area of food safety emergencies. The system collects Internet information using topic crawler, establishes data index on cross-media information and makes fast retrieval by sort labeling. The system supports image semantic retrieval and expansion retrieval based on Ontology. The cross-media retrieval provides a new technology for the research of emergencies field, and meets unique retrieval needs by the largest extend.
eXtensible Markup Language (XML) is widely used due to the characteristics of self-describing, scalable, and flexibility for representing different types of information in a single unified way. Information retrieval systems (IR) have become essential in finding relevant information within XML data. The growing volume of XML contents in many domains has motivated the research on how to retrieve XML data efficiently and effectively. XML information retrieval (XML-IR) systems main process is searching XML document fragments relevant to queries from a collection of XML documents. This paper aims at integrating IR and XML search techniques that could enable more sophisticated search on the structure as well as the content of the documents. This paper proposes an intelligent method of ranking XML documents with respect to an IR query by means of fuzzy logic. The proposed system deals with vague, uncertainty, and imprecise queries that allow it to be evaluated against an XML document collection. Experimental results show good performance of the proposed system compared to ADXPI system.
To capture the real intention of users’ needs more accurately from the increasingly abundant Mongolian information and return the retrieval results that best meet their needs, a Mongolian information retrieval method based on Word2vec and LDA topic model is proposed in this paper. Combining Mongolian grammatical features, this method builds a model based on LDA three-tier Bayesian structure to mine the hidden topic distribution and feature word distribution of documents, expands user queries according to Word2vec model to obtain words similar to user query keywords semantically, and then uses topic model to model extended vocabulary. Finally, according to the user’s query topic, the similarity between the query topic and the document topic is calculated, and the document with high relevance to the query topic is returned. The experimental results show that the effective combination of Word2vec and LDA model achieves better results than the traditional model with initial query in the representation of latent semantics.
With the appearance of semi-structured documents, such as XML documents, information retrieval has been challenging due to the introduction of the structural information known by his complex presentation. The system of information research must organize, store information and then provide the documents which correspond to user information needs. These systems are based on models of information retrieval and use similarity measures taking into account the structural and textual information. This paper presents a new similarity measure, inspired by that of CASIT model (in French: CAlcul de SImilarité Textuelle). It is adapted to semi-structured documents, specifically XML documents. This measure is used to calculate a rate of resemblance between a required XML document and each document of an XML database, by generating of interference wave presenting the existence and importance of the vocabulary of the required document in each document of database. Two important notions are used: the neighborhood that allows the valuation of terms and the path of tags followed to reach lexical units. This similarity measure has been exploited by a system of semi-structured information retrieval which we realized. We have used an experimental XML database and defined the time as criterion of evaluation. Consequently the running time is linear, which makes use of a huge database possible. Then tested in term of quality and answers relevance by the measure: recall / precision.
This article presents an ontology-based approach to designing and developing new representation IR system instead of conventional keyword-based approach. Such representation improves the precision and recall of document retrieval. Experiments carried out on the ontology-based approach and keyword-based approach demonstrates the effectiveness of the proposed approach.
In recent years the explosive growth of information affects the flood of information. The amount of information must be followed by the development of the effective Information Retrieval System (IRS) so that the information will be easily accessible and useful for the user. The source of Information contains various media format, beside text there is also image, audio, and video that called multimedia. A large number of multimedia information rise the Multimedia Information Retrieval System (MIRS). Most of MIRS today is monolithic or only using one media format like Google 1 for text search, tineye 2 for image search, youtube 3 for video search or 4shared 4 for music and audio search. There is a need of information in any kind of media, not only retrieve the document in text format, but also retrieve the document in an image, audio and video format at once from any kind media format of the query. This study reviews the evolution of IRS, regress from text-based to concept-based MIRS. Unified Multimedia Indexing technique is discussed along with Concept-based MIRS. This critical review concludes that the evolution of IRS follows three paces: content-based, context-based and concept-based. Each pace takes on indexing system and retrieval techniques to optimize information retrieved. The challenge is how to come up with a retrieval technique that can process unified MIRS in order to retrieve optimally the relevant document.
Information retrieval is usually referring to the text information retrieval, including information storage, organization, performance, many aspects, such as query, access and its core is the text indexing and retrieval of information. Under the trend of intelligent data analysis and mining, in this paper, we propose a novel information retrieval algorithm based on knowledge discovery and self-organizing feature map neural network. Knowledge discovery is one of the major intellectual activities of human, the current knowledge discovery activities are increasingly based on network data resources and environment. Enhance semantic correlation method, that is, on the basis of the existing association, found the correlation between the data source, a new connection between different sources of data, or further connection, this process is the process of knowledge discovery activities. For enhancement, we introduce the self-organizing feature map neural network into the method to integrate the semantic information. Since Kohonen self-organizing neural network is put forward, the self-organizing feature map algorithm as a kind of very effective clustering method, in the vector quantization and pattern recognition has been widely research and application. With the reasonable of the mentioned techniques, we propose the enhanced retrieval algorithm. The experimental simulation proves that our method obtains higher robustness and accuracy compared with the other state-of-the-art algorithms.
The explosion of digital data in the last two decades followed by the development of various types of data, including text, images, audio and video known as multimedia data. Multimedia Information Retrieval is required to search various type of media. There is comprehensive information need that can not be handled by the monolithic search engine like Google, Google Image, Youtube, or FindSounds. The shortcoming of search engine today related to their format or media is the dominance of text format, while the expected information could be an image, audio or video. Hence it is necessary to present multimedia format at the same time. This paper tries to design Unified Concept-based Multimedia Information Retrieval (UCpBMIR) technique to tackle those difficulties by using unified multimedia indexing. The indexing technique transforms the various of media with their features into text representation with the concept-based algorithm and put it into the concept detector. Learning model configures the concept detector to classify the multimedia object. The result of the concept detector process is placed in unified multimedia index database and waiting for the concept-based query to be matched into the Semantic Similarities with ontology. The ontology will provide the relationship between object representation of multimedia data. Due to indexing text, image, audio, and video respectively that naturally, they are heterogeneous, but conceptually they may have the relationship among them. From the preliminary result that multimedia document retrieved can be obtained through single query any format in order to retrieve all kind of multimedia format. Unified multimedia indexing technique with ontology will unify each format of multimedia.
Finding the valuable relevant information continues to be the major challenges of Information Retrieval Systems owing to the explosive growth of online web information. Among these challenges, we consider the XML Information Retrieval challenges as XML has become a de facto standard over the Web. In this paper, we tackle the issue of content-based XML information retrieval. We formulate the retrieval issue as a combinatorial optimization problem in order to generate the best set of relevant XML elements for a given keywords query. In our proposal, we define a genetic algorithm which maximizes similarity between a set of XML elements and the user query. The results based on the precision measure are very promising.
Novelty detection is already used in many Natural Processing Language (NLP) applications, such as information retrieval systems, Web search engines, text summarization, question answering systems …etc. This study aims to detect novel Arabic sentence level information patterns. The Length Adjusted (LA) model is based on sentence level information patterns is used, which depends on the sentence length. Test results show a significant improvement in the performance of novelty detection for Arabic texts in terms of precision at top ranks.
The challenge of medical information retrieval has attracted significant attention since the introduction of digital imaging in hospitals and other health facilities. Given the huge growth of medical imaging data produced in the past few years, studying new ways to index, process and retrieve medical images becomes an important subject to be addressed by the wider community of radiologists, scientists and engineers. At the moment, content-based image retrieval in medical imaging archives, although known to be beneficial to practitioners and researchers, is still rarely integrated into these archives. The aim of this paper is to present an overview of multimodal information retrieval for medical imaging studies, as well as the architecture of a solution for automatic medical image classification and retrieval using combinations of text and image queries. The complete solution was designed and implemented over an extensible open-source medical imaging archive software.
Research on cross-media topic analysis and information retrieval methods, which utilize semantic of multimedia data to describe cross-media documents. As the emerging of multimedia data on Internet, text based methods cause the problem of inadequacy of semantic, so a uniform semantic presentation on different media data is very important for information retrieval system. Cross-media topic analysis and information retrieval methods are proposed in this paper. Firstly, generative method and visual topic learning algorithm are used to construct visual topic model and map visual data to text topics. This method can solve the problem of consistent semantic description of cross-media data. On this basis, cross-media topic tagging and integrated search are achieved. Using the proposed method, a food safety information retrieval system is established and experiment results also show its effectiveness.
How to retrieve multimedia information effectively and efficiently is of great importance in information retrieval and computer vision. Therefore, in this paper, we aim to fully utilize the cross-modal hashing technology to solve the multimedia information retrieval problem. The main idea of this paper is to learn two different categories of hash functions for two different data modality, such as image modality and text modality. Then, the goal of this work is to learn several hashing functions, and then represent the samples to binary codes, and the proposed method generates hashing codes via connecting various modalities. In particular, we exploit the non-negative matrix factorization to learn a shared semantic space for different data modalities. Experimental results demonstrate that the proposed method can achieve higher accuracy of cross-domain multimedia information retrieval than other methods.
An Information Retrieval System is a system that is capable of storage, retrieval, and maintenance of an Information. In this context Information can be composed of text (including numeric and date data), images, audio, video and other multi-media objects. The TF-IDF weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. There exist various models for weighting terms of corpus documents and query terms. This work is carried out to analyze and evaluate the retrieval effectiveness of vector -- space model while using the new data set of FIRE 2011. The experiments were performed with TF-IDF and its variants. For all experiments and evaluation the open search engine, Terrier 3.5 was used. Our result shows that TF-IDF model gives the highest precision values with the new corpus dataset.
To solve the problem of low recall in traditional fast information retrieval methods, this paper proposes a fast retrieval method of massive library information based on data mining technology and supported by distributed open source framework Hadoop. According to the requirements of library resources retrieval, the system framework is designed, and the retrieval algorithm is compiled by using data mining technology. Then, under the system architecture, the implementation process of the system and the retrieval mechanism of library resources in the distributed environment are determined. The performance of the system is tested by multiple sets of experimental data. Compared with other similar retrieval systems, the retrieval efficiency of the designed system has been significantly improved, and with the increase of data volume, the improvement efficiency will be more obvious.
In order to improve the retrieval efficiency and accuracy of the student status file management system, this paper designs a student status file information retrieval system based on SSM framework. Based on the spring structure, the overall structure and frame of the management system are designed. The hardware system mainly includes information input module, information management module and information retrieval module. The software system uses the ontology semantic similarity method to calculate the feature vector and classification extraction threshold of the information of the student status file, realizes the classification extraction of the information of the student status file, and improves the efficiency and precision of the information retrieval of the student status file. The experimental results show that the designed system has high retrieval efficiency and precision, and can keep stable running state. The highest retrieval accuracy of the system can reach 96% .
The traditional information retrieval technology of vehicle navigation and positioning is very poor in real time. The result of retrieval often takes a long time to upload to the user, and the user can not make accurate judgment in time. In order to solve this problem, a new information retrieval technology of vehicle navigation and positioning is studied based on the environment of Internet of things. The work frame of the technology is analyzed, the working process of the technology is introduced, the focus on the real-time performance is studied, and the feasibility of the technology is verified by comparing with the traditional retrieval technology, and the result is compared with the result. It can be seen that the research technology can quickly retrieve vehicle navigation and positioning information in a short time, which has great market development space and is worth promoting.
The information retrieval system need to be upgraded constantly to meet the challenges posed by the advanced user queries as the search system becoming more sophisticated with time. These problems have been addressed extensively in recent times in several research communities to achieve quick and relevant outcome. One such approach is to augment the query where the automatic query expansion increases the precision in information retrieval even if it can cut down the results for some queries. Here, the above approach was tested with the present Urdu data collection obtained via different expansion models such as KL, Bo1 and Bo2. The current collection is quite large in size compared to other existing Urdu datasets. It comprises of 85,304 documents in a TRECschemes and 52 topics with their relevance assessment. In this paper we emphasize to enhance the retrieval model using the query expansion which is never done before on Urdu text. However, we show that a deep analysis of initial and expanded queries brings fascinating insights that could avail future research in the domain.
We explore whether the application of Blackboard Platform contributes to the improvement of the effects of Medical Information Retrieval teaching. Subjects for this study were 120 students majoring in Medicine in Hebei United University and they entered the university in 2010. They were divided into 2 groups according to the natural class (Group A and Group B). Each group included 60 students. The students in Group A were taught through traditional method and Blackboard Platform. The students in Group B were just taught through traditional method. After the study of a whole term, the teacher recorded and compared the final scores of the two groups and evaluated the students' satisfaction degrees of Medical Information Retrieval teaching. Results showed that Blackboard platform is an effective auxiliary teaching method in teaching Medical Information Retrieval. It could contribute to stimulating the students learning initiative, improving teaching efficiency and overall quality of teaching.
This article presents a novel information retrieval algorithms using genetic algorithm to increase the performance of information retrieval system. The novel matching functions called Overall Matching Function (OMF) and Virtual Center based Matching Function (VCF) are proposed for improving the retrieval performance. Overall Matching Function gives the results by finding the average of matching scores from classical matching functions and VCF is based on finding the virtual center from the set of centroids present in clustering space. VCF based Genetic Algorithm (VCGA) are used for information retrieval. Working of both the matching functions is compared to check the performance. We got promising results. This paper is presented as extension to our previous research papers in which the idea of GA based model for clustering and retrieval was proposed and the algorithms for VCF and VCGA was propose. The experimental results show improved values for precision and recall if retrieval is done using VCF and VCGA.
Searching for person names has been very popular among users of information systems and search engines. Thus, the effectiveness, accuracy and appropriateness of search results are strongly emphasized. Information retrieval (IR) methods provide high impact in influencing the searching results. Efforts in improving the IR methods have been made due to the fact that names are not unique and have varieties of spelling. This will caused errors during the process of getting accurate names. Searching based on phonetic is said to be a suitable method to solve the aforementioned problem because names have limited spelling standards. Phonetic method is used to recognize and retrieve words that have the same pronunciation. The main aim of this paper is to test the effectiveness of phonetic coding method on Malay name retrieval using Soundex and modified-Asoundex (Asoundex is an Arabic Soundex). The experimental approach used to perform this research consists of two stages; program development and testing Malay name data sets. The development of programs referred to the existing algorithms to generate name code. Code generated from program will be compared with data contained in the test data. The effectiveness of the result is determined by comparing the output with the result obtained from both phonetic approaches. Evaluation is based on the precision and recall measures. The contribution of the research is to provide comparative accuracy of Malay name retrieval using Soundex and modified-Asoundex coding method. Result show that an average of 38.38% improvement of the precision measure has been achieved.
Most of the knowledge intensive organizations are having their information resided in large text document repositories and most of these text repositories and databases are either unstructured or semi-structured. Recently various soft computing techniques have been used to improve information retrieval efficiency. More specifically genetic algorithms have been used for various information retrieval components like matching function learning, documents clustering, information extraction, query optimization [1 - 6]. In most of the cases in information retrieval matching function is based on term frequency. But the problem with this approach is that the syntactic information of the text document is lost and phrases are also not considered, so results in poor accuracy. In this paper we have proposed a new semantic based similarity measure in which each term can be a phrase or a single word and the weight assigned to each term is based on its semantic importance considering each sentence. We have used this semantic similarity measure along with other standard similarity measure as Jaccard and cosine to form the semantic-based-combined-similarity-measure. Standard genetic algorithm has been used to optimize the weight given for each similarity measure.
Vector based information retrieval system has been one of the trending methods in Natural Language Processing. The embeddings vector generated from a document helps in identifying most relevant document related to the query. There is various approach were embedding vectors can be generated and some of them which have implemented are Word2vec, Glove2vec and Sentence BERT. For information retrieval system also used word embedding transformation like PCA and Factor Analysis to improvise the model's performance. Most of information retrieval system involves getting query from the user, preprocessing of the query and generating most relevant information to the query. Results obtained by post processing methods such as PCA and Factor Analysis shows a comparatively better results with an increase of 2–3% of Mean average precision.
In this paper, a new indexing technique is proposed which retrieves the proximity of the keyword when exact match is not found. It is based on a signature graph along with n-ary tree which retrieves the relevant information when exact match does not exist. The index structure arranges signatures in aggregation hierarchy which leads to improvement of query evaluation. It also uses an n-ary tree data structure which is a hierarchy of keywords which is used to map in case of inexact match. As a proof of concept, a small application is built for a project manager to organize and retrieve resources efficiently. The application retrieves employee with required skill set and if such person is not available then it locates the closest match for the requirement. It also provides adequate resources to improve skill sets of the closest match selected. Thus, this system provides one point solution for the project manager.
We consider the problem of noisy private information retrieval (NPIR) from  N  non-communicating databases, each storing the same set of M messages. In this model, the answer strings are not returned through noiseless bit pipes, but rather through  noisy memoryless channels. We aim at characterizing the PIR capacity for this model as a function of the statistical information measures of the noisy channels such as entropy and mutual information. We derive a general upper bound for the retrieval rate in the form of a max-min optimization. We use the achievable schemes for the PIR problem under asymmetric traffic constraints and random coding arguments to derive a general lower bound for the retrieval rate. The upper and lower bounds match for  M=2  and M=3 , for any N , and any noisy channel. The lower and upper bounds show a separation between channel coding and retrieval scheme except for adapting the traffic ratio from the databases. We refer to this as  almost separation. Next, we consider the private information retrieval problem from multiple access channels (MAC-PIR). In MAC-PIR, the database responses reach the user through a multiple access channel (MAC) that mixes the responses together in a stochastic way. We show that for the additive MAC and the conjunction/disjunction MAC, channel coding and retrieval scheme are  inseparable unlike in NPIR. We show that the retrieval scheme depends on the properties of the MAC, in particular on the linearity aspect. For both cases, we provide schemes that achieve the full capacity without any loss due to the privacy constraint, which implies that the user can exploit the nature of the channel to improve privacy. Finally, we show that the full unconstrained capacity is not always attainable by determining the capacity of the selection channel.
With the rapid development and comprehensive popularization of information technology, in order to reduce resource consumption and environmental pollution, all walks of life have made efforts to promote the construction of information technology in response to the call of the state to achieve paperlessness. When more and more information data is accumulated at the PB level per second, usually we use a kind of information retrieval method to filter the files in the database that quickly and accurately search the data file which users care. This paper proposes a retrieval cluster structure based on distributed network design and performs unsupervised machine learning on the unstructured data content according to the classification learning model, so that the child nodes have the function of automatically identifying the semantics of the article, then the second inverted index is obtained by feedback learning results that it was based on the original index . The practical application results show that the design model is feasible, not only significantly improves the retrieval effect and the accuracy of data screening, but also provides strong support for subsequent big data analysis.
Most information seeking and retrieval systems are designed for individual use, and prototype collaborative systems are too limited to support use in heterogeneous environments. Easy Access to Digital Libraries (ezDL) attempts to bridge this gap by integrating familiar information sharing and management functions with communication tools into a flexible, scalable network in which individuals and groups function as nodes.
Design of visual platform for complex medical information retrieval considering complex information structure is studied in the paper. Before the software performs all operations, there must be a data structure that stores and describes the information transferred between the modules. The data transferred between modules is mainly the function information in the database. The function information mentioned here includes the function name, the return value of the function, the exit and entry parameter information of the function, and the description of the function in the database. From an application perspective, the semantic retrieval converts user query requirements into semantic concepts through semantic understanding and calculations, thereby retrieving the information related to this concept and the users really want, with this integration, the complex information analysis model is then designed. Through the testing on the different data sets, the performance of the model is then verified.
With the rapid development of artificial intelligence technology, traditional unimodal media information retrieval can hardly meet users’ retrieval needs for different data, therefore, cross-modal information retrieval is the key to breakthrough and solve the problem, however, the difficult problem faced by cross-modal information retrieval is how to effectively and accurately semantically match heterogeneous data and compare media objects of different modalities in different feature spaces. To address the difficult problem of cross-modal information retrieval, this paper proposes a cross-modal multimedia retrieval method with semi-supervised learning and category information alignment, which aligns different modal information by introducing category information to minimize the discriminative loss in the common representation space and the loss among individual modalities, and then constructs a homogeneous high-level semantic space based on the underlying feature space of different modal objects, and applies the semi-supervised learning method to cross-media retrieval. The semi-supervised learning method is applied to cross-media retrieval to achieve semantic matching of multimedia objects of different modalities. Finally, by comparing this method with traditional cross-media retrieval methods, the experimental results show that this method has greatly improved the accuracy of retrieval results compared with traditional cross-media retrieval techniques.
With the growing use of networked solutions and services in the Information Society, many citizen-oriented services are planned not only in utility or e-governance sectors, but also in other information-centric human applications. Conventional DBMS may have either two-tier or three-tier client/server architecture [19]. These are already being used on the Internet. Web Applications provide Network Interfaces to access information stored in one or more databases. The generic method of organizing and formatting these Network Interfaces is through the hyperlink documents, which are coded in languages like HTML or XML [3]. Further, the proliferation of Data Mining and Data Warehousing practices has necessitated the need for efficient e-Document retrieval, creating a requirement for robust Information Retrieval (IR) systems. In this paper we show that rough-set theory characterizes new and interesting viewpoint for electronics document (e-Documents) retrievals. We explore the characteristics of an e-Document and applicability of rough-set theory in detail.
Our paper discusses how active warden operates and why it is important for steganographers to understand the impending threat in which they possess. It was a common belief that the main adversary for steganography is coming from steganalysis detection. However, we have found in some situation, the destruction of hidden information is more easily achievable compared to the task of detecting it. Active Wardens are attackers of steganography which aims to demolish possible hidden information within a carrier media. If the enemy's objective is to disrupt the communication of hidden information, then the active approach is definitely a better choice compared to passive time consuming steganalysis.
Present days, large amount information stored in information sources, which is formally increased based on Knowledge Discovery from information warehouses with different formats of information. To acquire required and useful information from information sources, some of the techniques, methods and some of developed tools to combine large or high dimensional information sets. This procedure gives demand to implement novel research field i.e. information retrieval. The main task behind information retrieval is to extract required information from large size of information and change them into meaningful for further use in information retrieval. Classification and Grouping's are the main information retrieval approaches to classify and combine categorical information in a large set of information into required group set of class labels. So in this document, we provide comprehensive analysis of different classification and grouping methods in information retrieval to efficient information retrieval, which includes neural networks, Bayesian networks and decision trees. We also provide survey on some of semi supervised and supervised outlier detection techniques for categorical information on unlabeled information sets under large of instances in information sets with required instances in real time synthetic information. We bring out the keys aspects of different outlier and information retrieval approaches to information exploration.
The authors use the information visualization software - CiteSpace II to conduct the knowledge mapping based on the data of cross-language information retrieval(CLIR) downloaded from the Web of Science at Institute for Scientific Information (ISI), including SCI, SSCI, and A&HCI. The paper maps the co-citation network mapping to reveal the representative scientists and documents of CLIR. Then draws the term and keyword co-appearance network to analyze the research hotspots of CLIR, and concludes that query translation, query expansion and machine translation are the hot topics of CLIR. At last, we map the research front mapping to analyze the research fronts of CLIR and find that the research fronts and development trend are the methods, strategies, and evaluation of CLIR.
Online data generation is increasing rapidly due to technological development. In order to handle the huge volume of data, big data methodology is used widely. Though big data can able to process huge data, it has its own issues including information storage, data management, and retrieving data. In big data, data retrieval is an issue where the method of data retrieval and implementation environment differs. Though data mining has become evolving technology, it requires an optimal retrieval mechanism to extract data from the hugerepository.Therefore, an information retrieval method which in turn should effectively perform data retrieval in terms of both speed and accuracy need to be implemented. Topic relevance mechanism is used to retrieve an appropriate document from the repository, these documents are then ranked based on user preference using pseudo-relevance feedback mechanism. And hence the accuracy of the retrieved document is improved.
The objective of this study is evidence-based identification and characterization of the potential roles and applications of thesauri in Digital Information Retrieval Systems (DIRS). Contents of 77 relevant publications, indexed in Scopus and LISA, and 75 responses received from the administrators of open access repositories, registered with OpenDOAR have been analyzed. Apart from the potential roles of thesauri in information retrieval, i.e., indexing, searching and browsing, other potential applications of thesauri, like automatic keyphrase annotations, selective dissemination of information, ontology development and metadata harvesting, have also been identified. This paper aims to make an important contribution towards the design and practice of DIRSs by making cues for using the potentials of thesauri.
Literature information is an important content implicit in network. Focus on the lack of necessary ability of traditional search methods in literature information retrieval on the semantic comprehension, this paper combined with the existing web text mining, proposed a new information retrieval method using ontology concept for vector space model to integrate literature documents semantic information, and an appropriate evaluation methods is given. Application demonstrates the feasibility and effectiveness of this algorithm.
To achieve more accurate English Chinese cross language information retrieval, an English Chinese cross language information retrieval method based on association pattern mining is designed. Problem analysis of English Chinese cross language information retrieval based on association pattern mining. The main idea of problem analysis or problem understanding is to deal with the problem, extract the type and keywords of the problem according to certain means and methods, and provide basic support for the retrieval part. Keyword translation mainly adopts the combination of dictionary and network resource mining. Query word expansion is mainly to expand the semantic information of the original query words so that the original query contains more semantic information to increase the recall rate of document retrieval. The method used is a web search slice extension. We built an English Chinese cross language information retrieval model to realize English Chinese cross language information retrieval. The performance of the design method is tested. The test results show that the test results of each index of the method are ideal, which proves that the English-Chinese cross language information retrieval performance of the design method is good and can be applied.
The continuous growing datasets and the emergence terabyte-scale data pose great challenges to Information Retrieval (IR) systems. Tremendously, a large amount of data from various aspects is collected every day making the amount of raw data extremely large. As a result, indexing a large volume of data is a time-consuming problem. Therefore, efficient indexing of large collections is getting more challenging. MapReduce is a programming model for the computing of large document collections by distributing data and processing tasks over multiple computing machines. In this study, Solr and Terrier distributed indexing will be evaluated as they are the most popular information retrieval frameworks among researchers and enterprises. To be more specific, this paper will compare and analyze the distributed indexing performance over MapReduce for the indexing strategies of Solr and Terrier using 1GB, 3GB, 6GB, and 9GB datasets. In the experiments, the indexing average time, speedup, and throughput are observed as the number of machines involved in the experiments increases for both indexing frameworks. The experimental results show that Terrier is more efficient with large datasets in the presence of processing resource scalability. On the other hand, Solr performed better with small datasets using limited computing resources.
In this paper we describe an XML latent information retrieval model based on Dempster-Shafer theory and ontology that tries to find the latent information in XML documents in response to query. In this model, we utilize ontology to get the related instances in XML elements, propose two algorithms that use Dempster-Shafer theory of evidence to combine the related instances, which are in the same class within the ontology. Furthermore, we present the model's application in the detection of personal and enterprises' sensitive information in e-government domain. Experiments prove that the proposed model can retrieve latent information and has a higher performance than traditional methods.
This paper firstly studies the features of content-based multimedia information retrieval, then analyzes the system structure of content-based multimedia information retrieval and retrieval procedures, lastly the author discusses the exist limitations and prospects of content-based multimedia information retrieval.
With the evolution of internet, users are looking for documents that are accessible on demand. The growth in volume of stored information affects the performance of retrieval systems. In any information retrieval system, two main factors to be considered are document representation termed as indexing and retrieval. Most of the queries to an information retrieval system are casual, ordinary, bag of words that consists of two or three terms. In this paper, we implemented an index structure for fast phrase querying namely Word pair index using Terrier 3.5, a well-known open source for information retrieval. In our experiments, we used FIRE 2011 English data set. As we increased the size of corpus, time to create index structure has also increased linearly. When the word pair index structure is used for retrieval, the time to retrieve relevant results from the corpus reduced by 94.73%, when compared to traditional inverted index structure.
Intelligent information retrieval is an important area in computer application in the 21st century. In Tamil documents, Morphology (separating noun and verb) concept is used to retrieve the text. In this paper we use new approach on text processing in the information retrieval system. So we can widen our search criteria namely, Vowel - Kuril (Short), Nedil (Long); Consonant - Vallinam (Hard), Mellinam (Soft) and Idaiyinam (Medium). So it would not wait for the entire word to enter; perhaps the searching process starts immediately after the first letter is entered, because the Database table is segregated into 5 components rather than a single Database table. So to minimise the time constraint, memory space and to do a smart search a new IR system is introduced. In the proposed system, searches can be divided into three categorise, namely (i) Main topic search (ii) Subtitle search and (iii) Keyword search. So the system would search quickly and retrieve required information only. In addition to that, every poem is displayed with related pictures. So users will show more interest and desire to read those poems. In the classical system, the user should give the exact word to retrieve the information. But in the proposed system the misspelled word could be corrected and the information can be retrieved, because internally the system has its own spell checker. This would be useful for Tamil literates, Tamil students, Tamil scholars, etc.
In this paper, we propose mobile access to peer-reviewed medical information based on textual search and content-based visual image retrieval. Web-based interfaces designed for limited screen space were developed to query via web services a medical information retrieval engine optimizing the amount of data to be transferred in wireless form. Visual and textual retrieval engines with state-of-the-art performance were integrated. Results obtained show a good usability of the software. Future use in clinical environments has the potential of increasing quality of patient care through bedside access to the medical literature in context.
As the rapid development of information technology and agricultural applications, agricultural information has been widely distributed geographically in a region. It is observed that agricultural production and trading activities tend to be localized in many cases. Effective retrieval of geographically distributed information can facilitate agricultural production as well as trading. In this paper, an agricultural information retrieval approach is proposed for geographically distributed networks. The network structure of the unstructured peer-to-peer network is introduced. A topic-aware agent reputation model that combines individual- and social-level reputations is adopted to distinguish different information retrieval performance on various topics. A location-based network structure optimization method is proposed to gather geographically small distance agents together. A hybrid query routing approach is presented that combines topic-aware agent reputation scores and location-based ratings. Simulation experiments are conducted for the evaluation of the proposed approach. The results of the experiments show good performance of it.
Internet in today's times is the daily need of people. To retrieve right information efficiently is the constant desire. Expanding user queries by transforming some keywords to retrieve specific domain keywords has been a probable solution for information retrieval. Various methods have been combined with query expansion from time to time to improve the information retrieval results right from Classical IR methods to semantic methods or to natural language processing methods. All the methods have eventually minimized the mismatch problems and gave better retrieval results. This paper discusses the performance of various such methods that can be implemented to expand user query such that it gives high precision search results. The paper mainly focuses on word embeddings methods like Word2Vec - CBOW or Skip gram and Glove that are trained on real estate related legal datasets over classical methods. Experimental results show that word embeddings give better results with 87% mean average precision (mAP) values on all datasets.
Retrieval Systems have been developed in variety of domains, but due to information overload and its varieties in agriculture website, it is difficult to draw out the relevant information. Therefore, retrieval systems play an important role in filtering and customizing the desired information. Through the revision of specialized characteristic word weight and the revision of inquiry characteristic word weight, an improved vector space model (VSM) of retrieval systems in agricultural information field was proposed, which improved information retrieval performance both in precision and recall rates.
In recent years, information retrieval has been one of the vast area of research that focuses not only on a single mode of retrieval system but on multimodal (i.e. text, image, audio and video) information retrieval systems that binds multimodalities into a single repository. Multimodal retrieval system provides the features to search/retrieve information that are available in multiple formats. Systems have not reached the initial goal, i.e. to manage and search images in database we are unable to link the semantic sense of an image to numerical values. To meet the requirement as preprocessing step segmentation is used in Content Based Image Retrieval. In this context, we proposed statistical segmentation approach in this paper. After segmentation the features are extracted for the segmented images, which are helpful in understanding the contents of the image and retrieved the information.
With the rapid development of information technology, the volume of data has been explosively growing. The corresponding massive information storage, sharing and retrieval are focused on cloud computing. In this paper, cloud storage based on HDFS features was discussed. It is easy to scale up and down to provide an excellent platform for massive information management and application. The architecture of distributed information retrieval system was proposed, and three key problems were analyzed. Finally, according to P2P network characteristics, we point out that it is an effective tool for massive information sharing for real applications.
Information retrieval systems play the critical role of meeting information needs of users. Therefore, high effectiveness is expected from these systems since users make decisions based on the retrieval results. The effectiveness of these systems varies and is only known through retrieval evaluation and the main approach of evaluating these systems is the test collection model which comprises of a corpus of documents, topics and relevance judgments. One limitation of this approach is the cost of generating the relevance judgments. A recent solution to this limitation is the prediction of performance metrics at the high evaluation depths of documents using the system scores of other performance metrics computed at the low evaluation depths. However, this solution has a drawback of the inaccurate predictions of the non-cumulative discounted gain (nDCG) and the precision performance metrics at the high evaluation depths of documents using other performance metrics computed at the low evaluation depths. Therefore, this study addresses this drawback by proposing an approach that predicts the nDCG and precision performance metrics at the high evaluation depths of documents using topic scores of other performance metrics computed at the low evaluation depths. This study has shown that the proposed approach performs better predictions of the nDCG and precision performance metrics than the existing method.
The main goal of web Information Retrieval (IR) is to find highly relevant data for Internet users. Nowadays, huge billions of websites are populated day by day. The information growth is tremendous which is unbelievable. The current Information Retrieval (IR) system brings out very inadequate results because it is a keyword-based technique. The current IR system tools designed are very less capable to match exact document to the net users from the large web repository. Information ambiguity leads to poor information interconnection. It is impossible to handle many users and content to ensure trust at all levels. Hence to overcome these issues, in this novel proposes an Improve Firefly Heuristic Optimization Scheme (IFHOS) algorithm is used to identify the relevant document. The proposed first stage is pre-processing to remove stop words and tokenization etc. Then identify the web user interest using Density based Search Importance Rate (DSIR) technique. Later the proposed IFHOS technique identifies the appropriate information document from a user search. Thus, the proposed method achieves efficient performance than other comparative methods.
As the swift development of 3S (remote sensing, geography information system and global positioning system) technology and relevant information technologies, the demand for Geographic Information Retrieval has risen sharply. Based on user-centered principle and the problems in traditional geographic information retrieval results visualization, an improved scatter coordinate system was proposed. Firstly, the mapping relationship between the spatial entities and the coordinate point was established. Combining the spatial attributes and non-spatial attributes, the retrieval results were expressed graphically. Meanwhile, necessary interactive methods were provided to help users cognize and evaluate the results conveniently. Consequently, the objectives could be obtained more efficiently. Issues to be further studied and improved were discussed in the end.
Due to the relatively large demand for hospital case information retrieval and the high probability of concurrent requests, the information security is greatly threatened. Therefore, the design of hospital case information retrieval system based on improved AES algorithm is proposed. Take the ARK-1123h processor and ADK3390-DH21 power supply device that can meet the multi task Internet information processing as the hardware of the system. After normalizing the case information by using the linear function conversion, the gradient parameters are integrated into the AES algorithm to encrypt the information, and the corresponding information is displayed in combination with the user identity authentication results. The test results show that the designed system can effectively ensure the security of hospital case information under the condition of concurrent access.
Most of the existing textual information retrieval approaches depend on a lexical match between words in user's requests and words in target objects. Typically only objects that contain one or more common words with those in the user's query are returned as relevant. This lexical based retrieval model is far from ideal. In this research an approach to semantic based information retrieval of semantically annotated documents is presented. The approach operates based on: (i). natural language understanding, (ii). the Wordnet ontology, and (iii). the Semantic web standards. Not only the information is annotated and searched on a semantic basis, but also the retrieval process can be enhanced by the use of rich vocabulary knowledge in the ontology.
In these years, the number of biomedical articles has increased exponentially, which becomes a problem for biologists to capture all the needed information manually. Information retrieval technologies, as the core of search engines, can deal with the problem automatically, providing users with the needed information. However, it is a great challenge to apply these technologies directly for biomedical retrieval, because of the abundance of domain specific terminologies. To enhance biomedical retrieval, we propose a novel framework based on learning to rank. Learning to rank is a series of state-of-the-art information retrieval techniques, and has been proved effective in many information retrieval tasks. In the proposed framework, we attempt to tackle the problem of the abundance of terminologies by constructing ranking models, which focus on not only retrieving the most relevant documents, but also diversifying the searching results to increase the completeness of the resulting list for a given query. In the model training, we propose two novel document labeling strategies, and combine several traditional retrieval models as learning features. Besides, we also investigate the usefulness of different learning to rank approaches in our framework. Experimental results on TREC Genomics datasets demonstrate the effectiveness of our framework for biomedical information retrieval.
The goal of this research is to evaluate the use of English stop word lists in Latent Semantic Indexing (LSI)-based Information Retrieval (IR) systems with large text datasets. Literature claims that the use of such lists improves retrieval performance. Here, three different lists are compared: two were compiled by IR groups at the University of Glasgow and the University of Tennessee, and one is our own list developed at the University of Northern British Columbia. We also examine the case where stop words are not removed from the input dataset. Our research finds that using tailored stop word lists improves retrieval performance. On the other hand, using arbitrary (non-tailored) lists or not using any list reduces the retrieval performance of LSI-based IR systems with large text datasets.
The representation of document content is very important factors in retrieval process. The failure to create a good knowledge representation will definitely lead to failure in terms of its retrieval no matter how good the retrieval engine is. Therefore, this research focused on creating a reliable knowledge representation for our retrieval engine. We are using skolem to capture the information conveyed by multiple text documents and used skolem as an index language. This research also focuses on utilizing the skolem index as its knowledge representation in its question answering system. The system is capable of retrieving the answer as well as states the exact document in which the answer is derived from.
Regardless of the type of data set, it is frequently challenging to sift through the vast amount of data that is available on the Internet as a result of technological improvements. To deal with challenges mentioned above, we have come up with a ranking approach which is computed using NLP and vector space model. The approaches used for information retrieval start with a basic machine learning model and progress to multi-stage architectures and frameworks like language modelling and term matching. The main goal of this work is to use a standard retrieval process to glean insights from large amounts of data, which is the problem we are aiming to solve. The method utilised in the study is latent semantic analysis, which takes advantage of the semantic aspects at play and can be used to glean insights from lengthy texts.
Recently neural information retrieval systems have spurred many successful applications. Retrieval model to obtain a candidate document collection in the first retrieval stage, then use BERT to sort the candidate documents. Generally, the sentence score or paragraph score obtained using BERT is integrated into the document score to get the final ranking result. Semantic similarity is less often used to select query extensions and integrate semantic information into pseudo-relevance feedback. We propose a new strategy in this paper, selecting query extensions with semantic information using the BERT model. Incorporating semantic information weights into traditional pseudo-relevance feedback can better alleviate problems such as word polysemy and multi-word synonymy. Improve the performance of the retrieval system and return more accurate documents. The experimental results demonstrate that the query extensions selected by incorporating semantic information can help return more accurate results and improve the accuracy of the retrieval system, and the results of MAP and P@10 can prove the validity and feasibility of our proposed model.
Rich visual information is becoming increasingly important in today's Web, as evidenced by the popularity of social networks, the extensive use of video as a medium, and the compelling graphics and visual effects in movies and games. Here, the author examines the process of searching and retrieving images using a visual query--otherwise known as content-based image retrieval (CBIR).
The effective factographic information retrieval problem has been investigated in the paper. The primary objective of this paper is to outline factographic information retrieval and new Lemma related to properties of factographic information retrieval. The special curriculum has been developed for the set competences. The curriculum contains specialized disciplines. These disciplines use factographic information retrieval. Factographic information retrieval has effectiveness indicators. New Lemma about effectiveness and properties of factographic information retrieval was proved.
When the number of accessible sources of information is so much and even unlimited, the information retrieval process becomes very complicated. Visiting information resources one by one and comparing data or information from all the information sources visited will add much time to the process of rediscovering the information. It takes a technique that can gather information from multiple sources into a single entity to facilitate the process of information retrieval. This study uses 3 e-commerce websites as a source of information. By utilizing the crawling procedure will generate new variables that can store data from the source information, which then these data will be stored in a database. Web crawling works by taking HTML tags as needed, using scrapping techniques. Furthermore, by collecting the data in the database, the information retrieval process can be done easily, and by using the query, this process can be done in less time with 100% recall rate and 93.9 % precision rate.
This paper analyzes the deficiencies of traditional information retrieval and the advantages of visual information retrieval, studies the contents of visual network information retrieval, and describes a classic case on visual network information retrieval.
With the continuous development of biomedical, the scale of data has also continued to increase, which makes it difficult for researchers to extract information manually. In order to satisfy the researcher's information requirement, information retrieval techniques for the biomedical field have been resolved. Learning to rank is the field of machine learning and information retrieval combination, it has been shown to improve the retrieval efficiency to a large extent. This paper proposes a query optimization technology based on learning to rank. By using learning to rank for technical research on optimizing, a model combining query improvement and query expansion is proposed. Based on the improvement of the query, our method uses LTR to reorder the query expansion words, so that the original query can retrieve the document with higher accuracy. Only by biological field resources, the query expansion can obtain extended words, but it can not accurately describe the degree of relevance between extended words and query words. Introduce LTR methods can fully consider the relevance of extended words and original queries. Improve the inaccurate problem of single-extension. The experimental results show that compared with traditional algorithms, this algorithm improves retrieval efficiency by 3.31% on average.
As Internet technology has become a part of the lifestyle of the common man, research efforts are extensively made in the fields of Natural Language Processing (NLP) and Information Retrieval. Studying regional languages for developing the system to store, retrieve, extract the information from the database has gained lots of prominence nowadays. Case studies show that Ontological Information Retrieval has many advantages over keyword-based approach. In this paper we have focused on the general architecture of ontology-based Information Retrieval used for Kannada.
To get better performance, Some researchers have proposed relative work to exploit the position and proximity information of query terms in language model. However these models need large quantity of training data and its computation complexity is comparatively high. This paper presents an information retrieval model combining sentence level retrieval and use sentence as a unit to compute the relevant degree of the sentence to query. Experiment results show our model can get better performance than baseline models.
Bug localization involves the use of information about a bug to assist in locating sections of code that must be modified to fix the bug. Such a task can involve a considerable amount of time and effort on the part of software developers and/or maintainers. Recently, several automated bug localization techniques based on information retrieval (IR) models have been developed to speed the process of bug localization. Another code analysis technique involves locating duplicated sections of code in software projects, called code clones. We examine the application of code clone location techniques in the context of bug localization. We attempt to determine the advantages of extending existing code clone location techniques through the inclusion of IR models in the analysis process. We also examine a technique for extending the use of bug logging repositories and version control systems by analyzing the two using IR techniques.
In order to improve the effectiveness of semantic information retrieval, the improved fish swarm algorithm is proposed to carry out semantic information retrieval. Firstly, the system of semantic information retrieval is designed, and theory model of search engine is established. Secondly, the information retrieval model based on semantic similarity is constructed, and the mathematical model is deduced. Thirdly, the improved fish algorithm is established, and the analysis procedure of it is designed. Finally, the simulation analysis of semantic information retrieval is carried out, results show that the proposed model can obtain higher classification accuracy, precision rare and recall rate, therefore it has higher performance on semantic information retrieval.
Information retrieval is the process of searching through a database for material that meets a specific request. Due to the tremendous rise in popularity of knowledge discovery apps, end users are increasingly expected to craft sophisticated database search queries in order to access relevant data. Semantic information retrieval uses semantic analysis to return results that are more directly related to the user's original query. When this happens, ontology-based knowledge representation can be more efficient than other methods of representation like semantic networks and frames at facilitating semantic retrieval. Users in this category are required to understand not just the semantic connections between data as well as the structural complexity of complex databases. To get around these problems, Researchers have been focusing on enhancing the relation between data and search requests to provide outcomes that are more in line with users' research objectives to avoid these issues. As a result of this, ontologies are being used more and more for knowledge representation and interactive query creation. Considerations for ontology modelling, processing, and the translation of ontological knowledge into database search queries are all addressed in suggested ontology-based information retrieval strategy. Our findings show that the algorithm achieves state-of-the-art performance on a variety of measures of semantic information retrieval efficacy, including precision, recall, and f-measure. It has been shown through experiments that the algorithm speeds up the process of semantic information retrieval and enhances the system's capacity for expressing knowledge.
Mobile users have the capability of accessing information anywhere at anytime with the introduction of mobile web search. However, they still need to devote time and effort in order to retrieve relevant information in mobile devices. Conversely, context is proposed to reduce users time and effort. The recognition of context is supported by the availability of embedded sensors in mobile devices. In this study, the context acquisition and utilization for mobile information retrieval are proposed. The “just-in-time” approach is exploited in which the relevant information is retrieved without the user requesting it. This will reduce the mobile user's effort, time and interaction and present the relevant information to the user in the right time and at the right place.
In the paper, we propose a novel diversity-oriented biomedical information retrieval method based on supervised query expansion. Our method aims to obtain the most relevant and diversified terms to enrich user queries for better interpreting the information needs. We first propose a diversity-oriented labeling strategy to annotate the usefulness of candidate expansion terms. We then extract both the context-based and resource-based term features to represent terms as feature vectors. In model training, we propose a diversity-oriented group sampling method to modify the loss function of learning-to-rank for accurate biomedical term ranking. Experimental results on TREC Genomics datasets show that our method is effective in improving the performance of biomedical information retrieval in terms of both the relevance and the diversity.
Prompting diversity in ranking for information retrieval (IR) becomes an important topic in the past decade [2], [4] because of the increasing demand of personalization and disambiguation of user's queries. Beyond counting on relevance between documents and query, diversity IR takes consideration of relationship among documents in ranking order to promote diversity and reduce redundancy. To promote diversity means to provide various aspects of information in the ranking results list and to reduce redundancy aims to deduce repeatedly mentioned information. The application of diversity IR has drawn great attention and shown beneficial in previous studies when query turns out to be ambiguous, especially in the scenario of biomedical IR investigated in TREC 1 2006 and 2007 Genomics Tracks where biologists tend to query a certain type of entities covering different aspects that are related to the question, for example, genes, proteins, diseases, and mutations [1]. However, to the best of our knowledge, there is no learning-to-rank algorithm that processes the biomedical information retrieval in the perspective of addressing the domain specific features that may reflect the novelty of single document and the diversity of whole ranking list. We argue that it is promising to define and make use of diversity reflecting features to better model diversity information. Unlike previous studies, we tackle this problem in the learning-to-rank [3] perspective view. The main challenges are how to find salient features for biomedical data and how to tackle the problem of utilizing dynamic features with learning-to-rank technology. In this paper, we propose a novel approach to combine the dynamic diversified features with the learning-to-rank technology. Firstly we rank results using a general learning-to-rank model. Second, using Wikipedia, the topics of each retrieved results are detected which facilitate the generation of diversity-biased features. (Table I lists example of divers... (Show More)
Using the method of semantic ontology information fusion, this paper obtains the distribution fusion model of Chinese and English cross-language information from the perspective of systemic functional linguistics. A data distributed structure model of Chinese and English cross-language information under Systemic Functional Linguistics is constructed. Structural semantic hierarchical feature analysis method is used to fuse Chinese and English cross-language information under Systemic Functional Linguistics, feature extraction of Chinese and English cross-language information under Systemic Functional Linguistics is carried out in the reorganized feature space, and a fuzzy clustering model of Chinese and English cross-language information retrieval under Systemic Functional Linguistics is established by combining big data mining method. According to the semantic extension of verb-resultative phrases and dictionary learning results, the feature matching of Chinese and English cross-language data in personalized retrieval under the vision of Systemic Functional Linguistics is realized, and the Chinese and English cross-language information retrieval model under the vision of Systemic Functional Linguistics is established by adopting the method of grammatical attribute matching of semantic factors. Tests show that this method has higher accuracy and better matching in cross-language information retrieval between Chinese and English under the vision of Systemic Functional Linguistics.
With an ever-increasing data over the internet, efficient information retrieval of data for its users has always been on stake. The issue is deeper when it comes to e-governance based real estate scenario, where there is a lack of interoperability found in buying and selling of property formats and loads of miscellaneous legal terminology that fetches inappropriate legal documentation for the users. Hence in today's Semantic web scenario, a Real estate ontology has been proposed under the Real Estate Information Retrieval Model and the model is evaluated by applying various information retrieval measures. This paper shows the implementation and analysis of various popular information retrieval metrics, be it a set retrieval or rank retrieval that can be used for performance evaluation of web retrieval of data. The proposed system depicts better results in almost all the metrics as compared to the initial user query set.
This looks at targets to assess numerous automated fact retrieval strategies with appreciation for their potential to provide advanced data accessibility. The diverse methods might be evaluated by exploring how nicely they offer green statistics access, person-friendly systems, and the potential to handle massive datasets. The unique techniques include SEO (SEO), herbal language processing (NLP), content-primarily based retrieval, and disbursed databases. The impact of every method in the records accessibility procedure may be evaluated based on the extent of personal pleasure, typical performance, and scalability of the gadget. Sooner or later, the results can be used to offer pointers for improving data accessibility.
Recently, a novel form of web services had seen the light under the name of Cloud Computing which presents the dematerialisation of software, systems and infrastructures. However, in a world where digital information is everywhere, finding the desired information has become a crucial problem. In other hand, the users of cloud services starting asking about their privacy protection, particularly when they lose control of their data during the treatment and even some of them think about counting the service providers themselves as honest attackers. For that, new approaches had been published in every axis of the privacy preserving domain. One of these axis consists of a special retrieval models which allow both finding and hiding sensitive desired information at the same time. The substance of our work is a new system of private information retrieval protocol (PIR) composed of four steps the authentication to ensure the identification of authorised users. The encryption of stored documents by the server using the boosting algorithm based on the life of bees and multi-filter cryptosystems. The information retrieval step using a combination of distances by social bees where a document must pass through three dams controlled with three types of worker bees, the bee queen represents the query and the hive represents the class of relevant documents. Finally, a visualization step that permits the presentation of the results in graphical format understandable by humans as a 3D cube. Our objectives is to amend the response to users' demands.
A new method based on Latent Dirichlet Allocation (LDA) is proposed to retrieval information in Mongolian. Semantic information is also considered by Mongolian documents when consider relationship between keywords and retrieval documents. This method models Mongolian documents with LDA, parameters are estimated with Gibbs sampling and probability of word is represented, it can mine the hidden relationship between the different topics and the words from documents, get the topic distribution and compute the similarity of keywords topics. Finally, return to the most relevant documents with topics. Experimental results show that the method has a higher performance in topic semantic compared with vector space model and Language model.
Currently most of information is accessed from world wide web from many search engines. These search engine search information on the basis of keywords, and that keyword based search engine searches enormous amount of data. User takes more time to access relevant information. So, in order to overcome the limitation of keyword based search technic of conceptual search is done, i.e. search by meaning instead of search by literal string. Search engine interpret the meaning of user's query and the relations among the concepts that a document contains with respect to a particular domain. In this paper, we proposed Information Retrieval system in which, user enters a query and for that query meaningful concepts are Extracted using domain ontology, For all the terms (expanded and initial query terms), SPARQL query is formed and then it is fired on the knowledge base(ontology) that finds appropriate RDF triples in knowledge Base. In our proposed system we are converting the natural query to sparql query using quepy tool and creating the knowledge base system by using Protégé tool.
Traditional Chinese medicine (TCM) is the study of human physiology and pathology, and the prevention, diagnosis, and treatment of human diseases. The traditional Chinese medicine knowledge system is complex and its structure is huge. Therefore, it faces considerable obstacles in terms of knowledge sharing, exchanging and interfusing. Traditional information retrieval technology has made remarkable achievements in the Internet field. But information has defects of incomplete and high redundancy in the field of traditional Chinese medicine. Therefore constructing ontology-based knowledge base of traditional Chinese medicine domain is proposed and the information retrieval system based on ontology is developed. Firstly this paper introduces the present situation of information retrieval and expounds the relevant theories of ontology. Then the domain ontology of traditional Chinese medicinal construction is mainly introduced and the related techniques of information retrieval are analyzed and studied in detail, including query information processing, reasoning with the Jena, calculation of concepts correlation and sorting algorithms. Finally, with analysis and retrieval system of ancient texts of traditional Chinese medicine, the technology of information retrieval based on ontology is sufficiently validated.
This paper discusses the relationship between information retrieval (IR) and AI. Checking retrieval of texts, summarizes its key features and demonstrates the state of its art by introducing it one model that may have details, and other test results that show its value. The paper then analyzes this model and effective methods related to, focusing on and forgiving their weak use, unwanted representation and thinking. This paper describes some of the most effective ways uses intelligence-acquiring information retrieval (IR). Recovery of information is an important information management technology. It works together by searching for information and referencing, storing and categorizing information.
Current research usually adopts the standard process of Vector Space Model (VSM) in searching and retrieving information on Malay documents. However, this technique is less effective for semantic information retrieval from the collection. The system will only retrieve documents which contain the user's query terms and ignore semantic information among those terms. Therefore, several documents that have similar context are ignored and several document context that share a single term are retrieved. Due to this problem, Latent Dirichlet Allocation (LDA) model is applied for semantic information retrieval on Malay documents. An experiment was illustrated based on 6 queries text and 50 Hadith documents translated in Malay language, composed of Shahih Bukhari collections. Experimental results proved that the LDA model gives promising results in retrieving semantic information in Malay translated Hadith documents compare to existing techniques. Some limitation from this study can be explored for future work in order to improve the effectiveness of the retrieval results. Overall, LDA is an effective method for semantic information retrieval on Malay document, thus, it can help people to easily search and retrieve semantic information from Malay documents.
In Collaborative Information Retrieval, we construct formal concept lattices for a number of sub-text databases by the incremental update algorithms for generating the lattices algorithm. Then, respectively, we obtained the new formal concepts by merging the temporary formal concepts which are from the different lattices by measuring similarity between formal concepts and next, matching the new concepts to return the results which are to meet the user needs ultimately. The whole framework is easy to deploy in a distributed environment and match the query words in an imprecise way. It reflects the needs of humanity better.
Information retrieval system is taking an important role in current search engine which performs searching operation based on keywords which results in enormous amount of data available to the user, from which user cannot figure out the essential and most important information. This limitation may be overcome by a new web architecture known as semantic web which overcome the limitation of keyword based search technique called conceptual or semantic search technique. Natural language processing technique is mostly implemented in QA system for asking user's question and several steps are also followed for conversion of questions to query form for getting an exact answer. In conceptual search, search engine interprets the meaning of user's query and the relation among the concepts that documents contains with respect to a particular domain that produces specific answers instead of giving list of answers. In this paper, we proposed ontology based semantic information retrieval system and Jena semantic web framework in which, user enters an input query which is parsed by Standford Parser then triplet extraction algorithm is used. To all input query, SPARQL query is formed and then it is fired on the knowledge base (Ontology) that finds appropriate RDF triples in knowledge base and retrieve the relevant information using Jena framework.
The rapid development of technology resulted in the extensive information distribution. It takes a system that can organize a set of information and simplify information search. The concept of retrieval system can simplify information search process. Current studies on information searching are dominated by documents in English, yet very few about documents in Indonesian. Therefore this study focuses on document search in Indonesian language using vector space method. The vector space method processes data already in the index to calculate the value of similarity with the user-provided query. The result is a list of ranked and relevant documents to the user-provided query. The performance results obtained by the system were the average value of precision 0.45, recall 0.96, and f-measure 0.54. The system produced a high recall value compared to precision in the document search process. This study shows that the vector space method is able to search and produce relevant documents.
The basic principle of Classic traditional information retrieval model is the machine matching of the key word, namely retrieval based on keywords. This paper proposes a pre-clustering-based latent semantic analysis algorithm for document retrieval. The algorithm can solve the problem of time consuming computation of the similarity between the query vector and each text vector in the traditional latent semantic algorithm for document retrieval. It first clusters the documents using k-means clustering based on the latent semantic analysis, finds out the central point of each cluster, and then calculates the similarity between the query vector and each cluster's central points for retrieval. In view of the characteristics of document retrieval, it proposes a new method for calculating the feature weights and adopts the method of pre-clustering to preprocess document collection. The results of the experiment show that the new algorithm can reduce the search time, and improve the retrieval efficiency.
Qualitative geographic information retrieval (QGIR) is proposed in this paper. It is semantic matching oriented and based on the qualitative representation. Qualitative reasoning is used to get the relevance between documents and queries. Compared to the traditional geographic information retrieval (GIR), QGIR uses qualitative representation for both themes and place names. Semantic spatial relationships are taken into account. The qualitative representation is closer to human spatial recognition and can handle natural language better. A system is built to verify the qualitative theory and to compare the qualitative method with traditional GIR. The experiment in the mineral filed shows that QGIR can have better searching results and better fulfill people's geographic information retrieval request.
This paper conceptually proposes a context-aware recommendation system that gives/recommends optimal information for users based on 1) the content of an image using content-based image retrieval engines, 2) the contextual information of its similar images on the Web, and 3) user context, namely users' situations such as their location, we are aiming to extract the detailed information to the text-unpresentable images. It is expected to increase the precision of content-based image matching by combining the partial context obtained there from other types of data such as contextual information.
A variety of endeavors have been made to improve the performance of traditional information retrieval models in biomedical domain. However, majority of the studies have focused on improving the performance of individual information retrieval models, while few attempts have been made to the investigation of combining multiple information retrieval models and exploring their interactions in biomedical information retrieval area. In this study, a comprehensive performance evaluation of seven popular generic information retrieval models is conducted on a biomedical literature collection. In addition, an information fusion method called the Combinatorial Fusion Analysis is applied to perform extensive combinatorial experiments on these information retrieval models. Our experimental results have demonstrated that a combination of multiple information retrieval models can outperform a single model only if each of the individual models has different scoring and ranking behavior and relatively high performance.
With more information available in multiple languages, the need to search for relevant information is no longer fixated only on one language. Cross language information retrieval, a system that search for relevant information in different language, experienced a decrease in performance due to the loss of meanings during translation process or the initial query that was not descriptive of the information to be sought. One method to improve the performance of the information retrieval system relevance feedback query expansion. Another method utilizes external resources such as WordNet as the basis for generating query expansion term. This study utilizes the Wu-Palmer semantic similarity measurement in WordNet to improve the performance of pseudo relevance feedback query expansion. The terms contained in the feedback document considered as candidate expansion terms. Each candidate is given weight based on IDF score, Wu-Palmer similarity score, and document score using Okapi ranking function. Several candidate terms with the highest weight are used in query expansion. Experiment is conducted by comparing mean average precision value of query expansion based Rocchio feedback with query expansion based Wu-Palmer semantic similarity using NPL collection. The result show that the query expansion method of pseudo relevance feedback with WuPalmer semantic similarity can have better performance than the Rocchio pseudo relevance feedback for document feedback less than five. The highest mean average precision is obtained when two document feedback and ten expansion term is used with 0.5605 compared to Rocchio pseudo relevance feedback with only 0.4499.
The accumulation of Big Data and its constant growth determine the relevance and significance of the investigations in the field of basic domain description generation. The rapid development of network technologies and Internet contribute to the significant increase in available information resources and information about particular domain, which is often difficult for understanding. In order to increase the relevance of online searching, you should have the resource which gives a good impression of this subject. Such description can be represented by the created thesaurus of the investigated domain which is a tool for lexical system analysis and construction, terminological data classification and storage, information processing in search engines. Due to the thesaurus, a unified search image of the document and a search image of the user request is formed, contributing to the increase of the search processes efficiency in automated information systems.
This paper investigated the capability of reinforcement studying to optimize the retrieval velocity of big data from a database. Exclusive support for learning techniques and algorithms was explored, including Q-mastering, SARSA, and Deep Q-gaining knowledge. The paper supplied a case examination to assess the impact of the one-of-a-kind reinforcement gaining knowledge of algorithms at the retrieval velocity of a database. The results confirmed that the Q-learning approach yielded the best development inside the retrieval pace, reducing the time spent retrieving big data from the database. In addition, the paper counseled ability programs of the supplied reinforcement, gaining knowledge of strategies for numerous extensive statistics retrieval problems. The report concludes by presenting sensible advice and tips for incorporating reinforcement-gaining knowledge into optimizing considerable information retrieval speed.
Recent advances in the healthcare industry have led to an abundance of unstructured data, making it challenging to perform tasks such as efficient and accurate information retrieval at scale. Our work offers an all-in-one scalable solution for extracting and exploring complex information from large-scale research documents, which would otherwise be tedious. First, we briefly explain our knowledge synthesis process to extract helpful information from unstructured text data of research documents. Then, on top of the knowledge extracted from the documents, we perform complex information retrieval using three major components- Paragraph Retrieval, Triplet Retrieval from Knowledge Graphs, and Complex Question Answering (QA). These components combine lexical and semantic-based methods to retrieve paragraphs and triplets and perform faceted refinement for filtering these search results. The complexity of biomedical queries and documents necessitates using a QA system capable of handling queries more complex than factoid queries, which we evaluate qualitatively on the COVID-19 Open Research Dataset (CORD-19) to demonstrate the effectiveness and value-add.
This paper shows a proposal for a novel method for query information retrieval using one of signal processing techniques namely, the wavelet transform. We propose a wavelet transform based indexing as a new query information retrieval model. One demonstration example for the method of wavelet based indexing and the proposed method of wavelet based searching are shown. In addition, several disadvantages of traditional indexing technique and advantages of wavelet-based indexing and searching technique are presented.
Due to the increasing number of digital document repositories there is a heavy demand for information retrieval systems and therefore, information retrieval is still appearing as an emerging area of research. The information retrieval technology these days focuses on achieving better performance under different context by extracting documents most appropriate to the user’s query. Majority of the classical keyword based retrieval techniques does not focus on semantic meanings and therefore, are found to be less effective in reconstructing the actual information conveyed in the context. Also, retrieval of the relevant documents depends on appropriate analysis of the query terms. As words are polysemic, their actual meanings are influenced by their relationships with other words and their syntactic roles in the sentence. This work presents a fuzzy-cluster based semantic information retrieval model that considers these relationships to determine the exact meaning of the user query and extracts relevant documents as per their relevance scores.
With advances in information and communication technologies, vehicles on roads can cooperatively share and retrieve information in a distributed manner to support Intelligent Transportation Systems (ITS) services such as traffic management and infotainment services. A system is needed to retrieve information and data from moving vehicles and roadside facilities in an efficient manner. However, existing distributed vehicular systems rely on either vehicular ad hoc networks (VANETs) or application-layer peer-to-peer (P2P) protocols over an infrastructure-based wireless network suffering from low success rate or long latency in information retrieval. In this work, we propose a two-tier VANET/P2P system that integrates low-tier VANETs and a high-tier infrastructure-based P2P overlay. The proposed two-tier system aims to achieve high success rate, reduce latency, and minimize overhead for information retrieval in vehicular environments.
It is known that traditional precise ontology based information retrieval cannot finish implicit semantic information mining. To solve this problem, rough ontology is introduced into semantic information retrieval to meet the user needs to utmost extent. Firstly, semantic information retrieval is defined and its advantages are analyzed in detail. Secondly, rough ontology is employed to extend precise ontology. Thirdly, rough ontology based the semantic information retrieval model and its semantic similarity calculation method is given. Finally, a rough ontology based semantic information retrieval system named as ROSRS is designed and its implementation method is given. The experimental results show that system ROSRS can retrieval information semantically not only from precise ontology but also from rough ontology, and the implicit information can be gotten. Furthermore, the recall ratio and precision ratio of retrieval results can be improved.
English is an international language used for communication worldwide but still many cannot read, write, understand, or communicate in English. On the other hand, the World Wide Web has unlimited resources of information in different languages which English native find challenging to understand. To avoid such barriers, Cross-Language Information Retrieval (CLIR) systems are proposed, which refers to document retrieval tasks across different languages. This work focuses on the performance evaluation of different Information Retrieval (IR) models in CLIR system using Quran dataset. Furthermore, this work also investigated the length of query and query expansion models for effective retrieval. The results show that different length of queries has an impact on the performance of the retrieval methods in terms of effectiveness. Hence, after comprehensive experiments, an appropriate length of query for Arabic CLIR system is suggested along with the best query expansion and retrieval model.
Information on the dynamics of the deep-sea ecosystem is essential for conservation management. The marine soundscape has been considered as an acoustical sensing platform to investigate geophysical events, marine biodiversity, and human activities. However, analysis of the marine soundscape remains difficult because of the influence of simultaneous sound sources. In this study, we integrated machine learning-based information retrieval techniques to analyze the variability of the marine soundscape off northeastern Taiwan. A long-term spectral average was employed to visualize the long-duration recordings of the Marine Cable Hosted Observatory (MACHO). Biotic and abiotic soundscape components were separated by applying periodicity-coded nonnegative matrix factorization. Finally, various acoustic events were identified using k-means clustering. Our results show that the MACHO recordings of June 2012 contain multiple sound sources. Cetacean vocalizations, an unidentified biological chorus, environmental noise, and system noise can be accurately separated without an audio recognition database. Cetacean vocalizations were primarily detected at night, which is consistent with the detection results of two rule-based detectors. The unidentified biological chorus, ranging between 2 and 3 kHz, was primarily recorded between 7 p.m. and midnight during the studied period. On the basis of source separation, more acoustic events can be identified in the clustering result. The proposed information retrieval techniques effectively reduce the difficulty in the analysis of marine soundscape. The unsupervised approach of source separation and clustering can improve the investigation regarding the temporal behavior and spectral characteristics of different sound sources. Based on the findings in the present study, we believe that variability of the deep-sea ecosystem can be efficiently investigated by combining the soundscape information retrieval techniques and cabled hydrophone netw... (Show More)
In this paper we present a generic framework for ontology-based information retrieval. We focus on the recognition of semantic information extracted from data sources and the mapping of this knowledge into ontology. In order to achieve more scalability, we propose an approach for semantic indexing based on entity retrieval model. In addition, we have used ontology of public transportation domain in order to validate these proposals. Finally, we evaluated our system using ontology mapping and real world data sources. Experiments show that our framework can provide meaningful search results.
The World Wide Web easily becomes the largest repository of natural language text data. We are particularly interested in state-of-the-art methods in exploiting geospatial information the web. The survey is done in the context of its extraction methods, retrieval, visualization, and further possible mining or knowledge discovery scenarios in order to produce thematic maps automatically from the web corpus. We found that Web-based Geographic Information Retrieval (GIR) methods that returns selected relevant area instead of points is still lacking, even though area modeling is common in GIS. We also found that most GIR methods is still focused on places and buildings instead of theme or information around some area. Thus it indicates that the state of the art GIR methods are not yet sufficient for thematic extraction and retrieval to generate thematic maps from web corpus. Bayesian topic models such as Latent Dirichlet Allocation may serve as a good basis to serve such use cases.
The performance of information retrieval is dependent upon how effectively the documents can be ranked according to numeric similarity measure between the query and the document. The cosine and Jaccard are commonly used similarity measures. The authors have presented new similarity measure by combining Cosine and Jaccard similarity measures using fuzzy logic. The experiments are performed on CACM data collection. The proposed hybrid similarity measure gives better results than other two similarity measures.
The proliferation of information in cyberspace is increasing exponentially, leading to challenges for information retrieval systems to satisfy demands for performance and accuracy. How-ever, most existing works concentrate more on designing natural language processing (NLP) models than building such systems, which require massive efforts. In this study, we propose a modular framework for an information retrieval system consisting of several large-scale components capable of processing massive data. In addition, the proposed framework provides a high level of customization by assisting end-users in quickly replacing the NLP models to suit different contexts. This shortens the deployment from research to production of novel NLP models. The evaluation results of our prototype integrated with Vietnamese retrieval models show that the proposed framework is highly robust and scalable in big data contexts.
The information plays an inevitable role in the handling systems of numerical data existing today. The amount of information handled by various organizations in the world today is high and its management without the computer is no longer imaginable. Information Retrieval (IR) is a research area in computer science whose goal is to facilitate access to a set of documents in electronic form (corpus) and allow a user to find the relevant ones for him, that is to say, those whose content best matches the information needs of the user. We propose in this paper a system called “Neuro-RI” putting into practice our model of information retrieval based on neural networks with neighborhood.
Question and answer system, which is a full-text retrieval system based on dynamic knowledge library, is an important segment of remote education platform. Information retrieval technique is a key technology to make the success of Intelligent Question Answering System. The paper gave research and analysis on the inverted index technology and adopted the improved TFIDF weighting formula in order to improve the accuracy of retrieval explained; introduced a distributed multi-threading technology, cache technology in the retrieval system. Finally found the best program about the system, making it become intelligent interaction of the Question and Answering platform.
Multimedia information retrieval has been a challenging problem due to the diversity and size of multimedia data along with difficulty of expressing desired queries. This paper highlights key points of multimedia retrieval approaches that work. After providing discussion on the success of multimedia information retrieval, the paper analyzes the problem of retrieval challenge (i.e., the capability of retrieving every multimedia object) and proposes page-oriented precision as an alternative evaluation measure for the performance of multimedia information systems.
Legal professionals and law students often grapple with the arduous task of locating prior Supreme Court rulings for argument preparation and academic study, leading to a laborious and resource-intensive process. This study introduces an innovative approach to streamline the retrieval of legal information from Sri Lankan Supreme Court verdicts. The method centers on developing a Custom Named Entity Recognition (NER) model, boasting a remarkable accuracy exceeding 90%. This model efficiently extracts crucial legal entities from Supreme Court rulings, readily available on the Ministry of Justice's Supreme Court website. The high accuracy ensures precise entity extraction, followed by systematic organization within a dedicated database. Subsequently, a knowledge graph is formed by linking recorded legal entities, reducing information retrieval time to a mere 179 milliseconds, significantly outperforming existing methods. Moreover, a BART summarization model is crafted to generate concise, accurate, and insightful summaries of Supreme Court decisions, boasting an impressive ROGUE1 score of 85%. This approach revolutionizes legal information retrieval, delivering a user-friendly platform that enhances the identification of cases and fosters a deeper understanding, ultimately elevating the quality of legal research and practice.
With the development of technology, human computer interaction is continuously increasing. Parallel to this, information from web sites, social media, blogs and other applications reach enormous dimensions. It becomes a big problem to obtain the desired information from this mass of data. One way of solving this problem is to keep the information correctly indexed and searched by using information retrieval methods. Information retrieval is the study of finding documents of unstructured material which should satisfy users' information needs. Various term-weighting models have been proposed for information retrieval. This work is carried out to analyze and evaluate the retrieval effectiveness of recently developed term-weighting models (after the 2000s) using the earlier datasets (dating back as far as the 1980s) with the motivation of such comparison has not been done.The open source library Apache Lucene is used for all experiments and evaluation. As a result, we observe that the DFIC model is in general more effective than the other models. We note also that, although one model can be the most effective for one dataset, the same model can be the least effective for another dataset.
Quantum information methods are successfully used with classic methods in information retrieval. This paper reviews the formation of quantum concepts, their presentation and storage and comparison methods for algorithms of information retrieval. We survey the representation of quantum concepts with qubits and their comparison methods via quantum logic gates. The paper discusses fuzzy concepts ranking, relevance evaluation and machine learning methods to improve information retrieval.
Rapid advances in synthetic aperture radar (SAR) technologies have brought challenges in image interpretation toward the development of new Earth observation applications. Both novel scattering-imaging models and intelligent inversion techniques are required for advanced information retrieval and interpretation of high-resolution multidimension and multimode SAR data. As an example, scene reconstruction attempts to transfer the SAR image to human-understandable representation of man-made targets and natural environment. In this letter, a framework for scene reconstruction is outlined. It includes three key elements: a dictionary of parametric scatterer model, a method for scatterer recognition and parameter estimation, and a method for target reconstruction. A preliminary case is presented, where a simulated 3-D SAR image of a simple target is successfully reconstructed to a solid geometry. It uses a novel surface extension method to connect isolated scatterers to form a complete target geometry.
Cross Language Information Retrieval is a sub field of Information Retrieval that deals with retrieving relevant information stored in a language different from the language of user's given query. It plays a vital role in future because large volume of information is stored in the web is in English. So there is a necessity of proper mechanisms that can retrieve some required relevant information in a collection of information or in a database. Generally the collection of stored information may not be necessarily in one language. The simplest way to search for the information is to scan every item in the database and when the need to translate the languages being used arises, and then there will be a need of developing Cross Language Information Retrieval systems will take place. This paper reviews some of the recent research methods focus on topics in cross language information retrieval and there great role in On-going latest innovative research directions in wide area of information retrieval systems will take place.
In the new digital environment, digital literacy has gradually become the core skills of human survival. Since English teaching includes many modules, mobile English teaching platform is required to have the ability to deal with mixed information. At present, the existing mobile English teaching platform cannot deal with the mixed information of multiple modules, which leads to the problems of low precision and long time consuming. Based on the analysis of the problems existing in the construction of mobile English teaching information retrieval course and the requirements of the concept and ability of digital literacy, this paper constructs a multi-dimensional information retrieval teaching system from the perspective of digital literacy training from four aspects of teaching environment, teaching object, course content and teaching information. Experimental results show that the proposed system has ideal accuracy and running time.
Human beings spread technology through education, and information technology is now also rapidly promoting education. Combining with the information literacy standards of College students, we have reformed the teaching design of ESP and integrated information literacy into ESP teaching by using modern information technology. Training students to use modern tools for efficient reading and information retrieval, their ability to understand and summarize the original information, create and express new information has been significantly improved.
The traditional keyword-based information retrieval service can only return the results matching keywords according to the user’s query conditions, and the retrieval mechanism is relatively simple, leaving out a lot of information with similar content but different expressions. This paper proposes and implements an optimization method of network retrieval model based on big data algorithm. The model expanded by term similarity includes arcs that simulate the direct relationship between terms, which mainly expand the term subnet. According to the quantitative synonym relationship between terms, an extended BN (Bayesian network) retrieval model is established, and the retrieval results are presented to users in descending order of document relevance, thus realizing semantic-based information retrieval to some extent. The simulation results show that the average precision/recall ratio of the proposed retrieval strategy is higher than that of the traditional retrieval model, and the average precision ratio can reach 66.37%. Secondly, because the document is indexed offline, it is also better than the traditional retrieval model retrieval strategy in time consumption. Experiments show that compared with the traditional retrieval model, the proposed model has better retrieval performance. The model in this paper can expand the retrieval of related information without deviating from the user’s retrieval goal, and the ranking of related documents is more reasonable, and the quality of retrieval results is further improved.
The key for enabling smooth information sharing across channels is the efficient storage of the information in a language independent format so that it can be retrieved across many geographies without having multiple corpus. This paper explains how this complex requirement can be solved by using UNL. In the paper the architecture and implementation of the UNL interlingua based question answering system is given that is capable enough to handle user queries against a language-independent representation of document. Currently the system solves factoid based questions as where, what, why, which etc using english as source language but later can be extended to other languages once their enconvertors and deconvertors are available. The proposed model is tested on ELOSS documents achieving an accuracy of 82.22%.
With the progressive and availability of various search tools, interest in the evaluation of information retrieval based on user perspective has grown tremendously among researchers. The Information Retrieval System Evaluation is done through Cranfield-paradigm in which the test collections provide the foundation of the evaluation process. The test collections consist of a document corpus, topics, and a set of relevant judgments. The relevant judgments are the documents which retrieved from the test collections based on the topics. The accuracy of the evaluation process is based on the number of relevant documents in the relevance judgment set, called qrels. This paper presents a comprehensive study, which discusses the various ways to improve the number of relevant documents in the qrels to improve the quality of qrels and through that increase the accuracy of the evaluation process. Different ways in which each methodology was performed to retrieve more relevant documents were categorized, described, and analyzed, resulting in an inclusive flow of these methodologies.
The application of traditional information retrieval, based explicitly on vector space and probabilistic models, has been the basis of code search development. On the other hand, information-based and language-based models also need to be explored in information retrieval for code search systems. Simultaneously, further investigation into utilizing dense vector-based code search remains necessary. An investigation is required to assess the feasibility of constructing an application that uses information retrieval techniques for code search. This experiment uses a dataset from the CodeSearchNet Challenge as the basis for developing this code search. This study focuses on Java code snippets from the dataset. In the pre-processing step, this experiment uniquely extracts all code snippet identifiers. These extraction identifiers will become tokens to form a document in code search. The traditional information retrieval model application explores the TFIDF, BM25, Divergence from Randomness, Divergence from Independence, information-based models, and Language model methods. Based on their performance evaluation, approaches with probabilistic and information-based models provide slightly better performance than vector space models. However, the MRR-based performance rate of all traditional IR models is around 0.67. Meanwhile, the application of dense vectors in information retrieval uses the SentenceBERT model, which utilizes the all-MiniLM-L6-v2 pre-trained model. The performance evaluation for dense vector information retrieval uses K-NN ranking with K={5,10,15,20,25,50}. Retrieving the 10-nearest neighbors for vector distance between the query and source code delivers the best performance. Overall, the performance results of dense vectors provide better performance by about 8% than traditional information retrieval.
This paper reports a personalized model of an information retrieval system based on three-layer agent. This system includes a personalized user agent, an information retrieval agent, and an information filtering agent. It provides personalized servers for users through works of intelligent agents associated with each other.
The number of Web Users accessing the Internet becomes increasing day by day. Any kind of required information can be obtained anytime by anybody from the web. Information retrieval is the fact that there is vast amount of garbage that surrounds any useful information. Such information should be easily accessible and digestible. Internet is no longer monolingual and non-English content is growing rapidly. Speech is easy mode of communication for the people to interact with the computer, rather than using keyboard and mouse. This paper is new attempt to integrate speech recognition and cross language information retrieval system. Speech based queries retrieval performance is compatible with retrieval performance of text based queries. Both Speech recognition and Cross language information retrieval fields are very much challenging to integrate. On Forum for Information Retrieval Evaluation (FIRE) 2011 dataset speech and text query based Tamil- English Cross Language Information Retrieval (CLIR) system achieves 65% and 63% of monolingual retrieval. In future speech based CLIR system very much useful for visually challenged people.
In this paper, we investigate the problem of results merging in distributed information retrieval when overlapping databases are used. We focus on two issues: score normalization and weights assignment for each of the component results. Empirical study with the TREC data has the following three findings: 1. The cubic regression model and logistic regression model are better than the commonly used zero-one score normalization method, 2. The weighting scheme of uneven similarity is an effective method of weights assignment. 3. Score normalization and weights assignment can be used separately or together in a results merging method to improve effectiveness. The findings obtained in this paper are very useful for effectiveness improvement when implementing a distributed information retrieval system.
In this paper, we survey and classify most of the information retrieval (IR) approaches to Malay text in order to assess their benefits and limitations. We also summarized the information retrieval tools and related methods, in which ontology is a widely used tool for all countries' researchers. This research selects Malay language as the primary test collection because there are more issues in Malay languages, particularly those related to deep semantics, including the use of ontology. The traditional Malay retrieval system mostly focused on syntax extraction and keywords only. Mostly this technique will ignore the semantic element and the real meaning of query text and corpus which not fulfil the requirement of the user. Most of the previous study in information retrieval was using English and Arabic language as a test collection. Therefore, advance research is needed and it will be experimented in the future work. The finding of the paper will help other researchers discover the information and research gap regarding the Malay text.
The process by which information is retrieved from the repositories is called as information retrieval. This process had been a manual process since many centuries but since the advent of computers since the past few decades, it has become automated. Certain models form the basis of information retrieval in automated systems and are categorized as conventional and unconventional. This is because the motive of information retrieval systems is not only to render information to the regular users of internet but also, help the novice users of internet to skim out the documents that they need. In our paper, we have suggested that the information retrieval models must contain some questions regarding the user intent as well so that the search engine is able to search the websites that may prove to be informative for the users. These questions may enable the system to find out the user intent because entering one word may give varied results based upon the various uses to which it is put so that refining of requirements may be helpful in yielding appropriate results.
This paper proposes an "Information Orchestration System" that enables the closing of the information retrieval capability gap. In this proposed system, users can automatically pull up relevant information one after another just by tapping the desired information listed on screen devices. Users can also simultaneously distribute relevant information to different screen devices around users by tapping or motion operation. In this way, this proposed system realizes information orchestration in a way just like conducting a symphony orchestra.
This paper delivers the manipulation of the lingual perspective and its approaches in the Information Retrieval System. Research work explored the linguistic aspects of Information Retrieval, namely Bilingual, Cross-lingual, and Multilingual IR With the advancement and rise of globalization, the importance of the concept of information in form of Big Data has become a priority. The modern age and socialism give rise to large amounts of data due to which the requirement of information in different languages becomes a primary requirement and thus Multilingual Information Retrieval System approach for Big Data came into existence. Though in use, this field of Computer Science is still under research and optimization.
There are several information retrieval systems widely in use today. There are systems based on probabilistic models of relevance, language modeling and those based on DFR model. Though most of these can be applied in a cross-lingual environment, the retrieval efficiency varies widely in such a setting. In this paper we evaluate the performance of the IR models using different empirical parameters using English-Hindi corpus. We analyze the dependency of different models on the value of the empirical parameter and present the results. We observe that language modeling gives comparably better results even when relevance feedback is not available.
One of the important points in cloud service systems is information retrieval. After the failure of information made for a specific work in cloud environment, retrieving new information that builds the concept network is an important problem. In our solution when an information component in a concept network encounters failure, the adjacent components will be reported about the failure; by cooperating with work component, an information component that has required information type will be found. We will increase the availability in the information concept network.
With the increasing of multilingual information on the Internet, it is difficult for people to effectively interpret relevant resources when acquiring information. How to obtain the multilingual information resources needed by users from large-scale information has become an urgent matter, so cross-language semantic retrieval has become an important research direction of information processing technology. In order to solve the performance problems in cross-language semantic retrieval, this paper proposes a method of Tibetan-Chinese cross-language semantic retrieval based on Word2Vec. First, use the word vector to construct the Chinese semantic feature vector of the event keyword, then calculate the feature translation vector of the Tibetan event keyword, and finally complete the cross-language keyword alignment by calculating the similarity between the semantic feature vectors, thereby realizing the key to query automatic translation of words to complete cross-language semantic retrieval. Experiments were conducted on the constructed corpus and the effectiveness of the method was proved.
This paper proposes an automated ranking approach based on probabilistic information retrieval model. Firstly, based on the XML data and query history, this approach takes advantage of the probabilistic information retrieval model to capture the correlations between the unspecified and specified values of leaf nodes as well as the user preferences, and then constructs the scoring function and ranks the query results according to the ranking scores. Results of experiments demonstrate that ranking method proposed can meet the user's needs and preferences effectively.
During the past few years the readers interest on e-newspapers is significantly growing. E-newspapers play an important role by providing useful, informative and timely information to the readers. It is difficult task to retrieve relevant information from different newspapers, as the layout of a newspaper is not geometrically simple. The use of e-newspaper PDF format in the implementation of Information Retrieval System for Online Newspapers requires a robust information extraction system. The proposed system converts e-newspapers PDF format to text format. The PDF document is a platform independent file format. The file can be viewed on any information processing system with a PDF viewer. In this work the system analyses the tree structure of PDF file where it first locates the trailer part (root node of a file) and access the information of cross reference table such as the root object, location and the size of the cross reference table. From the cross reference table the system extracts the information of all the objects stored in the PDF document. This work mainly focuses on identifying each object and extracting the location of the contents of PDF documents.
This paper investigates the application of cluster-based data mining techniques in enhancing the efficiency and accuracy of graphical information retrieval. In the contemporary digital age, the exponential growth of graphical data necessitates advanced, scalable, and precise retrieval methods. Our research addresses the prevailing challenges by implementing a novel, cluster-based data mining strategy that segregates vast datasets into manageable, homogeneous groups for targeted information extraction. Utilizing an extensive dataset of graphical information, we employed cutting-edge algorithms and analytics tools to analyze, categorize, and retrieve pertinent data efficiently. The results indicate a significant improvement in retrieval precision, data processing speed, and user experience. Furthermore, the adaptability of the cluster-based approach ensures its applicability across diverse data volumes and types, marking a pivotal advancement in graphical information retrieval. The insights garnered from this study not only contribute to the theoretical discourse but also hold profound implications for practical applications, particularly for professionals and organizations reliant on swift and accurate graphical information access. This paper serves as a cornerstone for future research aimed at optimizing and expanding the applications of cluster-based data mining in information retrieval and beyond.
Due to the excessive increase in visual data set on the web it has led to the fuzziness of not able to search mage based on ranking and inconsistency. The Content Based Fuzzy Information Retrieval has led to the sustainability of achieving better results through visual identification and neglecting noise. The semantic gap between similar images and tags attached to it has led to challenges that are encountered in the present world. The proposed algorithm to quantify and evaluate algorithm are studied and surveyed in this paper.
Traditionally, the database is of text and numerical data only, which is having less attention nowadays because of the massive amount of multimedia content. In the multimedia and storage technology, the preceding two decades have resulted in a substantial progress that has led to building of a large repository of digital image, video, and audio data. Currently, the information retrieval from the multimedia content is having a great attention. For exploring through the myriad variety of media all over the world Content-based multimedia information retrieval provides paradigms and methods. In this paper, an extensive review on Multimedia content based retrieval is presented together with the classification by means of the multimedia content either it may be an image or audio or video. In addition, a concise description about multimedia content retrieval is presented. Further more, a brief description on image, audio and video retrieval is presented.
Exploring traffic accident locations is essential for making prevention strategy in order to improve traffic safety proactively. Quite a number of methods have been developed to acquire accident spatial data, by various detectors in roads. However, plenty of locations information is contained in textual alarm information transformed from telephone calls of 122 Alarm Reception. This paper, aims to obtain spatial data, longitude and latitude, of traffic accident locations based on textual alarm information. Natural language processing, database system and Geographic Information System (GIS) are used to get traffic accident spatial data. The proposed method involves three processes, location name identification, spatial information retrieval and information matching respectively. Validation results show that the method is of reasonably high accuracy for exploring traffic accident locations.
There have been very few studies on the use of conflation algorithms for indexing and retrieval of Malay documents. The two main classes of conflation algorithms are string-similarity algorithms and stemming algorithms. Stemming is used in information retrieval systems to reduce variant word forms to common roots in order to improve retrieval effectiveness. As in other languages, there is a need for an effective stemming algorithm for the indexing and retrieval of Malay documents. Again there are few research on n-gram string-similarity measures done on Malay. We have experimented on the application of stemming and string similarity matching on retrieving of verses from the Al-Quran in order to evaluated their effectiveness. Before retrieval effectiveness can be carried out an experimental data set need to be developed which comprises of a collection of documents, a set of queries, and their relevant judgements. In this paper we will describe the development of the experimental data set and the application of stemming and similarity matching algorithms in retrieving the verses from the Al-Quran. Inherent characteristics of n-grams and several variations of experiments performed on the queries and documents are discussed. The variations are: both non-stemmed queries and documents; stemmed queries and nonstemmed documents; and both stemmed queries and documents. Further experiment are then carried out by removing the most frequently occurring n-gram. The dice-coefficient is used as threshold and weight in ranking the retrieved documents. Beside using dice coefficients to rank documents, inverse document frequency weights are also used. Interpolation technique and standard recall-precision functions are used to evaluate the retrieval effectiveness.
Design of library information retrieval system based on Internet and information flow mining is studied in this paper. The designed service layer is directly connected to all readers of the library. Because the application function of the service platform is modular, it can accurately grasp the service content of different management operators, provide different reading services for different readers, and achieve humanized intelligent services. However, because the applicable functional services are composed of small modules, the connectivity between the functions is poor. When processing a large amount of reading information, the response time of reading services is increased, so that the reader's borrowing operation time is prolonged. With this theoretical model, the information mining algorithm is then designed for the systematic improvement. The numerical simulation has then shown the overall performance.
In the recent times, there has been considerable effort towards developing an effective and widely informative patient health record system. To meet this objective, diversified groups working on medical informatics have made concerted attempt to bind biomédical knowledge base with patient health data. This approach necessitates a strong need to make all relevant patient's clinical information available through extensive information search, thereby enabling better clinical management. Through this work we propose a method to bind patient event sequences, the temporal medical information and the medical knowledge. The run time extraction of information pose challenges such as (a) performance (b) a priori relation identification and (c) handling of memory overheads. This paper describes a practical implementation of a patient health record linked to a knowledge base and integrated with an information extraction platform ensuring a good balance between the performance complexity and memory usage.
Document Expansion is an effective approach to solve the common problem of word mismatch in information retrieval. This paper proposes a novel document expansion method based on clique that gotten from the document space in Markov network built from corpus. As demonstrated by our experimental results on ADI, MED and CRAN data set, the method which integrates clique into document Expansion can achieve better query performance.
The Krylov subspace based information retrieval (IR) approach has been shown to provide comparable accuracy to latent semantic indexing (LSI), while providing some computational advantages. Recently, in the area of numerical linear algebra, attention has been drawn to the block Krylov subspace methods, which are shown to be more efficient than the classic Krylov subspace methods in solving linear systems with multiple right hand sides. Such improvement in the algorithm gives us the opportunity to extend the original retrieval method, enabling single query searching, to multiple query searching. In this paper, we report such improvement in the retrieval algorithm, and demonstrate its performance by comparing to several other retrieval methods using the Medline corpus.
Checking construction specification in every construction phase is critical to ensure proper construction quality and to avoid contractual problems. However, manual review is inefficient, expensive, and error-prone. There have been efforts to automatically review specifications, but these studies are limited in their practical applicability. As a solution, the use of retrieval-based user interface (as known as a chatbot) can extract specific information from construction specifications as a user wants. For the development of an information retrieval chatbot for construction specifications, this paper tested the application feasibility of a question answering methodology using Bidirectional Encoder Representations from Transformers (BERT). By taking advantages of the pre-trained BERT, user-wanted information was successfully extracted from construction specifications. With this approach, variety of questions can be responded flexibly without time-consuming manual tasks such as labeling.
The continuing growth of information overflow has made it hard to obtain valuable information on the web. In this trend, the need for effective Information Retrieval (IR) technique has been increased. Although document data contain much more abundant information, users can retrieve necessary information only from the title and description in conventional web services. In order to meet the demands for fast and accurate retrieval of valuable information, we propose a fast and effective content-based document information retrieval system that retrieves the information from the actual content of a document. The proposed method is based on a topic model of Latent Dirichlet Allocation that is used to extract major keywords for a given document. The main contributions of our system are the increased flexibility, effectiveness, and fast retrieval of information. Our system can easily communicate with existing web service through the standard JSON format. In addition, we increase the speed of information retrieval by using NoSQL based database system with inverted indexing and B-tree based indexing. We validate the performance of our system on real data collected from the SlideShare service. The proposed system shows better retrieval performance over the existing IR system.
There is information explosion in this era; documents of all genres are available at the tip of our hands. The proliferation of digital data like images, audios and videos both on the Internet and on user's personal computers, have led the researchers across the globe to develop an efficient access and retrieval technique that could meet the real time need of the user. Since information exists in divergent modalities, therefore we are in need of a model where different modalities could be used to get access of information present in single modality. The explosion of data together with user's instantaneous need of accurate information signify that Multimodal Information Access and Retrieval techniques have emerged as a big research challenge in all domains including medical, defense, and aircraft etc. This paper introduces important hurdles encountered by MIAR systems and prominent research work which have been undertaken to address these challenges. Further it highlights problems in MIAR systems which are still not addressed. The objective of this work is to lay foundation for researchers willing to work in this area so that they could have deep knowledge of this challenging and exciting research field.
In our work we present a basic outline of information retrieval process using evolutionary computation and some of the basic models which are being employed. Different evolutionary techniques like particle swarm optimization, ant colony optimization, and genetic algorithm are used for optimizing the data using different sets of algorithms which are comparatively better than traditional computing techniques. It covers a brief overview of evolutionary computation and how optimization helps to retrieve better results using different techniques.
XML Information Retrieval is approach to identify the appropriate answer granularity and controlling to elements overlap. Recently, the demand for integrating Full Text Search and relational search has increased dramatically. The RDBMS implementation is generally much worse in the performance than the IR engine implementation. Especially, when a query is processed in the RDBMS, the number of join operation increases in proportion to the number of relationships in the query. In order to solve these problems, we propose in this paper a novel approach to extend the inverted index for support query processing, namely Absolute Document XPath Indexing that allows supporting and reducing the length of time on Score Sharing scheme. In terms of processing time, our system required an average of one second per topic on INEX-IEEE and an average of ten seconds per topic on INEX-Wiki better than GPX system.
The explosion of data and information on the Internet calls for efficient and reliable information retrieval methods. While textual information retrieval systems have significantly improved, content-based image retrieval using text inputs requires further study and optimization. This research paper proposes a system that uses CLIP (Contrastive Language-Image Pre-Training) model, which projects images and text into a multimodal embedding space to provide representations to compare the semantic meaning of words to embeddings of the image in a dataset. The output is a set of images closely resembling the text input, which is achieved through cosine similarity based on matrix operations. The models have also been optimized for use in production with the use of ONNX runtime is done to speed up inference timing. The application is full-stack and easily accessible, with a ReactJS frontend hosted on Netlify and Flask based Python backend hosted on AWS.
Imagine for a moment a Web where we can extract information from any website, know its context, and automatically assemble it with other information from other sources like databases, geo-maps, multimedia files etc. into a homogeneous document. Today the Web is populated by unstructured or semi-structured data, and the difficulty to consolidate it automatically makes this idea wishful thinking at the moment. This paper describes an attempt to use semi-automatic information retrieval and consolidation on the largest Austrian online encyclopedia Austria-Forum. We had access to the database of historic images of Austria with more than 40.000 images. We describe how we managed to incorporate some of those images suitable for the Austria-Forum without duplicates. The process comprised a large set of heuristics that accomplished a high percentage of the integration automatically. The results required only a moderate effort for a human expert to check if the images did indeed fit the entry proposed by the system.
With the rapid development of the cloud computing, massive information is distributed in the structure "cloud storage", leading to the heterogeneous data sources, and the "lost phenomenon" may occur in the process of information retrieval. To solve this problem, this paper builds the heterogeneous data integration model in the cloud computing environment, which includes three layers and five function modules: data acquisition and analysis interface in the cloud, data organization task scheduling engine, and heterogeneous data integrated interface based on the ontology. Also, common technologies used in unified retrieval of heterogeneous data resources are presented in this paper, such as parallel retrieval technology, server cluster retrieval technology, webpage's deep mining technology and retrieval database sharing technology.
Accumulated data has become enormous according to development and spread of Information technology. Generally the data is saved and is organized on the database system with some kinds of keywords and its clustering. In the case of performing the decision for some problems, we often refer to past cases that are similar to the problem. In such a case, if solutions of past cases are kept in the database as documents, it is very useful to solve a problem. We propose a new information retrieval system based on-heuristic key words extracted from documents and a set of clusters for the documents.
Most of information retrieval (IR) systems for Qur'an use text as their input query, whether they use the Alphabetic script or the Arabic script to represent the query. Thus, required IR user to know how to write the query. For searching the Qur'an verses, it is possible that IR user knows how to pronounce the query, but does not have enough knowledge about how to write Arabic letters to represent the query when search for a Qur'an verse. In this case, speech can be an alternative as the input to the IR system. In this work, we develop a spoken query IR based on the Hidden Markov Model acoustic models and the n- gram language model for its automatic speech recognition system. Both models are trained by using all verses of the Qur'an. The Inference Network Model and the well-known Vector Space Model are employed for its IR system. For the speech recognition system, average of word error rate are 7.41% for closed speakers, and 18.53% for open speakers. For the IR system, the best query formulation for the Inference Network is achieved by using input queries consisting of phrase of 2 words with the average value of Mean Reciprocal Rank is 0,922475, while for the Vector Space Model is achieved by using input query consisting of one word with the average value of Mean Reciprocal Rank is 0,9308.
Information retrieval methods, machine learning models, and humans can suffer from a failure in judging information representativeness. We refer to this problem as information bias. In this work, we propose a method to evaluate information bias through conjunctive fallacies. An experimental evaluation of different state-of-the-art entity retrieval models and human-curated benchmarks shows that both methods perform poorly on judging query-entity representativeness while statistically based methods perform considerably better than humans.
Web contains a vast amount of data, which are accumulated, studied, and utilized by a huge number of users on a daily basis. A substantial amount of data on the Web is available in an unstructured format, such as Web pages, books, journals, and files. Acquiring appropriate information from such humongous data has become quite challenging and a time-consuming task. Trivial keyword-based information retrieval systems highly depend on the statistics of data, thus facing word mismatch problem due to inevitable semantic and context variations of a certain word. Therefore, this marks the desperate need to organize such massive data into a structured format so that information can be easily processed in a large context by taking data semantics into account. Ontologies are not only being extensively employed in the semantic Web to store unstructured information in an organized and structured way but it has also raised the performance of diverse information retrieval approaches to a great extent. Ontological information retrieval systems retrieve data based on the similarity of semantics between the user query and the indexed data. This paper reviews modern ontology-based information retrieval methods for textual, multimedia, and cross-lingual data types. Furthermore, we compare and categorize the most recent approaches used in the above-mentioned information retrieval methods along with their major drawbacks and advantages.
This paper describes a semantic vector space model (SeVSM) and an information retrieval system based on the model. The SeVsmaims to improve information retrieval performance for domain-specific systems. In this model, we use an ontology to build the relations between any two keywords to solve the performance deficiency caused by the basic hypothesis of a vector space model (VSM) where keywords are mutually independent. Then we designed and developed a semantic ontology-based information retrieval (SemOIR) system based on the SeVSM model. An experimental study using 15 queries from different domains confirms the effectiveness of the SeVSM and the usability of the SemOIR system. The proposed model and the system contribute significantly to the application of semantic retrieval for digital libraries and e-commerce systems.
In the software development process, requirements traceability is a key part for ensuring the success of the entire project. It is very important to generate requirements traceability links, which can promote the software development and maintenance processes, such as software requirements integrity analysis, software evaluation, software testing, software validating, etc. However, the generation of the requirements traceability links is usually time-consuming and labor-intensive. In order to solve it, we designed and developed an automated software requirements traceability tool based on information retrieval (IR) model. The tool can not only automatically generate trace links but also evaluate trace links. It uses Vector Space Model (VSM) and the trace recommendation-based code class structure (TRCCS) links to generate trace links. We measure the performance in term of Precision, Recall, and  F2 are used to evaluate the trace links. The experimental results show that the tool can improve the efficiency of requirements traceability links generation and better support software development activities.
Information Retrieval is the activity of finding documents which is of unstructured nature and it should satisfy user's information needs. The term "IR" refers to the retrieval of unstructured records, that is, records which are free-form natural language text. There are various models for weighting terms of corpus documents and query terms. The probabilistic model captures the IR problem using a probabilistic framework, It tries to find the probability that a document will be relevant to a user query or not. In this we have a collection of user query, and there is an ideal answer set for each query, first of all initial set of documents are retrieved from the corpus or collection of documents. User inspects these documents for searching the relevant documents, then IR system use this information to find the description to get the ideal answer set. This work is carried out to analyze and evaluate the retrieval effectiveness of various probabilistic models with use of new data set i.e., FIRE 2011. The experiments were performed with different variants of probabilistic models. Terrier 3.5, which is an open search engine was used for all experiments and evaluation. Our result shows that IFB2 model gives the highest precision values with the news corpus dataset.
Information retrieval is the process of collecting the data or information from the large volume of data that are available in a local repository or internet according to the user query. Many information retrieval approaches are used by various organizations to facilitate the internet users. Even though, these approaches are consuming more time for the huge data. For reducing the retrieval time, many techniques proposed including clustering techniques for reducing the time. However, they failed to retrieve the more relevant data with less time. For overcoming these drawbacks, this paper proposes an intelligent subtype fuzzy cluster based relevant user data retrieval (ISFCDR) model for effective relevant data retrieval and classification for performing better performance in data recommendation. The proposed model developed by using the existing fuzzy clustering algorithm and classification. Moreover, the existing mutual information based preprocessing also has been done for enhancing the performance of the proposed model. The proposed system is evaluated by conducting various experiments and proved that is better than other existing works in terms of time taken and the content quality.
In the literature, the bag-of-concepts representation of textual documents is regarded as a convenient alternative to the bag-of-words representation, since the words that users choose as search terms may differ from the ones that the author of a particular document chose for referring to the same concept-thus reducing recall. Besides, the bag-of-words representation does not detect the differences of context in ambiguous terms, what reduces precision in search results. The objective of our research is to evaluate the applicability of the bag-of-concepts paradigm to information retrieval of educational resources. We built an information retrieval system that follows that approach and evaluated it with final users. The main contribution of this paper is the description of the architecture of the information retrieval system. First evaluation results show that the information retrieval system based on bag-of-concepts works well for retrieving educational resources. The practical implications of this research are that: it demonstrates that it is workable to build information retrieval systems based on bag-of-concepts and that they are efficient for retrieving educational resources. This makes them an a priori interesting alternative to be applied in other domains.
The number of users of an on-line shopping websites is continuously increasing. Such website often provides facility for the users to give comments and ratings to the products being sold on the websites. This information can be useful as the recommendation for other users in making their purchase decision. This paper investigates the problem of predicting rating based on users' comments. A classifier based on information retrieval model is proposed for the prediction. In addition, the effect of integrating sentiment analysis for the rating prediction is also investigated. Based on the results, an improvement in prediction performance can be expected with sentiment analysis where an increase of 54% is achieved.
Although much research on Music Information Retrieval (MIR) has been done in the last decade, the input of the current MIR to specify a user query for finding a similar piece of music is still either by the existing old-fashioned keywords or by music contents. We aim to realize a new type of MIR equipped with brain-computer interfaces using electroencephalogram (EEG) signals. Toward the new MIR, we propose an architecture of MIR driven by EEG signals in this paper. While the architecture contains many issues to be solved, the point of the architecture is to construct user's music query in multi-layered aggregation of EEG signals. We describe in this paper the preliminary experiments conducted for selecting some appropriate low-level features for our multi-layered query construction and matching. It is obtained that the mental states of users while listening to music can be classified with high accuracy by using EEG signal aggregated features. We are starting development of detailed design of the architecture using the results described in the paper.
Information Retrieval is an automated way to extract the information from various sources. It has always been a key area of research due to its wide range of applications in document searching, software traceability etc. We have presented an analysis of the three basic models of information retrieval i.e. Vector Space Model, Latent Dirichlet Allocation Model and Latent Semantic Indexing Model in this paper. These models are explained in detail according to their basic concepts, methodology adopted and area of application. We have provided the advantages of these topic models over each other and have discussed their limitations too. We have also highlighted the basic categories of information. The variants of some basic models are also described according to their concept and usage. This kind of analysis will be useful for the user to make a choice between these information retrieval models as to find out which one is to be used regarding a particular problem or application.
The semantic relationship in thesaurus is introduced into the current network information retrieval tool, which can realize semantic retrieval. Using a statistical language model to express query statements and return results in the form of probability distribution can more effectively complete the construction of user model and realize personalized retrieval. Firstly, this paper proposes a similarity calculation method based on the relationship between words in the thesaurus. On the basis of this method, combined with the idea of query expansion and weighted sorting, this paper proposes a semantic retrieval method of forestry information based on the thesaurus. Secondly, this paper uses a statistical language model to propose personalized retrieval methods based on three different user models: topic model, historical model and mixed model. Finally, a forestry information personalized semantic retrieval system is realized by using semantic retrieval and personalized retrieval method. Experimental results indicate that the proposed personalized semantic retrieval method can effectively improve the retrieval performance.
The rapid evolution of technology in Management Information Systems has enabled easy access to a huge amount of complex and specialized business information. The vast amount of business information has created the need for investigating ways to improve the efficiency and effectiveness of information retrieval techniques for querying business information. Among the different types of business information, information about business processes has been leveraged with dedicated languages, methods and tools. Business Process Management Notation (BPMN) is an international standard for modeling business processes. In this paper, we first develop the appropriate business process models of a complex real-life scenario using BPMN and we then represent process models in the form of an ontology business process model. We, next, query the two models with the same queries: the BPMN model using a query language for object-oriented databases and the semantic model using the SPARQL ontology-based search language. We compare the two query methods, discuss the results and explain the benefits of using semantics-based information retrieval techniques when dealing with business process information.
We propose a novel scheme that achieves the capacity of the PIR, which is the maximum number of bits of the desired messages that can be privately retrieved by a single bit information download from the databases. The proposed scheme is based on the stochastic approach, where the queries are generated randomly and the downloading information is the random linear combinations of the message bits. Consequently, we claim that one-shot query generation, based on the stochastic approach, can guarantee the privacy of the user and the correctness of the retrieved information while achieving the capacity, when the number of answering bits are sufficiently large.
Information retrieval systems enable the organization, storage, retrieval and distribution of information. Their main purpose is to retrieve information and return results that match the most the user's needs. The field of Information Retrieval (IR) has seen in recent years the appearance of a new axis which is Personalized Information Retrieval (PIR), in particular emotional PIR. This type of information search systems aims to extract the user's emotions and to include it in the information search process, in order to return results according to the user's sentiments and emotional state expressed through his queries. These emotion-based systems have shown interesting results in terms of recall and precision, however the search time for relevant information increased significantly compared to non-emotion-based systems, due to the extra treatments necessary for emotion extraction from the input queries, as well as the emotional profile construction operations. We propose in this work an optimization approach for emotion-based information retrieval systems, based on data mining techniques, more precisely clustering. We aim to reduce the execution time by grouping the documents of the system in clusters, thus transiting from multiple query/document comparing operations to fewer query/cluster operations while keeping quality of results similar to the original non-clustered system.
Current information retrieval is done using a search engine or information retrieval system, users write queries and search engines will display search results. Existing and widely used search engines now provide many search results (many documents are taken), so it takes time to determine the relevant search results. Therefore we need a way to classify the amount of information available, which is needed by the user, making it easier for users to get the desired documents. Information retrieval system is a system that is used to find information that is relevant to the needs of its users, by applying the system problem information retrieval Physics force can provide relevant results according to the needs of users. There are two main processes in information retrieval systems, namely indexing and retrieval. One of the weighting algorithms is the TF-IDF which is influenced by the frequency of occurrence of words in each document and the frequency of documents that have the word. The result in a recall value close to 1 in each test which shows that almost all relevant documents can be found by the system, the precision values are between 0.6666667 to 1 which shows that there are other documents besides the relevant documents found by the system.
How anyone can find the desired bit of information with respect to his/her own context from the ocean of information resided in multiple databases and text repositories growing at an enormous rate. Information retrieval systems (IRS) are use to find information as output with respect to the user query as input. Effectiveness of information retrieval system hugely depends upon the query formation. Various factors affecting query formation are media expertise, domain expertise and type of search [1, 2]. Search engines are necessary tools for information retrieval from World Wide Web. Conventional search engines like Google, Yahoo etc. have huge amount of data. To retrieve the information from these conventional search engines which serve the population on the whole without much concerning about user context require user query expressive enough about user context and need. In our paper we have proposed a model to build a context based search engine on the conventional search engine using genetic algorithm. We have tried to find out good query terms in context of user to find user specific retrieval. We have used these terms for query expansion or reformulation.
Multilingual information retrieval system (MLIR) retrieves relevant information from multiple languages in response to a user query in a single source language. Effectiveness of multilingual information retrieval is measured using Mean Average Precision (MAP). The main feature of multilingual information retrieval is the score list of one language cannot be compared with other language score list. MAP does not consider this feature. We propose a new metric Normalized Distance Measure (NDM) for measuring the effectiveness of MLIR systems. NDM considers the MLIR features. Our analysis states that NDM metric gives credits to MLIR systems that retrieve highly relevant multilingual documents.
Multimedia technology is known as the single repository source for information retrieval that includes images, audio and video. Nowadays, it is growing with tremendous amount in the field of communication technology. Traditional text based retrieval techniques of information retrieval are inefficient at searching user's intended information from the large multimedia database. To address this problem, content based image retrieval techniques are applied for information retrieval. Content-based image retrieval has been hot topic from past two decades by many researchers' community. This topic covers many fields such as image processing, artificial intelligence, pattern recognition, and data mining and beside other related area. In this paper image retrieval techniques on the basis of color, texture and shape are explained, studies and compared. While relevance feedback technique, system accuracy, efficiency have also been discussed. Emphasize on the concept of `semantic-based image retrieval techniques' have been proposed in this paper, which is better technique to overcome the problem of `semantic gap' in the context of image retrieval.
Information Retrieval (IR) is the complex of activities that represent information as data and rank the data representing information relevant to the user's information needs by a retrieval function. Such a function involves parameters. They can in principle be set irrespective of the specific set of documents and queries, but can in practice maximize retrieval effectiveness. However, algorithms to select retrieval function parameters must be efficient due to the large search space. We can remark that: (i) all the tested methods are similarly effective, but the plots of the maximum value of NDCG@20 at a given evaluation show that our algorithm is more efficient; (ii) performance metrics and datasets studied in this paper seem to yield objective functions with few, if any, local optima with large basin of attraction; (iii) our algorithm is considerably more efficient, quickly finding parameterizations of the retrieval function yielding high performance - much faster than line search.
In recent time conventional ways of listening to music, and methods for discovering music, such as radio broadcasts and record stores, are being replaced by personalized ways to hear and learn about music. There is abundant research and experimentation done over Western music. However, moderate amount of work noticed over Indian music and its related fields such as computational musicology and artificial intelligence to the realm of Indian music [1]. On the other hand, Indian music has adequate history and slowly it is becoming an international phenomenon. This has opened a wide opportunity for an Indian music discovery system which can suggest music based either on known artists or on simple descriptive terms. Hence researchers from various domains such as music processing, computer engineering and artificial intelligence etc. are contributing more towards Indian music processing. Eventually Indian music is broadly classified into South Indian Carnatic music and North Indian Hindustani music. Both the systems of music are rich in their own style and Carnatic music is much more complex in the way the notes are arranged and rendered [11]. Apart from these two broader categories, there are various types of Indian music. They are folk, tribal, bhajans or devotional, bhangra, Indi-pop, film songs, fusion and ghazals. This paper presents an overview of previous works on automatic Indian music information recognition, classification and retrieval. Furthermore the comparative study of the recognition, classification and retrieval techniques effectiveness based on various factors is also presented.
We present a private information retrieval (PIR) scheme that allows a user to retrieve a single message from an arbitrary number of databases by colluding with other users while hiding the desired message index. This scheme is of particular significance when there is only one accessible database-a special case that turns out to be more challenging for PIR in the multi-database case. The upper bound for privacy-preserving capacity for these scenarios is C=(1+1S+⋯+1SK−1)−1, where K is the number of messages and S represents the quantity of information sources such as S = N + U − 1 for U users and N databases. We show that the proposed information retrieval scheme attains the capacity bound even when only one database is present, which differs from most existing works that hinge on the access to multiple databases in order to hide user privacy. Unlike the multi-database case, this scheme capitalizes on the inability for a database to cross-reference queries made by multiple users due to computational complexity.
The increasing amount of available XML documents collections has led to the emergence of new challenges in information retrieval field. Therefore, multiple sources of evidence were used to retrieve XML elements at different levels of granularity. XML information retrieval combines textual and structural information to perform different information retrieval tasks. In this paper, we propose a new approach exploiting link evidence to re-rank XML retrieval results. Our approach, based on fuzzy logic concepts, combines both content and link evidence for all retrieved XML elements. The combination process generates a new ranked list from the initial returned list. Experiment based on INEX 2007 Wikipedia collection showed improvement of the interpolated precision values.
When an image database is queried with a particular example image ("show me similar images"), the corresponding feature vector is computed and the most similar feature vectors from the database are searched to display the most similar images in the database. This paper presents the design and implementation of a high-dimensional index application to facilitate the speedy searching in feature based image information retrieval, and the improvement for the k nearest neighbor query algorithm based on X-tree which is designed for high-dimensional indexing. Finally the performance evaluations are presented to show the merit of the algorithm.
Main goal of this work is to show the improvement of using a textual pre-filtering combined with an image re-ranking in a Multimedia Information Retrieval task. The defined three step-based retrieval processes and a well-selected combination of visual and textual techniques help the developed Multimedia Information Retrieval System to overcome the semantic gap in a given query. In the paper, five different late semantic fusion approaches are discussed and experimented in a realistic scenario for multimedia retrieval like the one provided by the publicly available ImageCLEF Wikipedia Collection.
Bibliometric techniques are not yet widely used to enhance retrieval processes in digital libraries, although they offer value-added effects for users. In this paper we will explore how statistical modelling of scholarship, such as Bradfordizing or network analysis of coauthorship network, can improve retrieval services for specific communities, as well as for large, cross-domain large collections. This paper aims to raise awareness of the missing link between information retrieval (IR) and bibliometrics / scientometrics and to create a common ground for the incorporation of bibliometric-enhanced services into retrieval at the digital library interface.
With the rapid development of Internet, information explosively grows at a geometric rate. How to retrieve useful information from the vast amounts of multimedia information resources is a hot research topic. A multi-feature-based image retrieval system on the condition of extracted multi-feature has been carried out. PCA method was used for dimensionality reduction. Relevance feedback technique is used to get the connection of system and users, and adjust the values of the features according to the feedback of users, to raise the accuracy of image retrieval and satisfy the users' need. The experiment shows that the proposed algorithm has better retrieval performance.
The main issue that currently faces research in the information society is the flood of information; a problem exacerbated by the massive diversity of information on the World Wide Web. It has given researchers access to millions of references, articles, news and services. Regardless of geographic location and language used, much of this information is unstructured data. There is a large body of research on mining unstructured Web data, but little effort for Web pages authored in Arabic. This paper investigates the Semantic Web (SW) support for handling documents that are authored and/or annotated in Arabic, and how to bridge the gap between the SW and Natural Language Processing (NLP). Moreover, to improve the intelligent exploration of unstructured documents in the Arabic domain.
This study introduces a worksheet to support students using ChatGPT for information retrieval and understanding. ChatGPT is a powerful tool; however, its value in education largely depends on how it is used. Therefore, this study aims to provide a design of a worksheet to help students assess the credibility of information through source checks, expertise, and consensus assessment alongside ChatGPT. Scoring criteria are established to determine how students use information judgment strategies through the worksheet. The worksheet and the use of ChatGPT have been introduced in computer science courses at the authors' university. Future work involves refining worksheet scoring and exploring students' use of ChatGPT and its impact on comprehension. This study proposes a potential positive use of ChatGPT in education.
Quantum cybersecurity is the study of all facets regarding the security of communication and computation in a distributed network. Significant developments in quantum technologies have outclassed their classical counterparts, thus envisioning the realization of a quantum internet. However, such quantum resources, in the hands of an adversary, can jeopardize network security. In this paper, we study two secure network connectivity concerns, namely,  privacy and anonymity, in quantum information retrieval systems. To this end, we propose a state-of-the-art single-server multi-user quantum anonymous private information retrieval (QAPIR) protocol. To actualize this, we utilize anonymous entanglement as a quantum resource. We show that the QAPIR protocol not only provides privacy but also introduces anonymity as an added layer of security in quantum networks. Furthermore, we also detail a comparative security analysis that establishes the desirable properties of our proposal.
With the advent of big data, the number of web pages and information are increasing exponentially. Much of the information we retrieve is mixed and redundant, which affects the effectiveness of retrieval. Therefore, information retrieval has become an indispensable technology. Compared with ordinary retrieval, the retrieval of laws and regulations needs higher relevance and accuracy. Aiming at the poor retrieval effect and unreasonable results in terms of legal retrieval, this paper proposes a legal and regulation retrieval system, our work is establishing a high-quality database, removing the stop word, and increasing hierarchical retrieval. Experimental results show that the proposed method in terms of legal retrieval is more effective than general legal search systems and wide-area search systems. In the end, we complete the design and visual display of the whole retrieval system to ensure the accuracy of retrieval results and the conciseness of retrieval contents.
With the increasing popularity of network applications, internet information retrieval has developed into an important retrieval method for people to query and obtain information, providing a guarantee for users to obtain valuable information from massive amounts of information and improve the accuracy of information acquisition. In internet information retrieval, the application of data mining technology can significantly improve the efficiency of information retrieval. This article introduces data mining technology, analyzes its application value in internet information retrieval, and analyzes the specific application of data mining technology in internet information retrieval.
This paper describes a brief history of the research and development of information retrieval systems starting with the creation of electromechanical searching devices, through to the early adoption of computers to search for items that are relevant to a user's query. The advances achieved by information retrieval researchers from the 1950s through to the present day are detailed next, focusing on the process of locating relevant information. The paper closes with speculation on where the future of information retrieval lies.
This paper presents two approaches for Information Retrieval (IR) from a collection of documents: Bayesian theory of probability and Dempster-Shafer theory of belief functions. Each method has been supported with essential derivations to prove their suitability for IR. The conclusions of derivations have been applied in illustrative examples. Finally, comparison of both the methods suggests the suitability of each for specific domains.
This paper contains a method to construct context data which can help an application grasp user intention in pervasive computing environment (PCE). There are various devices in which a user is interested for the user intention (intended behavior such as entering, reading and sleeping). And the core of PCE is to provide appropriate services adapted for grasped user intention through processing context information received from device sensors. Therefore, this paper suggests an approach based on co-occurrence and statistical method, kinds of information retrieval technique, to grasp user intention based on diverse device sensors (context information), including both physical and logical objects.
Based on the study of linguistics, Information Science and Library and Information Science, we research on the real-time news posted on the authority sites in the world's major countries. By analyzing the massive news of different information sources and language origins, we come up with a basic theory model and its algorithm on news, which is capable of intelligent collection, quick access, deduplication, correction and integration with news' backgrounds. Furthermore, we can find out connections between news and readers' interest. So we can achieve a real-time and on-demand news feed as well as provide a theoretical basis and verification of scientific problems on real-time processing of massive information. Finally, the simulation experiment shows that the multilingual news matching technology could give more help to distinguish the similar news in different languages than the traditional method.
Emotional conversation is attracting more attention in natural language understanding and machine intelligence. In this paper, we propose a single turn Chinese emotional conversation generation system. Given a post, our system generates appropriate responses from large-scale conversation datasets together with corresponding emotional labels. This system consists of four components, i.e., information retrieval (IR) system, language model, question answering (QA) system and emotion classifier. The experimental results on benchmark dataset show that our system is capable of retrieving appropriate responses with emotional labels.
This article shows the analogy between natural language texts and quantum-like systems on the example of the Bell test calculating. The applicability of the well-known Bell test for texts in Russian is investigated. The possibility of using this test for the text separation on the topics corresponding to the user query in information retrieval system is shown.
Information explosion and availability of information in various forms has changed the shape of information centres and nature of information profession and professionals. Information profession and professionals have been impacted by the exponentially increasing volumes of information available - as well as with changing attitudes and behaviour of information seeker toward electronic resources. It is very difficult to find the required pieces of information in the bundled of scattered information. The task becomes more challenging, when we find that over 90% of the information available is in unstructured and semi-structured forms, which is very difficult to search. Here text mining has come as a tool to help Information professionals to find the relevant information and deliver to its users. Text mining is used as a technology for analyzing large volumes of structured/unstructured textual documents. Text mining has very high knowledge and commercial values. The aim of Text mining is generally to strengthen decision making and internal operations processes of any organisation and generation of new domain of knowledge. These technologies help to increase the utilization of Knowledge Management (KM) systems and pro-actively help information professionals to improve their competencies and thus productivity of the organization. This article discusses the basic concept of text mining, its framework and text mining products and tools. It also discusses the benefits and challenges of text mining and examines the role of Information Professionals in Text Mining.
The fast development of cloud technology has brought about a new trend in the field of information service: more and more information is being transferred to the cloud as requested. However, the data, such as texts, images, sounds, and videos, before being moved to the cloud, in most cases, has to be encrypted so that intelligible information will not be obtained from unauthorized accesses. While having done a nice work in protecting the data privacy of its owners, this encrypting process, has produced a great challenge for retrieval of the document stored via traditional IR model based on document, query and relevance. In order to retrieve encrypted information from cloud, an alternative retrieval system is needed. To satisfy such a need, we have: 1) build a cloud information retrieval framework characterized by its retrieval risk formula, which, enables, for the very first time to the best of our knowledge, an effective retrieval of keywords from encrypted cloud data without undermining key word privacy and retrieval performance; and 2) upgraded the existing searchable encryption scheme that can only support simple equality queries on encrypted data and has been proved to perform slightly better than random selection, so that it can now support the state-of-art information retrieval methods, such as vector space, probabilistic, and language model. To evaluate the effect of the system proposed above, we've conducted a wide range of experiments on benchmark data sets, of which the results shows that solution can fulfill its purposes quite well in various settings.
City objects recommendation based on characteristics of users, location, time and weather is a challenging issue in geographical information retrieval (GIR). In the meanwhile, city objects recommendation is a computation-intensive and data-intensive application. Cloud computing has gained significant attention in recent years to process the large volume of data. MapReduce framework is currently a most dominant technology in cloud computing. Augmented User-based Collaborative Filtering (AUCF) algorithm which can effective deal with hybrid variable types is proposed firstly. Then, MapReduce for GIR (MRGIR) is presented and AUCF is implemented within MRGIR as an example. The MRGIR is implemented in Hadoop which is an open source framework for MapReduce. Experimental results shows that with moderate number of map tasks, the execution time of GIR algorithms (i.e., AUCF) can be reduced remarkably.
As a conception modeling tool, ontology can describe the information system in the semantic and the knowledge level, it can also express the relations of concepts, the inheritance hierarchy of concepts, potential relationships and axioms precisely, and so on. Meanwhile, it provides a solution for many problems of massive information caused by the rapid development of the Internet, such as how to organize, manage and maintain vast amounts of knowledge information, and provide users with efficient services to access them, also an effective way of sharing and representing knowledge. This paper proposes a keyword retrieval method based on thesaurus and ontology, to solve vary problems between user's keywords and words used by domain experts to describe the ontology, in order to improve the efficiency and accuracy of retrieval, aiming at combining the general retrieval system and domain ontology effectively.
This paper provides an extensive and thorough overview of the models and techniques utilized in the first and second stages of the typical information retrieval processing chain. Our discussion encompasses the current state-of-the-art models, covering a wide range of methods and approaches in the field of information retrieval. We delve into the historical development of these models, analyze the key advancements and breakthroughs, and address the challenges and limitations faced by researchers and practitioners in the domain. By offering a comprehensive understanding of the field, this survey is a valuable resource for researchers, practitioners, and newcomers to the information retrieval domain, fostering knowledge growth, innovation, and the development of novel ideas and techniques.
Post-war telecommunications network is completely damaged by aerial bombings as mostly towers and elevated structures are bombarded. This brings hindrances to the postwar information retrieval as it becomes extremely difficult to communicate using the previously established communication system. The aim of this paper is to fabricate a MANET based emergency communication system containing Damage Assessment Subsystem, Data Processing Subsystem and a Data Localization Subsystem. Apart from theoretically formulating the Post - War Information Retrieval System (PWIRS), this paper focuses on collaborative information gathering and independent processing of data through the MANET based networking infrastructure.
Information Retrieval Systems, often referred to by the acronym IRS, are essential tools in the field of data collection and analysis. Among IRS, Personalized Information Retrieval Systems stand out by offering the ability to personalize and adapt search results based on the user’s individual preferences. This ability to provide tailored information contributes to a more relevant and efficient search experience, meeting the specific needs of each user. A new type of personalized IRS has emerged, emotional IRS, which are distinguished by their ability to adapt search results based on the user’s emotions, represented by a structure called an emotional profile. This development is particularly notable thanks to major advances in the field of Natural Language Processing (NLP), which now allow a finer understanding of the emotions present in textual data. In this paper, we present a comparative study of different data structures for the representation of emotional profiles. The objective is to determine the optimal data structure to represent the emotional profile of the user and thus have an efficient and user-friendly emotional IRS.
The large scale of scholarly publications poses a challenge for scholars in information-seeking and sensemaking. Bibliometric, information retrieval (IR), text mining and NLP techniques could help in these activities, but are not yet widely used in digital libraries. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric and recommendation techniques which can advance the state-of-the-art in scholarly document understanding, analysis and retrieval at scale.
Arabic Information Retrieval faces distinct challenges stemming from the language’s morphology, script complexity, and contextual intricacies. The success of retrieval systems hinges significantly on the preprocessing steps applied to the textual data. These preprocessing techniques play a pivotal role in transforming raw text into a format that facilitates efficient indexing, retrieval, and enhances the overall performance of IR systems. By evaluating and comparing the performance of various preprocessing techniques, we aim to shed light on the best practices and methodologies that can be employed to enhance the accuracy, efficiency, and user satisfaction of Arabic Information Retrieval systems.
This paper presents preliminary result of research project, which is aimed to combine ontology information retrieval technology and process mining tools. The ontologies describing both data domains and data sources are used to search news in the Internet and to extract facts. Process Mining tools allows finding regularities, relations between single events or event types to construct formal models of processes which can be used for the next ensuing analysis by experts. An applicability of the approach is studied with example of the environmental technogenic disasters caused with oil spills, and followed events. Ontologies allow adjustment to new domains.
Fast growth of video data on the Internet requires managing these video data. In the last decade, Content-Based Video Retrieval (CBVR) became a considerable research interest to handle the large amounts of collecting video media on the Internet. Due to concerning a large ratio of the videos on the Internet to humans, human action retrieval is presented as a new topic in CBVR domain. In this paper, we seek to improve the current state-of-the-art retrieval algorithms for CBVR by using the statistical information. The statistical information is utilized to represent the video by a vector with m units instead of local points of m×n units and creating a histogram of Bag of Words and also, it decreases the complexity significantly. Furthermore, each vector is used to compare the videos and to find the similar videos to the query one instead of histogram of Bag of Words. The experimental results on KTH dataset illustrated that in contrast to the Bag-of-Words model and its various parameters, our method can perform better.
Today, with the rapid development of the Internet, text information is increasing rapidly. Therefore, there is a need to mine text information so that the relevant information can be detected and organized in the text. Especially, the multimodal information retrieval is mainstream, to solve the “heterogeneous gap” problem in cross-modal retrieval, most of the existing main methods rely on the idea of representation learning. Specifically, cross-modal retrieval algorithms generate isomorphic unified high-level feature representations for different modal data by setting up a model framework that can establish the semantic relationships between heterogeneous data. Hence, in this study, the filtering algorithm is firstly applied to clean the data sets, then, the modified HMM model is applied to achieve the information retrieval framework. In the experiment, from the aspects of accuracy and time usage, the proposed model is tested compared with the SVM and traditional HMM, the results show the proposed algorithm performs much better.
During the course of learning information retrieval skills, video is one type of teaching method for the students when they didn't catch what the teachers had taught them in class. In this article the main content and forms of the resources about information retrieval are proposed, including operating methods and relative technology, in order to improve the effectiveness of information retrieval skills and help readers find useful information which they need.
To help bridge the gap between consumer user's vocabulary and controlled vocabulary used to index health information, in this demo we implemented a Visualized Related Topics (VRT) browser system. The VRT was integrated into the “MeshMed” [2] system to support health information retrieval. The key technology behind the VRT browser is to select MeSH terms, which represent the related topics or subjects, from the top relevant documents. We rank these MeSH terms using the traditional Term Frequency-Inverse Document Frequency (TF-IDF) algorithm. The VRT browser displays a graphic representation of these MeSH terms by creating a visual where the selected MeSH terms stem from the centered user query. The design goal is provide users an overview of the key topics of the search results. In addition, VRT browser may also help users form better queries. Using the VRT browser we will be studying how to effectively assist in consumer users with their health information seeking.
The Quran provides valuable insights into various aspects of life, including information about the natural world, such as animals and plants. Developing an information retrieval system can greatly facilitate the search for specific content within the Quran. This system utilizes a cosine similarity approach and query expansion techniques to rank and expand the retrieved documents according to the user's needs. In this research, the Term Frequency-Inverse Document Frequency (TF-IDF) weighting method is employed, following preprocessing steps for both the documents and queries. The primary objectives of this study are to assess accuracy, precision, and recall values. Notably, the inclusion of query expansion has led to an improved recall value compared to previous research, resulting in a 99.32% accuracy rate, a 5.83% increase in precision (from 83.93%), and a system's ability to capture 8.75% of the relevant verse (compared to the previous 93.51%). This signifies that all verses can be invoked, displayed, or expanded, providing a more comprehensive query experience.
Establishing an information security policy that aligns with the business strategy of the organization usually depends on the stakeholders' expertise and experience. Due to the human factor constraint, this may lead to an incomplete and unclear policy. This research proposes a framework for the storage and retrieval of information security standards using information retrieval principles and security patterns based on security patterns, keyword search and the relationship among security patterns. The vector space model is used to compute the similarity between user queries and parts of security standard documents to present results relevant to the users' needs. Recall, precision and F-measure are used to evaluate the efficiency of the proposed methods.
Natural language processing is an important means to realize the communication between man and machine using natural language, and help computers quickly understand the meaning expressed by natural language. The most common application system using natural language processing technology is information retrieval system. On this basis, an information processing model based on BP neural network (BPNN) and statistical method is discussed, and the principle of BPNN is explained in detail. After analyzing these phenomena, researchers think that natural language processing is more suitable for tasks requiring accurate results, and the understanding level of natural language processing is divided into seven levels from low level to high level: pronunciation level → morphology level → vocabulary level → syntax level → semantics level → pragmatics level → context level. On this basis, the application of natural language processing in information retrieval system is discussed.
Effectiveness of Information Retrieval from Structured Database through Natural Language provides high utility value and ease of use. The challenge in Information Retrieval is to derive the users intended from limited number of query words (sometimes just one word) to extract the relevant information from structured database. It calls for correct interpretation, disambiguation and context resolution in natural language query processing. These difficulties can be reduced by limiting the domain of applications. In this paper we present our work in designing and developing Student Interface System by querying user in Natural Language (English). Our model consists of (i) semantically parsed the user inputted sentence (ii) transforming it into intermediate query form and finally (iii) convert it into structured query language. The system is developed in JAVA using JDBC for Student Information System. The same approach and methodology can be used in developing queries using vernacular languages.
In the field of text processing or Natural Language Processing, the increasing popularity of the use of words used in the field of Natural Language Processing can motivate the performance of each of the existing word embedding models to be compared. Semantic similarities have an important role in the field of language. Keywords are the most important thing in finding information. Seeing the benefits and studying the knowledge of Sirah Nabawiyah, especially from unreliable sources, a system to retrieve information was proposed for viewing and studying Sirah Nabawiyah. Based on the explanation above, the writer conducted a Systematic Literature Review (SLR) which was related to the semantic similarity comparison of the word embedding method, namely fast text and word2vec which contributed to the information retrieval system using the knowledge discovery in database method approach to documents in the form of Sirah nabawiyah text. Analysis and comparisons of several pieces of literature have been carried out. As a result, there are several criteria to complement the advantages and disadvantages of previous research.
With the exponential growth of various network resources, the use of search engine has become one of the most basic skills of everyone in today's society, and an efficient information retrieval model is also of more significance. The traditional text-based music information retrieval method can retrieve music data by inputting text information such as song name, composer, singer and album name. The content-based music information retrieval queries the target music through the input music melody information. In the actual music information retrieval scene, there is interaction between the user and the retrieval model. The user gives feedback on the retrieval results, and the retrieval model returns a new page of document according to this feedback. The existing ranking learning model regards ranking as a one-time process, ignores user feedback, and the ranking effect needs to be improved. With the increasing demand for digital music information and the continuous expansion of application fields based on massive music data sets, content-based music information retrieval method is attracting more and more researchers' attention.
Several shortages of Boolean retrieval, such as ignorance of the semantic relations among words and inability to rank the retrieval results in order of importance, are found by analyzing the essence of traditional text retrieval, in view of which an improvement of algorithm optimization based on topic words is proposed. Through enriching topic words to structure keywords library, the semantic distance and similarity of keywords are calculated on the basis of semantic retrieval framework. The improved algorithm is applied in the disaster case retrieval system at last, which retrieval results are then analyzed to detect performance. It is observed that the improved algorithm has a better improvement in retrieval both in precision rate and recall rate.
Wireless Sensor Network is a highly promising technique which is used for monitoring environmental conditions in marine areas. In marine areas it is very important to know the state of the sea conditions to direct gateways to be opened from the neighboring areas to allow water activities. Messages are sent from nearby sea areas stating the sea conditions. This paper proposes a novel framework for deployment of Wireless Sensor Networks in marine environment to carry out information retrieval and generate intelligent data. This method uses SentiWordNet, a lexical resource which is used for opinion mining. This framework extracts intelligent information from the aggregate message by first filtering the message and transforming it by lemmatization and then gathering the opinion words and calculating their scores with the help of SentiWordNet. Thereby polarity of the sentence is calculated and the message is conveyed. The proposed method is executed on the messages sent from various marine areas and the results prove the efficiency and accuracy of the method.
In this paper, a hybrid system combining ant based approaches and tabu search has been designed for the generation of materialized views in a relational data warehouse environment with the purpose of improving the queries performance. Two ACO algorithms were adapted for the views generation problem to take up the scalability challenge and information retrieval technologies are used in the search process. In addition, our approach manages dynamically the storage to include the best views determined by the bio-inspired approach. Experiments have been conducted to validate the designed algorithms and interesting performance is observed when comparing it with those of the previous related works.
A Topic-based Document Retrieval Framework (TDRF) is proposed in this paper to resolve the topic-based document retrieval. The TDRF includes nine parts, of which Corpus Topic Learning, Query Topic Learning and Relationship Sorting are the core. Experiments on similar document retrieval showed that TDRF's instance outperforms the Vector Space Model (VSM) in average precision, recall and f-measure. The value of TDRF may lie in that it provides a simple, universal and novel methodology for document retrieval.
In this paper, a novel approach is presented to construct a similarity function to make information retrieval efficient. This approach is based on different terms of term-weighting schema like term frequency, inverse document frequency and normalization. The proposed similarity function uses fuzzy logic to determine similarity score of a document against a query. All the experiments are done with CACM benchmark data collection. The experimental results reveal that the performance of proposed similarity function is much better than the fuzzy based ranking function developed by Rubens along with other widely used similarity function Okapi-BM25 in terms of precision rate and recall rate.
There are rich and complex knowledge networks behind the network big data. The effective use of network big data is the de redundant classification and refinement of data. This paper analyzes and discusses the well-known open network knowledge base at home and abroad and its supporting applications. This paper summarizes the construction of open network knowledge base, the application methods and technologies of information retrieval and data mining based on open network knowledge base, and analyzes the relationship between people and knowledge. This paper solves the matching problem of information retrieval, data mining and system application based on open network knowledge base. Experiments show that the method proposed in this paper has certain reference value to improve the accuracy of information retrieval method.
Information Retrieval (IR) is the process of retrieving relevant documents from a large collection of documents. The applications of these technologies are no longer restricted to only online search engines, instead, these are widely being used by organizations to facilitate different organizational information searches. Efficient Retrieval of documents from a large collection of organizational data is still a challenging task. In this paper, a domain categorization-based Information Retrieval system is presented for offline document repository. Several tests were conducted to evaluate the performance of the presented model. The performance of the model is also compared with existing retrieval techniques and the results obtained show the efficiency of the model in providing relevant documents quickly. The model achieves approximately 93% precision and 92% recall accuracy in the considered domains and data set.
Network crime information retrieval framework based on facial image recognition is designed in this article. Due to the intelligence and professionalism of crime methods, the variety of criminal methods, and the high degree of concealment of crimes, the detection of cybercrime is much more difficult than traditional cases, which brings great difficulty to the investigation of the case which leads to the detection with low rate. The face analytic model is used to construct the efficient crime data modelling system. The regular term of the ideal coding coefficient matrix is introduced to represent the coefficients of samples for the same types which are as close as possible. The representation coefficients of samples of different types are as different as possible, which also increases the discrimination of coding coefficients. The comprehensive analysis of the model is tested and compared with the state-of-the-art methodologies. The experiment shows the detection accuracy has been improved to 98.77%.
This explores using a system to gain knowledge of strategies that allow you to optimize the facts retrieval manner. Facts retrieval is a complicated technique; it is becoming even more complicated as advancements in generation have furnished tools to make the gathering, employing, and evaluating information less complicated and faster. Research techniques, including artificial intelligence, neural networks, and machine learning, can speed up records retrieval and improve outcomes' accuracy. This look examines the different machine learning techniques to optimize the facts retrieval process and the advantages and downsides of each technique. Furthermore, this study looks at how gadgetstudying algorithms improve a person's enjoyment of retrieval systems and how their use can be integrated into cutting-edge practices. Subsequently, methods for improving the performance of machine learning algorithms are discussed in addition to capacity destiny directions inside the use of gadget studying to optimize statistics retrieval.
The innovative brand “The internet of Me” is a recent research area that highlight the prevalence of personalization across the internet and focuses on the user habits and actions tracked from his interaction with the web content. This paradigm presents an efficient way to define the user experience, preferences useful in e-commerce, marketing, social and search purpose. In this paper we are interested in the integration of the advances of user profile in information retrieval systems by predicting the user preferences from his user interaction with social content. These data are exploited and modeled to be used for tailoring the query interpretation for the user and delivering more relevant and accurate results for the user. As for as the experiments, our proposed approach of user profile modeling shows promising results applied for the personalization of a defined set of ambiguous queries retained by thirty users having different profiling characteristics.
The product of this research work takes raw atmospheric science data as input, and generate clean, standardized, and redundancy-free data as output. There are two major tasks involved for the work: data processing and information retrieval. Data processing involves removing inaccuracies and resolving inconsistencies among data. Information retrieval involves identifying the information needed, extracting the data, and consolidating similar entries. Given the complexity of the process, various techniques have been used in the development. In particular, fuzzy matching and fuzzy rule-based inference engine have been used for removing inconsistencies among data entries, retrieving information from certain sections of data files, and consolidating information from different sources. A rule-based forward chaining system is chosen to represent the factors that associate with the type of measurement as well as their interrelationships, and then make decisions on the category of the measurement. The retrieval of instrument information is aided by a preprocessor based on the natural language processing (NLP) technique. The output of NLP is used to match with an entry in the instrument dictionary using a fuzzy rule-based system to determine the instrument type. A software package based on the algorithms presented in this paper has been developed using a single programming language; the package has been deployed for real-world applications.
The task of finding data files related to an information need from a group of information resources is known as Information Retrieval. In this work, the author propose a multi-lingual information retrieval system using deep learning. Input to the system is a question in sentencing form that can be processed by NLP tools. In the preprocessing phase, part-of-speech tagging of the input sentence is performed. A three layer neural network is used for creating word to vector representation. The word2vec model continuous-bag-of-words (CBOW) is used for this purpose. Then related words are obtained via word-2-vec using deep learning RNN. RNN is the recurrent neural network. Finally, results are obtained by calculating the cosine similarity score. For multi-lingual results, bilingual mapping is performed using CFILTs bilingual corpus. The tourism dataset is used for experimentation purposes.
In this paper we propose the use of multilevel classification techniques similar to concept of Bayesian belief networks for Combining Words and Pictures (Images) for Museum Information Retrieval. We have designed our own corpus on Allahabad Museum. This approach is static which allows one to compute the rank of documents of relevant words and pictures with respect to some query and a given corpus. In our case, we view combining words and pictures as a task in which a training dataset of tagged pictures is provided and we need to automatically combine the query relevant words and pictures. To do this, we first describe the picture using feature vector. We do static analysis over computed features to get distinguishing feature descriptors. Maximum similarity i.e. minimum distance allows us to find the query relevant combined pictures and associated relevant words. For textual part of the query we compute the concepts (keywords as well as synonyms of each keyword in the query and their categories). Using the concept of image hierarchy, we calculate the score of each labeled document and select top five documents with its associated pictures.
Text-based Question Answering (QA) is an essential task in natural language processing that aims to provide relevant and accurate answers to users’ queries. Traditional approaches to QA relied on rule-based systems and hand-crafted features. However, recent advancements in information retrieval and deep learning have shown promising results in improving the performance of QA systems. The combination of IR and deep learning techniques has led to significant improvements in QA performance. Hybrid models, such as the bi-encoder and tri-encoder architectures, have been proposed to leverage the strengths of both IR and deep learning models. Additionally, pre-training techniques, such as BERT and RoBERTa, have been shown to improve the performance of QA systems by providing pre-trained contextualized word embeddings. This study provides a brief overview of the QA system and its different domains and subdomains. This study also provides review of different proposed models and different datasets available for the task and evaluate their performance using some performance metrics. A comparison between various techniques are employed using the obtained result.
Musical genre and mood classification techniques combining lyric and audio features for Music Information Retrieval (MIR) have been studied widely in recent years. This paper investigates the performances of musical genre and mood classification using only lyric features. In this preliminary study, the Part-of-Speech (POS) feature is utilizes for classification of a collection of 600 songs. Ten musical genre and mood categories were selected respectively based on a summary from the literature. Experiments show that classification accuracies for mood categories outperform genres.
Delay and Tolerant Network (DTN) techniques are designed to address data communication challenges in network scenarios, which suffer from intermittent connectivity and frequent partitions. A lot of work about routing has been done, but there is little to provide an efficient information retrieval scheme. A content based information retrieval system for DTNs is studied in this paper. Differing from the denounced information access pattern of Internet, destination host of a request is not required. Content replication and indexing is adopted to accelerate response speed. Each node caches its data to the nodes which is more active, and some nodes stores the indexes to help determining the destination of the queries. Simulation results show that the mechanism of caching and indexing can acquire performance improvement.
The paper investigates various factors (System Reliability, User Efficacy, User behavior and User cognitive skills) that affect a user while retrieving information. These factors can be used to improve the efficiency of the system. In this research, we have created a model improving the classical IR model. The factors that affect the user are clubbed together to form a User Characteristic Data (UCD). The system interacts with the UCD to obtain the results as per the user query. The model takes in user's relevance feedback to update the UCD. The objective here is to retrieve better results every time the user wants some information. Therefore, the IIR models should consider these factors while evaluating the documents for the query given by the user for his information need.
In this paper, we study how to use the search session information to improve the retrieval accuracy. We propose a session-oriented retrieval model based on Markov random field. This model introduces the correlations between query terms as a retrieval factor into the retrieval process. It also presents a dynamic update algorithm based on the analysis of users' search behavior. Our model implements a complete session-oriented information retrieval framework finally. We use ClueWeb09 category B dataset and TREC 2010 (2011) Session dataset to quantitatively evaluate the model. Experimental results show that our model can improve retrieval performance substantially using the search session information.
Photos can be treated as life logs of photo owners. Photos can be reliable information to estimate patterns of actions and movements of the owners. Based on this discussion, we are developing an interactive technique to explore the recommended tourist spots based on their past personal travel photos. The technique extracts a set of keywords from the photo set applying a generic object recognition and constructs a tree structure to support the exploration of the keywords. When a user selects a set of interesting keywords, the system provides travel information related to the selected keywords. Our previous paper already introduced the visualizations that demonstrate the appropriateness of the structure of the keywords. This paper focuses on the mechanism for interactive travel information retrieval of our system and user evaluations with this system.
Introduction to the Semantic Web is the chances for easier and effective access to the constantly increasing heterogeneous data on the Web. Currently, the data is able to be retrieved semantically rather than through traditional keyword based searches, which usually return lots of irrelevant information. However, one of the main challenges of the Semantic Web is that data are stored in a structured RDF triple format and are retrieved using complex structured triple represented queries, such as SPARQL, instead of preferred natural language queries and this problem remains subject to research. The proposed AutoSDoR, meaning Automated Semantic Document Retrieval, enables the semantic formulation of natural language queries to structured triple representation based on the machine learning approach in order to retrieve documents from the structured RDF triple format. Additionally the research goes beyond small fragment queries, such as in FREyA to paragraph length query. Automatic disambiguation of query terms that are not covered in WordNet is also proposed, which contributes to the increase in precision and recall of the retrieved document.
The authors' information retrieval approach automatically extracts users' intentions when they interact with a device to access information, obviating the need for keyword inputs. The approach extracts these intentions by analyzing basic operations such as zooming, centering, and panning on a map, and applying them as a basis for retrieving related information.
Nowadays growing number of popularization in the World Wide Web promotes e-learning via web. During e-learning the users can easily share, reuse, and organize the knowledge. Using the search engine the e-learners search the web pages by set of keywords. But the pages which are unrelated for our tags come frequently are the major problem nowadays. There is always a semantic gab between searching the web pages and its representation. Ontology Based Text Mining (OBTM) with the help of human concept makes the search meaningfully and gives the relevance site first. Here e-learning with the help of several OBTM techniques such as Concept Weight Based Ontology (CWBO), NLP Based Text Mining (NBTM), Based on Data Quality (BDQ), Personalization (P), Question Answering System (QAS) and Rule Based Recommender System (RR) are analyzed. In this paper we proposed the ontology based information retrieval system using web ontology languages and analyzed the importance of handling concepts using tools Wordnet or Hownet. Here ontology is created after the preprocessing process with e-learning documents such as stop word removal, stemming, and whitespace removal and so on. We proved that the proposed ontology based NBTM information retrieval technique is efficient and effective in terms of precision and recall parameters.
We consider the problem of multi-message private information retrieval (MPIR) from N non-communicating replicated databases. In MPIR, the user is interested in retrieving P messages out of M stored messages without leaking the identity of the retrieved messages. The information-theoretic sum capacity of MPIR CP is the maximum number of desired message symbols that can be retrieved privately per downloaded symbol. For the case P ≥ M/2, we determine the exact sum capacity of MPIR as C=1/1+M-P/PN For P≤M/2, we develop lower and upper bounds for all M, P, N. These bounds match if the number of messages M is an integer multiple of the number of desired messages P, in which case, C = 1-1N/1-(1/N)M/P. Our results indicate that joint retrieval of desired messages is more efficient than successive use of single-message retrieval schemes.
Given the enormous volume of textual data generated in the healthcare sector, effective and accurate retrieval systems are essential. A major challenge is presented by the explosive growth of scientific publications and medical information. In this study, an advanced pipeline was developed to enhance information retrieval in the healthcare domain. The pipeline has two components: information retrieval and evaluation. The information retrieval component is composed of the retriever, which uses BM25, and the reader, which is powered by a pre-trained Large Language Model. The evaluation component, which uses standardized dataset formats such as SQuAD, provides a framework for evaluating system performance and comparing different parameters. Based on the Cancer category in the MedQuAD dataset, the retriever component showed a strong recall of 0.881 and a Mean Reciprocal Rank score of 0.804, demonstrating its effectiveness in retrieving relevant information and accurate ranking. A Semantic Answer Similarity score of 0.677 for the reader component indicates room for improvement. This work has implications for healthcare providers and the text-mining community working in health information retrieval.
For information retrieval systems, the quality of searching is most important. There are many techniques to improve the quality of search, the query expansion (QE) is considered in order to improve the search result in terms of recall and user satisfaction. This paper proposed the query expansion technique using keyword-based query. These semantic terms are added to the original query in order to reformulate query before sending to the searching process. The experiments were tested on twitter data collection which class is annotated by human. The results show that the retrieval effectiveness is considerably higher than using only original query. Compared to the base-line system, this method provides higher performance in terms of recall and precision.
The research reported in this paper explores the proposed semantic information retrieval (IR) system to overcome the problem of current IR systems by providing direct, relevant and accurate results to the end user in less time. The proposed system will drastically improve the performance over the current IR systems in terms of recall and precision.
Medical information retrieval plays an increasingly important role to help physicians and domain experts to better access medical-related knowledge and information, and support decision making. Integrating the medical knowledge bases has the potential to improve the information retrieval performance through incorporating medical domain knowledge for relevance assessment. However, this is not a trivial task due to the challenges to effectively utilize the domain knowledge in the medical knowledge bases. In this paper, we proposed a novel medical information retrieval system with a two-stage query expansion strategy, which is able to effectively model and incorporate the latent semantic associations to improve the performance. This system consists of two parts. First, we applied a heuristic approach to enhance the widely used pseudo relevance feedback method for more effective query expansion, through iteratively expanding the queries to boost the similarity score between queries and documents. Second, to improve the retrieval performance with structured knowledge bases, we presented a latent semantic relevance model based on tensor factorization to identify semantic association patterns under sparse settings. These identified patterns are then used as inference paths to trigger knowledge-based query expansion in medical information retrieval. Experiments with the TREC CDS 2014 data set: 1) showed that the performance of the proposed system is significantly better than the baseline system and the systems reported in TREC CDS 2014 conference, and is comparable with the state-of-the-art systems and 2) demonstrated the capability of tensor-based semantic enrichment methods for medical information retrieval tasks.
In order to provide high quality of network information service for enterprise, the integration system of network information resources is designed based on multi-agent collaboration. The system may be logically divided into three function modules: User Module, Processing Module and Search Module. This paper presents the essential function of each constituent part, collection workflow, evaluation workflow, Web information retrieval workflow, and image information retrieval workflow of network information resources. The key technologies are also discussed in detail, including User Interest Model matching algorithm for user personalized service, and content based image retrieval algorithm. The system is provided with intelligence, flexibility, and robustness.
Current agricultural information technologies mostly rely on single-modal data and lack the connections between images and text. For instance, image-based disease recognition requires first identifying the disease in the image and then retrieving relevant knowledge, which is less efficient in information retrieval and prone to information errors. In this paper, we apply cross-modal retrieval technology to the agricultural information processing field and propose a deep learning framework that realizes cross-modal image and text retrieval of citrus diseases and pests. Specifically, we use a CNN backbone to extract global spatial features of the image instead of Faster R-CNN used by mainstream methods for visual embedding. We fine-tuned RoBERTa to encode the language information of a sentence and trained new embedding representation for keywords in citrus domain sentences, then we used a multi-modal Transformer to learn cross-modal attention for different modal data and drive the model to learn how to perceive the similarity between input samples through matching loss and the label loss. We conducted experiments and ablation studies on a self-collected cross-modal retrieval dataset of citrus diseases and pests and compared them with mainstream methods, the results showed that our method achieves satisfactory results in image-to-text and text-to-image two retrieval tasks.
Biodiversity knowledge can be considered as complex information that involves several levels including genes, species, and ecology. As the data is complex, it may exist in form of unstructured and unorganized. The ways researchers collect their data according to their interest needs can worsen this situation because there are no standard ways of doing it. Also, the imperfect collaboration between the biological scientist and the information technology community can result in inconsistent data retrieval. These problems can be overcome by the development of retrieval application as it assists in managing and retrieving biodiversity information. The collection of biodiversity data is stored in the database. By doing this way, it will be easier to be managed as it is kept in the most structured way. For retrieving the information, this application received the user’s query as an input. The query will go through several natural language processing techniques including tokenization of a query, removing the stop words, and lemmatizing the generated tokens to produce better output for the user. The process will be repeated until the user is satisfied with the output he or she is looking for. Several test cases have been done in the testing phase and this application has proven to work accordingly based on the conducted analysis. Recall and precision metrics were also being conducted and the result is quite promising. Further research is recommended for producing a better result by improving the query processing techniques and adding more data to the database.
With the continuous improvement of Internet technology, how to quickly and accurately find the target content in the Internet's massive big data has become the focus of current IT research. This paper first explained the function and the architecture of Lucene, and discussed the index technology and search technology of Lucene. Moreover, the basic architecture of a small-scale traffic illegal act retrieval system based on Lucene technology was analyzed in detail. This system was designed effectively from three aspects of crawling information, index database construction and data retrieval. The results will provide the application basis for better embedding Lucene toolkit into the station search engine.
This work presents the Data-Information Retrieval based Automated Ontology framework for Service Robots. The paper focuses on the Dynamic Semantics which implemented the Object Finding scenario. The goal is to present a framework regarding automated ontology building without human involvement and the Object Finding scenario aims at solving the problem of dynamic ontology. The Object Finding scenario combined FileN algorithm and the Least Recently Used and the Least Frequently Used technique in order to calculate the future location of object. The result of scenario will reveal the effective way to the prediction of future location of specific object.
To determine which documents are relevant and which are not to the user query is one central problem broadly studied in the field of information retrieval (IR). Learning to rank for information retrieval (LR4IR), which leverages supervised learning-based methods to address the problem, aims to produce a ranking model automatically for defining a proper sequential order of related documents according to the given query. The ranking model is employed to determine the relationship degree between one document and the user query, based on which a ranking of query-related documents could be produced. In this paper we proposed an improved RankGP algorithm using multi-layered multi-population genetic programming to obtain a ranking function, trained from collections of IR results with relevance judgments. In essence, the generated ranking function is consisted of a set of IR evidences (or features) and particular predefined GP operators. The proposed method is capable of generating complex functions through evolving small populations. LETOR 4.0 was used to evaluate the effectiveness of the proposed method and the results showed that the method is competitive with RankSVM and AdaRank.
In modern healthcare practices, diagnosis and treatment for certain complex illnesses require specific information on the patients' background, genealogy, heredity, demographic data etc. Even with a similar diagnosis, treatments may need to designed specifically to adapt well to the patients' genetic, cultural, and lifestyle aspects. Precision medicine mainly deals with enabling personalized care based on a given patient's conditions in a scientifically rigorous way. Because this entails recommending personalized therapies to patients and has the potential to affect the health of other people, the performance of a designed system must be accurate and exact. In this paper, a precision information retrieval system is proposed that leverages structured and unstructured data to retrieve relevant knowledge for enabling personalized recommendations. The proposed pipeline is validated with the clinical trial dataset of the Precision medicine track of TREC 2017. A set of relevant ranked clinical trials for a given condition/disease that could not be cured using any of the traditional treatments suggested are retrieved using structured and unstructured patient data. We employ multiple IR techniques like Best Match 25, query reformulation and re-ranking facilitated through deep neural networks, focusing on extracting highly accurate and relevant trials. The proposed pipeline achieved a high score of 0.58 in terms of Normalized Discounted Cumulative Gain (NDCG) score for ranking the relevant clinical trials, outperforming the state-of-the-art approaches.
Geographic Information Systems (GIS) do not allow the current management information to be changed continuously. The information value is assumed to be constant, and evolution occurs in a discrete manner depending explicitly requested changes to the system. The mobile agent can migrate autonomously to sites it deems interesting. This new approach has increased the applications robustness for distributed systems, ensuring the network load reduction, reliability and autonomy of components and performance. Our work is in the field of geographic information retrieval whose goal is to provide an approach to find and the acquisition of the relevant geographical information, using the agent paradigm.
Design of smart cheerleading competition assistant evaluation system with big data and information retrieval sorting algorithm is studied in the paper. The information retrieval and sorting method has well attracted more and more researchers' attention in recent years because of its better retrieval speed and lower storage cost. Its core idea is to transform the feature vector in the high-dimensional space into the Hamming space. low dimensional. For a data sequence a, sorting refers to arranging all data in a sequence from small to large, and then determining the position of any data in a in the sequence. With this theoretical basis, the smart cheerleading competition assistant evaluation system is implemented. For the efficient analysis, the combination pattern is integrated, and the UI is implemented. Through the experiment, the system details are demonstrated.
In this paper musical instrument recognition and retrieval for fifteen musical instruments from different instrument families are discussed. The system is implementing in three stages; first stage is pre-processing, second is feature extraction and third is recognition and retrieval. Musical instruments are retrieved using most important and distinguishable features like temporal and cepstral features. Kohenon self organizing map has been used as classifiers. The average accuracy is achieved for fifteen instruments are recorded 92.98%. The experimental results also show that the better recognition rate is obtained for LPCC as compared to MFCC and temporal for all the musical instruments.
A price information retrieval (IR) system allows users to search and view differences among prices of specific products. Building product-price driven IR system is a challenging and active research area. Approaches entirely depending products information provided by shops via interface environment encounter limitations of database. While automatic systems specifically require product names and commercial websites for their input. For both paradigms, approaches of building product-price IR system for Vietnamese are still very limited. In this paper, we introduce an automatic Vietnamese IR system for product-price by identifying and storing Xpath patterns to extract prices of products from commercial websites. Experiments of our system show promising results.
Productive and effective content retrieval methods are critical in dealing with the expanding measures of textual information accessible in electronic form. However, information retrieval is an overwhelming job. The paper presents an enhancement over naive information retrieval model i.e., Vector Space Model (VSM), an algebraic model combined with Cover Density Ranking (CDR), leading to a better computational approach to deal with documents with the same rank. Most of the Retrieval techniques depend upon keyword indexing, frequency of keywords or index terms called as term frequency(tf). But unfortunately, term frequency alone can't be enough to catch the contents of the document, resulting in poor retrieval performance since several documents may fall under the same rank. To obtain the most relevant document there is a need in improvising VSM by not just taking the term frequency into consideration but also the positions of the terms in the document.
We consider private information retrieval (PIR) of a single file out of K files from N non-colluding databases with heterogeneous storage constraints m = (m1, ⋯, mN). The aim of this work is to jointly design the content placement phase and the information retrieval phase in order to minimize the download cost in the PIR phase. We characterize the optimal PIR download cost as a linear program. By analyzing the structure of the optimal solution of this linear program, we show that, surprisingly, the optimal download cost in our heterogeneous case matches its homogeneous counterpart where all databases have the same average storage constraint μ = 1/N Σ n=1 N Thus, we show that there is no loss in the PIR capacity due to heterogeneity of storage spaces of the databases. We provide the optimum content placement explicitly for N = 3.
Pseudo relevance feedback (PRF) enhances the retrieval performance of the relevance feedback. Pseudo relevance feedback assumes that the k highest-ranking documents in the first retrieval are relevant and extract query expansion from them. Rocchio algorithm is a classical algorithm for implementing relevance feedback into vector space models. The Rocchio algorithm forms a new query moves toward the centroid of the relevant documents and keeps away from centroid of the irrelevant documents. However, in the relevance feedback method, irrelevant documents are ignored. In this paper, we conduct a method for pseudo irrelevance feedback (PIRF) documents components that effectively applied to the Rocchio algorithm. Documents with a high ranking outside of k relevant documents and those documents dissimilar to any k relevant documents can extract good query expansion if the documents are applied as irrelevant documents. The Rocchio algorithm uses PRF as a component of relevant documents and this research method for irrelevant documents as a component of irrelevant documents denoted by Roc PRF PIRF (filter). Experiment on CISI dataset show that Roc PRF PIRF (filter) improved performance by testing several variations the number of irrelevant documents compared to the standard Rocchio algorithm and Rocchio algorithm with irrelevant documents but without proposed method).
This paper investigates the effectiveness of a state of the art information retrieval (IR) system in the verse retrieval problem for Quranic text. The evaluation is based on manually indexed topics of the Quran that provides both the queries and the relevance judgments. Furthermore, the system is evaluated in both Malay and English environment. The performance of the system is measured based on the MAP, the precision at 1, 5 and 10, and the MRR scores. The results of the evaluation are promising, showing the IR system has many potential for the Quranic text retrieval.
In recent years, the rapid development of Internet of Technology (IOT) makes the intelligent home come true as people expect. The intelligent home system creates the more comfortable, safer, humane and intelligent living environment. It can resolve the problems facing by the people who have busy schedules and get a very less amount of time to spend at home which is increasing rapidly around the world. For the solution of this problem, user can depend on the automated machines and gadgets like smart phones. These smart gadgets are using cloud computing which sends and receives signal o the cloud. The data that is of our use can be fetched by matching some key values using the concept of information retrieval. The key objective of this paper is to create a full-fledged application which could let user to operate the lights of their house from any remote location. The user have a list of options to select which light is to be on and when. The only requirement is to have working wifi at home to which the lights are connected. The is developed in Lua Language by using the Esplorer Integrated Development Environment (IDE). We have also used the micro controller chip ESP 8266 to build our board.
Science and technology literature retrieval system is commonly used by researchers as document retrieval tools. The classic information retrieval models such as Boolean Model [1], Vector Space Model and Probabilistic model neglect the position of query words in every paper. We propose a weighted term frequency model (WTFM) based on term frequency, and through a simulated annealing algorithm to learn the weighted factor. The results of experiments show that our weighted factors model gets better performance than ordinary models.
This article presents a new Unification Matching Scheme (UMS) for information retrieval using the genetic algorithm. The selection of appropriate matching functions contributes to the performance of the information retrieval system. The proposed UMS executes the Unification function on three classical matching functions for different threshold values. The main objective is to utilize all the base functions to increase the relevancy of a users query with the data objects. The best results from each matching function define the new generation on, which the other matching functions are applied. The results from each generation are optimized using the Genetic Algorithm. The working of UMS is compared with individual classical matching functions. A significant improvement is seen in the experimental results in terms of precision and recall. The performance increased/increases gradually, in each generation thereby, producing the relevant results.
Retrieval of proper information from the collection of available data on Internet in a cross-lingual platform is a tedious task. For the retrieval of information, the user specifies the required data in the way of query. In some cases, the query could not clearly define the required information specifically owing to the ambiguities and is solved by the use of Google translator. This paper presents a new Tamil-English cross lingual information retrieval (CLIR) where the queries in Tamil language are used to retrieve related documents from English language. The proposed model uses Maximum Entropy Principle based Document Ranking with Term Selection Analysis (MEPDR-TSA) for CLIR. The presented MEPDR model distributes the term frequency weights between the documents subjected to a provided query. In TSA, the term selection value is used to select the retrieved documents from each query. In addition, Google translator is used to translating the query into English and then again translate the retrieved English documents into Tamil language. The performance of the MEPDR-TSA model has been tested against several Tamil language based queries and the results are investigated interms of precision, recall, and F-score.
As predicted by Internet Data Center (IDC), the amount of global language data will exceed 40ZB by 2020. With the globalization of information, it has become an urgent matter for current web retrieval to break the barriers between languages. In this paper, we propose to integrate semantic and lexical information to deal with the task of cross-language information retrieval (CLIR). The approach does not rely on external knowledge bases thus to avoid that knowledge bases cannot deal with net neologism. Experiments on Sogou dataset show the feasibility of the approach.
We present VisIRR, an interactive visual information retrieval and recommendation system for large-scale document data. Starting with a query, VisIRR visualizes the retrieved documents in a scatter plot along with their topic summary. Next, based on interactive personalized preference feedback on the documents, VisIRR collects and visualizes potentially relevant documents out of the entire corpus so that an integrated analysis of both retrieved and recommended documents can be performed seamlessly.
This paper examines the way of relevance feedback issue in persistent presentation space with regards to media information or data retrieval. Accentuation is put on investigating the unlikeness of issue and looking at the presumptions, executions, and benefits of different solutions in the literary works. This paper shows the study of relevance feedback techniques that has been utilized as a part of past research, suggests a use of different query remodeling mechanism that can be utilized in many retrieval frameworks or systems, and generates some rules for proficient outline of retrieval system's element i.e. relevance feedback.
Educational materials, including guides, tutorials, and master plans, are universally presented in e-book formats. This facilitates a comprehensive understanding for academics, encompassing both technical intricacies and broader conceptual frameworks. E-books offer several benefits, such as searchability and the incorporation of links to additional information sources. However, many individuals express concerns that e-books are not particularly comfortable for extended reading periods. In the other hand, a Generative AI approach is employed for the development of an intelligent chatbot. Our primary contribution lies in an automated information retrieval method, involving the design of a PDF-Driven Chatbot using Large Language Models (LLMs) in the context of faculty guidelines question answering. This research utilizes the LangChain Framework, OpenAI’s Chat-GPT (GPT3.5 Turbo), and Pinecone for generating responses. The outcomes demonstrate that the chatbot is capable of generating coherent responses closely aligned with the context of the PDF document.
Recently, researchers mainly focus on three categories of models in the field of Information Retrieval (IR), namely vector-space models, probabilistic models, and statistical language models. The existing studies have always developed IR models through refining or combining these traditional models. However, some new frameworks (e.g., digital signal processing (DSP)-based IR framework) have not been well-developed. In our research, we propose a new DSP-based IR Framework (DSPF) introducing the theories from the field of the DSP and present two corresponding DSP-based IR models, denoted as DSPF-BM25 and DSPF-DLM, which incorporate the term weighting schemes from two well-performed probabilistic IR models, the BM25, and the Dirichlet Language Model (DLM). In particular, first, we consider each query term as a spectrum with Gaussian form. Second, instead of transforming the signals from the time domain to frequency domain, we directly represent the query terms in the frequency domain. It is much more controllable and precise to adjust the values of the parameters for getting better performance of our proposed models. To testify the effectiveness of our proposed models, we conduct extensive experiments on seven standard datasets. The results show that in most cases our proposed models outperform the strong baselines in terms of MAP.
Cloud Computing is emerging technology that provides services of storage and platform software to large organizations, but some of them are still hesitant to shift their setups on the cloud due to security issues and risks. Thus, it is important to address the security issues and problems in cloud systems. In this research we contributed to a multilevel security (MLS) framework based on data sensitivity and security that provides adequate level of data security based on various classifications and categories. The proposed multilevel security embedded information retrieval tool in this paper encompasses suitable access control combined with Security Enhanced Linux (SELinux) that facilitates classification of the data based on subsequent changes in the sensitivity levels of the data and changes in the security measures to cope with the dynamic and vulnerable changes in cloud security threats. To implement the proposed MLS framework, the SELinux system is applied as a testbed to retrieve information and track the history of the data retrieved.
Stemming is a technique used to reduce inflected and derived words to their basic forms (stem or root). It is a very important step of pre-processing in text mining, and generally used in many areas of research such as: Natural language Processing NLP, Text Categorization TC, Text Summarizing TS, Information Retrieval IR, and other tasks in text mining. Stemming is frequently useful in text categorization to reduce the size of terms vocabulary, and in information retrieval to improve the search effectiveness and then gives us relevant results. In this paper, we propose a new multilingual stemmer based on the extraction of word root and in which we use the technique of n-grams. We validated our stemmer on three languages which are: Arabic, French and English.
Based on the popularity of social networks, a large amount of information is generated every day. This information contains a lot of useful data, but there is also a lot of useless or harmful information. This article focuses on how to construct information retrieval (IR) and classification model of social network by using graph neural network (GNN) algorithm. Firstly, the social network data are preprocessed by data cleaning, deduplication and labeling, so as to eliminate redundancy and noise information and meet the needs of subsequent model training. Next, the GNN model is trained by using the preprocessed data set, and the model suitable for IR and classification is established. Finally, the model is evaluated and tested to fully consider its accuracy and efficiency. The results show that this algorithm can make full use of graph structure information in social networks, find key information and hidden community structure, improve the efficiency and accuracy of IR and classification, and provide an effective theoretical basis for practical application in the field of law.
Over the years, the volume of information available through the world wide web has been increasing continuously, and never has so much information readily available and shared among so many people. Unfortunately, the unstructured nature and huge volume of information accessible over network have made it difficult for users to shift through and find relevant information. The information retrievals commonly used are based on keywords. These techniques used keyword lists to describe the content of information, but one problem with such list is that they do not say anything about the symantic relationships between keywords, nor do they take into account the meaning of words or phrases.
This research paper targets to look at the usage of machine gaining knowledge of techniques and algorithms in terms of massive statistics retrieval overall performance. Specifically, this paper explores the efficacy of supervised and unsupervised studying based fashions for large facts retrieval situations, such as type, clustering and optimization obligations. Additionally, this paper examines the challenges associated with each method, consisting of scalability, complexity, and accuracy. Sooner or later, this paper presents a comparison between various systems mastering algorithms in order to higher apprehend the alternate-offs among distinct techniques when applied to comparable hassle regions. Via this studies, its miles meant to offer a complete overview of the benefits and disadvantages of using machine gaining knowledge of models for large information retrieval.
The purpose of the following report is to introduce a model that makes it possible to efficiently search data by using keyword-based concept network for reliable access of information which is rapidly increasing in the mobile cloud. A keyword-based concept network is a method with the application of ontology. However, the proposed model is added by association information between keyword concepts as a method for a user's efficient information retrieval. Furthermore, the proposed concept network consists of the keyword centered concept network, expert-group-recommended field concept network, and process concept network.
Identifying the location of faults in real-world programs is one of the most costly processes during software debugging. In order to reduce debugging effort, many fault localization techniques have been proposed. One of the most widely studied technique is called Spectrum-based fault localization (SBFL), which uses the coverage information and execution results of test cases to do fault localization. Most SBFL techniques only consider the binary coverage information and ignore the execution frequency, so their fault localization accuracy is limited, especially when faults occur in the iteration entities or loop bodies. In this paper, we propose IRBFL, a novel fault localization technique based on information retrieval to extract information from execution frequencies of program entities. IRBFL uses mutation analysis to reduce the low suspicious classes, and then it adopts information retrieval techniques to calculate the suspiciousness value. We evaluate IRBFL on 205 real-world faults from 5 programs in Defects4J benchmark. The experimental results show that our proposed method outperforms the other five state-of-the-art SBFL techniques. More specifically, no matter in single-fault or multi-fault programs, IRBFL can identify 2 to 3 times more faulty methods than the other five SBFL techniques when checking the top 1 method. More empirical results in terms of other metrics, including acc@3, acc@5, EXAM, MRR, and MAP, also indicate that IRBFL technique is better than the other five SBFL techniques.
Standardized Electronic Healthcare Records (EHRs) data have a complex structure. They are based on dual model approach where domain knowledge is saved as an archetype [1, 2]. It is technically very difficult to generate query language for medical expert to retrieve the data. It gives rise to issue of retrieval and usability of electronic healthcare records databases. To improve the retrieval process, Graphical User Interface (GUI) must be simplified. Other issues related with electronic healthcare record systems are mobility, reachability and optimal use of advancements in information technology. This paper highlights the problems and issues of electronic healthcare record systems. It investigates the available graphical user interfaces for electronic healthcare record databases and compares their usability. Finally, it presents the comparison of a graphical user interface for a standardized Electronic healthcare records application for various handheld mobile devices.
The work presented in this paper ascertains role of Long Sort-Term Memory (LSTM) neural network in Math Information Retrieval (MIR). Motivated from promising performances of the LSTM for sequence-to-sequence tasks, an LSTM based Formula Entailment (LFE) module is implemented for recognizing entailment between mathematical user query and document formulae. The LFE module is trained and validated using a symbol level Math Formula Entailment (MENTAIL) dataset. The relevance of a document is determined by the fraction of document formulae which entail the user query. A reasonable score of 0.45 for the P_5 evaluation measure substantiates competence of the implemented MIR system in retrieving relevant documents corresponding to a mathematical user query.
We study the problem of weakly private information retrieval (W-PIR), where a user wishes to retrieve a desired message from N non-colluding servers in a way that the privacy leakage regarding the desired message’s identity is less than or equal to a threshold. We propose a new code construction which significantly improves upon the best known result in the literature, based on the following critical observation. In previous constructions, for the extreme case of minimum download, the retrieval pattern is to download the message directly from N−1 servers; however this causes leakage to all these N−1 servers, and a better retrieval pattern for this extreme case is to download the message directly from a single server. The proposed code construction allows a natural transition to such a pattern, and for both the maximal leakage metric and the mutual information leakage metric, significant improvements can be obtained. We provide explicit solutions, in contrast to a previous work by Lin et al., where only numerical solutions were obtained.
A Tsunami Detection System can be created using highly efficient Wireless Sensor Networks comprising a network of distributed sensors around water bodies. Tsunamis occur in coastal areas which can be detected by sensors that are deployed underwater. A Tsunami Detection System is used to detect tsunamis and spread warning messages to prevent loss of life and establishments. This paper presents a novel framework for implementing Intelligent Information Retrieval Technique in a Tsunami Detection System using Wireless Sensor Networks. This system generates intelligent data and spreads a warning message about the occurrence of tsunami. The proposed method uses SentiWordNet, a lexical resource explicitly used for opinion mining. This framework extracts intelligent information from the messages which are transmitted by first filtering the redundant text from them followed by stemming and stop words removal and then gathers all the opinion words and calculates their sentiment values using SentiWordNet. Thereafter polarity of the text is calculated and a warning message is conveyed. The proposed method is implemented for determining the magnitude of earthquakes, which is very essential to generate tsunami warnings.
This study focuses on the preprocessing of medical paper retrieval, aiming to address the challenge of extracting information from lengthy medical texts. This process is crucial in facilitating effective medical research. We propose BioBERT Preprocessing Model (BBPM). By employing the cutting-edge medical BERT model, we predict the subject words of the paper’s abstract. Subsequently, based on these subject words, we calculate secondary similarity using the corresponding medical model. Finally, we incorporate the most strongly associated words to augment the associative vocabulary of the article, thereby enhancing its effectiveness in subsequent text retrieval. Experimental results show that BioBERT shows superior performance in predicting subject words in long medical texts, with a basic text similarity of about 90%. The primary contribution of this paper lies in the integration of the latest BioBERT model for preprocessing the abstracts of medical papers. This yields additional subject terms necessary for retrieval. These terms are then integrated with the title keywords into the final retrieval system, leading to more efficient retrieval outcomes. This approach promises to provide fresh insights into medical information retrieval.
This paper describes the cross-language plagiarism detection method CLAD (Cross-Language Analog Detector) between the test document and indexed documents. The main difference of this method from existing versions is the detection of plagiarism among multiple languages not only two languages and checking all synonym versions of every term which using during compare documents. While translating terms, it uses the dictionary-based machine-translation method. CLAD's working process consists of document indexing and detection process phases. In this paper, we have described both of these phases.
With the development of natural language processing and artificial intelligence, remarkable progress has been made in the field of information retrieval In this paper, we selected papers on information retrieval from web of Science in the past ten years, and constructed a Co-word network based on the co-occurrence relationship of keywords in the same paper. By analyzing the evolution of the Co-word network community, the core evolution and the number of keyword status, we discussed the evolution of research focus in the field of information retrieval in the past ten years. The results show that natural language processing, medical information retrieval, image retrieval and social media retrieval are the focus of information retrieval research in recent years.
With online publications, the current Web has become the largest source of digital documents, often stored in HTML, XML, PDF or DOC. Among the features of documents, note especially their logical structure, which represents their components such as chapters, sections, paragraphs, the document title, chapter titles, sections, etc. The section headings are meaningful; they are a good indicator of the content of paragraphs. For this reason we pay particular attention to these titles during the indexing process and research. Our objective is to provide relevant access to digital documents, by the process of all sections titles to take advantage of their mining and importance in the research process. Experiments on a large corpus, INEX 2009 show effectiveness of our proposition an improvement in the precision of the results in IR.
Relevance feedback techniques are important to Information retrieval (IR), which can effectively improve the performance of IR. The feedback includes positive and negative relevance one. The most of the previous work using feedback have focused on positive relevance feedback and pseudo relevance feedback in IR. In recent years, some work has been done and investigated the negative relevance feedback in IR. However, this paper highlights the incorporation or integration between the language models based positive and negative relevance feedback in IR, and through positive and negative feedback documents proportion on queries classification, with different parameters adjustment of positive and negative feedback ratio, where both types of feedback are used to modify and expand the user's query model. Our experimental results of using several TREC collections show that this method is significantly outperform the relevance feedback and pseudo relevance feedback in terms of the retrieval accuracy.
Music is possibly the most impactful bonding over the society and culture. The process of perusing the music in classrooms is considered as a costly affair for the humans in rural areas. Although the tutorials (i.e., video lectures) are usually available for free access in internet, the process of learning and evaluation yet depends on conventional teacher-student affair. So, the need for an automated tool designating the process of analysis cum assessment of music is formulated in the music eternity. This proposed model focuses on providing a solution for this use case exactly where the users can play notes or music pieces on a musical instrument (i.e., piano ) and followed by evaluating their performance against a chosen benchmark audio file, named as 'teacher' file. The technique considers various features such as loudness, tempo, rolloff frequency, kurtosis, skewness and centroid associated with piano for evaluation process. The model emphasizes towards to distinguish the characteristics of different keys and it was achieved with the help of Essentia onset function and LibROSA python package. The evaluation was carried through a set of stages such as normalization of features, pattern matching to extract an effective grading sheet of the played tune (i.e., 'Student' file). The drawbacks such as identification of the sequence of musical tones and noise onsets were avoided using a pattern matching framework, named as REMOVE_NOISY_ONSETS. The performance factor is fixed based on the notes that were missed by the user and the extra notes played by the user. Hence, this technique is of cutting-edge technology in the expertise of music education using technology trends. The proposed tool also helps people, those were deprived of learning the instrument of their choice by giving them an easy-to-use software tool to evaluate themselves and also give a lucid user interface to review their performance.
Large, scalable computer resources are made available to consumers as a service via the cloud computing system, which runs over the Internet. Both the business and scientific communities have shown a lot of interest in this technology. Nevertheless, there are no mechanisms for information retrieval and knowledge discovery in the concept of cloud computing. In other words, clouds need autonomy and intelligence. Within cloud computing settings, Web Services are an essential part of Service Oriented Computing (SOC). A single Web Service is unable to get targeted and specialized information from Web Services inside a cloud environment. Agents may be used to help in the successful composition of Web Services. The main goal of this project is to propose a system to extract information in cloud environment by employing web services along with concept of multi-agent systems.
Conventional information retrieval doesn't meet users' personalized requirements. Furthermore, personalized retrieval in recent few years focuses on mainly personalization of web information retrieval regardless of the difference between traditional information and spatial information. In this paper, a personalized retrieval strategy of spatial information is further put forward, which combines user profile with query request, and consists of following stages: Firstly, original data collected by system is cataloged into different levels, and useful information is refined from the levels; Secondly, user profiles are constructed on the basis of refined information; Thirdly, the search for spatial information from query conditions is implemented and the candidates are got accordingly; Lastly, utility degrees of the candidates aren't identical for different users in spite of the same query conditions, and so the utility degrees are achieved based on user profiles and personalized retrieval results are available after descending the candidates by utility degrees.
Markov network information retrieval model integrates computer, Atopology and possibility which is a powerful tool for representation and inference of possibility knowledge. This paper mainly introduces background and the general theory of the model. Several information retrieval models based on the Markov network are summed up from different perspective of Markov network space. Finally, the future work on the model is proposed.
Information Retrieval is the task of satisfying an information need by retrieving relevant information from large collections. Recently, contextualised pre-trained systems have surpassed traditional systems on benchmarks that provided vast amounts of context. In contrast, pre-trained systems underperform in environments where context is insufficient. Hence, this work aims to improve the performance of pre-trained systems under such resource constraints. To this aim, we turned to context augmentation. Since sentences typically consist of few terms, we recognised sentence retrieval as a context-constrained environment. However, most context augmentation methods were either ineffective in sentence retrieval or computationally expensive, while those methods that were effective in sentence retrieval and computationally cheap were unable to capture contextual information. Hence, we developed a novel context augmentation method for sentence retrieval that preserves contextual information while maintaining low latency. The proposed method allowed the pre-trained system to achieve a 19% performance gain in a context-constrained environment. Its effectiveness was also validated by participation in the EPIC-QA challenge. Specifically, it allowed the pre-trained system to rank third among 55 competing systems, with less than a 1% performance gap to first place.
In this paper we propose a path expression-based smoothing method of query likelihood model for XML element retrieval (QLMER). Though the query likelihood model, one of the statistical language models, is regarded as an accurate term weighting scheme in document retrieval, it has not been surveyed enough in XML element retrieval. Some term Weighting schemes for XML element retrieval utilize the idea of a path expression and it is effective for accurate retrieval. Therefore, we propose the path expression-based smoothing method. We are also interested in the potential power of QLMER compared with a commonly used term weighting scheme, BM25E, which is a classic probabilistic model. Our experimental evaluations showed that the proposed smoothing method is more effective than the existing one. In addition, BM25E is more effective than QLMER even though the effectiveness improved with the proposed method.
Dimensionality reduction of high-dimensional data for visualization has recently been formalized as an information retrieval task where original neighbors of data points are retrieved from the low-dimensional display, and the visualization is optimized to maximize flexible tradeoffs between precision and recall of the retrieval, avoiding misses and false neighbors. The approach has yielded well-performing visualization methods as well as information retrieval interpretations of earlier neighbor embedding methods. However, most of the methods are based on slow gradient search approaches, whereas fast methods are crucial for example in interactive applications. In this paper we propose a fast multiplicative update rule for visualization optimized for information retrieval, and show in experiments it yields equally good results as the previous state of the art gradient based approach but much faster.
In order to improve the availability of mixed English teaching resources, it is necessary to cluster the teaching resources, but the traditional clustering method is difficult to retrieve all the teaching resources information, resulting in the low accuracy of mixed English teaching resources. Therefore, a mixed English teaching resources clustering method based on information retrieval is proposed. Firstly, the structural characteristics of mixed English teaching resources are analyzed, and the piecewise linear fusion retrieval method is used to construct the multi-dimensional feature distribution constraints of mixed English teaching resources. Secondly, according to the beam domain calculation results of mixed English teaching resources retrieval, the distribution structure model of mixed English teaching resources is constructed to complete the retrieval of English teaching resources. Finally, in the collection of mixed English teaching resources, the distance between the sample and the centroid in each cluster is the distance within the cluster, and the clustering objective function completes the clustering of mixed English teaching resources. The experimental results show that the retrieval accuracy and clustering accuracy of this method are significantly improved compared with the traditional teaching resource clustering method.
In recent years, cross-modal retrieval has gradually become the frontier and hotspot of academic research at home and abroad, and it is an important direction for the future development of information retrieval. Some current methods improve the performance of image-text retrieval by exploring more comprehensive global image-text alignment information or capturing region-word local fine-grained alignment. However, previous methods did not mine more useful information to obtain more accurate matching scores. In this paper, we propose a location attention and similarity filtering network for image text retrieval. Specifically, we enhance visual-text joint embedding learning with global and local alignments. We then enhance more reliable relationships between images and text sentences by exploring the location information of objects in images through location attention. In addition, we use a similarity filtering mechanism to selectively focus on important and representative alignment information while leaving the distraction of meaningless alignment information aside to effectively integrate these alignments. Experiments on our proposed method on the public datasets Flickr30K and MS-COCO validate the effectiveness and superiority of our method.
Myanmar information retrieval (MIR) is the system of searching for the information in a document, searching for documents themselves, and also searching for texts in databases, which is written in Myanmar language. Unfortunately, many problems remain in MIR, such as word segmentation, indexing, semantic ambiguous words and many others. This paper proposes a method of semantic query expansion by using WordNet to resolve semantic ambiguous words problem in MIR. The experimental results showed improvements in evaluating information retrieval potential for the Burmese language with 65.4% precision, 78.9%recall and 70.96% F-measure.
There has been more content with various types needed by users. It comes from specific websites or databases. It causes a need to access the content through a retrieval system that can produce content that meets the demand. The existing searching applications still provide services aimed at only one particular modality, such as text, audio, or video. Therefore, research aimed at developing multimodal searching systems is still a significant opportunity in this field. In this paper, we review the multimodal information retrieval systems that have proposed in the previous research. And they discussed the components and mechanisms of the multimodal retrieval used by them. In this way, it will be clear on how they apply their approach to problem-solving on multimodality content searching. At the end of the discussion, there are some ideas or issues related to the problem.
To facilitate professional users to find multimedia files composed of music information more quickly and realize audio frequency content information retrieval, this paper proposes a solution of automatic classification using HMM. Considering the traditional timbre features, the preprocessing process of speech signal is analyzed, and the common feature parameter extraction methods are compared to denoise audio information from the aspects of structure and state number. Then HMM model based on notes is used for training and recognition to realize the feature extraction of teaching audio/video files. The simulation results show that the audio classification performance of the hidden Markov model proposed in this paper has better performance, and the optimal classification accuracy is more than 90%.
In order to overcome the problem of low retrieval efficiency in the field of clinical medicine. According to the TREC evaluation task, the paper proposes a method for constructing a clinical medical retrieval system for treatment plan to improve retrieval efficiency, aiming at being able to obtain effective treatment options by entering patient-related information to facilitate the solution given by physicians or biomedical scientists. For the specificity of the dataset, this method considers genetic and demographic characteristics to preprocess documents and topics, and improve the query using the query expansion technique based on Medical Subject Headings (MeSH); Set up a Lucene-based retrieval system to read in the converted files, and get the primary feedback results by inputting the processed query into the retrieval, and then filter the result document to get the final result document. Experimental results show that this method can obtain documents with high relevance, which is of great significance to the field of clinical medicine information retrieval.
Vector-space model is one of the most popular information retrieval models and it has been successfully implemented in retrieving many textual document collections. However, not many vector-space implementations employed open source technology. This paper discusses the integration of Oracle 10g Express edition database and Java language in realizing the development of an online document retrieval system for an Archive and Museum Unit in a public university. The retrieval system is tested on thirty publication documents and natural language queries entered managed to retrieve and ranked the documents successfully.
Nowadays, users of computers store a lot of information on the Web. For this reason, the Internet is a good place to search information on any subject. Due to the large amount of information, some users would search information on specific websites that they consider interesting (e.g. www.wikipedia.com, news sites, etc.). Traditional models represent webpages by using the frequency of terms or the structure of links in order to assign weight to terms of webpages. This paper presents a semantic information retrieval to represent specific websites. This proposal integrates text mining algorithms based on natural language processing and traditional representation models with the aim to improve the quality of webpages recovered by searching. Each webpage of the website is represented as a vector of topics, instead of a vector of terms. In a similar way, the query is represented as a vector of topics. Thus, a similarity measure can be applied over this vector and vectors of documents to retrieve the most relevant documents.
In view of some problems existing in current information retrieval system, this paper puts forward a multi-agents information retrieval system based on intelligent evolution, which is divided into nine modules: user Agent, communication Agent, mining Agent, personal Agent, intelligence evolution Agent, information retrieval Agent, group Agent, intelligence evolution filtering Agent and clustering Agent. According to the user history query information, current retrieval information and feedback information, this system rectifies the judgment of the user preferences constantly, makes the returned query results can reflect the user demand more and more, that is: provides users with the more and more accurate query result. Experimental results show that the proposed method provides the more accurate query on the document than the existing intelligent retrieval systems, the average of query accuracy is 81.6%, and along with the using system by users, and with the system running continually, query accuracy will be higher, which reflects the effect of the intelligent evolution.
Information Retrieval is a process of finding the documents in a collection based on a specific topic. The information need is expressed by the user as a query. Documents that satisfy the given query in the judgment of the user are said to be relevant. The documents that are not of the given topic are said to be non-relevant. An IR engine may use the query to classify the documents in a collection, returning to the user a subset of documents that satisfy some classification criterion. There are several search engines to find information in the given repositories containing large amounts of unstructured form of text data. However, the task of ad hoc information retrieval is, finding documents within a corpus like Bible, that are relevant to the user remains a hard challenge. Sometimes the relevant documents may not contain the specified keyword. The lack of the given term in a document does not necessarily mean that the document is not a relevant. Because more than one terms can be semantically similar although they are lexicographically different. In this paper a new algorithm called "Semantic based Boolean Information Retrieval" (SBIR) is proposed to retrieve the documents with semantically similar terms to enhance the performance of Boolean Information Model by improving the recall and precision.
With the development of cloud storage services, more and more users choose to store their information and data in the cloud and use Cloud Service Provider to manage their data. In order to ensure the security of the data in the cloud, users will encrypt their data before store it in the cloud. Although the encryption ensure the security of the data, it brings some problem to users when searching their data at the same time. At present, the research on the technology of the plain text has been relatively mature, how to study for a retrieval technology over cipher text in the cloud is the key to cipher text retrieval technology. In this paper, we analyze the existing search algorithm over cipher text, for the problem that most algorithm will disclosure user's access patterns, we propose a new method of private information retrieval supporting keyword search which combined with homomorphic encryption and private information retrieval. This method can achieve the query operation without disclose private information of both sides. In view of the efficiency problem of the private information retrieval scheme, we introduce the MapReduce into the search algorithm which can meet both the efficiency and privacy requirements.
Construction of intelligent sign language corpus of the tourist attractions under the background of the big data information retrieval is discussed in this paper. The distributed fusion system is that before the detection report of each sensor enters the fusion, its own data processor generates the local multi-target tracking track, and then sends the processed information to the fusion center, which is based on the data of each node. Considering the above advantages, the designed model adopts the SQL model, and the information mining and retrieval algorithms are combined to construct the efficient smart system. The proposed model is simulated on the collected public massive data set. Through comparing with the other traditional methods, the proposed framework performance better.
The main objective of information retrieval models is to searching and retrieving information from databases and provides users with that information which satisfy their needs. There are various information retrieval models have been developed till now. All these models are categorized based on their characteristics. To compute the efficiency of information retrieval model, we require a database of documents, detailed description suite of user needs expressible as queries and accurate assessment of each query-document pair. The information retrieval models have been used to judge effectiveness of information retrieval systems and considered a baseline for search engines evaluation. This paper focuses main feature of IR models and provides a comparison of these models.
Parsers have been shown to be helpful in information retrieval tasks because they are able to model long-span word dependencies efficiently. While previous work focused on using traditional syntactic parse trees, this paper proposes a new approach where, unlike previous work, the parser parameters are discriminatively trained to directly optimize a non-convex and non-smooth IR measure. The relevance between a document and a query is then modeled by the weighted tree edit distance between their parses. We evaluate our method on a large scale web search task consisting of a real world query set. Results show that the new parser is more effective for document retrieval than using traditional syntactic parse trees. It gives significant improvement, especially for long queries where proper modeling of long-span dependencies is crucial.
Base on information retrieval system's search function (such as the retrieval rule, searchable fields), user retrieval interface, result output (such as output content, sorting way), system revelation (help system) and so on, the paper analyzes the necessity of the IRS standardization and puts forward some proposals on the standardization.
In order to help people obtain useful information from patent documents in different languages. This paper proposes a cross-language retrieval system to search Chinese and English patent documents simultaneously. This system consists of query translation module, document retrieval module and user interaction module. Query translation module is used to translate query based on bilingual dictionaries. Document retrieval module consists of monolingual retrieval system using standard vector space model. In order to retrieve in highly parallel, we use the MapReduce model to calculate the similarity. User interaction module provides users with interactive mechanism used to improve the retrieval accuracy in the system. It contains two parts: the second translation and relevance feedback. The experimental results show that our system has good performance.
Medical information is valuable for a health care professional to empower diagnosis and clinical decision process. In addition, it can also support communities with health knowledge to support healthy living. Ontology-based information retrieval supports the retrieval of such information with the semantically better result. In this paper, we propose a framework of lightweight semantic-based medical document retrieval in Bahasa Indonesia. The medical information is focused on diagnosis, etiology, epidemiology, clinical manifestation, and guidelines. The framework firstly annotates queries' by using the ontology, analyse the information networks within the ontology and extracts the context of the queries. When a query falls within one context of disease information, a modified search query will be then generated and passed to an existing keyword-based search result. The framework is evaluated using a case study in infectious tropical disease domain. The evaluation results show that the framework could perform well with precision equals or more than 0.9 to find the relevant medical document through the semantic networks and the enhancement process.
Video synthetic aperture radar (VideoSAR) has attracted significant attention in the computer vision-based information retrieval field. In this paper, a unified three-dimensional reconstruction framework is proposed for both the straight and circular VideoSAR modes, and is suitable for height retrieval with/without geometric prior knowledge. First, two VideoSAR imaging modes are presented to explain the formation of dynamic observation. Then, structure-from-motion retrieval method with/without geometric prior knowledge is proposed by exploiting shadow information. Finally, experimental results based on VideoSAR fragments measured by the airborne and MiniSAR systems demonstrate the effectiveness of the unified retrieval method with a higher accuracy compared with that of a single SAR imagery.
Information retrieval is the research area in which many researcher have been done and many are still going on. The rapidly growing web pages make it very crucial to search up to date documents. In continuation of research works on learning to rank, this research focuses on implication of machine learning techniques for IR ranking. SVM, PSO and hybrid of both are the main techniques implemented for IR ranking. In case of SVM, selecting appropriate parameters is difficult, but it gives potential solutions for the ranking. One of the optimization methods i.e. PSO is easy to implement and has global search capability. Thus to find the fitness function to optimize the ranking of document retrieval Hybrid SVM-PSO model is proposed.After the comparative study it has been calculated that the ranking parameters gives best result for RankSVM-PSO over RankPSO and RankSVM. The result has been calculated based on single term queries and multi-term queries. The study shows RankPSO gives the better result than RankSVM and RankSVM –PSO gives better result than RankPSO, so it has been concluded that RankSVM-PSO gives best result among the three techniques.
Information Retrieval is a process to find back the information that is needed by system. News is not only communicated via the print media, but also through online media. The rapid technology makes people more up to date to on news or current information. Detik.com is one of the online news website that serves a variety of the latest information. Based on the results of questionnaires taken from 30 respondents, the results obtained percentage of 100% which states that online news is important But in detik.com website visitors often get articles that are not in accordance with what is referred to, is evidenced by the results of the percentage is 66.7%. It is claimed that the keywords entered are not relevant to the search results. This research was conducted by applying a weighting method TF-IDF (Term Frequency Inverse Document Frequency). There are several preprocessing stages that conducted in the search for relevance weighting value starting from tokenizing process, Sitering process, stemming process followed by a TF-IDF weighting method. The weighting of the results obtained weight value relevance of each article from highest to lowest weight. This research resulted a web applications Information Retrieval on the site detik.com using TF-IDF weighting method. The test results showed recall value of 1 indicating that the relevant articles can be found by the system and the precision value of 0:50 indicates there are relevant articles that are not found in the system. Recall and precision resulted in a value of 1 if the query (keyword) which included having one term (word). Precision low value indicates that the average accuracy of the keywords entered by the article irrelevant search results.
Given the widespread integration of Artificial Intelligence across various domains, the exploration of its applications in the legal field remains understudied, emphasizing the critical need for the development of effective deep learning approaches. This paper describes on our approaches to legal information retrieval (Task 1) and legal question answering (Task 2) in the Automated Legal Question Answering Competition (ALQAC 2023). Specifically, we employed the Paraformer model for information retrieval and leveraged large language models for answering questions. In the competition, we achieved the 1st place for Task 2. In this task, our findings demonstrate that fine-tuning appropriate prompts helps the large language models to achieve better performance. Our prompts are available at https://github.com/DucLong06/Legal-Prompts
In relevance feedback of information retrieval system, selective sampling is often used to alleviate the burden of labeling by selecting only the most informative data to label. The traditional batch labeling model neglects the data's correlation and thus degrades the performance; while the theoretical optimal one-by-one training model is not efficient enough because of the high computational complexity. In this paper, we propose a Moving Virtual Boundary (MVB) strategy for informative data selection. We adopt a novel one-by-one labeling model, using the previous labeled data as extra guidance for the selection of next, and achieve better experimental results.
FAQ retrieval systems are one of the fields of information retrieval and perform the task of retrieving appropriate question-answer pairs from user queries. Recent studies separately train deep learning models using dense representations of query-question similarity and query-answer relationships. However, since the sentences of a question-answer pair may contain different information, retrieval performance can be improved by using both sentences simultaneously when measuring similarity to the user's query. In this paper, we propose a learning method that can improve retrieval performance by training the relationship between queries and question-answer pairs with a single encoder. We evaluated P@5, MAP, and MRR performance using human-labeled queries on the StackFAQ dataset. To show that performance can be increased by improving the learning method, we trained and verified using the same model as in the previous study and the same query dataset created with GPT-2. Furthermore, we showed that performance can be further improved by training using queries created with GPT-3.5.
The article describes computational experiment and further research work in the area of identification of destructive information influence in social networks. The problem of distribution of suicidal content via open sources is presented. On the basis of calculations there was made a conclusions about the prospects of using the methods of information retrieval in the task of identification of the destructive informational influence. The procedure of singular value decomposition of the matrix "pattern-message" is considered as a path of the future research.
Content-based video retrieval is very interesting point where it can be used in our daily life. Video retrieval is regarded as one of the most important in multimedia research. The development of multimedia data types there is demand of video retrieval system. Video retrieval can be used for video search and browsing which are useful in web applications. Selection of extracted features play an important role in content based video retrieval. There are two types of feature extraction, low level feature extraction and high level feature extraction. Low level feature extraction based on color, shape, texture, spatial relationship. The main goal of this paper is that, user can give the two different types of input in the form of image query and the text query. First one is that give the input in the form of image query and retrieved image which is similar to the query image by using the CBIR algorithm. CBIR is still developing science. Retrieval of images based on visual features such as color, texture and shape. In this paper gives a detail description of a system developed for retrieving images similar to a query image from a different large set of image. Second one is that give the input in the form of text query and retrieved image by using the ABIR technique. ABIR is powerful algorithm for the information retrieval it can utilize their powerful natural language. Annotation is never complete.
With the rapid development of information technology, the smart and digital transformation of power grid has become an inevitable trend. However, this trend has brought about increasingly large and complex power grid security standard documents, posing significant challenges to power grid research and management. In order to effectively address the risks and issues related to power grid security management, this paper aims to conduct in-depth research on the intelligent retrieval and question answering techniques for power grid security standards, exploring how to intelligently process and apply power grid security standard documents.
Specialists working in the global information space, as organizers and participants in information processes, become witnesses to the problems that arise in this space. The user has difficulty making the right decisions, especially in the event of a conflict of interest in the information space. In some cases, a “conflict of interest” leaves the consumer with a choice. Such cases are more common on social networks. The consumer of information sometimes participates in this information space as a distributor of information. In some cases, the user becomes a disseminator of disinformation because the reliability of the information is “in doubt”. In some cases, disinformation is more interesting because of its attractiveness. We also see an increase in the number of “products of disinformation” in the global information space. The article discusses a project aimed at resolving conflicts of interest in the global information space. Conflict of interest is a widespread problem in the global information space. Between “information monopolists” and the state, between sellers and consumers, between sources of information and consumers of information, etc. As a result of systematic discussions, it can be assumed that new approaches to the problem of conflicts of interest may appear that can contribute to “accelerating progress.” In this regard, the idea that libraries are a reliable source of information, especially on global networks, is supported and encouraged. The purpose of the article is to analyze the features of a project aimed at solving the problem of conflict of interest, based on modeling within an integrated system and finding optimal solutions in the context of various corporate interests. This will exclude the manipulation of conflicts in the global network. The abundance and dynamics of information growth, the problem of the reliability of information products and attempts to manipulate the growing information products show that the problem of information rel... (Show More)
During the software development cycle, software products can undergo many changes in requirements and design. In one of those products, the source code of the system, these changes can impact the original specifications and design, leading to tangled and scattered code that affects the design and maintenance processes. Research efforts have been made to analyze the scattered code for concerns identification and extraction. In object oriented systems, these concerns can be distributed across many classes, leading to the definition and identification of crosscutting concerns. Many methods have been proposed to identify, extract and separate crosscutting concerns. In this paper, a method for crosscutting concerns identification using information retrieval techniques is presented. This work aims to identify the core crosscutting concerns, which are the main tasks a system has to satisfy whose code is scattered over the different classes of the system. Information retrieval provides advantages over common parsing techniques since the code is treated as unstructured text, making the method programming language independent.
According to the problem of the constant increase of internet data and the current Internet information retrieval system has been unable to meet the needs of data and information retrieval, this paper puts forward the data mining based Internet information retrieval system. The paper first uses data mining to excavate association rules, and evaluates the entire behavior simulation of users resources need according to a certain factor of user's retrieval resources, and then uses bayesian network algorithm to correlate the mining data. Experimental simulation results show that the application of bayesian network algorithm into Internet information retrieval has implemented intelligence and personalization of information retrieval to a certain extent, which has certain research value.
For text search systems, the ambiguity of short queries often leads to poor performance. To solve this problem, relevance feedback via query-expansion is considered as one effective technique. However, many methods of relevance feedback barely use the knowledge of search results and the improvement of effectiveness is limited because the knowledge used is limited. In this paper we try to include Wikipedia's category knowledge to improve the poor retrieval performance. A method of category feedback is proposed, which is based on the information of Wikipedia categories. Categories instead of terms and documents are provided to users for feedback. Finally, an experimental search system is developed which demonstrates the effectiveness of our method.
The peer-reviewed articles and textual data are main source of data in biology. Text mining is solution to extract information from textual data sources that are usually in bulky quantities, messy and disorganized. Dealing with this situation needs to deploy innovative methods and techniques. In this paper, we identify the current heavily used methods for biomedical text mining, their capabilities and developments, some proposed solution and how to evaluate performance. Biomedical specific challenges in text mining context have been studied with respect to proposed answers and then main future trends based on current needs and requirements have been discussed.
We consider private information retrieval (PIR) for distributed storage systems (DSSs) with noncolluding nodes where data is stored using a non maximum distance separable (MDS) linear code. It was recently shown that if data is stored using a particular class of non-MDS linear codes, the MDS-PIR capacity, i.e., the maximum possible PIR rate for MDS-coded DSSs, can be achieved. For this class of codes, we prove that the PIR capacity is indeed equal to the MDS-PIR capacity, giving the first family of non-MDS codes for which the PIR capacity is known. For other codes, we provide asymmetric PIR protocols that achieve a strictly larger PIR rate compared to existing symmetric PIR protocols.
Although there are a lot of researches on Information Management and Information System education in China, little research is done in examining the impact of IT on doctoral curricula by Information Management and Information System programs in China to reveal: what courses are offered? what is the most popular subject area? how Information Management and Information System schools change their curricula to meet today's challenges? In order to address above questions, this study gives a brief summary of the curricula of Information Management and Information System doctoral programs in China. Findings show that there is no national consensus of what should be the core of the Information Management and Information System discipline for PhD education in China. The largest percentage of the courses offered relate to Management Science and Engineering. Interdisciplinarity is also featured within the Information Management and Information System PhD curricula in China.
It is known that combining multiple information retrieval systems can improve the combined systems performance over the performance of individual systems in many cases. It has also been known in these cases that the performance improvement of the combined system is mainly due to: (a) performance of each of the individual systems, and (b) the diversity between individual systems. However, it remains a challenging problem to quantify these two conditions. In this paper, we investigate these issues using live TREC datasets, TREC 2-6 (1993-97). Six systems in each dataset are selected either by random choice or by precision. We then compare performance of combining these six systems selected by random v.s. by precision from each of these datasets. It is demonstrated that, in each of the live datasets, the sum of x + y for positive cases (performance of combination of A and B is better than or equal to the individual systems) is larger than for negative cases (other than positive cases), where x is the performance ratio P l/P h and y is the diversity (between A and B), both normalized to [0, 1]. In addition, it is also demonstrated that combinations of t systems, t = 2,3,4, 5 , and 6 overall on the 6 systems selected by precision performs better than on the 6 systems selected by random.
This paper presents an approach based on optimized data search and data analysis of high dimensional clinical health records, for the purpose of identifying the patients with significant risk of fracture to be introduced to the Fracture Liaison Service (FLS) for further investigations and treatment. The main objective of the work is to speed up and automate the process of identifying the patients suitable for FLS and provide a real decision support for the clinical team by fast information retrieval and structuring. The performed analysis and the current results are promising and support our research aim for the optimization of the process in the real life scenario.
With constantly advancing of informatization construction and application, many enterprises have accumulated a wealth of information resources, desiderating to solve the problems of information effectively sharing and developing. Based on discussing the connotation and characteristic of enterprise information resources, this paper introduces the integration model of enterprise information resources. The integration scope covers network information resources, internal information resources and commercial database resources. The integration method of internal information resources, functions and characteristics of integration model are presented in detail. In order to utilize information resources integration technology for providing information service, the deployment scheme of information service system based on cloud computing is proposed in the article, and system implement is also explained at full length.
In a digital world information retrieval used for several application such as searching information in a document, searching for documents themselves, searching for metadata that depict data and for databases such as text, HTML, XML, image, audio etc. many universities, schools and companies use this IR systems to provide access to books, journals and other documents required in companies. Search engines are the most observable IR application. Our purpose is to compose system which search in our collection of datasets and retrieve the document which matches the user query. These collections of documents are text files, PDF's, HTML, XML files which we call heterogeneous data sets. For getting relevant documents in final outcomes, first we need to index the documents then rank them according to the score of cosine similarity values for each document which matches to the user query that we are entering. We are using cosine similarity technique in vector space model mainly for our analysis purpose and comparing with moderated IDF-Cosine similarity for better searching results using Rstudio. Our approach gives better results on real world corpus as comparing with enhanced IDF-Cosine similarity approach in vector space model.
Based on the study of traditional retrieval models, an improved model of vector space information retrieval has been proposed in the paper. The model has fully considered the effect of file structure on the importance of words. It can avoid the disadvantages of low recall and precision in the vector space model if the improved model is selected to calculate the similarity.
Cross-language information retrieval research has favored system-centered approaches in the past. The user is not an integral part of the translation and retrieval processes. In this paper, we investigate the problem of personalized cross-language information retrieval by exploiting query expansion techniques. The original query is augmented with terms mined from the user's historical usage information in one language, with the aim of retrieving more relevant results in another language. Experiments semi-automatically constructed by using bilingual Wikipedia documents showed that in general personalized approaches work better than non-personalized approaches. We also found that an individual user model generated from one language can be used to enhance the personalized cross-language information retrieval.
Peer-to-Peer (P2P) approach in information retrieval systems has drawn significant attention recently. P2P networks provide obvious advantages like scalability, reliability and, therefore, recent researchers are looking for a way to adapt these techniques to Information Retrieval fashion of nodes with heterogeneous documents. The greatest attention is paid to different semantic-based searching such as Gnutella Efficient Search (GES) proposed by Zhu Y et al., which derives from Vector Space Model. This paper proposes conceptual design of P2P unstructured information retrieval (IR) with heterogeneous documents on independent nodes.
The traditional information retrieval technology is not concerned about the information temporal, but the temporal nature of the data is playing a particularly important role in some fields, so the search of temporal information has become a hot spot. With the relational database information retrieval technology emerge, it is urgent to integrate temporal attributes into the relational database information retrieval technology. In this paper, temporal information retrieval method based on data graph is proposed, and the validity of the method is verified by experiments.
CCS CONCEPTS • Information systems → Information retrieval; Personalization;
We consider the problem of private information retrieval through wiretap channel II (PIR-WTC-II). In PIR-WTC-II, a user wants to retrieve a single message (file) privately out of M messages, which are stored in N replicated and non-communicating databases. An external eavesdropper observes a fraction μn(of its choice) of the traffic exchanged between the nth database and the user. In addition to the privacy constraint, the databases should encode the returned answer strings such that the eavesdropper learns absolutely nothing about the contents of the databases. We aim at characterizing the capacity of the PIR-WTC-II under the combined privacy and security constraints. We obtain a general upper bound for the problem in the form of a max-min optimization problem, which extends the converse proof of the PIR problem under asymmetric traffic constraints. We propose an achievability scheme that satisfies the security constraint by encoding a secret key, which is generated securely at each database, into an artificial noise vector using an MDS code. The user and the databases operate at one of the corner points of the achievable scheme for the PIR under asymmetric traffic constraints such that the retrieval rate is maximized under the imposed security constraint. The upper bound and the lower bound match for the case of M = 2 and M = 3 messages, for any N, and any μ = (μ1, · · · , μN).
The big data revolution has created a hefty demand for searching large-scale electronic health records (EHRs) to support clinical practice, research, and administration. Despite the volume of data involved, fast and accurate identification of clinical narratives pertinent to a clinical case being seen by any given provider is crucial for decision-making at the point of care. In the general domain, this capability is accomplished through a combination of the inverted index data structure, horizontal scaling, and information retrieval (IR) scoring algorithms. These technologies are also being used in the clinical domain, but have met limited success, particularly as clinical cases become more complex. One barrier affecting clinical performance is that contextual information, such as negation, temporality, and the subject of clinical mentions, impact clinical relevance but is not considered in general IR methodologies. In this study, we implemented a solution by identifying and incorporating the aforementioned semantic contexts as part of IR indexing/scoring with Elasticsearch. Experiments were conducted in comparison to baseline approaches with respect to: 1) evaluation of the impact on the quality (relevance) of the returned results, and 2) evaluation of the impact on execution time and storage requirements. The results showed a 5.1-23.1% improvement in retrieval quality, along with achieving 35% faster query execution time. Cost-wise, the solution required 1.5-2 times larger space and about 3 times increase in indexing time. The higher relevance demonstrated the merit of incorporating contextual information into clinical IR, and the near-constant increase in time and space suggested promising scalability.
Various natural hazards can be detected by well trained sensors deployed at the target sites. This paper proposes a novel framework of a Tsunami detection system using wireless sensor nodes deployed in coastal areas around earth's magnetic poles. The proposed framework implements intelligent information retrieval technique to generate a Tsunami alert. This method makes use of proposed parameters that defines the underwater condition in terms of magnetic field, electric field, and wave gradient and heat energy. The proposed framework is further used to extract the intelligent information from above mentioned parameters and study the behavioural as well as physiological pattern changes in marine animals. The proposed method filters the messages, transformed by condition tagging which are further evaluated to generate Tsunami alert. The proposed framework is validated by the help of a case study to analyse the behavioural patterns observed in sea turtles when exposed to evident change in electro-magnetic field intensity.
Due to the wide availability of huge amount of multimedia data in various modalities such as image and text documents, having a great amount of similarity among them is inevitable. In this paper, we present an efficient model which correlates the similarity among documents belonging to various modalities to achieve cross-media retrieval. Cross-media retrieval is a content based information retrieval system where heterogeneous data is mined to retrieve results of various modalities, i.e., input object and returned results may be of different modalities. For example, text objects can be retrieved as a result to image input. First, features are extracted from multimedia objects by which the objects are labeled. Using the labels, similar documents are grouped to generate Multimedia Documents. We construct a cross-media correlation graph with documents as vertices, where positive weight is assigned to every single edge according to the amount of similarity between vertices. The cross-media retrieval system identifies the input document and as a result returns required number of documents with highest weights.
Medical information retrieval demands terminology understanding which indicates the needs for domain specific knowledge. This paper presents a medical retrieval model called IntelligentMedicalSearch system (IMS). The contribution of the paper is four-fold: (i) medical-aware query expansion technique; (ii) MeSH-manipulated indexer which entails medical terminologies and their synonym (iii) extended Boolean matching to measure similarity between query and documents; (iv) comprehensive ranking method which prioritizes matched expanded query size besides sole total occurrence score. Result shown that (i) enrichment of query expansion using synonym term can improve mean average precision (MAP) value as opposed to standard query expansion; (iii) our comprehensive ranking method achieved high recall and according to MAP score we are in the top four run system amongst submitted run systems in ImageCLEF2010 medical task.
Although recently crowd sourcing has become popular, the reliability of crowdsourced results has been questioned because of workers' varied degrees of attention, skill, and accuracy. Therefore, understanding the factors that affect the reliability of crowd sourcing is crucial. Using crowdsourcing has recently proposed as an alternative to create relevance judgments for the evaluation of information retrieval systems as traditional methods are expensive and scale poorly. The aim of this study is to measure selected cognitive ability (general reasoning skill) of crowdsourced workers, and investigate the effect that this characteristic has upon judgment reliability, as measured against a human gold standard. A significant correlation was found between judgment reliability and measured general reasoning skill. Our findings show that general reasoning skill influences the accuracy of crowdsourced workers who create the relevance judgments set.
Future Information Retrieval, especially in connection with the internet, will incorporate the content descriptions that are generated with social network extraction technologies and preferably incorporate the probability theory for assigning the semantic. Although there is an increasing interest about social network extraction, but a little of them has a significant impact to information retrieval. Therefore this paper proposes a model of information retrieval from the social network extraction.
In this paper, we propose a new method for applications of Augmentative and Alternative Communication (AAC) systems. These systems aid communication for people with impairments or comprehension issues with spoken or written languages in a dialogue situation. In previous studies, the user selected symbols as substitutes for displayed words and the system generated sentences based on these selections. In this study, our proposed method can generate the required sentence using minimum keystroke input and reduced input errors. Here, the system takes the lead in suggesting symbol candidates to the user. In our experiments, we use a decision tree. This is a machine learning method for automatically setting strategies in order to set the order of the symbol candidates. In our experiments, the proposed method reduces the number of key strokes by 87.6%.
The collection and dissemination of increasing quantities of information representing multiple digital media objects are promising, combined with a number of technical advances in ICT. Even though the information is maintained and distributed by different owners, systems, formats and locations, their contents might tell the same story, share the same space-time extension, and they might be closely related to each other. Previous efforts in information retrieval and integration have been limited to classical keyword-based and content-based indexing and querying for multiple media documents. Furthermore, the concept-based approach, which is becoming a very popular method, still focuses on specific single media documents. Thus, a concept-based system architecture that allows access to information in multiple format - text, image and graphic, audio, video and spatial information - within heterogeneous sources is presented in this paper. This architecture conforms to the Service Oriented Architecture standards and is designed with the use of ontologies in order to comprehend the semantic matchmaking between users' queries and multi-format information through metadata. The basis of the ontology model is already in place, but the architecture still undergoes its incremental progress as a research prototype within the disaster management domain.
The significance of time in information production and consumption has been recognised in information retrieval research. Temporal information plays an important role in the webpage retrieval. The webpage has both the temporal metadata and temporal semantics in the content. However, the existing search engines conduct the information retrieval based on text keywords rather than temporal semantics. To address this issue, a Temporal semantics Information Retrieval (TSIR) System is proposed to deal with the Chinese temporal information retrieval. The TSIR system is deployed on Hadoop and implemented by the means of MapReduce. Firstly, the Chinese temporal regular expression rule is introduced to extract the explicit and implicit temporal phrases in the query keywords and webpages. Secondly, the scores of webpages are re-evaluated by taking text relevance and temporal semantics relevance into account and the returned results are ranked according to re-evaluation. Experiment shows that TSIR system could precisely and effectively match the keywords queries related to temporal expression.
Most of traditional information retrieval and automatic text classification methods with vector space model almost need determine the weighting of the feature terms. Term weighting plays an important role to achieve high performance in information retrieval and text classification. The popular method is using term frequency (tf) and inverse document frequency (idf) for representing importance and computing weighting of terms. But the tf-idf model is not introduced class information, the important information such as title, abstract, conclusion, and the synonymous words information. This paper provides an improved method to compute weighting of the terms. The above information is involved. The experimental results show that the performance is enhanced. The role of important and representative terms is raised and the effect of the unimportant feature term to retrieval and classification is decreased. In addition, the F1 based on new algorithm is higher than based on traditional tf-idf model.
Query-document vocabulary mismatch, the lack of query expressiveness for user needs and the phenomenon of short queries are the main issues associated with information retrieval systems. Query Expansion (QE) is one of the well-known alternative for overcoming these problems. It mainly involves finding synonyms or related words for the query terms. There are several approaches in the query expansion field such as statistical and semantic approaches; they focus on expanding the individual query terms rather than the entire query during the expansion process. An other category of approaches deals with the whole query by using a neural approach based on Pseudo Relevance feedback (PRF) documents. In this work, we carried out an ablation study to measure the impact of the classical and semantic (word embedding, order, context) based query expansion on the retrieval performance. The experiments conducted on the Arabic EveTAR dataset reveal that our hybrid proposed approach combining classical (PRF) and transformer (AraBERT) is competitive with the state-of-the-art methods. In fact, the obtained result in terms of the Mean Average Precision (MAP) is up to 0.72.
Billions of devices are connected in the Internet of Things (IoT)-based sensor networks and they continuously generate a large volume of data. In order to get access to specific data, which is crucial to enable a myriad of new intelligent applications, efficient information retrieval becomes an imminent need for IoT. However, sensor information in the physical world can be heterogeneous, high dimensional, and voluminous due to the complex and dynamic environments. In this paper, we first investigate several IoT search scenarios and propose a uniform representation model for sensor information recordings. Four query models are designed to represent all possible information query styles. With these models, we develop information retrieval architecture for IoT. In essence, an indexing mechanism called efficiency maximization and cost minimization is proposed to solve the property selection problem in the process of index construction and update. Meanwhile, a novel real-time grid R-tree structure is designed to support historical and realtime search for spatiotemporal observation data. Simulation results based on real-world IoT data sets show that storage space is considerably reduced with the sensor model. Furthermore, the proposed indexing mechanisms can improve retrieval efficiency and accuracy, and ensure scalability for large-sized data simultaneously.
In the current retrieval methods, the Hamming distance measure is used to calculate the relevance of information resources, which leads to the lower average accuracy of retrieval text. Determine the number and density of key words of information resources in the resource database, adopt the technology of key word density to calculate the semantic score of information resources in the resource database, and assess the semantics of information resources; from the sequence of information resources in the resource database, design and improve the steps for calculating the relevance of information resources in the resource database based on grey relational degree, and determine the relevance of information resources in the resource database; adopt the DCMH method, set up the retrieval model, extract the characteristics of information resources in the resource database, design the retrieval objective function, and retrieve the information resources in the resource database of subsidies for poor students. The experimental results show that the average accuracy of retrieving the text information of the three sets of datasets is higher than that of the two comparison methods chosen in this experiment in the three dimensions of 16, 24 and 32, and 0.09, 0.12, 0.055, 0.065, 0.025 and 0.04 when the recall rate is 0.2%. Obviously, the retrieval method of this research has the strong retrieval performance.
In this paper, we show that direct search methods are more suited and simpler to implement than the other search techniques for information retrieval. Two novel PSO algorithms are designed for the purpose of validating this important result. One of these algorithms is sequential and the other one is a parallel version. We discuss the advantages of these algorithms and demonstrate that not only they are suited for web information retrieval but they also outperform the existing algorithms from the design and experimental points of view. Extensive experiments were performed on CACM and RCV1 collections. Performances in terms of solution quality and runtime are compared between these algorithms and exact methods. Numerical results exhibit the superiority of parallel PSO on all the others in terms of scalability while yielding comparable quality.
Regression testing is widely used in practice for validating program changes. However, running large regression suites can be costly. Researchers have developed several techniques for prioritizing tests such that the higher-priority tests have a higher likelihood of finding bugs. A vast majority of these techniques are based on dynamic analysis, which can be precise but can also have significant overhead (e.g., for program instrumentation and test-coverage collection). We introduce a new approach, REPiR, to address the problem of regression test prioritization by reducing it to a standard Information Retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection. REPiR does not require any dynamic profiling or static program analysis. As an enabling technology we leverage the open-source IR toolkit Indri. An empirical evaluation using eight open-source Java projects shows that REPiR is computationally efficient and performs better than existing (dynamic or static) techniques for the majority of subject systems.
Knowledge plays an essential role in efficient functioning and decision-making. The relevance of information retrieval (IR) depends on an efficient knowledge representation (KR). Ontology constitutes a rich set of knowledge formalism for KR, but inconsistency, vagueness, incompleteness, etc., are major limitations that show uncertainty. This paper proposes a rough Bayesian (RB) approach to uncertain ontology. Under this work, we have presented decision rules to determine attribute reduction, estimated the outcomes of a set of queries, and created a decision class for the PIMA Indians Diabetes Ontology. The model identifies a group of relational rules, reduces calculations, and minimal rules and uses them in inferences and query retrieval with the help of probabilistic BN. The model captures the ontology's knowledge as a whole, and accurately predicts the average belief with 91 % accuracy for four queries in terms of precision, recall, and accuracy and 98 % accuracy for the decision class on the approximation.
Research Domain Criteria (RDoC) is a classification framework for mental illness, recently introduced by the National Institute of Mental Health. The RDoC Task at this years' BioNLP Open Shared Tasks 2019 workshop is a competition for inviting text mining groups around the world for developing informatics models for two subtasks: information retrieval and sentence extraction for mental health using RDoC. For competing in the RDoC task, we developed RDoCer, which uses a Term Frequency Inverse Document Frequency-based similarity measure for information retrieval and supervised machine learning models for sentence extraction. In comparison to the other models competed at the RDoC task, RDoCer performs competitively across the two subtasks while notably ranking first for the Sustained Threat RDoC construct in the sentence extraction subtask. The findings of this study have implications for mental health informaticians as well as researchers, curators, clinicians working in this domain.
Nowadays, the development of information technology both in terms of hardware and software has been growing rapidly. Many people used technology to search many types of information. On the other hand, there are information on Sundanese ancient manuscript as cultural heritage with many valuable information still accessed manually, such as the manuscript which located in Situs Kabuyutan Ciburuy, Garut, Jawa Barat. Many people with many purposes (i.e. Research, education, etc.) came to access directly. Its becoming a problem because due to lack of maintainability, the manuscripts became fragile. Furthermore, the information will be lost. So, the emergency solution should be proposed to preserve this valuable heritage. We proposed a retrieval system using the probability approach that can be effective for search the information in ancient Sundanese language. The two probability approaches: Bayesian Networks and Divergence form Randomness have been compared. The 50 queries for three different types of structured query have been run and evaluated by Mean Average Precision (MAP). The results give that the effective methods come from Bayesian Network. There is another finding during the building process of retrieval system. The interactive visualization can be delivered by the retrieval system according to the given query.
Semantic search has become a grand vision for improving retrieval effectiveness in today's scenario. Most of the existing ontology based semantic search models requires user to enter a query in formal query languages. It hinders the usability of the retrieval system. Aiming to solve the above limitations and improve the retrieval effectiveness, a framework for ontology based information retrieval is proposed. In order to overcome the usability limitations, a query interface which requires the user to enter the query in natural language is provided. A domain-specific ontology based on movies is used to develop a prototype of the proposed model which improves search accuracy.
We propose an intuitive operation based information retrieval system “Clickable Real World (CRW)”. If a user which uses this system takes a picture of a landmark in the world, some related information is displayed on a smartphone. One of the key research issues is how to estimate appropriate keywords in order to retrieve information related to the target. Our strategy utilizes a lot of training samples, which consist of images, tags and geolocation where the image was taken, shared on the Web (such as flickr and Picasa). Then, a keyword table is created based on the consistency of geolocation and visual feature. We developed a prototype version of CRW on a smartphone, and conducted a field experiment in Kyoto city, Japan.
Thesaurus is used with many Information Retrieval (IR) models such as data integration, data warehousing, semantic query processing and classifiers. Considering the existence of various thesauri for a particular domain of knowledge, output quality of an IR model when using different thesauri in the same domain is not predictable. In this paper, we propose a methodology to study the performance of thesaurus in solving schema matching as a case study of IR models. The paper also presents initial results of experiment conducted using different thesauri. Precision, recall, and F-measure were calculated to show that the quality of matching was changed according to the used thesaurus.
The development of plagiarism checking tools has been widely used to help improve academic performance. But contrary to that, many acts of fraud are also carried out to deceive these various tools. One of them is because there is still limited advanced technology to complete the tool. This paper is presented in order to provide information to readers about the need for optimization and development of information technology in the use of similarity checker. As one of the features used by a authors to check the originality of writing, Turnitin is one of the effective similarity checker currently used by many people. The advantages of Turnitin are inversely proportional to the number of individuals who commit fraud so that their writing is not detected, which is similar to data from previous research. The authors presents a finding as well as theoretical recommendations that can be done to develop technology in this Turnitin. One of them is the use of a Turnitin workflow that should be able to go through a procedure of checking the incoming text first before comparing it directly with existing data.
Lexical or statistical information retrieval confronts challenges such as the semantic gap and vocabulary mismatch. In the context of medical data, these difficulties are compounded by users' diverse backgrounds, resulting in disparities in perspective and vocabulary. The intricacies of medical language, including spelling variations, frequent acronyms, and ambiguous concepts, further amplify the semantic gap in medical texts. However, adopting a semantic approach can address these issues, albeit introducing a new challenge in the form of a soft matching nature leading to lower recall. In response, we propose a hybrid information retrieval method that combines semantic and lexical approaches. In contrast to recent experiments with the emphasis on utilizing semantic methods as re-ranker, our current approach diverges by incorporating semantic techniques as a fundamental part of the retrieval model. This shift aims to explore the efficacy of semantic methodologies in the initial retrieval stage rather than exclusively relying on them for post-retrieval refinement. The results obtained from both the semantic and lexical retrieval approaches are subsequently subjected to reranking through Reciprocal Rank Fusion. The proposed method outperforms lexical methods such as BM25L, Jaccard Similarity, and Query Likelihood Model, along with semantic methods, including doc2vec, multilingual BERT, IndoBERT, and MiniLM. It has additionally been demonstrated to be more effective than other hybrid models, PLM-based dense retrieval. This technique has successfully capitalized on the strengths of both semantic and lexical methods, resulting in enhanced overall performance in retrieving relevant documents.
There are two problems in using words to represent document contents and query in information retrieval: ambiguity and different words which represent the same concept. These problems can be addressed by using query expansion. We focused on analysing the implementation of query expansion, word sense disambiguation (WSD), iterated relevance feedback, and some retrieval variations to retrieval performance. In this paper, WSD is implemented in Lucene using query expansion with thesaurus and relevance feedback. Extended Lesk algorithm was re-implemented to disambiguate the query using WordNet. Expansion terms were limited up to 20 words chosen from expansion term candidates from disambiguated query's senses information, co-occurrence terms, and most frequent terms using Kullback-Leibler Distance. We iterated the process to find the best number of expansion iteration. We found that the method using WSD to query can extend search process time to 161 times longer at worst. Query expansion using disambiguated sense information did not affect the performance much while using information from relevance feedback did. This experiment provides better understanding of WSD in information retrieval system performance.
This paper introduces the concept algebra (CA) theory as a basis for the conceptual representation and the derivation of text processing to realize a semantic based retrieval system. We also take advantage of Hownet to create the concept attributes space for concept algebra. With the help of LTP, we get the key words and their dependent relations of every sentence to build the CA concept representation of the content with a five-tuple. Concepts make it possible to express both the keyword itself and the semantic relation with its context. According to the demands of text retrieval, some CA operations are optimized to calculate the relations and similarity between concepts. Besides, a text retrieval system framework which processes information based on the concept relations at a concept level is also proposed to verify the advantages of our method.
We consider private information retrieval (PIR) of a single file out of K files from N non-colluding databases with heterogeneous storage constraints m = (m1, ⋯, mN). The aim of this work is to jointly design the content placement phase and the retrieval phase in order to minimize the download cost in the PIR phase. We characterize the optimal PIR download cost as a linear program. By analyzing the structure of the optimal solution of this linear program, we show that, surprisingly, the optimal download cost in our heterogeneous case matches its homogeneous counterpart where all databases have the same average storage constraint μ = 1/N Σ n = 1 Nmn. We show the optimum content placement explicitly for N = 3.
There are many attempts to optimize information retrieval using various social concepts such as social annotations and social book marking. This paper explores the convergence of the information retrieval technique, the social network and the context-aware computing in order to improve information retrieval services. We propose a new ranking algorithm for the context-aware information retrieval. To do this, we get ideas from the social network, previous graph theory and the context-aware computing. Therefore, we investigate the problem of the information retrieval system in ubiquitous computing environment.
Distributed information systems tend to be highly heterogeneous, integrating different computer platforms, data storage structure, different database management systems and schemas which converted data structure under one model to data structure under a different model. For these reasons, a probabilistic pattern matching framework based on neural network is introduced. Our approach gives a probabilistic interpretation of the prediction weights of the candidates, selects the rule set with highest matching probability. Pattern matching is the problem of finding correspondences (mapping rules, e.g. logical formulae) between heterogeneous schemas e.g. in the data exchange domain. Through the union formulae, distributed information system may achieve uniform retrieval and provide a strong reference for the research of information retrieval.
Query-by-example information retrieval provides users a flexible but efficient way to accurately describe their information needs. The query exemplars are usually long and in the form of either a partial or even a full document. However, they may contain extraneous terms that would have potential negative impacts on the retrieval performance. In order to alleviate those negative impacts, we propose a novel term-based query reduction mechanism so as to improve the informativeness of verbose query exemplars. We also explore the notion of term discrimination power to select a salient subset of query terms automatically. Experiments on the TDT Chinese collection show that the proposed approach is indeed effective and promising.
The growing demands on the Arabic language analysis in the computing field highlights the importance of Arabic information retrieval systems in particular, such as search engines, and it's gaining more interest. One way of enhancing the effectiveness of any information retrieval system is adopted via incorporating query expansion techniques. In this paper, Arabic query expansion is performed by using Arabic WordNet Ontology, and the whole work is evaluated to expose how beneficial is it comparing to the English WordNet Ontology. In fact the medicinal corpus doesn't show much benefit in the Arabic expansion in any of evaluation measures implemented, which are system's precision and recall.
Information retrieval provides useful information suitable for policy-making. How to apply information retrieval technology into daily use is a hot topic that deserves our foucs. Take journals evaluation for example, this paper do research on how to use the database inquiry technology provided by China National Knowledge Infrastructure (CNKI) to collect data of Journal of Guangdong University of Business Studies from the year 2009-2011. We analyze the data that collected from CNKI and do computing according to the formula of the journal evaluation indexes, and attain first-class, objective and valuable information that can used for suporting the future development of the journal.
We report the first realisation of a provably-secure symmetric private information retrieval (SPIR) scheme supported by a quantum-secure key-exchange network, offering secure retrieval of fingerprint data from a database with 800 entries.
This paper is to study cross-media semantic retrieval method of information resources based on deep learning. By analyzing the concept of deep learning, the depth structure and the prerequisites for deep learning, this paper studies the relationship between deep learning and information resources cross-media semantic retrieval, and points out the cross-media correlation learning technology of information resources on the basis of depth structure, buildings a distinct framework for information retrieval. As a new information retrieval model, the combination of deep learning and cross-media semantic retrieval can solve the problems of retrieving semantic information across the media and processing complex dimensional data, greatly improving the efficiency of data retrieval and integration. This model will replace the existing information retrieval tools, to become a sword to enhance the level of knowledge service in the era of big data.
We focus on smooth locally decodable codes (SLDC) and Private Information Retrieval (PIR). Recently, a relationship between SLDC and PIR are studied using information theoretical notations. In this paper, we clarify a relationship between SLDCs and PIR using set theoretical notations mainly.
On the basis of the analysis of the major elements and the keystone of the CALS, this paper proposed the information database and retrieval system of aircraft development based on CALS, which is divided into four parts, Client browser, Mesosphere, Heterogeneous database system and Communications layer.
In this paper, we present a supervised dictionary learning method for optimizing the feature-based Bag-of-Words (BoW) representation towards Information Retrieval. Following the cluster hypothesis, which states that points in the same cluster are likely to fulfill the same information need, we propose the use of an entropy-based optimization criterion that is better suited for retrieval instead of classification. We demonstrate the ability of the proposed method, abbreviated as EO-BoW, to improve the retrieval performance by providing extensive experiments on two multi-class image datasets. The BoW model can be applied to other domains as well, so we also evaluate our approach using a collection of 45 time-series datasets, a text dataset, and a video dataset. The gains are three-fold since the EO-BoW can improve the mean Average Precision, while reducing the encoding time and the database storage requirements. Finally, we provide evidence that the EO-BoW maintains its representation ability even when used to retrieve objects from classes that were not seen during the training.
Information Retrieval is the process of extracting an appropriate document from a huge dataset in accordance with the user's intention. Retrieval of relevant document tends to be a serious issue in the Information Retrieval system. Ranking models in Information Retrieval system eliminates this difficulty by ranking the documents based on relevancy measure. Thus improving the IR system's performance. Several ranking models are developed to cope up with the increasing demand for accurate Information Retrieval. The merits and demerits of various ranking models are discussed with a comparative analysis.
Privacy of the outsourced data is one of the major challenge. Insecurity of the network environment and untrust-worthiness of the service providers are obstacles of making the database as a service. Privacy concerns exist wherever personally identifiable information is collected and stored. Public databases and resources are potential source of risk to user privacy. An intentional database owner can guess user details by practically monitoring their queries. Hence the major challenge here is to share the data by protecting personal information. A data retrieval scheme allowing the users to query the database with out comprpmising the privacy in data item is generally sought The naive solution for confidentiality is to encrypt data before outsourcing. Query execution, key management and statistical inference are major challenges in this case. The proposed system suggests a mechanism to store private information and secure retrieval of this data using secret sharing based Secure Multiparty Computation(SMC). The idea is to develop a mechanism to store private information with a highly available storage provider which could be accessed from anywhere using queries while hiding the actual data values from the storage provider. Multiparty Computation facilitates application of join functions over their private inputs and SMC performs these functions by keeping the input data private. This is achieved by making secret shares of the inputs and manipulating the shares to compute some functions.
All the existing replicated database Private Information Retrieval (PIR) schemes have failed to achieve 1) Breaking the dependency of both user privacy and data privacy on a single intractability assumption 2) Maintaining information-theoretically secure queries even on colluding and 3) Non-trivial server communication cost 4) Verifying the integrity of the communicating information. In this paper, we have introduced a computationally bounded 2-database colluding resistant information-theoretic private information retrieval scheme with non-trivial communication cost. Since intractability assumption independent user privacy, colluding resistant information-theoretically secure query generation and the non-trivial communication have been involved in the proposed scheme, we coin the proposed scheme as Hybrid Private Information Retrieval (hPIR). In this, colluding resistant information-theoretically secure query generation (to preserve user privacy) is achieved by extending from quadratic residuosity property dependent index inputs to quadratic residuosity property independent index inputs and non-trivial communication is achieved by a new quadratic residuosity based recursive 2-bit encryption method (to preserve data privacy). Note that the generation of colluding resistant information-theoretically secure query supports user privacy whereas application of recursive 2-bit encryption method supports both data privacy and non-trivial communication.
Digital preservation is one of the ways to make history of a nation keeps alive. The history of a nation rooted in its cultures and nature. The wealth of these cultures and nature spread in many entities and in many areas. Their existences are in a vulnerable position. One of the efforts to preserve cultural heritage and natural history is by utilizing information and communication technologies (ICT) as enabler. In this paper we overview a series of research progress on integrated approach to digital preservation of eCultural Heritage and Natural History (eCHNH) Framework. The research results show that the availability and accessibility of cultural heritage and natural history are improved. Furthermore, the unification of these assets that are spread in several entities and several geographical areas, are established through seamless integration.
Latest advances in artificial intelligence (AI) have given upward push to automated mastering engines which can be capable of leveraging big quantities of records to become aware of and extract applicable information with top notch accuracy. This has enabled more green records retrieval strategies, which has capability to revolutionize how human beings use and get admission to facts. computerized studying engines are capable of acting complex duties consisting of natural language processing, sentiment analysis, and object recognition, all of which require extensive computation strength. because of this possibilities for more suitable statistics retrieval systems are getting an increasing number of available to organizations.the automatic mastering engine is particularly beneficial in situations wherein there's an abundance of statistics. The engine is able to filter and narrow down the facts to identify and extract simplest the most beneficial data. This allows to keep away from clutter and false positives from the hunt consequences, accordingly main to greater accurate and reliable results. moreover, their capacity to examine from the records they process way that they can be tailor-made to particular use instances, thereby making searches greater green and powerful. furthermore, automated mastering engines can perform at a miles quicker fee than traditional seek techniques. through automating the technique of statistics collection, evaluation, and usage, time is stored and the efficiency of the whole information retrieval system is substantially improved. this may have a ways-reaching implications.
A sequence of recent papers, including in this journal, has considered the role of measurement scales in information retrieval (IR) experimentation, and presented the argument that (only) uniform-step interval scales should be used. Hence, it has been argued, well-known metrics such as reciprocal rank, expected reciprocal rank, normalized discounted cumulative gain, and average precision, should be either discarded as measurement tools, or adapted so that their metric values lie at uniformly-spaced points on the number line. These papers paint a rather bleak picture of past decades of IR evaluation, at odds with the IR community’s overall emphasis on practical experimentation and measurable improvement. Our purpose in this work is to challenge that pessimistic assessment. In particular, we argue that mappings from categorical and ordinal data to sets of points on the number line are valid provided there is an external reason for each target point to have been selected. We first consider the general role of measurement scales, and of categorical, ordinal, interval, ratio, and absolute data collections. In connection with the first two of those categories we also provide examples of the knowledge that is captured and represented by numeric mappings to the real number line. Focusing then on information retrieval, we argue that document rankings are categorical data, and that the role of an effectiveness metric is to provide a single value that summarizes the usefulness to a user or population of users of any given ranking, with usefulness able to be represented as a continuous variable on a ratio scale. That is, we argue that most current IR metrics are well-founded, and, moreover, that those metrics are more meaningful in their current form than in the proposed “intervalized” versions.
The work presented in this paper aims to develop a system for information retrieval which has the particularity of being adapted to the Arabic language and providing personalized results based on the user's preferences / interests. For this reason, we proposed a query expansion method based on both semantic knowledge from existing ontologies for the Arabic language and information from the users' profiles. The proposed method is interesting for several reasons. Personalized information related to the users' profiles to enrich the query should be considered since it is entirely appropriate and justified to improve traditional IR systems. Moreover, this idea has been little exploited for IR systems in Arabic. In addition, the coupling between information from the user's profiles and that contained in the ontologies is also an original and interesting track to develop. The suggested query expansion method has been implemented and tested on 150 queries for 50 users or 7500 queries of evaluation. The results of this evaluation are promising.
In case based reasoning the new problems occurred are solved with the help of the solutions used for similar problems occurred in the past. Information retrieval is very important step in case based reasoning. Similarity knowledge is used to retrieve the data in CBR. Many of the existing systems use similarity knowledge as well as the association rules to retrieve the information. {But many existing algorithms are mainly dependent on the similarity knowledge and they don't consider the other available forms of which are helpful for the information retrieval process. This paper uses Apriori algorithm for extracting the expected relevant cases which are dependent on association rules and also on the correlation methods. The main goal of this paper is to provide details of information retrieval in CBR with the help of different methods and also to show the efficiency of the Eclat algorithms.
Information retrieval systems are generally used to find documents that are most appropriate according to some query that comes dynamically from users. In this paper a novel Fuzzy Document based Information Retrieval Model (FDIRM) is proposed for the purpose of Stock Market Index forecasting. The novelty of proposed approach is a modified tf-idf scoring scheme to predict the future trend of the stock market index. The contribution of this paper has two dimensions, 1) In the proposed system the simple time series is converted to an enriched fuzzy linguistic time series with a unique approach of incorporating market sentiment related information along with the price and 2) A unique approach is followed while modeling the information retrieval (IR) system which converts a simple IR system into a forecasting system. From the performance comparison of FDIRM with standard benchmark models it can be affirmed that the proposed model has a potential of becoming a good forecasting model. The stock market data provided by Standard & Poor's CRISIL NSE Index 50 (CNX NIFTY-50 index) of National Stock Exchange of India (NSE) is used to experiment and validate the proposed model. The authentic data for validation and experimentation is obtained from http://www.nseindia.com which is the official website of NSE. A java program is under construction to implement the model in real-time with graphical users' interface.
The Semantic Web technologies, especially ontologies are used for applications representation and knowledge management. In this paper we focus on the presentation of the map in ontological form and the use of the mapping ontology to serve the user query. The proposed approach consists of two parts; the first part is the construction of ontology representing the components of the map where an instance represents an entire map. The second part represents an information retrieval system by exploiting the ontology previously constructed through the use of SPARQL queries. The proposed approach is validated through a prototype exploiting a set of tools (Microsoft Expression Web, PHP, SPaRQL,...) as an adaptation of a classic architecture of Web application.
In recent years, personal information management has become a hot topic in the field of information management. This paper introduces the development and some relevant definitions of personal information management, and points out a series of problems and explores some countermeasures concerning these issues.
Private information retrieval (PIR) protocols ensure that a user can download a file from a database without revealing any information on the identity of the requested file to the servers storing the database. While existing protocols strictly impose that no information is leaked on the file’s identity, this work initiates the study of the tradeoffs that can be achieved by relaxing the perfect privacy requirement. We refer to such protocols as weakly-private information retrieval (WPIR) protocols. In particular, for the case of multiple noncolluding replicated servers, we study how the download rate, the upload cost, and the access complexity can be improved when relaxing the perfect privacy constraint. To quantify the information leakage on the requested file’s identity we consider mutual information (MI), worst-case information leakage, and maximal leakage (MaxL). We present two WPIR schemes, denoted by Scheme A and Scheme B, based on two recent PIR protocols and show that the download rate of the former can be optimized by solving a convex optimization problem. We also show that Scheme A achieves an improved download rate compared to the recently proposed scheme by Samy et al. under the so-called  ϵ -privacy metric. Additionally, a family of schemes based on partitioning is presented. Moreover, we provide an information-theoretic converse bound for the maximum possible download rate for the MI and MaxL privacy metrics under a practical restriction on the alphabet size of queries and answers. For two servers and two files, the bound is tight under the MaxL metric, which settles the WPIR capacity in this particular case. Finally, we compare the performance of the proposed schemes and their gap to the converse bound.
This paper focuses on retrieval of web documents with improved response time and similarity using particle swarm optimization (PSO) technique. Since the nature of the web data is distributed, volatile and uncertain, an accurate and speedy access is required. Hence a novel approach on evolutionary bio-inspired Swarm Intelligence techniques to optimize search process in Web Information Retrieval systems is proposed and developed. Here, a novel algorithm has been proposed using basic PSO technique which works on both small CACM and huge RCV1 collections. This is applied on the pre-processed documents to retrieve most similar documents with a very less response time. This paper also reveals a comparative study with the existing method.
A retrieval profiling is to capturing the metadata of search process and the details of retrieval results. According to the theory of information retrieval (IR) that relevance judgment should be justified by user's needs, the aim of this research is to propose a retrieval profiling framework (RPF) for pooling relevant interpretations of al-Quran based on thematic search. A thematic interpretation of al-Quran is called tafseer al-maudu'iy (urdu language sentence). The domain tafseer al-maudu'iy is sensitive in IR due to its interrelation of Quranic interpretation within its verse(s). On account of this concern, this paper has proposed a RPF to develop a standard procedure for themes/topics of al-Quran in order to keep the interrelation in search. It has undertaken mix methodology with a comparative study on two thematic search engines of al-Quran and two sources of thematic interpretation of al-Quran to accomplish the procedure. With a counting number of themes and experts' justification, it encountered a pilot test to justify the validity of the RPF and found effective. Despite the limitation of data and experts, it is assumed that this RPF can contribute to users in thematic search of al-Quran as well as it can practically guide the IR system developers for generating a good retrieval platform based on thematic search of Quranic interpretation.
Software systems introduce an increasing number of configuration options to provide flexibility and customizability for users. As configurations in mature systems become increasingly complex, configuration errors have become one of major causes of software failure. Due to the complexity of the correlation between configuration options and operating environment, misconfiguration diagnosing faces many challenges, such as low efficiency of program analysis and incomplete statistical data. In order to improve the efficiency and accuracy of diagnosing configuration errors, we propose a method to construct a database of historical configuration errors, and use information retrieval to troubleshoot configuration errors. We design and implement ConfErrShooter, which first extracts log features for known configuration errors and recommends potential misconfigurations by detecting the similarity between the new failure logs with known log features. We use manual and real-world error cases from MySQL, PostgreSQL and Httpd to evaluate the effectiveness. The result shows the accuracy reaches 86% for the manual error cases, and 73% for real-world cases.
In this paper, we present a Persian dataset designed specifically for the task of text information retrieval through the web. Unlike most available datasets, we incorporate real-world user queries in this dataset distinguishing it from the existing ones in the field. We believe this dataset serves as a valuable resource for researchers and practitioners involved in the development and evaluation of information retrieval systems tailored for the Persian language. Besides the 951 anonymized Persian queries collected by the Zarebin search engine, this dataset consists of documents from the web retrieved based on the queries by both Google and Zarebin search engines. Another interesting feature is that the relevance of each retrieved document to the original query is human annotated. Overall, this corpus consists of 18568 pairs of query and document, each with a relevant/irrelevant tag
In a world characterized by rapid information sharing and constant consumer feedback, the ability of brands to understand and respond to changing public sentiment is crucial. The proposed research responds to the imperative of comprehending public sentiment regarding brands relative to their competitors. It aims to not only track how people feel about a brand but also provide valuable insights by comparing it to how they feel about similar brands in the market. The proposed research incorporates Information retrieval techniques such as Textual Similarity calculation to compare textual data and Machine Learning techniques to compare product similarity for Competitive Analysis. We aim to create user-friendly visualizations and tools to help brands stay up-to-date with public opinion, empowering them to make informed decisions in an ever-evolving landscape of consumer sentiment and competition. This research aims to offer a practical solution for brand reputation management and market research strategy in the digital age.
The importance of effective Thai information retrieval (IR) increases as more businesses in Thailand undergo digital transformation. However, previous research on Thai IR systems has mainly focused on web search engines. This study will focus on using query correction to reduce user errors to improve Thai IR. Experiments are conducted on our business-oriented Thai IR task (bTIR). Our investigation presented three notable findings. First, cognitive errors are less of an issue in a business setting. Thus, homophones correction methods provide very little to no benefit for bTIR. Second, approximation based spelling correction methods can significantly reduce search performance. Thus, partial matching on a full dictionary, such as symmetric delete indexing (SymSpell), should be preferred over non-optimal search methods. Third, we introduce a re-ranking algorithm for query corrector, which features multiple sub-correctors (e.g., ThaiQCor 2.0), which results in better performance across multiple configurations.
A private information retrieval (PIR) protocol guarantees that a user can privately retrieve files stored in a database without revealing any information about the identity of the requested file. Existing information-theoretic PIR protocols ensure perfect privacy, i.e., zero information leakage to the servers storing the database, but at the cost of high download. In this work, we present weakly-private information retrieval (WPIR) schemes that trade off perfect privacy to improve the download cost when the database is stored on a single server. We study the tradeoff between the download cost and information leakage in terms of mutual information (MI) and maximal leakage (MaxL) privacy metrics. By relating the WPIR problem to rate-distortion theory, the download-leakage function, which is defined as the minimum required download cost of all single-server WPIR schemes for a given level of information leakage and a fixed file size, is introduced. By characterizing the download-leakage function for the MI and MaxL metrics, the capacity of single-server WPIR is fully described.
Events formulate the world of human being and could be regarded as the semantic units in different granularities for information organization. Extracting events and temporal information from texts plays an important role for information analytics in big data. This paper surveys research work on text-based event temporal resolution and reasoning including the identification of events, temporal information resolutions of events, the rule-based temporal relation reasoning between events and the temporal representations. We point out the shortcomings of existing research work and the future trends for advancing the identification of events and the establishing/reasoning of temporal relations in the future.
The researchers of this study chose 242 Arabic abstract doucments. Computer science and information systems are mentioned in all of these abstracts. The researchers created an Arabic-specific autonomous information retrieval system, the system was written in the C# NET programming language and its compatible with IBM/PCs and other microcomputers. For this corpus, The researchers used an automatic indexing strategy. The system was created using the Vector Space Model (VSM). In this model, the researcher take all measurements and utilize the Cosine, Dice, Jaccard, and Inner Product Similarity measures. Using the Vector Space Model, the researchers compared the retrieval results. In Arabic documents, the researchers discovered that the retrieval result for cosine is better than the retrieval result for other measures.
Deep stacking networks (DSN) are a special type of deep model equipped with parallel and scalable learning. We report successful applications of DSN to an information retrieval (IR) task pertaining to relevance prediction for sponsor search after careful regularization methods are incorporated to the previous DSN methods developed for speech and image classification tasks. The DSN-based system significantly outperforms the LambdaRank-based system which represents a recent state-of-the-art for IR in normalized discounted cumulative gain (NDCG) measures, despite the use of mean square error as DSN's training objective. We demonstrate desirable monotonic correlation between NDCG and classification rate in a wide range of IR quality. The weaker correlation and more flat relationship in the high IR-quality region suggest the need for developing new learning objectives and optimization methods.
The paper introduces an advance to connect various current information retrieval (IR) techniques with change impact analysis (CIA) and Bag of Words, trying to recognize the possible results of a replacement or manage the required changes for affecting the necessary change. In this paper a neural network-based LSTM-RNN Algorithm is proposed to find the similarity document. Also, RMSprop Optimization model is used to reduce the vibrations in the upward path and it can improve our learning rate and precision rate. The experimental result shows that the proposed method performs better accuracy compared to the current methods.
Information retrieval is a technique used in search engines, advertisement placement and cognitive databases. With increasing amounts of data and stringent response time requirements, improving the underlying implementation of document retrieval becomes critical. To this end, we consider a Bloom filter, a simple randomized data structure that answers membership queries with no false negative and customizable false positive probability. Mainly, we focus on the speed-up of the algorithm by using a Graphics Processing Units (GPU) based implementation. Starting from a regular CPU implementation of the Bloom filter algorithm, we employ different optimization techniques on the two basic Bloom filter operations: mapping and querying. An important speed-up is achieved for both operations: over 300x for mapping, and over 20x for querying. Furthermore, we show that the number of hash functions used during the mapping operation, the number of files, and the number of query words have a significant effect on the execution time and the speed-up.
In the era of big data, volume of online text documents has exploded through social media, search engines, and many other channels. Rapid growth of data demands more efficient processing and information retrieval approaches, among those Latent Dirichlet Allocation (LDA) and Word2Vec are commonly used. LDA explains a global relationship by investigating latent topics from a collection of documents and assigning each document with a probability distribution over all topics. In contrary, Word2Vec projects words to vectors with semantic meanings by local contexts. However, LDA concentrates on handling high probability words and neglects low probability but meaningful ones. While Word2Vec explores only surroundings of the target words within sentences, which makes global improvement of word representations very difficult. Therefore, we propose an improved text information retrieval method that integrates both global and local text information. Experimental results show that our proposed document representation method significantly enhance accuracy of classification analysis.
Traditionally, in the financial services industry, a large amount of financial analysts’ time is spent on knowledge discovery and extraction from different unstructured data sources, such as reports, research notes, SEC filings, earnings call transcripts, news etc. In addition to inefficiency, this manual information retrieval process can be prone to human error, subjectivity, and inconsistency. Recent advances in representation learning provide a reliable platform for mapping a large volume of unstructured data to a high dimensional vector space where similarities and differences between data points can be quantified and used for featurization, pattern recognition and information retrieval. In this work we demonstrate that by properly representing terms, documents and companies in the same informative vector space and applying a simple self-supervised learning framework, relevant companies and documents can be retrieved with a good level of accuracy given the topics of interest, even with no prior labeled data.
Medical search technologies are crucial to enable the user to rapidly and effectively discover useful information from massive medical and clinical data. Because of the complexity of medical terminology, traditional information search methods have not fully expressed the intention of the query request and explored the potential semantic knowledge in the document. In this paper, we propose a multi-analysis approach by considering the medical ontology as a semantic resource, which can excavate latent semantic information of a user's query request. In addition, we also recognize topics of medical documents to express text contents for providing support for calculating the similarity between query keywords and documents. Our experiments on PubMed medical article collections show that the semantic-based multi-analysis approach is feasible and efficient compared with other traditional approaches in medical retrieval.
Users' information seeking behavior is an important topic especially for organizations that seek to understand how users browse and search for information. Information nature is elusive and hard to describe given that different people use information for different purposes and different ways. This study aims at evaluating and reviewing information seeking models that have emerged in Information Retrieval (IR) literature. In this paper, the classic IR models are explicitly reviewed. Previous IR models are categorized into two main types reflecting its interactive and passive nature. A holistic model of IR is suggested to cover for missing points of previous models and to account for the interactive nature of today's social and dynamic information environment. The paper concludes with general observations on the current state of IR and how users' IR needs are changing with change in the IR environment.
Image retrieval is mainly a text-based image retrieval technology, which uses text description to describe the characteristics of images, such as the author, age, genre and size of paintings. Later, image retrieval techniques that analyze and retrieve the content semantics of images, such as color, texture, layout, etc. Feedback technology has become an important technology to improve retrieval efficiency in content-based image retrieval. However, in oil painting image retrieval, there is a huge semantic difference between high-level semantics and low-level features. The traditional relevance feedback technology requires multiple feedbacks to obtain satisfactory results, which makes the retrieval task of users time-consuming and cumbersome. In order to further improve the accuracy of retrieval, many systems combine relevant feedback technology to collect the feedback information of users on retrieval results. What they achieve is a gradual refinement of image retrieval process. In the same retrieval process, they need to constantly interact with users. Therefore, this paper proposes a retrieval technology of oil painting image through the feedback mechanism of oil painting image retrieval. On the one hand, the analysis and transformation of user needs form questions that can be retrieved from the index database. On the other hand, collect and process oil painting image resources, extract features, analyze and index, and establish image index database. The last aspect is to calculate the similarity between user questions and records in the index database according to the similarity algorithm, extract records that meet the threshold as the result, and output them in descending order of similarity.
Urdu is a widely spoken language with 163 million speakers across the globe. Information Retrieval (IR) for Urdu entails special consideration of research community due to its rich morphological features and a large number of speakers. In general, IR evaluation task is not extensively explored for Urdu. The most important missing element is the availability of a standardized evaluation corpus specific to Urdu. In this research work, we propose and construct a standard test collection of Urdu documents for IR evaluation and named it Collection for Urdu Retrieval Evaluation (CURE). We select 1,096 unique documents against 50 diverse queries from a large collection of 0.5 million crawled documents using two IR models. The purpose of test collection is the evaluation of IR models, ranking algorithms, and different natural language processing techniques. Next, we perform binary relevance judgment on the selected documents. We also build two other language resources for lemmatization and query expansion specific to our test collection. Evaluation of test collection is carried out using four retrieval models as well using the stop-words list, lemmatization, and query expansion. Furthermore, error analysis is performed for each query with different NLP techniques. To the best of our knowledge, this work is the first attempt for preparing a standardized information retrieval evaluation test collection for the Urdu language.
Social networking sites like Facebook and Flickr allows users to upload billions of images, famous e commerce web sites such as flipkart, amazon etc. are also provides number of product related images to user. Images which are present in social multimedia networking websites are accompanied by different tags, comments, annotations and other related information which tend to form an image rich information networks. Due to existence of different related information to images (tag, annotation etc.) retrieval of images from image rich information networks is very challenging and complex task. In this paper we have proposed an integrated approach (S-Cbir) to retrieve an images from image rich information networks. The proposed algorithm consists integration of two different techniques such as link based image retrieval and content based image retrieval systems. K-Simrank algorithm is first applied to find out the node similarity in image rich information networks and then CBIR technique is applied to find out relevant images. SIFT, CEDD, feature and texture extraction descriptors are used in CBIR technique. The search results shows that proposed algorithm refines the search results and also improves the accuracy of search.
In the three aspects of information retrieval, scientific paper evaluation, revealing the knowledge structure evolution, citation analysis plays a vital role. With the appearance of full-text literature repositories, Citation analysis entered the 4 Era-full-text citation analysis age. However There is no Chinese full-text literature database. Secondly, in the two key points of citation content analysis, the citation topic analysis and the citation sentiment analysis are still lack of effective methods especially for Chinese literature. These have greatly restricted the research and application of full text citation analysis in Chinese Literature. In this project, our basic idea is: Through the study of establish a set of effective research for Chinese large-scale literature citation content analysis two key research points and efficient method for the analysis Chinese full citation analysis based on data, to enhance the effectiveness of citation analysis in our country in the field of science and technology. The main research topics include: (1) Studying the methods of real-time construction of standardized Data Set for citation content analysis based on spark. (2) A scientific and technical literature retrieval platform based on Citation content is proposed. In order to improve the research efficiency of science and technology in China and to improve the accuracy of sci-tech literature searching, the full text citation analysis is promoted.
Document retrieval is the process of discovering a set of documents related to a query or another document, as per comparable similarity to some significant extent. Such as legal law, cases, or judgments. The current work under the area of the Document retrieval domain focuses on creating a system that will examine various features or information contained in the supporting documents with the given query document and suggest the possible top-ranked documents which have a higher similarity score. This work is particularly useful in helping out the lawyers by suggesting similar cases or judgments and allowing them to explore significantly less number of previously recorded cases/judgments. The main focus of this work is to retrieve relevant documents for particular Supreme Court cases in India from a set of prior case documents. To categorize documents and do the document retrieval task, we have used a published data set of 2000 prior cases. For each distinct document, the current system lists a set of documents with their ranking scores using N similarity, Fast-text, and BERT. Then, the system returns the relevant documents based on their ranking score. In order to compare our system to other similar ones already in use, we computed the accuracy and recall of our system. We have discovered that various methods perform better in certain areas where a few systems have higher precision and others with higher recall values. Our developed system provides comparably better accuracy, recall, and mean average precision than other current systems.
The exponential growth of digital information requires advanced methods for efficient information retrieval. This paper proposes a novel algorithm that integrates data structure fusion optimization for information retrieval tasks in computer networks. Based on a comprehensive literature review, key challenges are identified and explored existing solutions in the field of information retrieval. The proposed methodology focuses on improving the accuracy and efficiency of information retrieval through innovative data fusion techniques. The presented robust framework seamlessly integrates data structure optimization to improve the performance of the existing retrieval models. Experimental evaluations conducted on the “WebNews” dataset demonstrate the effectiveness of the proposed algorithm. A series of experiments were conducted with 20 groups of experimental settings, for four different methods. The results show that the Proposed method consistently outperforms the other methods in terms of accuracy and stability in the majority of experimental settings.
Social networks are becoming an integral part of people's life to share and disseminate information instantly. Users share their views on a particular topic or events and receive feedback from their followers, friends and colleagues in form of comments, like, dislike, upvote, downvote etc. Since the user has a very limited control on previously shared information and has limited storage space in his/her account, the management of users information is a very tedious task. In this paper, we discuss different types of social networking services and user's information shared on these services. We categorize the content based information into two categories namely textual content based information and visual content based information. Then we discuss the major efforts made for retrieval of these information from different social networks. We also outline a procedure for content based information retrieval from multiple social networks.
Cross-Language Information Retrieval (CLIR) is a sub field of Information Retrieval (IR). Like IR, in CLIR for a particular information need, we have to find relevant information or documents containing such information. In CLIR multiple tools must be developed to match terms containing same meaning in different languages. The usual solution is to translate the query and/or the documents before performing the search. So translation is pivotal activity in CLIR. In CLIR a wide range of techniques were proposed in the literature for translation. This paper presents a detailed insight of these techniques, with a special emphasis on recent developments.
Medical image retrieval (MedIR) is a challenging field in Visual information retrieval, due to the multi-dimensional and multi-modal context of the underlying content. Traditional models do not take the intrinsic characteristics of data into consideration and have achieved limited accuracy in application to medical images. Manifold Ranking (MR) is a technique that can be used in further optimizing precision and recall in MedIR applications as it ranks items by traversing a dynamically constructed content-specific information graph. In this paper, a MedIR approach based on Manifold Ranking is proposed. Medical images being multi-dimensional, exhibit underlying cluster and manifold information which enhances semantic relevance and allows for label uniformity. Hence, when adapted for MedIR, MR can help in achieving large-scale ranking across datasets as is the case in most medical imaging applications. In addition, a relevance feedback mechanism was also incorporated to support a learning based system. We show that MR achieved significant improvement in retrieval results with relevance feedback as compared to the Euclidean Distance (ED) rankings. This showcases the importance of analyzing the inherent latent structure in medical image data for better performance over traditional methods.
This paper focuses on personalization of "user information needs" by applying clustering techniques of data mining. In the current data centric world, it is very important to analyze data properly and draw the crux from the available for effective creation and maintenance of user profiles. This paper throws explains the use of an indispensable technique called Clustering Technique, which is used to group items exhibiting similar behavior into different item sets.
Domain ontology in military transportation is applied to information retrieval to create corresponding personalized user interest models and personalized information retrieval model, construct a knowledge base on basis of domain ontology in military transportation, in order to to improve the efficiency in information retrieval and to provide users with timely, accurate and high-quality personalized services.
Locating bugs is important, difficult, and expensive, particularly for large-scale systems. To address this, natural language information retrieval techniques are increasingly being used to suggest potential faulty source files given bug reports. While these techniques are very scalable, in practice their effectiveness remains low in accurately localizing bugs to a small number of files. Our key insight is that structured information retrieval based on code constructs, such as class and method names, enables more accurate bug localization. We present BLUiR, which embodies this insight, requires only the source code and bug reports, and takes advantage of bug similarity data if available. We build BLUiR on a proven, open source IR toolkit that anyone can use. Our work provides a thorough grounding of IR-based bug localization research in fundamental IR theoretical and empirical knowledge and practice. We evaluate BLUiR on four open source projects with approximately 3,400 bugs. Results show that BLUiR matches or outperforms a current state-of-the-art tool across applications considered, even when BLUiR does not use bug similarity data used by the other tool.
With the rapid development of biomedicine, the number of biomedical articles has increased accordingly, which presents a great challenge for biologists trying to keep up with the latest research. Information retrieval seeks to meet this challenge by searching among a large number of articles based on given queries and providing the most relevant ones to fulfill information needs. As an effective information retrieval technique, query expansion has some room for improvement to achieve the desired performance when directly applied for biomedical information retrieval because there exist many domain-related terms both in users' queries and in related articles. To solve this problem, we propose a biomedical query expansion framework based on learning-to-rank methods, in which we refine candidate expansion terms by training term-ranking models to select the most relevant terms. To train the term-ranking models, we first propose a pseudo-relevance feedback method based on MeSH to select candidate expansion terms and then represent the candidate terms as feature vectors by defining both the corpus-based term features and the resource-based term features. Experimental results obtained for TREC genomics datasets show that our method can capture more relevant terms to expand the original query and effectively improve biomedical information retrieval performance.
Compared with people's urgent desire for information, the current information retrieval still has problems such as slow speed and low precision. In response to this problem, this paper proposes an information retrieval method based on NLP. Firstly, the semi-supervised learning algorithm is used to describe the natural language under the manifold condition, and the undirected graph is constructed for all the data. For the undirected graph after construction, we use the label propagation algorithm to simplify the undirected graph to reduce the computational complexity in the information retrieval process. Then the keywords will be extracted by TextRank algorithm to achieve information retrieval. Finally the experimental results show that the retrieval efficiency can be improved by using this algorithm.
Image retrieval using free hand drawn sketch image is addressed in the presented paper. As the volume of image information's are rapidly increasing, retrieval of information from the image data set is observed to be a tedious task. Majority of image retrieval are developed using an image querying system, however, unknown images which is given input as a free hand sketch is less addressed. The recent developments in the sketch-based image retrieval (SBIR) were developed using spectral image representation. However, the problem to the existing SBIR approach using random distribution of feature set builds a larger search overhead and the diverse distribution leads to misclassification. The objective of the proposed work is to develop a faster sketch-based image retrieval system with higher accuracy. This paper presents a new clustering-based approach for spectral feature distribution using cluster gain and information selection approach. The relative distribution and the information gains are considered in the development of the proposed approach for classification. The proposed approach observes a higher retrieval accuracy and classification performance in comparison to the existing spectral feature classification.
This paper introduces the Chinese Semantic Retrieval model and the key technology. It initially points out the core idea of mapping natural language to RDF triples. Based on the Language Technology Platform dependency relationship, it classifies the Chinese question. Then, the article outlines the concrete method of mapping Chinese question to SPARQL query with examples. At last, it implements the system and gives the retrieval result, which decreases the irrelevant searching return compared with the tradition information retrieval method based on keyword matching.
There is an emergent interest in the use of emotion data to improve information retrieval processes. Our study examined whether the knowledge of searchers' emotions can be used to predict their actions (and vise versa). We investigated associations between information retrieval behaviors (e.g., examination of search results) and patterns of emotional expressions around those behaviors, and found that individual search behaviors were associated with the certain types of emotional expressions. The findings can inform classification of emotions and search behaviors, and in turn lead to the development of affect-sensitive retrieval systems.
Today, Internet of Vehicles has given us the opportunity to understand our vehicle in digitized way. To track the vehicle, information regarding the vehicle registration number, the real time location of the vehicle, speed of the vehicle, detail of the vehicle owner etc. are needed both by the traffic authorities and the driver while driving itself. An approach to track the vehicle and to notify the traffic authority as well as driver is discussed in this paper. An algorithm is implemented to transmit the data from tracked vehicle. As part of the solution, a mobile application is implemented and used to track the location and speed of the vehicle. We are proposing a context aware application using RFID sensor, head up display to track, analyse and control vehicle motion and speed in real time. The solution gives all the information related to the vehicle and driver to the traffic authority.
Multimodal Question Answering (MMQA) has emerged as a challenging frontier at the intersection of natural language processing (NLP) and computer vision, demanding the integration of diverse modalities for effective comprehension and response. While pre-trained language models (PLMs) exhibit impressive performance across a range of NLP tasks, the investigation of text-based approaches to address MMQA represents a compelling and promising avenue for further research and advancement in the field. Although recent research has delved into text-based approaches for MMQA, the attained results have been unsatisfactory, which could be attributed to potential information loss during the knowledge transformation processes. In response, a novel three-stage framework named UniRaG is proposed for tackling MMQA, which encompasses unified knowledge representation, context retrieval, and answer generation. At the initial stage, advanced techniques are employed for unified knowledge representation, including LLaVA for image captioning and table linearization for tabular data, facilitating seamless integration of visual and tabular information into textual representation. For context retrieval, a cross-encoder trained on sequence classification is utilized to predict relevance scores for question-document pairs, and a top-k retrieval strategy is employed to retrieve the documents with the highest relevance scores as the contexts for answer generation. Finally, the answer generation stage is facilitated by a text-to-text PLM, Flan-T5-Base, which follows the encoder-decoder architecture with attention mechanisms. During this stage, uniform prefix conditioning is applied to the input text for enhanced adaptability and generalizability. Moreover, contextual diversity training is introduced to improve model robustness by including distractor documents as negative contexts during training. Experimental results on the MultimodalQA dataset demonstrate the superior performance of UniRaG, surpa... (Show More)
From aspects of domain ontology construction, concept similarity computation based on ontology and semantic query expansion study the related technologies of information retrieval system based on ontology; establish food safety domain ontology and ontology-based concept similarity computation model, put forward a new semantic query expansion method based on concept similarity computation; design and implement food safety semantic retrieval system. The experiments show that this food safety semantic retrieval system is superior to the retrieval system based on keywords both in the recall ratio and the precision, and realize certain intelligent retrieval.
This paper explores various cloud computing distributed technologies and constructs a SaaS cloud computing architecture for a human resource management system. Each module’s functions within the system are detailed. To enhance information security management in the HRMS, the concept of information trust degree is introduced, improving retrieval speed. The effectiveness is tested, revealing a retrieval time of approximately 24s for 100 to 1000 information groups. The optimized information trust degree yields security coefficients of 98.83% and 96.86% for human resources information in sending and receiving endpoints, respectively. The utilization of SaaS cloud computing enhances HR management system efficiency, and information trust optimizes HR information security during transmission.
We consider constructing capacity-achieving linear codes with minimum message size for private information retrieval (PIR) from N non-colluding databases, where each message is coded using maximum distance separable (MDS) codes, such that it can be recovered from reading the contents of any T databases. It is shown that the minimum message size (sometimes also referred to as the sub-packetization level) is significantly, in fact exponentially, lower than previously believed. More precisely, when K > T/ gcd(N, T) where K is the total number of message in the system and gcd(·,·) means the greatest common divisor, we establish, by providing both a novel code construction and a matching converse, the minimum message size as lcm(N -T, T), where lcm(·,·) means the least common multiple. On the other hand, when K is small, we show that it is in fact possible to design codes with a message size even smaller than lcm(N - T, T).
Extracting the users expected information from a large text collection based on some query is the aim of a Information Retrieval (IR) system. Now a days Assamese Digital documents are increasing at a huge rate and to collect the information efficiently from them we are in need of an Assamese IR system for retrieving documents. Comparing query and document term on lexical level and the shorter length query implies the problem like word mismatch. Adding additional term with user's query means expanding the query can improve the IR system's performance. The electronic lexical database, WordNet can help identifying the synonymous expressions and linguistic entities that are semantically similar with the input query. Here we present an Assamese IR system based on vector space model and show our result by considering the query vector as the original user's query and the query is extended using the Assamese WordNet synsets.
Multilingual Information Retrieval (MLIR) System deals with the use of queries in one language and retrieves the documents in various languages. Here the Query translation plays a central role in MLIR research. In this paper, the language-independent indexing technology is used to process the text collections of English, Telugu and Hindi languages. We have used multilingual dictionary based word-by-word query translation. The experimental results are evaluated to analyze and compare the performance of Average Precision (AP IR) and Mean Average Precision (MAP IR) metrics of IR system with esteem to the Average Precision (AP MLIR) and Mean Average Precision (MAP MLIR) metrics in MLIR system. Experimental result shows that the effective retrieval and performance of MLIR system has improved by 31.4% over IR system.
Efficient in-network information storage and retrieval is of paramount importance to sensor networks and has attracted a large number of studies while most of them focus on 2D fields. In this paper, we propose novel Reeb graph based information storage and retrieval schemes for 3D sensor networks. The key is to extract the line-like skeleton from the Reeb graph of a network, based on which two distance-sensitive information storage and retrieval schemes are developed: one devoted to shorter retrieval path and the other devoted to more balanced load. Desirably, the proposed algorithms have no reliance on the geographic location or boundary information, and have no constraint on the network shape or communication graph. The extensive simulations also show their efficiency in terms of sensor storage load and retrieval path length.
Information retrieval (IR) becomes a crucial process in different applications like digital libraries, medicine, entertainment, and so on. Presently, the deep learning (DL) techniques are found to be applicable in several computer vision areas like object tracking, image classification, and natural language processing. So, the DL models are mainly applied in this work for the retrieval of text and images effectively. This paper develops a collaborative text and image based information retrieval system using DL models. The presented model involves two distinct processes namely text retrieval and image retrieval. Primarily, the convolutional neural network (CNN) based residual network 50 (ResNet) model called CNN-ResNet50 with Manhattan distance based similarity measurement technique is applied for the effectual retrieval of images. On the other side, bi-directional long short term memory (Bi-LSTM) technique optimized by differential evolution (DE) is applied for text retrieval. The proposed system highlights that the accuracy can be gained by increasing the depth( intermediate layers ) in DL model and providing an optimal result when using differential evolution algorithm with Bi-LSTM. A detailed implementation analysis was carried out to highlight the proficient results of the presented model. The outcomes indicated that the DE-BiLSTM technique has resulted in effectual outcome with the precision of 99.08%, recall of 99.24%, and F-score of 99.21%. Similarly, the CNN-ResNet50 model has also proficient retrieved the images with the average precision of 0.96 and an average recall of 0.91.
In recent years, one of the main areas of research in information retrieval has been configured by Genetic Algorithms, a practice within the field of Artificial Intelligence. His research, through the work of numerous authors, has allowed us to develop a genetic algorithm and implement it in a case of DSS (Decision Support Systems) based on Competitive Intelligence and Technology Monitoring processing which gives a better understanding of how those can contribute to the improvement of information retrieval through optimization of users queries in Information Retrieval Systems and how in turn, the results obtained can be applied to knowledge generating tools for organizations and provide positive results.
This paper investigates the crossroads between Large Language Models (LLMs), Information Retrieval (IR), and the growing issue of misinformation in this current age of the Internet. Large Language Models, including the GPT and LLaMA series (both implemented in this paper), have given us profound insight into how we interact on the Internet and in real life. They have also innovated the way we interact with information. However, these innovations also pose significant shortfalls, particularly in the domain of misinformation from the text. The main contribution of this paper lies in developing a proposed strategy to mitigate the risks of misinformation seen on the Internet and generated from LLMs with a focus on public individuals by recanting statements they have made and the retrieval of said statements. We propose a multi-faceted approach that includes utilizing GPT3.5 and the open source LLaMA2 LLMs, finetuning data curation, and integrating accuracy mechanisms to ensure the most relevant and accurate information is retrieved. The efficacy of this methodology is measured using a cosine similarity metric. Considering that the recanting of this model must be at or as close to the original statement as possible, this metric is deemed most fitting. Findings later in this paper deemed a similarity recall of 90.42% on average with the GPT3.5 variant and 88.29% on average with the LLaMA2 variant, both in zero-shot examples, indicating the core semantic meanings were retrieved with variations on the format of illustration.
There are billions of web pages available on the Internet. Search Engines always have a challenge to find the best ranked list to the user's query from those huge numbers of pages. A lot of search results that corresponding to a user's query are not relevant to the user need. Most of the page ranking algorithms use Link-based ranking (web structure) or Content-based ranking to calculate the relevancy of the information to the user's need, but those ranking algorithms might be not enough to provide a good ranked list. So, in this paper we proposed an Efficient Hybrid Usage-based Ranking Algorithm called EHURA. EHURA was applied to 1033 English Corpus to measure its performance. The result shows improvement of the precision for using EHURA over the Content-based ranking algorithm representation while realizing approximately the same recall percentage.
In this study, we propose a user behavior-based query optimization strategy to address the commonly observed suboptimal query performance and user experience in the field of video retrieval. This strategy primarily involves analyzing user behavioral data during the video retrieval process to expand query keywords, enhancing their relevance and diversity, thus optimizing search results. We achieved this by augmenting the original dataset to generate simulated user behavioral data and Ground Truth data. Subsequently, an automated query expansion algorithm effectively applied this data to video retrieval. Through experimental assessment on the video retrieval dataset (MSR- VTT), we validate the effectiveness of this method. The results demonstrate that this user behavior- driven query expansion approach significantly improves the accuracy of video retrieval and user satisfaction.
This paper proposes methods for information processing of audio streams using methods of information geometry. We lay the theoretical groundwork for a framework allowing the treatment of signal information as information entities, suitable for similarity and symbolic computing on audio signals. The theoretical basis of this paper is based on the information geometry of statistical structures representing audio spectrum features, and specifically through the bijection between the generic families of Bregman divergences and that of exponential distributions. The proposed framework, called Music Information Geometry, allows online segmentation of audio streams to metric balls where each ball represents a quasi-stationary continuous chunk of audio, and discusses methods to qualify and quantify information between entities for similarity computing. We define an information geometry that approximates a similarity metric space, redefine general notions in music information retrieval such as similarity between entities, and address methods for dealing with nonstationarity of audio signals. We demonstrate the framework on two sample applications for online audio structure discovery and audio matching.
Concept lattices constructed by formal concept analysis have been successfully applied in structuring stored information to facilitate its retrieval. However, the models used in retrieval determine the results, and we propose an improved retrieval model to rank query results. The proposed concept similarity enables concept approximation, using both attribute extent and object intent, as well as occurrence frequencies in formal concepts. This approach avoids the computation of a Hasse diagram, only using the concepts of a formal context in query-based retrieval.
Query refinement is useful for automatically generating term., which gives direction for the user to improve the query. The user can modify the terms in the query by considering and adding the suggested terms with the original query conjunctively for improving the precision of retrieval. Query refinement mechanism use n-grams corpus with sequence of terms having the length of N. It uses probabilistic to determine the prediction of the next item in a sequence of a (n-l) order. In this paper., n-gram corpus is generated from the inverted index posting list and effectively used for retrieval application. This method reduces the time and space required for generating the corpus compared to the conventional approaches., which generate the corpus from user queries. This approach generates the corpus using only the terms available in the documents along with their frequency of occurrence. The conditional probability is calculated for matching the pattern to refine the user query for n-grams. The performance of the proposed approach is evaluated using the documents and corresponding inverted index fetched from http://www.nitt.edu/. We found that the proposed approach gives better precision of retrieval.
Topic Detection and Tracking (TDT) refer to the technologies and techniques in analysing and handling the vast amount of information arriving continuously from the news stream. Some issues in news monitoring regarding interface design are considered. In this paper we present the techniques and works done in TDT domain. We discuss the named entities approach used in document representation and in user interface. Finally, we present the suggested approach to improve users' performance and to facilitate them to perform an effective TDT task.
We proposes a privacy-preserving location sharing scheme based on Learning with Errors (LWE) encryption, emphasizing its ability to resist quantum computing attacks and provide a high level of security. In order for real-world usage, we focus on its efficiency, flexibility, and adaptability for streaming. Location privacy has has emerged as a critical concern in today's data-driven society, due to the widespread usage of location-based services and the growing demand for personalized information. We leverages for fast and accurate information retrieval while maintaining robust privacy guarantees, achieving peer-to-peer transmission, 128-bit security or higher and an unparalleled throughput of 10 GB/s/core Through extensive experimental evaluations conducted on real-world deployments, we demonstrate the remarkable performance of our scheme in terms of preserving privacy and retrieval speed, particularly for streaming location data.
This technical briefing presents the state of the art Text Retrieval and Natural Language Processing techniques used in Software Engineering and discusses their applications in the field.
Several approaches have been introduced in the field of information retrieval. Although these approaches are effective but sometimes they are not able to provide accurate information to the user. In this paper an ontology based approach of information retrieval has been presented that uses fuzzy set of various documents for a specific domain. An algorithm for fuzzy based classification of web documents is proposed to create semantic index. The proposed algorithm differs from others as: it utilizes K-means clustering algorithm to find semantically similar terms and domain ontology as well. The retrieved results would always be semantic as they are limited to a particular threshold of classified range.
Spoken content retrieval refers to directly indexing and retrieving spoken content based on the audio rather than text descriptions. This potentially eliminates the requirement of producing text descriptions for multimedia content for indexing and retrieval purposes, and is able to precisely locate the exact time the desired information appears in the multimedia. Spoken content retrieval has been very successfully achieved with the basic approach of cascading automatic speech recognition (ASR) with text information retrieval: after the spoken content is transcribed into text or lattice format, a text retrieval engine searches over the ASR output to find desired information. This framework works well when the ASR accuracy is relatively high, but becomes less adequate when more challenging real-world scenarios are considered, since retrieval performance depends heavily on ASR accuracy. This challenge leads to the emergence of another approach to spoken content retrieval: to go beyond the basic framework of cascading ASR with text retrieval in order to have retrieval performances that are less dependent on ASR accuracy. This overview article is intended to provide a thorough overview of the concepts, principles, approaches, and achievements of major technical contributions along this line of investigation. This includes five major directions: 1) Modified ASR for Retrieval Purposes: cascading ASR with text retrieval, but the ASR is modified or optimized for spoken content retrieval purposes; 2) Exploiting the Information not present in ASR outputs: to try to utilize the information in speech signals inevitably lost when transcribed into phonemes and words; 3) Directly Matching at the Acoustic Level without ASR: for spoken queries, the signals can be directly matched at the acoustic level, rather than at the phoneme or word levels, bypassing all ASR issues; 4) Semantic Retrieval of Spoken Content: trying to retrieve spoken content that is semantically related to the quer... (Show More)
In order to solve the problems of poor retrieval performance and low retrieval quality in traditional retrieval method, a non-metallic pipe retrieval system based on ontology was designed. Firstly, the theory of ontology and information retrieval technology were introduced, and then an ontology model that can describe different information of oilfield non-metallic pipe was built. The ontology files in the type of html were described by OWL language. After that, they were saved in My SQL database based on Jena. Secondly introduced the retrieval algorithm and retrieval process of the system, realized the semantic analysis of query condition, expanded query and other functions. At last, the domain ontology-based oilfield non-metallic pipe information retrieval system was designed. Compared with the traditional retrieval method, the domain ontology-based non-metallic pipe information retrieval system can significantly improve the recall and precision rate. It has great advantage and potential in the field of intelligent information retrieval.
Private information retrieval (PIR) is a privacy protection that allows users to retrieve information from a database without revealing any information about the retrieved data to the server. Since the pioneering work of Chor et al. (FOCS `95) and of Kushilevitz and Ostrovsky (FOCS `97), PIR has been extensively studied (especially the single database setting) in the past two decades. However, most protocols only allow users to retrieve only one data at a time, which leads to high communication costs. To solve this issue, this work proposes a multi-value private information retrieval protocol using group homomorphic encryption, which allows users to retrieve multiple values at a time. We compared our work with that of Ostrovsky and Skeith (PKC `07) and show that retrieving multiple data at a time significantly can significantly reduces communication costs. Furthermore, we provide rigorous proof that if the underlying group homomorphic encryption is secure, then no attacker can know what data the user has retrieved.
Information retrieval in associative memories was studied in a recent paper by Yaakobi and Bruck (2012). Associations between memory entries give us the t-neighbourhood of an entry. In their model, an information unit is retrieved from the memory with the aid of input clues, which are chosen from a reference set. In this paper, we consider the situation where the information unit is found unambiguously using the associated t-neighbourhoods of the input clues. A varying number of input clues are allowed, but a limit mu on the maximum number of them is imposed. Of course, we would like mu to be as small as possible. We consider the problem over the binary Hamming space Fn and focus on the minimum of mu, denoted by ν(n; t). Using linear reference sets, we show that ν(n; 2) ≤ 5 for any n ≥ 9. We also give infinite families of reference sets, which provide good bounds on ν(n; t) for t = 3. In addition, efficient methods are given to obtain bounds on ν(n; t) for any t from known reference sets. We also discuss the applications of this model to the Levenshtein's sequence reconstruction problem and the sensor network monitoring.
We study the problem of intermittent private information retrieval with multiple servers, in which a user consecutively requests one of K messages from N replicated databases such that part of requests need to be protected while others do not need privacy. Motivated by the location privacy application, the correlation between requests is modeled by a Markov chain. We propose an intermittent private information retrieval scheme that concatenates an obfuscation scheme and a private information retrieval scheme for the time period when privacy is not needed, to prevent leakage incurred by the correlation over time. In the end, we illustrate how the proposed scheme for the problem of intermittent private information retrieval with Markov structure correlation can be applied to design a location privacy protection mechanism in the location privacy problem.
NA
The medical field at present is an example for appropriate innovation in the information technology sector where it can bring benefits to physicians and patients and also the medical tools to prevent disease. Among the examples of innovation in information technology are in the field of medical imaging technology, laser technology, nanotechnology and others. As we know, medical imaging technology is very important because it can help medical experts in conducting research on patients. A medical image is known as DICOM. DICOM format is unique, it is not only contain the image of the patient, but it also contains information related to the patient. This paper will show the development of software to retrieve the information from the digital X-ray image (DICOM) by using tag-info method. This software is then tested with a number of patient data and results showed that the software is able to find the information contained in the patient X-ray images accurately.
Find relevant documents for a given query in search process information seems a challenge for all models and techniques embodied in an information retrieval system. We continue this challenge by proposing a new measure of similarity between two queries. This similarity measure is based on two aspects: the first is statistical based on the statistic X2 (method CHIR) and the second is semantic based on the mutual information. Our proposed method shows its efficiency and performance according to tests performed on the standard corpus of research information CACM.
Creating an intelligent search and retrieval system for artwork images, particularly paintings, is crucial for documenting cultural heritage, fostering wider public engagement, and advancing artistic analysis and interpretation. Visual-Semantic Embedding (VSE) networks are deep learning models used for information retrieval, which learn joint representations of textual and visual data, enabling 1) cross-modal search and retrieval tasks, such as image-to-text and text-to-image retrieval; and 2) relation-focused retrieval to capture entity relationships and provide more contextually relevant search results. Although VSE networks have played a significant role in cross-modal information retrieval, their application to painting datasets, such as ArtUK, remains unexplored. This paper introduces BoonArt, a VSE-based cross-modal search engine that allows users to search for images using textual queries, and to obtain textual descriptions along with the corresponding images when using image queries. The performance of BoonArt was evaluated using the ArtUK dataset. Experimental evaluations revealed that BoonArt achieved 97 % Recall@10 for image-to-text retrieval, and 97.4 % Recall@10 for text-to-image Retrieval. By bridging the gap between textual and visual modalities, BoonArt provides a much-improved search performance compared to traditional search engines, such as the one provided by the ArtUK website. BoonArt can be utilised to work with other artwork datasets.
Information Retrieval Models (IRM) that integrate term dependencies are based on the assumption that the retrieval performance of an Information Retrieval System (IRS) usually increases when the relationships among the terms, contained in a given document collection, is used. These models have to deal with two problems. The first is how to obtain a set of relevant dependence relationships efficiently form a document collection. The second problem is how best to use the set of the obtained dependencies to retrieve relevant documents, given a user query. In this work, a new information retrieval model based on Bayesian networks is proposed. Its aim is to achieve a good retrieval performance by restricting the set of dependencies between terms to most relevant ones. In order to achieve this objective, this model searches for dependence relationships within each document in the collection. Then, it creates a final list of dependencies by merging the set of lists obtained locally form each document. Experiments carried out on four standard document collections have proven the efficiency of the proposed model.
A huge volume of images is uploaded to the mobile cloud environment every day with rapid growth of mobile devices. Different from retrieving image using a personal computer, infor-mation retrieval on mobile devices requires higher efficiency due to limitations in data transmission and memory cost. Therefore, an efficient scalable image retrieval model is proposed in this paper which consists of a two-layer bloom filter. The first layer of the proposed bloom filter generated from the asymmetric cyclical hashing (ACH) is used for primary image existence verification. In order to reduce the false positive rate commonly happening in bloom filters, the second layer of the bloom filter based on secure hashing is applied for verification in the second stage. The proposed model realizes approximated nearest neighbor retrieval with limited cost of storage and protects the stored data from illegal access simultaneously. Experimental results show that the proposed model yields significant better retrieval accuracy than other retrieval methods with more than 6 times faster in retrieval time and a smaller space requirement.
Because of the unique attributes of archive information, it is challenging to manage and effectively retrieve archive information in the archive information management practice. This paper designs and develops the first general higher-order Neural Network Model for archives. Based on the analysis of the correlation, the relevance of the weight model, the study of technical methods about the core weight, the direction weight retrieval, and the statistical ranking of the results, this paper designs a corresponding archive information analysis system. Finally, this paper adopts the B/S development model by applying the relevance ranking weight algorithm into the comprehensive archive retrieval activities, which not only enhances the intelligence and efficiency of the archive retrieval, but also can act as a standard example to demonstrate informatization construction for archive management. This paper compares this algorithm with two other existing retrieval algorithms and verifies the practicability of the relevance algorithm by evaluating the algorithm and the default retrieval algorithm using the NDCG evaluation method.
Information Retrieval (IR) is the activity of obtaining information resources relevant to a questioned information. It usually retrieves a set of objects ranked according to the relevancy to the needed fact. In document analysis, information retrieval receives a lot of attention in terms of symbol and word spotting. However, through decades the community mostly focused either on printed or on single writer scenario, where the state-of-the-art results have achieved reasonable performance on the available datasets. Nevertheless, the existing algorithms do not perform accordingly on multiwriter scenario. A graph representing relations between a set of objects is a structure where each node delineates an individual element and the similarity between them is represented as a weight on the connecting edge. In this paper, we explore different analytics of graphs constructed from words or graphical symbols, such as diffusion, shortest path, etc. to improve the performance of information retrieval methods in multiwriter scenario.
In recent years the extreme growth of digital documents brought to light the need for novel approaches and more efficient techniques to improve the precision and the recall of IR systems. In this paper I proposed a novel Similarity Matching Algorithm for Ontology-Based Semantic Information Retrieval Model to measure whether two ontologies are matching or not from the name, the attribute and the theme of the concepts. Simulation shows that for the same recall, the proposed algorithm can increase the precision and flexibility compared with the traditional semantic similarity matching methods.
Words that frequently occur in a document but carry less significant meaning are called stopwords. Identification and removal of stopwords can result in effective indexing of documents. Mean average precision (MAP) is the metric used to measure the efficiency of information retrieval (IR) tasks. In this paper, we have experimented with elimination of Gujarati stopwords to measure the improvements in Adhoc monolingual information retrieval of Gujarati text documents. Results show that elimination of stopwords improve the MAP values of Gujarati IR.
Today's web is a human-readable where information cannot be easily processed by the machine. At the same time, the enormous amount of data has made it increasingly difficult to find, access, present, and maintain relevant information. The present data retrieval techniques are based on the full content coordinating of keywords, which lack the semantic data. In this paper the semantic based information retrieval methodology is proposed to get data from the web archives in a specific domain by gathering the domain relevant information with web crawler. By utilizing ontology and semantic information matched with a given user's query is used for retrieval. The results are discussed for retrieving attractive tourist places in a particular city. Analyses are carried out by comparing results obtained with and without wordnet, with ontology and both ontology and wordnet respectively. The analysis carried out reveals that the proposed system is working efficiently.
The augmented adoption of XML as the standard format for representing a document structure requires the development of tools to retrieve and rank effectively elements of the XML documents. It's known that in information retrieval, considering multiple sources of relevance improves information retrieval. In this work some relevance features are defined and used in a learning to rank approach for XML information retrieval. Our aim is to combine theses features to derive good ranking function and show the impact of each feature in the relevance of XML element. Experiments on a large collection from the XML Information Retrieval evaluation campaign (INEX) showed good performance of the approach.
The ability to access existing patents and patent-related information can have significant impacts on technology development and commercial markets. While such information is now available online, requesting the information about a specific technology remains a nontrivial task. The reason is that the online information often resides in different storage sources, is based on different modeling paradigms, and is managed by different independent entities. Ontology is one modeling paradigm that can capture semantically rich knowledge about a specific domain and its existing patents. That knowledge can be used to facilitate information sharing and information interoperability. This paper focuses on the bio-medical domain, where numerous patents have been filed in recent years. Specifically, this paper describes a computational framework that can facilitate the retrieval of intellectual property (IP-) related information in the bio-medical domain.
Modernization of medical industries experiences numerous challenges. The modernization requires an innovative solution to determine diagnosis of diseases and the best treatment. This solution discovers related diseases to doctors' original diagnosis and quickly reassesses the situation if their diagnosis is incorrect. It also should eliminate unnecessary treatments and testing. It shortens time spent in hospitals for patients. This research, as reported in this paper focuses on creating such a solution in the form of an Intelligent Information Retrieval Lifecycle Architecture Based Clustering Extended Genetic Algorithm Using Service-Oriented Architecture (SOA). The purpose of the solution is to develop concepts and techniques based on the architecture to accelerate processing time of information retrieval at lower cost. The solution provides medical tasks that support strategic decision making and operational business processes. In a SOA environment, the study of this research develops new intelligent concepts of integrating approaches of search methodologies, information retrieval, clustering, genetic algorithm, and intelligent agents. A prototype is created and examined in order to validate the concepts.
In the rapid development of internet technologies, search engines play a vital role in information retrieval. To provide efficient search engine to the user, Link Based Search Engine for information retrieval using K-Means clustering algorithm has been developed. The traditional search engines provide users with a set of non-classified web pages to their request based on its ranking mechanism. In order to satisfy the needs of the user, an improvement to the search engine called Intelligent Cluster Search Engine (ICSE) has been proposed. The improvement of information retrieval process can be divided into two parts such as: comparison of co-occurrence terms and clustering of documents. In this information retrieval, the relevancy of documents is obtained based on the number of occurrences of each co-occurrent term (in links and out links) in a particular web page. The clustering of these relevant documents can be done based on the threshold values assigning to cluster and then the web pages are grouped into that cluster. When the web pages are clustered, a boost up factor is given to a web page based on the relevancy of content from title and summary. The documents can be classified into most relevant, relevant and irrelevant clusters. K-Means clustering algorithm is used to cluster the relevant web pages in order to increase the relevance rate of search results and reduce the computational time of the user.
This paper presents a text retrieval method based on vector space feature conversion. Even though knowledge-based semantic fingerprint information and semantic information obtained by the Tag-LDA topic model are two different representations of semantic features of the same text, they are incompatible with semantic information. Here we introduce the vector space as a bridge to transfer knowledge-based semantic fingerprint information space into the Tag-LDA model space and prove the rationality of the conversion process with correlation theory. The compatible semantic fingerprint information into the Tag-LDA model generate a new topic model STag-LDA. The Stag-LDA model has certain disambiguation effect for the semantic information of the tag. So, it can mine text semantic information more accurately to improve the retrieval efficiency.
The basic aim of this paper is to present a broad picture of Big Data and to show how information can be retrieved using different methodologies and especially using evolutionary computation techniques that help in the information retrieval process in a better way compared to traditional retrieval techniques. Big data is considered as a collection of large data which are heterogeneous in nature and cannot be handled by traditional computing techniques and require more advanced statistical procedures to extract relevant information from a voluminous pool of data. This paper also covers some of the basic information retrieval models which are used and present a basic outline of evolutionary computation techniques.
Personal Information Management (PIM) process consists of four activities; acquisition, organization, maintenance and retrieval. Obviously this process is due to human limitations and expanding scope of personal space of information which has become an exhausting and difficult process. Use of technology and tools related to PIM could facilitate this procedure. This survey study aims to investigate PIM behavior and its four activities among graduate students in the University of Birjand and was performed on the basis of digital environment. Population of this study was all graduate students at the University of Birjand in the 2013–2014 academic year. Self assessment questionnaire was used to collect information. The results show that the overall PEVI and all four activities among graduate students at the University of Birjand were less than estimated desired rate and there were significant differences between PIM in various course studies, also among master's and doctoral students. Computer and laptop was most used tools for storage of personal collection, the most widely used method for organizing based on content and finally the most common method for retrieval of information was based on the explanatory notes.
Due to the rapid growth of information available about individual patients, most physicians suffer from information overload when they review patient information in health information technology systems. In this manuscript, we present a novel hybrid dynamic and multi-collaborative filtering method to improve information retrieval from electronic health records. This method recommends relevant information from electronic health records for physicians during patient visits. It models information search dynamics using a Markov model. It also leverages the key idea of collaborative filtering, originating from Recommender Systems, to prioritize information based on various similarities among physicians, patients and information items We tested this new method using real electronic health record data from the Indiana Network for Patient Care. Our experimental results demonstrated that for 46.7% of test cases, this new method is able to correctly prioritize relevant information among top-5 recommendations that physicians are truly interested in.
A narrative review of the literature on the importance of affective factors in the information retrieval (IR) behaviours of entrepreneurs is presented in this paper. Through the lens of Media Richness Theory, we examine the importance of the richness of information and medium to the success of the IR process. The results show that IR system does not only serve entrepreneurs regarding their information needs but also their emotional needs. The richness of the IR system, referring to the accessibility, is an essential factor for entrepreneurs’ preference and use of the IR medium. This paper contributes to literature by showing how affective attributes are associated with other factors, in addition to their effect on the IR behaviours. The findings reveal that the affective characteristics are both an individual need and a determining component in the process.
Performance evaluation is one of the main research topics in information retrieval. Evaluation metrics are used to quantify various performance aspects of a retrieval method. These metrics assist in identifying the optimum method for a specific retrieval challenge but also to allow its parameters fine-tuning in order to achieve a robust operation for a given set of requirements specification. In this work, we present RETRIEVAL, a Web-based integrated information retrieval performance evaluation platform. It offers a number of metrics that are popular within the scientific community, so as to compose an efficient framework for implementing performance evaluation. We discuss the functionality of RETRIEVAL by citing important aspects such as the data input approaches, the user-level performance metrics parameterization, the evaluation scenarios, the interactive plots, and the performance reports repository that offers both archiving and download functionalities.
With technological progress, how to efficiently process and retrieve large amounts of multi-modal multimedia data has become a challenge. Although multi-modal hash algorithms have been applied in this field, their performance potential has not been fully realized. To fully leverage the performance of the model, a supervised discrete multi-modal hash algorithm has been proposed to improve the efficiency of multi-modal retrieval. The adaptive online multi-modal hash algorithm is used to dynamically adapt to changes in query samples. The topological semantic multi-modal hash algorithm is applied to further improve the retrieval performance of multi-modal hashes. According to the results, in the MIR Flickr dataset, the Mean Average Precision (MAP) reaches 0.8048, 0.8155, and 0.8185 at 32-bit, 64-bit, and 128-bit, respectively. Fusion Graph Convolutional multi-modal Hashing (FGCMH) exhibits the bestin different datasets. From this, the designed method has high processing power in handling large-scale and high-dimensional multimedia data.
Images become more important as carriers of information. But information retrieval and discovery is still mainly based on words and text. In recent years progress is made in exploiting image-based information also. Searching for images using a text query is now already a classical method. Search by image or reverse image search is a more recent method in which the query consists not of text but of an image. This approach allows us to find and reveal exact and modified copies of a known image that is used as source image in the query. Since a few years, this method even makes progress in order to find images on the World Wide Web (WWW), which are not only visually similar, but even semantically related to the query/source image; the same search action can also reveal related text information. All this has been demonstrated in previous tests by this author. A leading developer in this area is the company Google. More recently Google has changed their service for storage of photos by creating the new service named Google Photos; there also, the system applies the improving methods of automatic semantic analysis to the submitted photos. This results in automatic classification/categorization/annotation/tagging of photos, according to their contents. Here a case study of this feature is reported. It turns out that categories are created with high precision, as hoped; however, specificity is only low, as expected. This demonstrates at least that automatic semantic analysis of images is growing in importance. So managers of a digital library that includes images should keep an eye on this evolution in order to maximize the information discovery process of their users.
Searching for new information requires talking to the system. In this research, an Open-domain Conversational information search system has been developed. This system has been implemented using the TREC CAsT 2019 track, which is one of the first attempts to build a framework in this area. According to the user's previous questions, the system firstly completes the question (using the first and the previous question in each turn) and then classifies it (based on the question words). This system extracts the related answers according to the rules of each question. In this research, a simple yet effective method with high performance has been used, which on average, extracts 20% more relevant results than the baseline.
Most content-based image retrieval systems consider either one single query, or multiple queries that include the same object or represent the same semantic information. In this paper, we consider the content-based image retrieval problem for multiple query images corresponding to different image semantics. We propose a novel multiple-query information retrieval algorithm that combines the Pareto front method with efficient manifold ranking. We show that our proposed algorithm outperforms state of the art multiple-query retrieval algorithms on real-world image databases. We attribute this performance improvement to concavity properties of the Pareto fronts, and prove a theoretical result that characterizes the asymptotic concavity of the fronts.
Nowadays Designing of an efficient information retrieval system for multimedia dataset is a big task to understand the trend. Searching for the repeated pattern within a particular genetic sequence has become a much required task in the data mining sectors. Due to the increasing size of text and audio data over internet, various techniques are needed to help with the finding and extraction of very specific information relevant to a user's task or finding the new trends. Matching the unique patterns and generate the rules by using different Pattern Matching, Rule Generation Algorithm. In addition to extracting questionnaires and curiosity based sentences patterns from large database some different implementation of the pattern matching algorithms is proposed. Real world data is large in size like images, speech signals holding high dimensions to represent data. Multiple dimensional data are more critical for detecting and developing the associations among terms. Dimensionality reduction is a technique used for reducing complexity and gives the most frequent item from high dimensional data. It reduces the dimensions of the original input data. Singular value decomposition this dimensionality reduction technique is used for large data reduction on Apache MAHOUT of Hadoop framework. Finally, retrieve and extract the curiosity and questionnaires based unique pattern from large data size using the Map Reduce Framework and compress the result using the SVD technique to give curiosity and questionnaires based subject.
The identification of duplicated and plagiarized passages of text has become an increasingly active area of research. In this paper, we investigate methods for plagiarism detection that aim to identify potential sources of plagiarism from MEDLINE, particularly when the original text has been modified through the replacement of words or phrases. A scalable approach based on Information Retrieval is used to perform candidate document selection-the identification of a subset of potential source documents given a suspicious text-from MEDLINE. Query expansion is performed using the ULMS Metathesaurus to deal with situations in which original documents are obfuscated. Various approaches to Word Sense Disambiguation are investigated to deal with cases where there are multiple Concept Unique Identifiers (CUIs) for a given term. Results using the proposed IR-based approach outperform a state-of-theart baseline based on Kullback-Leibler Distance.
We evaluate the suitability of latent and explicit semantic spaces of documents for Information Retrieval (IR) tasks using a dataset obtained from the Q&A community Stackexchange. In addition, the ability of the latent semantic spaces to reconstruct human relevance judgments is explored. The latent semantic spaces are generated with Latent Dirichlet Allocation (LDA), while explicit semantic spaces are modeled using Explicit Semantic Analyis (ESA). In the first part of the experiment, a series of ad-hoc information retrieval tasks is performed, interpreting closeness in the semantic and explicit spaces as a criterion for relevance. In the second part, it is investigated whether the latent semantic representation allows to infer user defined quality assessments of answers. The findings suggest that the semantic spaces show a correlation between query and relevant information items, however, both algorithms are outperformed by a simple Vector Space Model using TF-IDF. In addition, no significant correlation between the user defined order of relevant answers to a question and the similarity-based order (using closeness in the latent semantic space as similarity function) could be demonstrated.
Compared with the traditional teaching mode, the use of the network teaching platform of document retrieval course has many compelling advantages. The network teaching platform of document retrieval course should be designed to constructivism learning theory as the foundation and basis and implement principles of openness, interactivity, dynamic, and maintainability, and so on. Thus it sets up a “learner centered” main framework with its own characteristics full of Microsoft's technology. The framework of document retrieval course network teaching platform includes the following subsystems, network classroom, communication consultation, exam evaluation, and teaching management. They do their own work and cooperate closely to complete the revolutionary evolution of the document retrieval course teaching mode under network environment.
The exponentially increasing of resources generated and stored in various forms and made available within the internet has posed numerous challenges inside information retrieval (IR) operations and consequently accelerated a myriad of studies in the realm of IR technologies. In the IR system, the large part of the researches is centred on how to satisfy an information requirement from a query to voluminous document sets. In this regard, the importance is on improving the relevance quality of the results (i.e. retrieved documents). The effectiveness of the IR system depends on the efficacy of the respective adopted IR model and strategy. Understanding of the IR models, strategies and their challenges is important in choosing an appropriate and viable strategy toward the development of effective IR systems for a specific/predefined role. This paper surveys the basic IR models, challenges and adopted strategies to enhance the IR systems are also highlighted.
The quality of care can be significantly enhanced and healthcare cost can be substantially reduced if healthcare providers can have access to patient records that are scattered among several paper and electronic based systems. Major challenges of Healthcare Information Exchange result from patient's medical records being kept in several healthcare provider offices, clinics and hospitals in different formats. There are two major problems with healthcare information retrieval. The first problem is lack of visibility and knowledge as to where patient's medical records reside. The second problem is lack of access to information and also incompatibility of data formats. A considerable coverage of Electronic Information Exchange among Electronic Health Record (EHR) systems remains to be implemented despite extensive standardization efforts toward providing solutions. The adoption pace of available standards and solutions has been slow with the exception of some public/government entities. This paper describes a comprehensive and practical solution based on a distributed system with independent subsystems that control and manage processes and data flow of information exchange. The Registrar Subsystem creates a directory of healthcare providers and patients. The Security Subsystem provides authentication and authorization services across all subsystems. The Locators address patient and medical location lookup. The Agents act on behalf of healthcare providers to communicate with other subsystems. The Mediators facilitate information retrieval. The Solution comprises of three levels of participation that allows healthcare providers to join the system easily by starting from basic semi-manual information exchange level and then migrating to a fully electronic and automated information exchange. The Solution is modeled based on variety of standards and protocols used in Internet and other application fields as well as healthcare specific standards and proposals. (Show More)
Research articles in biomedicine domain have increased exponentially, which makes it more and more difficult for biologists to manually capture all the information they need. Information retrieval technologies can help to obtain the users' needed information automatically. However, it is a great challenge to apply these technologies to biomedicine domain directly because of some domain specific characteristics, such as the abundance of terminologies. To enhance the effectiveness of the biomedical information retrieval, we propose a novel framework based on the state-of-the-art information retrieval methods, called learning to rank, which has been proved effective to rank documents based on their relevance degree. In the framework, we attempt to tackle the problem of the abundance of terminologies by constructing ranking models, which focus on not only retrieving the most relevant documents but also diversifying the searching results to increase the completeness of the resulting list for a given query. In the model training, we propose two novel document labeling strategies, and combine several traditional retrieval models as learning features. Besides, we also investigate the usefulness of different learning to rank approaches in our framework. Experimental results on TREC Genomics datasets demonstrate our proposed framework is effective in improving the performance of biomedical information retrieval.
This technical briefing presents the state of the art Text Retrieval and Natural Language Processing techniques used in Software Engineering and discusses their applications in the field.
The most important problem of automated ontology building is intellectualisation, and the related data integration and qualitative information retrieval. In the light of the issues of data science and advanced analytics, the task of automated ontology formation is quite relevant. A common criterion for the quality of an ontology is based on an assessment of the convenience and efficiency of working with it and the connection of cognitive processes with semantic ones for its construction and updating. The main difference of the analysis, the results of which are presented in this paper, is the cognitive information technologies of the subject area, cognitive and semantic analysis based on the theory of categories, mathematical logic and universal algebra, construction of an ontological dictionary, ontological constructions, which was carried out for the first time. The information interaction of the intellectual environment of the object with the subject is determined. For the formation of the concept in the intellectual environment due to the connected information space, the possibility and justification of its use for normalising the concept and its features within the framework of the problematic is determined.
With the increase in multi-media data over the Internet, query by example spoken term detection (QbE-STD) has become important in providing a search mechanism to find spoken queries in spoken audio. Audio search algorithms should be efficient in terms of speed and memory to handle large audio files. In general, approaches derived from the well known dynamic time warping (DTW) algorithm suffer from scalability problems. To overcome such problems, an Information Retrieval-based DTW (IR-DTW) algorithm has been proposed recently. IR-DTW borrows techniques from Information Retrieval community to detect regions which are more likely to contain the spoken query and then uses a standard DTW to obtain exact start and end times. One drawback of the IR-DTW is the time taken for the retrieval of similar reference points for a given query point. In this paper we propose a method to improve the search performance of IR-DTW algorithm using a clustering based technique. The proposed method has shown an estimated speedup of 2400X.
Private information retrieval scheme for coded data storage is considered in this paper. We focus on the case where the size of each data record is large and hence only the download cost (but not the upload cost for transmitting retrieval queries) is of interest. We prove that the tradeoff between storage cost and retrieval/download cost depends on the number of data records in the system. We propose a class of linear storage codes and retrieval schemes, and derive conditions under which our schemes are error-free and private. Tradeoffs between the storage cost and retrieval costs are also obtained.
The adaptation of a Query Expansion (QE) approach for Arabic documents may produce the worst rankings or irrelevant results. Therefore, we have introduced a technique, which is to utilise the Arabic WordNet in the corpus and query expansion level. A Point-wise Mutual Information (PMI) corpus-based measure is used to semantically select synonyms from the WordNet. In addition, Automatic Query Expansion (AQE) and Pseudo Relevance Feedback (PRF) methods were also explored to improve the performance of the Arabic information retrieval (AIR) system. The experimental results of our proposed techniques for AIR shows that the use of Arabic WordNet in the corpus and query level together with AQE, and the adaptation of PMI in the expansion process have successfully reduced the level of ambiguity as these techniques select the most appropriate synonym. It enhanced knowledge discovery by taking care of the relevancy aspect. The techniques also demonstrated an improvement in Mean Average Precision by 49%, with an increase of 7.3% in recall in comparison to the baseline.
Most of the existing user privacy preserving techniques rely on the existing intractability assumption based data privacy concepts that greatly reduces the chances of providing  perfect privacy(i.e., no information about the user interest is revealed over the curious server) to the user. In order to fill this gap, we have presented a perfect privacy preserving single database scheme with non-trivial communication cost using the concept called Private Information Retrieval (PIR). We have successfully overcome the basic requirement of Ω(n) communication for any single database information-theoretic private information retrieval as claimed by Chor et.al [1] by breaking the dependency of both user privacy and data privacy on a single intractability assumption where n is the size of the database. In the proposed scheme, information-theoretic queries are generated by extending the query input domain to ℤ N+1(i.e., all queries can select their inputs from ℤN+1 with identically distributed probability) where N is the RSA composite modulus which is an improvement to the quadratic residuosity based user privacy preserving technique presented in [2] (Note that in [2], query may contain computationally indistinguishable input either from quadratic residue set or from quadratic non-residue set). We have presented a new recursive trapdoor bit based linking function which takes the information-theoretic query and the database and produces a non-trivial communication response. Additionally, we have extended the proposed scheme to a new scheme with reduced communication by using a new compression method.
Cross-modal retrieval has drawn much attention in recent years due to the diversity and the quantity of information data that exploded with the popularity of mobile devices and social media. Extracting relevant information efficiently from large-scale multi-modal data is becoming a crucial problem of information retrieval. Cross-modal retrieval aims to retrieve relevant information across different modalities. In this paper, we highlight key points of recent cross-modal retrieval approaches based on deep-learning, especially in the image-text retrieval context, and classify them into four categories according to different embedding methods. Evaluations of state-of-the-art cross-modal retrieval methods on two benchmark datasets are shown at the end of this paper.
To deal with the characteristics of current information retrieval model for computer crime evidence which is limited to keyword matching, this thesis proposes aninformation retrieval model for computer crime evidence based on domain ontology through an organized method of domain ontology. This model accepts user's evidence retrieval requirements natural language input. By calculating the correlation and similarity between words and ontology concepts, mapping retrieval words to knowledge is achieved. On this basis, the query semantic expansion of user forensics needs is realized further. The experimental results showed that proposed evidence retrieval method obtained better retrieval results overcoming the shortcomings of keyword matching search of the traditional information retrieval method.
In this paper we propose a novel system for automatic assessment of narrative answers using information retrieval algorithms. It is designed to help professors to evaluate the answers that they receive from their students. It is a Java application that communicates through a REST API. This REST API has at its core the Lucene library and exposes all the great functionalities that Lucene has. The application has one UI for the students and one UI for the professor. The student will select the professor, select the question, upload the answer and send it. The professor will evaluate the student answer using the algorithms that will be discussed in this paper. Also in this paper a series of experiments will be presented, and their result will give us a better understanding of the algorithms and have a taste of how they work.
Network science is a relatively new yet widely researched discipline using network paradigm and a graph theoretic approach. This paper investigates web mining techniques as information retrieval methods, as well as network science paradigm to develop an information retrieval and network analysis system for mapping and visualizing the connections between courses, in order to improve the quality of education.
This article studies the process of searching for files of interest in a peer-to-peer network, in an autonomous fashion. The system aims to provide users with the most up-to-date information that is spread over a large network of heterogeneous devices. Our motivation is that in today's world there is a large increase in data so the tools that enable searching for it need to adapt. The advantage of the system overall is that users are notified of updates related to the information they search for whilst accessing a large amount of data through the peer-to-peer network.
Most of the traditional Information Retrieval models are based on the assumption that query terms are independent of each other and a document is represented as a bag of words. Nevertheless this assumption may not hold in practice. In this talk, I will discuss how the query terms associate with each other and how to incorporate the term proximity information into the classical probabilistic IR models. I will discuss the relationship between document length and its relevance and how to balance between the Verbosity and Scope hypotheses by modeling document length within the probabilistic weighting model. I will also present how to incorporate this relationship into the classical BM25 models. Through extensive experiments on standard large-scale TREC Web collections, I will show that the extended models are able to markedly outperform the BM25 baseline and at least comparable to the state-of-the-art model. The talk will conclude with a discussion of novel challenges raised in extending probabilistic Information Retrieval and several applications such as promoting diversity in ranking for biomedical IR, sentiment analysis for predicting sales performance and EMR data analysis for effective health care.
Now-a-days due to increase in the availability of computing facilities, large amount of data in electronic form is been generated. The data generated is to be analyzed in order to maximize the benefit of intelligent decision making. Text categorization is an important and extensively studied problem in machine learning. The basic phases in the text categorization include preprocessing features like removing stop words from documents and applying TF-IDF is used which results into increase efficiency and deletion of irrelevant data from huge dataset. This paper discusses the implication of Information Retrieval system for text-based data using different clustering approaches. Applying TF-IDF algorithm on dataset gives weight for each word which summarized by Weight matrix.
In paper are considered existing algorithms for automatically isolating the bases for a number of natural languages and possible ways of synthesizing a normal form of a word for the Kazakh language. In paper are described the complete system of endings of the Kazakh language. The paper proposes a new approach to constructing a lexicon-free stemming algorithm for the Kazakh language on the basis of a complete set of endings of the Kazakh language. The stemming algorithm will be used for Kazakh language information retrieval to finding specific words in the documents by stemming base of word.
Currently keyword search is a prominent data retrieval method for the Web because the simple and efficient nature of the keyword processing allows it to process a large amount of information with fast response. However, keyword search approaches do not formally capture the clear meaning of a keyword query and fail to address the semantic relationships between keywords. As a result, its recall rate and precision rate are often unsatisfactory, and therefore its ranking algorithms fail to properly reflect the semantic relevance of keywords. This means that the accuracy (the precision and the recall rate) of search results is often low. Therefore, we proposed a new 3-tuple query interface and corresponding ranking algorithm. And we presented a comparison of the search accuracy of our new query interface and conventional search approaches.
The fundamental limits of private information retrieval (PIR) with unknown cache prefetching at the user are investigated in this paper. To this end, a novel random linear combination (RLC)-based PIR scheme that can solve the basic PIR problem and its variation is proposed. The proposed scheme is based on random coding approach and achieves the capacity of the basic PIR asymptotically. Then, we investigate PIR with unknown cache prefetching (PIRC) problem at different cache-to-file size ratio. Specifically, we propose RLC-based PIRC method, which prefetches RLC-based side information and leverages them to retrieve desired information at small download cost. Furthermore, by applying time and memory sharing on the proposed RLC-based PIRC, RLC-based basic PIR and some other known approach in literature, we derive the achievable normalized download cost bound of PIRC. The derived achievable bound outperforms the existing bound in literature and the case study provides numerical results that verifies it.
This research investigates information retrieval from distributed databases. The most popular example of such systems are the regular web search engines. Thus, this paper discusses in depth the search engines systems and have made an evaluation of different search engines based on popular queries searched. We first explores the structure of the search engines and explain its functions behind the scenes, how they can get the results for a specific query. The key idea behind search engines is the ranking algorithms used to bring the appropriate results. This research has chosen Google, Yahoo, and Bing to do evaluation experiments, since they are top search engines used nowadays.
The amount of cybersecurity-related information is extraordinarily increasing, given the fast-growing number of cybersecurity attacks and the significant influence brought by them. How to efficiently obtain and precisely understand the relevant knowledge in the sea of information on cybersecurity becomes a challenge. In this article, we propose an innovative cybersecurity retrieval scheme that supports automatic indexing and searching of cybersecurity information based on semantic contents and hidden metadata. The proposed scheme leverages a customized neural model that incorporates new linguistic features and word embedding by identifying the entities related to cybersecurity incidents from the text. We implement a novel cybersecurity search engine to demonstrate effective, understandable and pragmatic cybersecurity information retrieval based on the proposed schema. Comprehensive performance evaluation over real-world datasets has been conducted to validate the new algorithms and techniques developed for cybersecurity information retrieval. The new engine makes it possible to conduct augmented search, cybersecurity analytics, and visualization, with the ultimate goal of providing direct and efficient results to help people obtain and truly understand cybersecurity information.
Recently, linking Machine Learning (ML) to the new trend of Automated Machine Learning (AutoML) is a vital step towards improving the performance of information systems. This research comprehensively explores this new trend, which promotes innovation in ML technologies, facilitates computing, and improves data understanding and utilization. In addition, the study investigates the significant impact of AutoML in the field of Information Systems Management. This paper also discusses the benefits and applications of AutoML, focusing on how these applications contribute to tasks such as automatic data processing, improving data quality, simplifying data integration, and facilitating information retrieval. Moreover, addressing ethical issues, ensuring data security, protecting privacy, validating models are important tasks that arise when using AutoML, which need to be focused on. Looking to the future, we are taking full advantage of AutoML capabilities to innovate information and systems management, make super discoveries, and make more accurate predictions. The purpose of this study is to show the importance, possibilities, and prospects of using AutoML in various information systems.
Based on multi data fusion, this paper introduces a retrieval model for audio frequency data, which is combined with big data and applied to information retrieval. Firstly, the model is pre trained, and then the network parameters are fine-tuned through the network training method. By adjusting the network parameters, the recognition ability of the model is improved. In view of the assumption that whether external factors can interfere with external dependent variables on dependent variables, the multi group structure model is used for analysis. This method is a general method that can quickly find the target data under massive data. In the experiment, the features are used as the key words of the inverted table index to establish the table, and all the file information in the reference database will be stored in the corresponding entries in the inverted table according to its underlying features. When the feature of each query file finds data in the table, it only needs to use its eigenvalue to calculate the complexity once, and the information of the reference data with this feature can be searched.
The information retrieval experience has a significant impact on the use of university library database resources. In order to improve the efficiency of users' information retrieval in the database, a controlled experimental method is adopted to explore the effects of form controls and user types on the performance, cognitive load, and satisfaction of university library digital resource information retrieval tasks, and provide design optimization suggestions. The study found that form controls have a significant impact on cognitive load, task performance, and subjective evaluation, and there is an interactive effect between user characteristics and form controls on cognitive load and task performance. Novice users have the highest retrieval efficiency when using drop-down list boxes, and experienced users are more suitable for using combo boxes.
The Medical Coordination, which is the application of logistics techniques to emergency context, is responsible for providing appropriate resources, in appropriate conditions to appropriate patients. A system for medical coordination of emergency requests was developed in 2009, although some activities related to medical coordination decision making are extremely subjective. Aiming to decrease subjectivity on activities like prioritization of requests and coordination flow, new technologies of decision support were incorporated to that system. These technologies include textual and semantic processing of clinical summaries and machine learning tools. Results indicate that automated tools could support decision on medical coordination process, allowing coordinators to focus attention on critical cases. These features may streamline the medical coordination, avoiding mistakes and increasing the chances of saving lives.
At present, the progress of Internet technology has made the music recommendation system develop rapidly, and the online music platform has gradually become the first choice for people to listen to music. Then in the field of music analysis, this text conducts in-depth research on similarity calculation methods for music analysis models of various data types, in order to draw more accurate conclusions. According to the method framework and research methods proposed in this text, we have successfully constructed a music data analysis, retrieval and storage platform, which provides comprehensive services for music analysis and retrieval. Through this platform, the information or resources that users are interested in can be effectively found from massive data, and the relevant music resources that users need can be quickly and accurately located. The score dataset are 0.69 and 0.62 respectively. Compared with existing platforms, the method proposed in this text shows remarkable effectiveness and practicality in practice, which is fully proved.
Data are everywhere. Often the sheer quantities of data that is stored or archived in repositories or digital libraries make it difficult to navigate data; information in the data is obscured, particularly where we have Big Data. Traditionally, we record metadata on our data items to assist data classification and some information retrieval. The Semantic Web enables us to further unlock and enrich our data, by exploring how different data are related or connected. Using ontologies and Linked Data we can declare, navigate and discover semantic relationships. Relationships exist both locally, within the data, and globally, such that we can enhance our data with information retrieved from a wider context. To illustrate how the application of Semantic Web technologies aids data discovery and information retrieval, I discuss two case studies: (1) Sharing Ancient Wisdoms (SAWS), a Dynamic Library of information on selected ancient wisdom literature; and (2) the DEFRA DTC archive, a repository of data about freshwater quality in the UK.
Geospatial web service of agricultural information has a wide variety of consumers. An operational agricultural service will receive considerable requests and process a huge amount of datasets each day. To ensure the service quality, many strategies have to be taken during developing and deploying agricultural information services. This paper presents a set of methods to build robust geospatial web service for agricultural information extraction and sharing. The service is designed to serve the public and handle heavy-load requests for a long-lasting term with least maintenance. We have developed a web service to validate our approach. The service is used to serve more than 10 TB data product of agricultural drought. The performance is tested. The result shows that the service has an excellent response time and the use of system resources is stable. We have plugged the service into an operational system for global drought monitoring. The statistics and feedbacks show our approach is feasible and efficient in operational web systems.
Traditional knowledge graph retrieval techniques ignore node relationship weights, making it difficult to achieve targeted retrieval. Therefore, a nonlinear model AGCN-WGAN is proposed to solve retrieval tasks in relational networks, fully utilizing the advantages of Graph Convolutional Networks (GCN) and Generative Adversarial Networks (GAN). Firstly, AGCN is used to capture the local topological features of a single node; In addition, the use of GAN enhances the ability of AGCN models to generate reasonable weight distribution maps, effectively extracting correlations between nodes, thereby improving the performance of the model in handling large-scale data retrieval tasks. In order to verify the effectiveness of the method, the dispatching operation data in a real business scenario of a city power grid is used for experiments. The experimental results show that the proposed data retrieval method has significantly improved accuracy compared to existing methods.
Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. Learning to rank is useful for document retrieval, it uses feature vector to rank, but there is no feature about document topic. Our paper combines LDA and learning to rank, adds a topic feature into the feature vector of learning to rank algorithm. And the experiment results show that the topic feature has improved the rank result effectively.
Information retrieval (IR) is an essential aspect of modern-day generation, especially with the fast increase and expansion of the net and its related technologies. IR aims to broaden systems that can correctly and appropriately retrieve applicable statistics from considerable data, assisting customers in their search and records accumulating wishes. AI has performed a tremendous function in advancing IR techniques, making systems more innovative and efficient in retrieving data. However, with the ever-growing extent of facts on the net and the want for real-time admission to statistics, there may be a growing desire for side computing in IR systems. Area computing refers to the processing and garage of records at the brink of the community instead of sending them to a critical area for processing. This method gives numerous advantages, including reduced latency, stepped-forward information privacy and safety, and efficient use of community bandwidth. In recent years, researchers have explored superior side strategies in AI to broaden high-overall performance IR systems. These techniques include aspect caching, facet gadget mastering, and edge-based statistics filtering, Part caching entails storing frequently accessed data at the community's edge, reducing the need to retrieve information from a critical server. This technique can improve IR systems' performance by reducing latency and community congestion.
We study the symmetric private information retrieval (SPIR) problem under arbitrary collusion and eavesdropping patterns for replicated databases. We find its capacity, which is the same as the capacity of the original SPIR problem with the number of serversN replaced by a number F∗ . The number F∗ is the optimal solution to a linear programming problem, and it is a function of the joint pattern, which is the union of the collusion and eavesdropping pattern. This is the first result that shows how two arbitrary patterns collectively affect the capacity of the SPIR problem. We draw the conclusion that for SPIR problems, the collusion and eavesdropping constraints are interchangeable in terms of capacity, i.e., the two patterns play the same role in the SPIR problem and the capacity remains unchanged if we exchange the colluding and eavesdropping patterns. As corollaries of our result, the capacity of the SPIR problem under arbitrary collusion patterns, and the capacity of the PIR problem where each colluding set is included in some eavesdropping set, are also found. Some extensions with restrictions to finite message lengths are provided, and in this case, upper and lower bounds on the capacity are given. The lower bound is described with a solution to an integer linear programming problem.
The current popular Sina microblog application has better support for text retrieval, but limited support for image retrieval. We try to build a cross-modal data set based on the characteristics of Sina microblog data, design a cross-modal retrieval method aiming at luxury official microblog, and implement cross-modal luxury microblog retrieval. In response to the lack of image retrieval on Weibo, a cross-modal graphic information retrieval is constructed in this article, and a cross-modal retrieval and identification method for luxury microblogs based on deep learning is proposed. The retrieval model in this article is composed of a convolutional neural network and a TF-IDF model. The convolutional neural network is used to extract the features of Weibo pictures, and the similarity measurement method is used in the high-level semantic space to perform feature matching to achieve cross-modal retrieval. This research has important theoretical guiding significance for improving the design of microblog multi-modal data retrieval, and has high engineering application value for the design, realization and improvement of future Sina Weibo retrieval methods.
The importance of knowledge organization and information retrieval techniques has been evident throughout human history, becoming even more crucial in the digital age. While computer systems and the web have facilitated information retrieval, challenges arose with the increasing volume of data. The introduction of Semantic Web technologies aimed to enhance precision and accuracy by converting the web into a structured data format. The Cultural Heritage (CH) community has been at the forefront of adopting Semantic Web practices to promote interoperability and shared understanding. In this study, we present a comprehensive conceptual framework that spans cultural heritage, information modeling, and information retrieval. Our model addresses early solutions in knowledge organization systems, highlighting the evolution from classification systems and controlled vocabularies to the significance of metadata schemas. We delve into the limitations of traditional knowledge organization systems and the necessity of formal ontologies, particularly in the cultural heritage domain. The comparative analysis of CRM vs. EDM, ontology-based metadata interoperability, and ontology technologies elucidate our contributions to the field. This paper outlines the process from the initial steps of adopting Semantic Web technologies in the CH domain to the latest developments in CH information retrieval. In this paper, we also reviewed intelligent applications and services developed in the CH domain after establishing semantic data models and Knowledge Organization Systems. Finally, challenges and possible future research directions are discussed. The findings revealed that GLAMs (Galleries, Libraries, Archives, and Museums) are excellent and comprehensive sources of CH information. The CH community has put in a lot of time and effort to develop data models and knowledge organization tools; now it’s time to use this valuable resource to construct smart applications that are still in their e... (Show More)
Web-based archive retrieval system (ARS) is gaining its importance for the convenient access, and how to have it effective is worth paying attentions. The study aims at presenting and analyzing critical viewpoints during the planning and implementing processes of a web-based historical ARS. In-depth interviews were conducted with librarians in charge of the Digital Archives of the Diplomatic and Economic Records in Modern History Project, and with IT personnel of the Computer Center of Academia Sinica participating in the construction of the ARS. A comparative study on viewpoints among archivists, IT personnel and the requirements of users has been conducted to identify relevant content of a web-based ARS, including key players' requirements and concerns, system functions, and system characteristics. By the research findings, communications between users and system developers are improved, and agreements can be made for future web-based ARS construction reference.
By the advent of new information resources, search engines have encountered a new challenge since they have been obliged to store a large amount of text materials. This is even more drastic for small-sized companies which are suffering from a lack of hardware resources and limited budgets. In such a circumstance, reducing index size is of paramount importance as it is to maintain the accuracy of retrieval. One of the primary ways to reduce the index size in text processing systems is to remove stop-words, frequently occurring terms which do not contribute to the information content of documents. Even though there are manually built stop-word lists almost for all languages in the world, stop-word lists are domain-specific; in other words, a term which is a stop-word in a specific domain may play an indispensable role in another one. This paper proposes an aggregated method for automatically building stop-word lists for Persian information retrieval systems. Using part of speech tagging and analyzing statistical features of terms, the proposed method tries to enhance the accuracy of retrieval and minimize potential side effects of removing informative terms. The experiment results show that the proposed approach enhances the average precision, decreases the index storage size, and improves the overall response time.
Non-technical loss (NTL) detection is a persistent challenge for Distribution System Operators. Data-driven solutions have been widely used nowadays to analyze customers’ energy consumption and to identify suspicious fraud patterns for a posterior on-field inspection. However, the usage of such techniques, in particular the current deep learning methods, is not trivial and requires special attention to tackle imbalanced-class and overfitting issues. In this paper, we propose a new non-technical loss detection framework, which combines the effectiveness of convolutional neural network feature extractors with the efficiency of the Information Retrieval paradigm. In our solution, state-of-the-art pre-trained convolution neural networks (CNNs) extract deep features from electricity consumption time series represented as images. Next, these deep features are encoded into textual signatures and indexed using off-the-shelf solutions for posterior fraud searching. With this framework, the user can search for a specific fraud pattern in the utility database without having to train any classifier. The experiments performed in a real dataset provided by CPFL Energia, one of the largest electric utilities in Brazil, presented promising results both in terms of effectiveness and efficiency for the detection of fraudulent customers. In the conducted comparative study, we evaluate different time series image representations and CNN feature extraction approaches with regard to NTL detection results. Experimental results demonstrate that the combination of the Recurrence Plot image representation with the VGG16 CNN presented the best performance in terms of both effectiveness and efficiency.
Current search engines like Google, Yahoo give the search results, indeed users are facing problems in information retrieval. The main problem is because of word mismatch and availability of many resources. Petabytes of information is available because of Internet. From that huge available data, for a naïve user it becomes hectic to distinguish between relevant and irrelevant information to individual interest. Also, another reason of getting irrelevant information is incompatibility between terms that users are using and keywords present in documents. Query expansion is an adding keyword to the original query. The main issue in query expansion is selection of appropriate terms from user's original query. Vocabulary database helps to solve this issue. Identification of the similar words and language entities that are similar in meaning is done by vocabulary which is frequently incorporated in information retrieval system. Thesaurus has been used across a large area of in information retrieval also many applications and natural language processing. In this work, to improve performance of a search query, BM25 model is used for query expansion. Cosine similarity is used to determine similarity between two keywords. Rocchio algorithm is used to calculate the relevance feedback. Experimental result shows better results using Rocchio algorithm.
Traditional text retrieval techniques greatly consume system resources. Although some file-sharing software realizes file positioning and high-speed downloads, they have no enough capacity to analysis variety format Chinese documents and to extract keywords. At the same time, during the operation of system, it exist hot issues in network routing. This paper proposes an intelligent distributed text retrieval system, which adopts two routing strategies based on the index sub-node and peer nodes and can effectively solve these problems, in the same also can improve the efficiency of finding specific information from the massive data.
Being such a vast resource of information, World Wide Web has become irreplaceable but the outburst of information available over the internet has made web search a time consuming and a very complex process. Now days if someone need to retrieve any information using internet they come across huge number of web pages because of large amount of data on web. It becomes difficult task to find information so in order to retrieve meaningful and intelligent information we have many information retrieval (IR) techniques. To gather the significant information from such a vast available resource Information Retrieval (IR); a method of retrieving such information resources which are relevant to an information need is applied. This paper would review few of these methods for intelligent information retrieval systems on web based on the Ontology information retrieval techniques.
The application of Information Retrieval (IR) techniques to software traceability link recovery has been the focus of many studies. These studies have formulated the task of establishing valid trace links between two types of software-artifacts as a retrieval problem, where one type of artifacts is selected as the set of queries and the other as the corpus. Previous work selected the sets of queries and corpus artifacts for a study up front, therefore pre-imposing a retrieval direction for finding all trace links. This decision was usually made based on intuition or previous work. We argue that the choice of the query and corpus sets (i.e., retrieval direction) can significantly impact the results of IR-based traceability link recovery and should be made with context in mind, as the best choice may be dependent on the properties of each dataset. More than that, we argue thateven within the same system, different traceability links maybe best recovered by using different retrieval directions. In this paper we provide the first evidence to support these claims, showing that retrieval direction can have a significant impact on IR performance for traceability link recovery at both the project and individual link level. Moreover, we propose futurere search directions aimed at predicting the most efficient retrieval direction, as well as approaches leveraging information from both retrieval directions simultaneously.
This article describes the process of Question-Answering Systems Development Based on Big Data Analysis. The methods and tools of the developed product, namely Naive Bayer Classifier, Linear Support Vector Machin, Logistic Regression Natural Language Processing, Information Retrieval and Python, are also proved, and why we chose them is substantiated. In the paper, experiments were performed. And a model was built, and in the next section, its work was shown on a control example. A comparative analysis of the construction and speed of all models was conducted.
In the modern era, their exists a need for efficient Information Retrieval (IR), from large number of documents; consequently, leading towards automation. In the past decade, we have observed an exponential increase in data influx, particularly in the case of unstructured data, which includes images, videos, and textual documents. When textual data sources are taken into consideration, like in the case of resumes, there is no standard format, and hence, are liable to subjective experience. On the other hand, current automated information extraction techniques assume a standard format for documents. Previous researchers have employed Rule-based methods, supervised methods and semantics-based methods to extract entities from the resumes. Though these methods heavily depend on large amounts of data, that is usually in an unstructured format. Furthermore, these techniques are time-intensive and are prone to some limitations. Our study includes the selection of a two-step hybrid Information Retrieval methodology. Sequentially it can be broken down into text block classification which employs Boolean Naive Bayes with Laplcian smoothing and a tri-gram approach followed by Entity recognition using BERT-cased. Our approach had an Average F1 Score of 0.80 for Text Block Classification and an average F1 score of 0.52 for Named Entity Recognition.
During the last decade, the demand for content-based retrieval among the mass information continues increasing. Unfortunately, the traditional key words matching method has been useless. Therefore, how to retrieve the expected results in semantic level has become a tough challenge for information retrieval. Theoretically, many methods have been proposed to solve it, among which latent semantic analysis (LSA) model is the most famous one. However, while conventional LSA methods, for example singular value decomposition (SVD), do make advances in this issue, they consume large memory and generate elements with negative value in some dimensions, which is meaningless. With such consideration, in this paper, we propose a content-based retrieval model using non-negative matrix factorization method, which generates a latent semantic space representation with non-negative parts as a low rank approximation to term-document matrix. Experimental results show that our approach gets a good performance in content-based retrieval.
Term proximity statistic, which consists of rewarding documents where the matched query terms occur in close proximity, has proved its effectiveness in document retrieval performance. However, this field of research remains unexplored for Arabic information retrieval (IR) despite of the non diacritical text and the rich morphology of Arabic language which complicate the retrieval process. In this paper, we propose to boost the Arabic information retrieval performance by using proximity information. Our aim is to evaluate proximity features for Arabic language in order to go beyond the bag-of-words, and to overcome the problems related to text preprocessing. We investigate several state-of-the-art proximity models, including the Cross-Term model (CRTER), the Markov Random Field model (MRF), the divergence from randomness (DFR) multinomial model, and the Positional Language Model (PLM). For preprocessing purposes, Khoja and light stemming algorithms have been used. Experiments are performed on the Arabic TREC-2001/2002 collection using Terrier IR platform. The obtained results show significant improvements by using proximity based-models for Arabic IR.
This paper presents our contribution to enhance literature-based discovery with information retrieval techniques. We propose the joint use of a flexible Information Retrieval model and MeSH Concepts for knowledge discovery in biomedical literature. The Information Retrieval model contributes to filter MEDLINE biomedical literature to the most relevant documents. Utilizing MeSH concepts allows to quickly identifying candidate concepts that could potentially validate a hypothesis. We have tested our approach by replicating the Swanson's first discovery on fish oil and Raynaud's disease correlation. The obtained results show the effectiveness of our approach.
The user is a basic component in the whole information management process and for this reason s/he has to be taken into account in the design and implementation of frameworks and systems for information retrieval. For this reason, the problem of defining efficient techniques for knowledge representation in a user's prospective is becoming a challenging topic in both academic and industrial community. Moreover, the large amount of available data induces several problems from different points of view and it is stressed in the bigdata vision. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of user-based information retrieval applications. In this paper we propose the use of a semantic approach to design the structure of a multimedia BigData. In addition, the recognition of multimodal features to represent concepts and its attributes together with linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. Information about users (i.e. position) is also used to recognize his/her behaviour and improve the quality of the information retrieval process. Our framework has been implemented using a NoSQL graphdb populated from very large knowledge sources and mobile technologies. Extended experiments are presented to show the effectiveness of our approach.
Every corporation’s day-to-day activities entail dealing with a vast array of diverse data formats, such as work orders, techno’s, maintenance papers, and so on, many of which are selectable or scanned PDFs. These tasks demand several hours of human labor to extract the necessary data from these papers for further processing, and analysis incurring significant financial toll to these corporations. As a result, there is enormous potential for the creation of a digital solution that enables sophisticated OCR implementation, leading to the automation of the entire information extraction process. This paper provides a thorough examination of information extraction process focusing to deliver a high-quality complete functional solution and suggests a solution that incorporates critical preprocessing required for accurate information extraction and makes use of the capabilities of Faster R-CNN for document layout analysis as well as a range of approaches for efficient data extraction depending on data type. The multistage document analysis and information extraction tool also provides options for template definition enabling their reusability for batch processing large amounts of unstructured data.
Physicians often seek scientific evidence regarding how to best care for their patients, while making clinical decisions These scientific evidence are available in the form of published medical articles, reports and clinical trials. Considering the volume of already existing medical literature and the pace at which medical research is growing, getting the most relevant information will be a tedious task. In this paper, we describe an empirical approach to fetch relevant medical articles from the PubMed (about 733,328 articles of 45.2 gigabytes) collection, based on a given query. Our IR system comprises of three parts: inverted indexing using Lucene, lexical query expansion to increase recall with MetaMap and reranking aimed at optimizing the system. Word sense of ambiguous terms are introduced to limit the negative effects that synonymy-based query expansion may have on precision. The subsequent ranked list was then re-ranked with learning to rank algorithms. We evaluated our system using 30 medical queries and the results show that our system can handle various medical queries effectively and efficiently. Also, the final results demonstrate that the ensemble approach performs better than the Lucene baseline by boosting the ranking of articles that are near the top of several single ranked lists.
Cross Language Information Retrieval (CLIR) is a sub domain of Information Retrieval. It deals with retrieval of information in a specified language that is different from the language of user's query. In this paper, an improved English-Hindi based CLIR is proposed. There are various un-noticed domains in this broad research area that are required to be worked upon in order to improve the performance of an English-Hindi based CLIR. Not much research effort has been put up to improve the searching and ranking aspects of CLIR systems, especially in case of English-Hindi based CLIR. This paper focuses on applying algorithms like Naïve Bayes and particle swarm optimization in order to improve ranking and searching aspects of a CLIR system. We matched terms contained in documents to the query terms in same sequence as present in the search query to make our system more efficient. Along with this our approach also makes use of bilingual English-Hindi translator for query conversion in Hindi language. Further, we use Hindi query extension and synonym generation which helps in retrieving more relevant results in an English-Hindi based CLIR as compared to existing one. Both of these techniques applied to this improved approach gives user a change to choose more appropriate Hindi query than just by using the single translated query and hence improving overall performance.
Personalized Information Retrieval (PIR) exploits the user's data in order to refine the retrieval, like for instance when users with different backgrounds may express different information needs with the same query. However, this additional source of information is not supported by the classical Information Retrieval (IR) process. In order to overcome this limit, we propose to generate the user profile out from his profile and social data. Then, we introduce several Personalized Information Retrieval models which integrate this profile at the querying step, allowing to personalize the search results. We study several combinations of the initial user's query with his profile. Furthermore, we present a PIR test collection that we built from the social bookmarking network Delicious, in order to evaluate our PIR models. Our experiments showed that the PIR models improve the retrieval results.
Stemming and Lemmatization are two significant natural language processing techniques extensively used in Information Retrieval for query processing and Machine Translation for reducing the data sparseness. Most of the existing Stemmers and Lemmatizers are based on some language dependent rules which require the supervision of a language expert. Some probabilistic approach needs vast amounts of monolingual corpus. Both Stemming and Lemmatization minimize inflectional structure, and sometimes derivationally related structure of a word to a common base form. This paper proposes an unsupervised stemming which is hybridized with partial lemmatization for four morphologically different languages such as English, French, Tamil and Hindi. An innovative attempt is being made to develop a stemming algorithm for a novel conflation method that exploits the quality of words and uses some standard Natural Language Processing tools like Levenshtein Distance and Longest Common Subsequence for Stemming process. This approach can even support other Indian and Non-Indian languages.
This paper is to gain an overall view of the current research status of semantic retrieval systems based on knowledge graph, and construct such a system of rice field. The authors selected several well-known semantic retrieval models of various industries, such as Google, Bing, Elsevier, Baidu ZhiXin, etc., and comprehensively analyzed them from the aspects of entity type, field, entity source, retrieval mode, and information presentation, and then proposed a semantic retrieval system on basis of ontology, knowledge graph and nature language processing with multi-source of rice. The survey result shows that semantic retrieval is stepping toward the multi-field and instantiation stably, and the future research is concerned with semantization depth and multi-field knowledge graph. The proposed system can provide services of entity and knowledge retrieval, visual navigation and analysis, which meet users' retrieval needs in semantic to a certain extent.
Traditional geospatial data retrieval is data-driven and system-oriented, but which suffers from the problem that users are not provided with personalized results. In order to improve the quality of geospatial data retrieval, the idea of user-oriented geospatial data retrieval is introduced that is demand-driven. Consequently, the user-oriented geospatial data retrieval is divided into GRoED and GRoFD. Then, a user-oriented approach to the geospatial data retrieval is further proposed. For GRoED, after reordering the candidates by user profiles from the standard retrieval, personalized results that reflect the importance of the candidates for users are available accordingly. For GRoFD, the combination of retrieval conditions and user profiles are employed to express the user's information demands, and as a result personalized retrieval results are achieved.
Aiming at the traditional image retrieval methods, the image retrieval method based on genetic algorithm and rough set is used to get the simplest visual feature subset of the image. As a machine learning algorithm, genetic algorithm has many advantages compared with other machine learning algorithms because of its inspiration from biological evolution. In order to make the image retrieval algorithm based on multi-features have better retrieval effect, this paper proposes an adaptive image retrieval algorithm based on genetic algorithm. Focusing on the adaptive image retrieval model based on genetic algorithm, based on the research of traditional image retrieval algorithm based on color moment, this paper puts forward a block weighted color moment algorithm to express the color features of images. In order to change the traditional way of finding the best weight by giving the weight manually, and make the algorithm more intelligent, this paper puts forward a genetic usurpation method to get the best solution of the weight and assign the weight automatically. An image retrieval system with human-computer interaction characteristics and easy operation is designed and implemented. This system can verify the performance of the proposed and improved algorithm and the traditional algorithm. Through statistical analysis of experimental data, the feasibility of this algorithm is proved.
Requirements traceability supports many software engineering activities such as change impact analysis and requirements validation, providing benefits to the overall quality of software systems. Factors such as lack of communication, time pressure problems, and unsuccessfully implemented traceability practices result in developers losing track of requirements. Requirements traceability is a primary means to address completeness and accuracy of requirements. It is an active research topic for software engineers. Textual analysis and information retrieval techniques have been applied to the requirements traceability recovery problem for many years, due to the textual components of requirements and source code. Information retrieval techniques are semiautomatic techniques for recovering traceability links and on occasion, they have become the baseline for automatic methods applied to requirements traceability recovery. We evaluate the performance of IR techniques applied to the requirement traceability recovery process. The most popular information retrieval techniques applied to the requirements traceability recovery problem are the IR Probabilistic, Vector Space Model, and Latent Semantic Index approach. All three approaches rank documents by using one of the documents for extracting queries and the other as the documents being search using those extracted queries; however, they apply different internal logics for establishing similarities. We compared IR Probabilistic, Vector Space Model, and Latent Semantic Index approaches to evaluate their performance for requirement traceability recovery using the metrics of precision and recall. Experimental results indicate a low precision and recall for the LSI technique and high precision and low recall for both the IR probabilistic and the VSM techniques.
Effective information capture and retrieval is crucial for organizations to meet their business needs and remain competitive in today's fast-paced digital world. Traditional information management systems, while useful, are often limited in their ability to handle large amounts of data and provide accurate and relevant results. This paper proposes the use of machine learning-based systems to enhance the process of information capture and retrieval. These systems leverage the power of artificial intelligence to automatically learn from data patterns and improve their performance over time. This approach has shown promising results in various industries, such as finance, healthcare, and e-commerce. Through the utilization of machine learning algorithms, organizations can achieve more efficient and accurate information capture and retrieval, leading to better decision-making and improved business outcomes. The benefits and challenges associated with using machine learning-based systems for information management are discussed, along with potential future research directions in this field. Overall, this paper highlights the importance of utilizing machine learning-based systems for effective information capture and retrieval and the potential impact they can have on organizational success.
A branch of Distributed Information Retrieval(DIR) is Peer-to-Peer Information Retrieval (P2PIR). Peer-to-Peer (P2P) networks are a recently evolved paradigm for distributed computing and many researchers are developing new algorithms for such systems. Evaluate algorithms performance, in a P2P network, is already a challenging task caused by the lack of realistic test beds. The construction of standard test beds to evaluate search approaches and to compare them is a major open problem. Indeed, the poverty of documents and queries representation, in test bed, limits retrieval opportunities. This study addresses the lack of an adequate test bed that can be used to evaluate peer-to-peer information retrieval approaches. In particular, we propose, in this paper, a test bed based on an already existing test bedand on a semantic resource enrichment.
In any larger engineering setting, there is a huge number of documents that engineers and others need to use and be aware of in their daily work. To improve the handling of this amount of documents, we propose to view it under the angle of a new domain for professional search, thus incorporating search engine knowledge into the process. We examine the use of Information Retrieval (IR), Recommender Systems (RecSys), and Knowledge Management (KM) methods in the engineering domain of Knowledge-based Engineering (KBE). The KBE goal is to capture and reuse knowledge in product and process engineering with a systematic method. Based on previous work in professional search and enterprise search, we explore a combination of methods and aim to identify key issues in their application to KBE. We list detected challenges, discuss information needs and search tasks, then focus on issues to solve for a successful integration of the IR and KBE domain and give a system overview of our approach to build a search and recommendation tool to improve the daily information-seeking workflow of engineers in knowledge-intense disciplines. Our work contributes to bridging the gap between Information Retrieval and engineering support systems.
Voice enabled Information Desk, to empower the rural Indian farmers with latest and most upgraded information. This paper will discuss how Information Desk can explain/guide/respond features of government scheme and other information in a native/regional language of the user. Interaction between a user and an Information Desk is done by utilizing a voice command generated by the user. User friendly interaction between the user and the voice enabled Information Desk. Information Desk will provide the user with advices / suggestions in regional language. Experimental results for accuracy and error is 89.58% and 10.42% respectively.
In information retrieval, documents are usually retrieved using lexical matching which matches where words in a user's query with words found in a set of documents. A significant model used in information retrieval is the vector space model where these words are represented as a vector in space and are assigned weights using a favorite weighting technique called TFIDF (Term Frequency Inverse Document Frequency). In this thesis, we have devised three new weighting techniques to improve the TFIDF weighting technique. The first technique is Dispersed Words Weight Augmentation (DWWA) which gives more weight to the words distributed in most of the document's paragraphs; we consider that those words are more significant than words found in few paragraphs. The second technique is called Title Weight Augmentation (TWA) which gives more weight to the words found in the document's title and first paragraphs. The third technique is called First Ranked Words Weight Augmentation (FRWWA) which increments further the weight of the most frequent words in a document. We tested the three techniques, and we found more relevant documents were retrieved in our system.
The paper presented here intends to demonstrate the usage of Organizational Multi Agent Software Engineering Methodology now on referred to as O-MaSE for the purpose of analyzing, modeling and designing custom intelligent agents for an organization specifically relevant for context aware systems. The paper discusses Genomic Information Retrieval. O-MaSE is an extension over MaSE and is a more comprehensive agent development methodology as compared to existing methodologies. The biologists generate a big amount of unprocessed data in sequencing organisms and the data is distributed and heterogeneous, dispersed in varied formats and diverse platforms. The need is to devise a system that fills this gap to quite an extent and this is carried through the help of O-MaSE and its modeling tool, Agent Tool 3, AT3. A method to model the concepts required for building a multi-agent system for organizations is presented including goals, roles, agents, protocols, plans and the mapping of agents over roles.
Search engines can return ranked documents as a result for any query from which the user struggle to navigate and search the correct answer. This process wastes user's navigation time and due to this the need for automated question answering systems becomes more urgent. We need such a system which is capable of replying the exact and concise answer to the question posed in natural language. The best way to address this problem is use of Question answering systems (QAS). The basic aim of QAS is to provide short and correct answer to the user saving his/her navigation time. The concept of Natural Language Processing plays an important role in developing any QAS. This paper provides an implementation approaches for various categories of QAS such as Closed Domain based QAS, Open Domain based QAS, WEBBASED QAS, Information Retrieval or Information Extraction (IR/IE) based QAS, and Rule based QAS which will be helpful for new directions of research in this area.
Digital Resource Objects (DROs) suffer from shortage of its contents, and this leads to reduction in the effectiveness in its retrieval results. In order to increase the retrieval effectiveness of DRO, adding extra information to its contents is required. However, the extra information must be related to the structure of DRO. Each document contains metadata units with multiple topics. Document expansion (DE) methods utilize the unstructured documents to increase the document contents. In the same way, in this paper, an Enhanced Document Expansion (EDE) method is proposed by utilizing structured documents. DE method is a way of feeding and providing documents with new information to increase the effectiveness of the documents. Usually, traditional DE methods add terms to the original documents. In the proposed EDE method, a new procedure to increase the information content according to specific steps is added and aimed at adding new information which is more relevant and closer to each metadata unit in each document. The proposed EDE method calculates the nearest sentences to the content of the metadata unit by improving the probability estimation equation. The experiments which are conducted on cultural heritage CHiC2013 collections show a statistically significant improvement over the traditional document expansion methods.
Malayalam, a classical language in India, is spoken by over 40 million people. This paper proposes an effective information retrieval system for Malayalam, which retrieve Malayalam documents relevant to the user's information need. The proposed system improves effectiveness by considering synonyms and negations of the terms specified in the query. Though the evaluation of the system is performed over a small corpus, the results are promising. The proposed system is thus relevant for various natural language processing tasks in an highly agglutinative language like Malayalam.
Information Technology brought many applications of Information Retrieval as simple as possible through Web and other Digital Information Access Environment. There is a demand in building Applications of IR in Local languages, which allows common people with minimal knowledge in at least one language to avail the information services. Word mismatch is a common problem in IR System Applications. The research is started to overcome this problem in 4 decades back and still the problem is unsolved. The methods were moving from word replacement to knowledge replacement to solve word mismatch and concept mismatches. This paper mainly explored towards solving word mismatch problem in India language context particularly Telugu language. IR in Telugu Language is in inception phase and need huge research to solve many issues in information processing and retrieval. Cross Lingual IR is quite comfortable stage with parallel rule conversion techniques, where as it is difficult to process Telugu alone and build Information Retrieval System. Telugu is morphologically rich with high conflational rate. In this paper we build IR in Telugu Language and extended to solve vocabulary mismatch problem using Synset relationships. When compared to base Retrieval system, expanded search yield offered results. This paper identified many issues related to topical search and given direction to solve with base proof with implementation results. We found improvement in Recall and precision with Expanded Search with Non-Expanded Retrieval.
Automated fault localization aims to reduce software maintenance's workload during software development's evolution. Applying different features extracted from bug reports and source files can help locate faults. However, these approaches consider programming languages as natural when measuring similarity features, considering only precise term matching and ignoring deep semantic similarity features. Furthermore, existing bug localization approaches need to utilize the structural information extracted from source files, where program languages have unique structural features compared to natural languages. In this paper, we proposed SemirFL, a model combining both Convolutional Neural Network(CNN) and revised Vector Space Model(rVSM), which is feeded with four metadata features (bug-fixing recency, bug-fixing frequency, collaborative filtering score, and class name similarity). SemirFL has been studied on four open-source projects. The experimental results show that SemirFL can significantly outperform the existing representative techniques in locating faults in the buggy source files.
Geographic information retrieval (GIR) is developed to retrieve geographical information from unstructured text (commonly web documents). Previous researches focus on applying traditional information retrieval (IR) techniques to GIR, such as ranking geographic relevance by vector space model (VSM). In many cases, these keyword-based methods can not support spatial query very well. For example, searching documents on “debris flow took place in Hunan last year”, the documents selected in this way may only contain the words “debris flow” and “Hunan” rather than refer to “debris flow actually occurred in Hunan”. Lack of spatial relations between thematic activates (debris flow) and geographic entities (Hunan) is the key reason for this problem. In this paper, we present a kernel-based approach and apply it in support vector machine (SVM) to extract spatial relations from free text for further GIS service and spatial reasoning. First, we analyze the characters of spatial relation expressions in natural language and there are two types of spatial relations: topology and direction. Both of them are used to qualitatively describe the relative positions of spatial objects to each other. Then we explore the use of dependency tree (a dependency tree represents the grammatical dependencies in a sentence and it can be generated by syntax parser) to identify these spatial relations. We observe that the features required to find a relationship between two spatial named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency tree. Therefore, we construct a shortest path dependency kernel for SVM to complete the task. The experiment results show that our dependency tree kernel achieves significant improvement than previous method.
The exponential increase in textual unstructured digital data creates significant demand for advanced and smart stemming systems. As a preprocessing stage, stemming is applied in various research fields such as information retrieval (IR), domain vocabulary analysis, and feature reduction in many natural language processing (NLP). Text stemming (TS), an important step, can significantly improve performance in such systems. Text-stemming methods developed till now could be better in their results and can produce errors of different types leading to degraded performance of the applications in which these are used. This work presents a systematic study with an in-depth review of selected stemming works published from 1968 to 2023. The work presents a multidimensional review of studied stemming algorithms i.e., methodology, data source, performance, and evaluation methods. For this study, we have chosen different stemmers, which can be categorized as 1) linguistic knowledge-based, 2) statistical, 3) corpus-based, 4) context-sensitive, and 5) hybrid stemmers. The study shows that linguistic knowledge-based stemming techniques were widely used for highly inflected languages (such as Arabic, Hindi, and Urdu) and have reported higher accuracy than other techniques. We compare and analyze the performance of various state-of-the-art TS approaches, including their issues and challenges, which are summarized as research gaps. This work also analyzes different NLP applications utilizing stemming methods. At the end, we list the future work directions for interested researchers.
Document retrieval will become challenging when it deals with the unique capability of natural languages to present content in different forms using synonyms, usages, and their complex combinations. Most of the existing information retrieval systems are struggling to retrieve documents with a similar meaning, and they are helpful only to get documents based on matching keywords. The query expansion is a logically simple and straightforward technique to improve the effectiveness of information retrieval in this background. The existing statistical approach depends mainly on the term frequency to generate candidate documents for the expanded or normal query. Most of the existing works do not consider the ways in which the content in a particular document can be represented differently by keeping the same context. This paper proposes a novel Synonym Weighted - Vector Space Model and query expansion technique for an effective synonym-incorporated method for document retrieval. The combination of modified Term Frequency - Inverse Document Frequency(TF-IDF) and synonym extended VSM has given a promising outcome for the experiments throughout the study. The proposed method is validated with two English-written publicly available datasets - CACM and CISI. The quantitative measures, like mean average precision, precision, recall, and F-measure obtained in the experiments are found to be better for the proposed method compared with the classical VSM and other baseline methods in the problem domain. We could obtain the highest precision of 0.83 and 0.65 for the CACM and CISI datasets respectively.
Private information retrieval protocols guarantee that a user can privately and losslessly retrieve a single file from a database stored across multiple servers. In this work, we propose to simultaneously relax the conditions of perfect retrievability and privacy in order to obtain improved download rates when all files are stored uncoded on a single server. Information leakage is measured in terms of the average success probability for the server of correctly guessing the identity of the desired file. The main findings are: i) The derivation of the optimal tradeoff between download rate, distortion, and information leakage when the file size is infinite. Closed-form expressions of the optimal tradeoff for the special cases of “no-leakage” and “no-privacy” are also given. ii) A novel approach based on linear programming (LP) to construct schemes for a finite file size and an arbitrary number of files. The proposed LP approach can be leveraged to find provably optimal schemes with corresponding closed-form expressions for the rate-distortion-leakage tradeoff when the database contains at most four bits. Finally, for a database that contains 320 bits, we compare two construction methods based on the LP approach with a nonconstructive scheme downloading subsets of files using a finite-length lossy compressor based on random coding.
Semantic retrieve as one of the development direction of the intelligent information retrieve technology occupies extremely important position in intelligent information retrieve field. Aiming at the disadvantage of the traditional retrieve system based on the key matching, a new model of the semantic retrieve based on the Ontology is proposed in this paper. In order to test the performance of the semantic retrieval model, this paper does experiments by comparing this model with traditional model. The result of experiment shows that the semantic model is better than traditional model. It has important reference value in the process of implementing semantic retrieve system in other fields.
Nowadays, the resources available on the web increases significantly. It then has a large volume of information, but without mastery of content. In this immense data warehouse research of current information retrieval systems do not allow users to obtain results to their requests that meet exactly their needs. This is due in large part to indexing techniques (key words, thesaurus). The result is that the user of the web wasting much of his time to examine a large number of Web page by searching for what he needs, because the Web does not provide service in this direction. The Semantic Web is the solution; this new vision of the web is to make web resources not only understandable by humans but also by machines. To improve the relevance of information retrieval, we propose in this paper an approach based on the use of domain ontology for indexing a collection of documents and the use of semantic links between documents in the collection to allow the inference of all relevant documents. The work involves the implementation of a system based on the use of OWL ontology for research pedagogical documents. In this case, the descriptors are not directly chosen in the documents but in the ontology and are indexed by concepts that reflect their meaning rather than words are often ambiguous. To perform a search based on meaning, documents and their descriptors are stored in OWL ontologies describing the documentary features of a document. The objective is to design two types of OWL ontologies: document ontology reserved for storage of all pedagogical documents and domain ontology reserved for well-structured of documents stored in the level of the document ontology and each document is indexed by its keywords and their synonyms.
Semantic information retrieval has been a topic of great interest in recent years. Its purpose is to improve the effectiveness of information retrieval methods by exploiting the semantic information in documents and queries. Many approaches have been proposed to retrieval semantically the information. In this article, we propose a new approach based on domain ontology and natural language processing (NLP) techniques. In our approach, we represent resources and queries by a list of concepts. This allows us at the second level to create a semantic map of concepts and terms extracted from resources. Through this map, we propose to the users a proximity champ allowing them to find, in the one time, pertinent resources according to their query and in the second time to discover a new knowledge by navigating in semantic proximity relations.
Intelligent transportation system (ITS) services have attracted significant attention in recent years. To support ITS services, architecture is required to retrieve information and data from moving vehicles and roadside facilities in an efficient manner. A two-tier system that integrates low-tier vehicular ad hoc networks (VANETs) and a high-tier infrastructure-based peer-to-peer (P2P) overlay, which can achieve a high lookup success rate and low lookup latency for information retrieval, has been developed. However, conventional information lookups in the two-tier VANET/P2P system may introduce extra lookup messages and latencies because the lookup queries are simultaneously performed over the VANET/P2P networks. This paper proposes an adaptive lookup protocol for the two-tier VANET/P2P system to improve the efficiency of information retrieval. The proposed protocol uses a Bloom filter, which is a space-efficient data structure, to collect reachability information of road segments; therefore, adaptive routing of queries between low- and high-tier networks according to reachability probability can be employed. Simulations based on the SUMO traffic simulator and QualNet network simulator demonstrate that compared with the conventional two-tier lookup mechanism, the adaptive lookup protocol can reduce the lookup latency by 12%, reduce the P2P lookup overhead by 20%-33%, and achieve a high success rate in information lookups.
In the age of Web 2.0, community user contributed questions and answers provide an important alternative for knowledge acquisition through web search. Question retrieval in current community-based question answering (CQA) services do not, in general, work well for long and complex queries, such as the questions. The main reasons are the verboseness in natural language queries and the word mismatch between the queries and the candidate questions in the CQA archive during retrieval. To address these two problems, existing solutions try to refine the search queries by distinguishing the key concepts in the queries and expanding the queries with relevant content. However, using the existing query refinement approaches can only identify the key and non-key concepts, while the differences between the key concepts are overlooked. Moreover, the existing query expansion approaches, not only overlook the weights of key concepts in the queries, but also fail to consider concept level expansion for them. In this paper, we explore a key concept identification approach for query refinement and a pivot language translation based approach to explore key concept paraphrasing. We further propose a new question retrieval model which can seamlessly integrate the key concepts and their paraphrases. The experimental results demonstrate that the integrated retrieval model significantly outperforms the state-of-the-art models in question retrieval.
Extracting knowledge from multimedia contents represents recently a big challenge. Organizing and analyzing multimedia collections requires specific tools for extracting knowledge from the contents to enable effective and efficient filtering, searching and retrieval. The use of knowledge models, such as Ontology, is gaining interest among multimedia retrieval researches. This paper builds and integrates a multi-modality ontology to the conventional image annotation and retrieval methodology. The proposed knowledge-model integration highly improves the searching process. Two ontologies are proposed, domain and visual description ontologies. Experiments have demonstrated the efficiency of the proposed multi-modality ontology method when compared against the classical retrieval technique. The results show that using ontologies increases the performance to reach 1, 0.91 and 0.94 for Precision, Recall and F-measure respectively.
this paper provides an in-depth survey of challenges in the design of intelligent information retrieval systems, pointing out some similarities and differences in the core data mining and web based search operations. The procedures for evaluation of search engine performance and implicit feedback of the user with respect to a search result are studied with references to the different algorithms. We have proposed a novel neural satisfaction based feedback vector in contrast to the existing activity pattern based feedback as a future research direction in Intelligent IR. We addressed the select research work in the area of soft information retrieval using fuzzy sets, artificial neural networks, genetic algorithms and probabilistic information retrieval. As an instance of information retrieval, web mining is a set of operations to retrieve relevant documents from preprocessed, crawled and indexed web, and it can be categorized into more specialized tasks of web content mining, web structure mining and web usage mining. We have given a survey of the important reviews on topic of web mining and its associated tasks. On the basis of Identified challenges in information retrieval in general, and web mining in particular, we have concentrated on applicability of soft computing techniques and their hybrids in web mining, the performance related issues of select solutions, future of web mining and the next generation user's expectations from a search engine.
Enterprise information management (EIM) deals with the demands upon enterprise unstructured information placed by applications such as eDiscovery, compliance, information lifecycle management, etc. Each of these applications poses a unique challenge to the retrieval and data mining of enterprise unstructured information. However, the study of EIM as a research field has long been hampered by the lack of availability of enterprise corpora. Due to this paucity of enterprise datasets, much of the research on information retrieval and mining that is meant for EIM is benchmarked on corpora that are vastly different in their structure than a typical enterprise corpus. An important category of enterprise corpora are those that arise during technology creation in an enterprise. Such corpora take center stage, for example, during eDiscovery requests arising from technology patent related litigation: an area of immense commercial impact. In this paper, we highlight the primary characteristics of enterprise corpora that arise during technology creation. At a high-level, these properties are project structure, heterogeneity, collaborations, and skew ness along various axes. We then study these characteristics in a carefully chosen enterprise corpus from a technology creation effort at a Fortune 10 corporation. In summary, we study the salient features of enterprise corpora and emphasize their structural properties. We hope that our study will spur effort in devising retrieval and mining techniques that are designed for EIM.
In the Dempster-Shafer's theory of evidence, for incorporating uncertainty, the valuation assigns to the data tables the degrees of belief for these data. Firstly, we are looking for the answers to the following questions. Is there a valuation-based system in which combination and marginalization operate on valuations? Has this system prosperities analogical to the t-norm system? In the t-norm system of the valuation for the specific database attributes configuration can be described the algebra of possible data set in which can be interpreted the Information Retrieval Logic.
Many software maintenance activities need to find code units (functions, files, etc.) that implement a certain concern (features, bugs, etc.). To facilitate such activities, many approaches have been proposed to automatically link code units with concerns described in natural languages, which are termed as concern localization and often employ Information Retrieval (IR) techniques. There has not been a study that evaluates and compares the effectiveness of latest IR techniques on a large dataset. This study fills this gap by investigating ten IR techniques, some of which are new and have not been used for concern localization, on a Linux kernel dataset. The Linux kernel dataset contains more than 1,500 concerns that are linked to over 85,000 C functions. We have evaluated the effectiveness of the ten techniques on recovering the links between the concerns and the implementing functions and ranked the IR techniques based on their precisions on concern localization.
The study aims to develop a taxonomy based on the pool of information resources in the areas of teaching and learning and academic leadership available in different formats at the Institute of Leadership, Evaluation and Development (iLEAD), Universiti Teknologi MARA, Malaysia which conducts various training programmes for academic staff and has accumulated a substantial amount of training resources over the past few years. Data is gathered through the content analysis method to develop candidates of taxonomy terms. The product of this research would considerably enhance the retrieval of information by participants of the training programmes and members of the university and could be applied to similar agencies under the umbrella of the Ministry of Higher Education as a guide for information managers to add value and repackage their information resources.
We study the problem of pliable private information retrieval with side information (PPIR-SI) for the single server case. In PPIR, the messages are partitioned into nonoverlapping classes and stored in a number of noncolluding databases. The user wishes to retrieve any one message from a desired class while revealing no information about the desired class identity to the databases. In PPIR-SI, the user has prior access to some side information in the form of messages from different classes and wishes to retrieve any one new message from a desired class, i.e., the message is not included in the side information set, while revealing no information about the desired class to the databases. We characterize the capacity of (linear) single-server PPIR-SI for the case where the user’s side information is unidentified, i.e., the user is oblivious of the identities of its side information messages and the database structure. We term this case PPIR-USI. Surprisingly, we show that having side information, in PPIR-USI, is disadvantageous, in terms of the download rate, compared to PPIR.
In order to overcome the problem of low retrieval efficiency. The paper proposes a method of deep learning-based distributed vector representation for query expansion. The classic task of improving queries to improve retrieval performance is query expansion, refining user intent by filling in extended terms to fully understand the needs to achieve retrieval accuracy. In the query expansion, how to select the extended words is the key issue, and the quality of the extended words determines the performance of retrieval. The main feature of this method is to optimize the expansion words, improve the extended word relevance labeling strategy based on learning to rank, and use the word vector to construct features for the extended words for the construction and optimization of the extended word ranking model. Experimental results show that the method has a high accuracy rate on TREC public dataset, with a 4.45% improvement compared to the traditional method, which has important implications for the research of deep learning in information retrieval.
Fast and scalable Content-Based Image Retrieval using visual features is required for document analysis, Medical image analysis, etc. in the present age. Convolutional Neural Network (CNN) activations as features achieved their outstanding performance in this area. Deep Convolutional representations using the softmax function in the output layer are also ones among visual features. However, almost all the image retrieval systems hold their index of visual features on main memory in order to high responsiveness, limiting their applicability for big data applications. In this paper, we propose a fast calculation method of cosine similarity with L2 norm indexed in advance on Elasticsearch. We evaluate our approach with ImageNet Dataset and VGG-16 pre-trained model. The evaluation results show the effectiveness and efficiency of our proposed method.
Query expansion (QE) and document expansion (DE) have been proved effective for improving the retrieval performance in language modeling approach. However, the issue that which expansion technique is more effective in information retrieval (IR), has not been well studied and discussed. To address this issue, this paper performs an empirical study on QE and DE to examine their effects. Moreover, since QE and DE exploit different corpus structures, we also examine the potential effectiveness of incorporating QE and DE. Experimental results on several TREC test collections show that both QE and DE significantly outperform the classical language model, but the effectiveness of QE and DE is varied in different settings of retrieval. In addition, incorporating QE with DE does not always bring about the best performance.
In the Dictionary-based String Matching (DSM) problem, an Information Retrieval (IR) system has access to a source sequence and stores the position of a certain number of strings in a posting table. When a user inquires the position of a string, the IR system, instead of searching in the source sequence directly, relies on the the posting table to answer the query more efficiently. In this paper, the Statistical DSM problem is proposed as a statistical and information-theoretic formulation of the classic DSM problem in which both the source and the query have a statistical description while the strings stored in the posting sequence are described as a code. Through this formulation, we define the communication efficiency of the IR system as the average cost in retrieving the entries of the posting list from the posting table, in the limit of an infinitely long source sequence. This formulation is used to study the communication efficiency for the case in which the dictionary is composed of (i) all the strings of a given length, referred to as k-grams, and (ii) run-length codes.
Recent rapid advances in handheld devices make it possible for users to retrieve information anytime and anywhere. Existing information retrieval techniques usually require the users to spend much effort to refine their queries. Information retrieval in mobile devices would represent an extremely challenging search environment. Due to the existing challenging constraints in technology, there is a need to have a simple and easy information retrieval in mobile with an attractive interface to attract more users to use this application. The M-Hadith prototype is designed to provide searching Malay Hadith text based on query word using stemming algorithm to get results on the query. Since, stemming is a basic text processing tool where it is used for efficient and effective text retrieval and machine translation, therefore, it has been adapted in the development of M-Hadith. Early results in the mobile simulator show compatibility and efficiency in various type of mobile devices.
With the rapid growth of networking, cyber–physical–social systems (CPSSs) provide vast amounts of information. Aimed at the huge and complex data provided by networking, obtaining valuable information to meet precise search needs when capturing user intention has become a major challenge, especially in personalized websites. General search engines face difficulties in addressing the challenges brought by this exploding amount of information. In this paper, we use real-time location and relevant feedback technology to design and implement an efficient, configurable, and intelligent retrieval framework for personalized websites in CPSSs. To improve the retrieval results, this paper also proposes a strategy of implicit relevant feedback based on click-through data analysis, which can obtain the relationship between the user query conditions and retrieval results. Finally, this paper designs a personalized PageRank algorithm including modified parameters to improve the ranking quality of the retrieval results using the relevant feedback from other users in the interest group. Experiments illustrate that the proposed accurate and intelligent retrieval framework improves the user experience.
Natural language processing technology can not only enrich the functions of computers, but also fundamentally promote the development of artificial intelligence technology. Based on natural language processing technology, many useful systems for people's survival and life have been produced, such as the question-and-answer system described in this thesis. This system mainly uses natural language processing technology and information retrieval technology. Although it is based on text retrieval, it is quite different from traditional search engine. Traditional search engines point out that users need to input a series of keyword combinations, and users can only get a variety of related websites, but also rely on their own discrimination ability to select useful information. However, the question answering system can allow users to input a question in the form of natural language. Finally, according to the search and judgment, the system can get a short and accurate answer to the user, which greatly improves the convenience of people's production and life. This thesis mainly elaborates the content of question answering system of natural language processing and analyses how to optimize it.
The current pandemic has spread everywhere. Various effects of pressure in the economic, educational, and social sectors are forced to adjust. So that people really need information about efforts to prevent the spread is very necessary. Search Engine is a program that is used as a tool to find more information on the internet. Search Engine is one of the discussions in the field of Information Retrieval. This system is a document search of unstructured properties. Thus, being able to provide the information needs of a large set of documents (on a local computer server or the internet). The vector space model is one of the many models in Information Retrieval that is used to get the distance and direction between keywords and documents by representing them into vectors. Then the results of ranking using cosine similarity with a dataset of 90 articles about covid19 along with 4 keywords will be tested with precision, recall, and accuracy calculations. The results of the precision calculation get a value of 60% - 73%, recall gets a value for each of the keywords 81%-100% and gets an accuracy value of 85% -89%. The results of these experiments indicate that information retrieval with vector space model is effective with good and stable performance used for information retrieval.
In order to improve effectively the identification ability of agricultural product information, the ontology technology is introduced to the searching field of agricultural product information, and the classification of agricultural product information technology based on ontology is proposed. The ontology model is defined, and the classification standards are carried out, then the retrieval model is made, the learning algorithm is used to extract topic knowledge of agricultural product, the help is from the semantic description of the agricultural product information, unstructure or semi-structure information are classified by the field of ontology framework of agricultural product, then which are made the structure knowledge ontology base of agricultural product information. The practice has shown that the classification of agricultural product information based on ontology has high accuracy, then improve the reliability and stability of information searching of agricultural product.
In this paper, we introduce t-revealing codes in the binary Hamming space Fn. Let C ⊆ Fn be a code and denote by It{C; x) the set of codewords of C which are within (Hamming) distance t from a word x ϵ Fn. A code C is t-revealing if the majority voting on the coordinates of the words in It(C; x) gives unambiguously x. These codes have applications, for instance, to the list decoding problem of the Levenshtein's channel model, where the decoder provides a list based on several different outputs of the channel with the same input, and to the information retrieval problem of the Yaakobi-Bruck model of associative memories. We give t-revealing codes which improve some of the key parameters for these applications compared to earlier code constructions, namely, the length of the output list L of the decoder and the maximal number of input clues m needed for information retrieval.
Cloud computing is one of the most useful environment that provides various information services in which required information can be retrieved through many web-based tools and applications. Now the new surge of interest in cloud computing is accompanied with the exponential growth of data sizes. There is a need to find the desired content quickly and efficiently by simply consulting the index. Thus there arises a question of how to effectively process these immense data sets is becoming increasingly urgent. Our existing system is searching the content through ontology in cloud which practically suffers from maintaining a consistent logic for the input documents given and taking this advantage into consideration we have brought in new concepts called Document Retrieval Algorithm. In this paper we discuss how effectively and efficiently information can be retrieved from cloud taking into account their storage space too, where we store the metadata of file in cloud and not the entire file which holds a lot of space in distributed environment. Thus we bring in the concept of Named Entity Recognition and Universal Word List with Term frequency, which maximizes the information retrieval more effective and efficient and also to bridge the gap between the semantic web and the users which reduces the complexity met by them in information retrieval.
This study describes a preliminary research on Information Retrieval (IR) systems. We developed a prototype tool that runs on a master-slave model and uses distributed processing in order to decentralize the workload while retrieving information from the Internet. Later, its viability is demonstrated with a set of executions and the discussion follows with an analysis of the master process overload versus the number of slaves connected.
Health Information Retrieval is another alternative for retrieving the health information on the internet as can be seen by the increasing number of the information providers on the topic of health information for the modern medicine and the Thai traditional medicine also called Thai herb. However, there are a few of Thai herb informative service interacts with modern medicine. This is partly due to the language that the information are kept by which the information on the modern medicine are available in English but the information on the Thai traditional medicine are mostly available in Thai. Furthermore, another reason could be the matching problem due to the wordings or vocabularies used in the user query that may have the same or similar meaning such as fever, flu, sick. This research is proposing to use the semantic and cross-language information retrieval for Thai herbs and modern medicine by Latent Semantic Analysis technique in order to benefit those who are searching for the basic health information and to enhance further studies and researches on the related topic area.
It is very necessary to build a customized retrieval system in the era of the big information explosion. This paper gives a framework of petroleum information retrieval system which will be used by petroleum exploration and development researchers. First, we use the open source framework SCRAPY to build a crawler system to crawl the information that business people pay attention to. Then k-means algorithm is used to cluster the crawled documents, therefore the key information is extracted and presented in the system. The actual effect in production and operation shows that this customized retrieval system is efficient and agile, it improves the efficiency, accuracy and automation level of the work.
Most of the information stored in web pages contains geographic context, such as place names, address, and coordinates. Such geographic information often contains great values and is worth retrieving and analyzing. Traditional search engine is limited in its capability of extracting meaningful geographic messages from mass of unstructured and textual source. Even with the retrieved geographic information, specific systems are still needed to show the spatial distributions of such data. This paper presents an integrated framework which can retrieve and analyze geographic information in web pages. This framework integrates the following core functions: geographic information retrieval; geocoding; spatial analysis and statistical analysis. We also demonstrate the effectiveness of this framework by employing it to retrieve and analyze the geographic information from a particular website.
Ontology has rich internal structure. There are relations and constraints between the concepts that is why Ontology has a richer internal structure. Ontology can be used for information retrieval. Ontology is a halfway determination of a conceptual vocabulary to be utilized for formulating knowledge-level hypothesis around a domain of discourse. The key part of ontology is to help knowledge sharing and reuse. The process of allotting descriptions to documents in an IRS is called indexing. In this paper a technique is proposed which improves results. In this technique web pages are stored in xml database. WordNet is used as dictionary for finding synonyms of user's query. The technique is based on context of word. Using context of words helps improvise the search results. The technique is called ontological indexing. This technique is compared with text based search. The words on the web pages are mapped to concepts in ontology. Mapping score is generated for each word. Results of search depend on value of mapping score. Moreover the issue of word sense disambiguation is solved up to some extent using parts of speech tagger.
The aim of this paper is to assess the impact of the uncertainty representation for retrieving items with missing data. The problem of information retrieval from incomplete incident databases is addressed in this paper. After a brief survey on the problem of missing data with an emphasis on the information retrieval application, we propose a novel approach for retrieving case records with missing data. The general idea of the proposed data driven approach is to model the uncertainty pertaining to this missing data. We chose the general model of belief functions as it encompasses as special cases both classical and probability models. Several uncertainty models are then compared based on (1) an expressiveness criterion (non-specificity or randomness) and (2) objective measures of performance typical to the Information Retrieval domain. The results are illustrated on several datasets and a simulation controlled missing data mechanism.
This paper presents ARGOsearch, an information retrieval system which improves the search quality in library management systems. We have proposed a general and extensible architecture for ARGOsearch based on relevance criteria that sorts the user query results considering text similarity and context information such as statistical data and the user profile. We have experimentally evaluated our approach using the vector space model as baseline.
This paper proposes an audio information retrieval model based on Manifold Ranking (MR) and improving ranking results by relevance feedback algorithm. Timbre component has been employed as the main feature. To compute the timbre similarity, it is necessary to extract the spectrum features for each frame. The large set of frames is clustered by a Gaussian Mixture Model (GMM) and Expectation Maximization. The typical spectra frame from GMM is drawn as the data points, manifold ranking assigns each data point a relative ranking score, which is treated as a distance instead of traditional similarity metrics based on pair-wise distance. Furthermore, manifold ranking algorithm can be easily generalized by adding these positive examples by relevance feedback algorithm, and improves the final result. Experimental results show the proposed approach is effective to improve the ranking capability of the existing distance functions.
Towards improving the performance in various music information processing tasks, recent studies exploit different modalities able to capture diverse aspects of music. Such modalities include audio recordings, symbolic music scores, mid-level representations, motion and gestural data, video recordings, editorial or cultural tags, lyrics and album cover arts. This paper critically reviews the various approaches adopted in Music Information Processing and Retrieval, and highlights how multimodal algorithms can help Music Computing applications. First, we categorize the related literature based on the application they address. Subsequently, we analyze existing information fusion approaches, and we conclude with the set of challenges that Music Information Retrieval and Sound and Music Computing research communities should focus in the next years.
Along with the growth of Islamic religion in Indonesia, the need for information of Hadits becomes very important. Hadith as the second source of law in Islam after Al-Quran has a high position in moslem life. But application related with Hadith Retrieval is still limited. This limitation especially found in non-arabic language environment. Existing Hadith Retrieval System in Indonesia execute input query directly to DBMS, causing low quality search result. To overcome the problem, we try to implement thesaurus in the system. This study carried out research on the application of thesaurus for query expansion in Indonesian Hadith Retrieval System. Thesaurus construction done automatically using co-coccurence analysis followed by manual validation process. The thesaurus documents will be used in the process of query expansion while searching process. Performance measurements includes Precision at 10, Mean Average Precision (MAP) and Recall generated by the system after applying the thesaurus for query expansion. From the results, the accuracy for Precisison at 10 (P@10) increased by 34% between searches with query expansion using a thesaurus and without query expansion using thesaurus. As for the MAP value increased by 16% and 34% for recall.
Information retrieval evaluation based on the pooling method inherently poses biasness towards systems that contributed to the pool of judged documents. This may distort the results about the relative effectiveness of different retrieval strategies, or rather, the retrieval systems and thus result in unreliable system rankings. The purpose of this study is to suggest a technique to estimate the reliability of the retrieval system effectiveness rank in a list of ranked systems based on its performance in previous experiments. This can be also defined as the strength of rank for the individual retrieval system. By doing so, we will be able to predict the performance of each system in future experiments. To validate the proposed rank strength estimation method, an alternative systems ranking method is proposed to generate a new list of systems rankings which is used together with the proposed rank strength estimation method. The experimentation shows that the correlation coefficients remain above 0.8 across different number of experiments which means the new systems ranking is highly correlated with the gold standard. It suggests that the rank reliability estimation methods have effectively predicted the strength of the system ranks. And also, the results from both TREC 2004 and TREC 8 show the similar outcome which further confirms the effectiveness of the proposed rank reliability estimation method.
Graduate students are active information seekers. Their experiences with information seeking range from novices to experts. Graduate students' understanding of the needed information and the different types of information sources accessible are essential for them to search for the required information effectively and efficiently. Previous studies concluded that insufficient knowledge about information seeking behaviour is one of the factors that contributed to failure in searching for useful information. This paper discuss about a study conducted to evaluate a measurement model of information seeking behaviour factors, which is a part of the methods used to develop Research Information Seeking Behaviour Model. The data was collected through survey method by using web-based questionnaire. The measurement model was validated using Structural Equation Modeling with AMOS 16.0. Eight factors were extracted from exploratory factor analysis and labeled as preliminary research needs, data research needs, printed information sources, electronic information sources, media information sources, library information search, electronic information search and information use. Results of the assessment of model fit showed that all the relevant measurements fit a value that exceeds the value recommended by previous researchers. In conclusion, the measurement model provides a reasonably good fit and suitable for the development of Research Information Seeking Behaviour Model.
In this paper, we try to exploit the semantic richness of Arabic language for Information Retrieval (IR). The semantics of Arabic words may be extracted from dictionaries or corpora, which are analyzed and minded using Natural Language Processing (NLP) and text mining tools. This allows modeling the contextual dependencies between words, which help identify the meaning of queries in the search process. Thus, the queries are enriched by semantic knowledge, which enhances search performance. In this context, this paper describes a text mining-based approach for Arabic semantic IR, which considers senses of query terms. Experiments and results based on a standard Arabic Test collection are discussed through this communication. In the one hand, we compare dictionary versus corpus-based approaches for modeling semantics. On the other hand, we compare some Arabic NLP tools in the preprocessing step. Thus, we study the effect of Arabic morphology on the semantic interpretation of queries.
At present many commercial database using Xpath and XQuery as XML retrieval standard, grammar about XPath and XQuery is complicated , it is difficult for general user to use. In this paper, the research work is focused on the keyword retrieval technology of XML document, designs a XML full-text information retrieval similar to HTML full-text information retrieval,proposes inverted-file retrieval method based on the Deweycoding, designs a SLCA retrieval algorithm about multi-keyword .
This paper aims to enlightening a unique technique influenced by modern methodologies of advanced learning implemented in various domains of knowledge. Faster communication and manipulation of data are the aspects of IT that contribute lot to the learning minds by keeping the continuity and captivating the learner to absorb more and more in efficient manner. Harvesting these qualities of IT and merging it for easier, better and structured learning of holy Quran is the main objective of this paper. Arabic language in general and dialectal of holy Quran in particular is under the focus of many researchers in the field of Natural Language Processing (NLP). Arabic has morphology that differs from English in very basic level but it does follow systematic pattern when inflectional or derivational morphology is discussed. The challenges in the structuring of such a vast variety of data and their successful accomplishment are also added in brief but strengthen with examples to provide the depth of work carried out in this domain. Initially 23 attributes per word are applied to Quranic words (70% of words are covered till to date) based on Arabic parts of speech. These attributes are defined in such manner that language hierarchy is conserved to its maximum. The said application can be further enhanced to micro level data structuring. This effort did lead to implementation of information retrieval system that provides the proper tagging to each step. This approach can be further utilised in the field of Quranic education and research.
As the interactive interface of the library automation system ultimately facing users, OPAC (Online Public Search Catalog) is the most important window for the library to communicate with readers, and it plays a role in communicating readers and collection resources, readers and resource services. The article chooses Windows NT as the network environment to realize online retrieval of campus literature information. The design content of the campus document information online retrieval system includes system objectives, system composition and environment, system principles and functions, database design and types, and user interface design. The study found that the application of this system has greatly improved the efficiency of document retrieval in academic libraries.
Nowadays information retrieval systems get more attention due to the increasing use of multimedia technologies. The information may be in the form of video, image, sound and/or text. Application of surveillance, digital libraries, web applications and various other applications that handle enormous volume of data essentially have information retrieval components. This paper demonstrates an image retrieval system based on multiple regions that give a client interface for helping to identify the watershed regions-of-interest inside of an input image. The relationship between semantic ideas and visual elements is established by supervised Bayesian learning from positive bags. For comparison, feature vectors of regions which have similar regions code to the regions of query image can be used during retrieval. On standard datasets the proposed algorithm has been applied and accomplishes great annotation performance.
In this paper, we present an approach for effective content based image retrieval by color and texture based on genetic algorithm and euclidean distance method to achieve good image retrieval performance in Android mobile environment. In recent years the Google's Android play an important role in mobile operating system. It can provide number of application to the user like e-mail, web browser, video and audio player, maps, google play and so on. The content based image retrieval plays an important role in image retrieval methods. Where the color feature extraction is done by color Histogram refinement method and the texture feature extraction is acquired by gray level co-occurrence matrix (GLCM). This integrated feature of color and texture can provide better result for image retrieval in Google's Android mobile operating system.
Nowadays, like the English and other languages, Bangla also plays a significant role to strengthen the web repository. The storing rate of Bangla information is augmented day-by-day. Because of the numerous documents in the World Wide Web, it is very difficult for a user to retrieve the desired information. Furthermore, finding the useful documents tends to be more time spending as well as an annoying job. These demands emerge to develop an Information Retrieval (IR) system to document ranking for Bangla language. In this paper, we have built such a retrieval system where users can find their needed documents which correspond to their own query strings throughout the ranking index. Although a lot of works have been done for English and other languages to rank the documents, unfortunately, we have found a very negligible amount of contributions in Bangla Language. Many methods such as - Boolean model, Maximal Marginal Relevance (MMR), Portfolio Theory (PR), Quantum Probability Ranking Principle (QPRP), Query Directed Clustering (QDC), Vector-based TFIDF and so on, have been proposed to implement the document ranking system. Here, we have applied a new approach, called Latent Semantic Indexing (LSI) to do the same task for Bangla documents. LSI uses the mathematical method called Singular Value Decomposition (SVD). After that, we have applied the cosine similarity to rank all the documents. We believe that the performance result of our proposed system has reached the trustworthy level.
Combining multiple retrieval systems is a commonly used method to improve the retrieval performance. However, it is still a challenging problem to figure out when and how the combined system can perform better than its individual systems. In this paper, we study these issues by using an information fusion paradigm: Combinatorial Fusion Analysis (CFA). TREC datasets are used as our experiment data. We measure the cognitive diversity between different individual systems by using a rank-score characteristic (RSC) function. Our results demonstrate that: 1) The performance of combination of p systems does not always increase with p, 2) Rank combination is better than score combination in particular when RSC diversity between two individual systems is large enough, and 3) combination of two systems can improve performance only if the two individual systems have relative good performance and are diverse.
Information Retrieval is a process through which we obtain and present related information from a very large collection of information resources according to user's query. Information Retrieval can vary according to user's wish, it can be an information retrieval for documents, music, pictures, etc. In a classic search model we have a user who has to fulfil a task. And for that to happen that user needs some information and to search the information needed user should make a query based on the information. So that query will act as an input in a search box, and on the basis of that query our information will be retrieved. But it is required that no misconception happens to user which may lead user to believe that the information needed can be wrong. And if that happens it may lead to improper formation of the query, meaning the query formed on the basis of information can be wrong thus resulting in a wrong result through information retrieval. So it is important for us to pay proper attention to every step that is going to take place during information retrieval. This paper presents a basic IR model and describes how an information retrieval is done following user's query. This paper also addresses how Information Retrieval and Data Retrieval are two different things and are often confused to be one.
Information retrieval is the methods of retrieving all the relevant information from a large collection of information. We will focus on the performance of the different information retrieval model based upon these models properties. Search engines used the property of these models for identifying the users given query for retrieving the information from the World Wide Web and form some specific application domain. If the user query is matched the search engine is able to precede the user query and retrieve the relevant information that is being searched by the user. Search engines which usually allow the information retrieval accuracy by using the Boolean Model, Vector Space Model and Probabilistic Model. In our paper we analyze precision and recall of these three models and compared by using different parameters.
Through the literature search, the paper summarizes the experience of information capacity training of European and other developed countries. Combining the fact that information awareness of college students in China is not strong and the actual information capacity is not high, the paper looks for the existing problems and shortcomings, and conducts in-depth analysis and discussion. On this basis, countermeasures and suggestions of the process of designing and building model are put forward to help train information capacity of college students.
We consider a multi-user variant of the private information retrieval problem described as follows. Suppose there are D users, each of which wants to privately retrieve a distinct message from a server with the help of a trusted agent. We assume that the agent has a subset of M messages whose indices are unknown to the server. The goal of the agent is to collectively retrieve the users' requests from the server. For this problem, we introduce the notion of individual-privacy - the agent is required to protect the privacy only for each individual user (but may leak some correlations among user requests). We refer to this problem as Individually-Private Information Retrieval with Side Information (IPIR-SI).We first establish a lower bound on the capacity, which is defined as the maximum achievable download rate, of the IPIR-SI problem by presenting a novel achievability protocol. Next, we characterize the capacity of IPIR-SI problem for M = 1 and D = 2. In the process of characterizing the capacity for arbitrary M and D we present a novel combinatorial conjecture, that may be of independent interest.
Focusing on the issue that formula retrieval could not be realized with traditional full-text retrieval technology because of the spatial features of formulae, this paper proposed a formulae retrieval method with the full considerations of the hierarchical structures of formulae. The index was constructed according to the clusters of formulae with their baseline level structures. Based on the index, the math retrieval algorithm was designed with different query modes, such as precision matching and structural matching. The experimental results show the average query response times in accurate retrieval mode and structural mode are acceptable, which indicates the practicability of proposed method.
Temporal Information Retrieval is a coming up research area in the field of Information Retrieval due to the immense amount of data in the World Wide Web (WWW) as well the contents of documents are actively time-dependent, it is very stiff for the user to retrieve the relevant documents. The time dimension available in the documents should be incorporated with document ranking for efficient retrieval. This paper gives an preface to Temporal Information Retrieval as well as explores the distinctive approaches related to time-aware retrieval models and temporal ranking approaches for specific types of time-sensitive queries along with deals with analysis and comparison of diverse temporal taggers to normalize the temporal expressions of the document in some standard format. On the basis of various parameters find out their advantages and limitations for the retrieval of relevant information and also point outs promising direction of research work for the further analysis.
When using Information Retrieval (IR) systems, users often present search queries made of ad-hoc keywords. It is then up to the information retrieval systems (IRS) to obtain a precise representation of the user's information need and the context (preferences) of the information. To address this problem, we investigate optimization of IRS to individual information needs in order of relevance. The goal of this article is to develop algorithms that optimize the ranking of documents retrieved from IRS according to user search context. In particular, the ranking task that led the user to engage in information-seeking behaviour during search tasks. This article discusses and describes a Document Ranking Optimization (DROPT) algorithm for IR in an Internet-based or designated databases environment. Conversely, as the volume of information available online and in designated databases is growing continuously, ranking algorithms can play a major role in the context of search results. In this article, a DROPT technique for documents retrieved from a corpus is developed with respect to document index keywords and the query vectors. This is based on calculating the weight (wij) of keywords in the document index vector, calculated as a function of the frequency of a keyword kj across a document. The purpose of the DROPT technique is to reflect how human users can judge the context changes in IR result rankings according to information relevance. This article shows that it is possible for the DROPT technique to overcome some of the limitations of existing traditional (tf × idf) algorithms via adaptation. The empirical evaluation using metrics measures on the DROPT technique carried out through human user interaction shows improvement over the traditional relevance feedback technique to demonstrate improving IR effectiveness.
In recent year, XML has become the major means for information representation and exchange on the Web. Due to the increasing number of XML documents, XML similarity becomes essential in a wide range of applications like information extraction, data integration etc. In this paper we present a clustering approach that calculate similarity between XML elements to identify appropriate information unit for retrieval task. Preliminary experimental results conducted using XML data sets of a biliographical site and a conference site shows that the proposed approach is promising to be further extended for other possible XML sources.
The ways to improve the efficiency of submission information using multimedia technology were examined in this report, thus ensuring the use of a powerful new tool for perception of information by disabled persons. The formation peculiarities of multimedia information content for disabled persons were analyzed. This paper considers digital library as an information system, where information is formed and collected from different sources, sorted, structured and intellectually processed and it's proposed a set of information technology services, making multimedia information content accessible for disabled persons.
Text Matching is the task of examining two pieces of texts, such as query and documents, and determining whether they have the same meaning. Text Matching is very important in many NLP tasks, such as document retrieval, question answering, automatic conversation, machine translation, etc. In recent years, there existed some representation-based and interaction-based neural networks which have achieved some improvements. However powerful attention mechanism is rarely used in these models. Inspired by the success of attention in machine translation and document classification, in this paper, we propose a Deep Hierarchical Attention Networks for Text Matching, namely Deep-HAN-Matching. Specifically, Deep-HAN-Matching extracts meaningful matching patterns and rich contextual features hierarchically from words to total document at the query term level using the recurrent neural network and attention mechanism, and finally rank the matching score produced by the fully connected neural network. Experimental results on WikiQA, a popular benchmark dataset for answer sentence selection in question answering, show that our model can significantly outperform traditional retrieval baseline models and some recent deep neural network based matching models.
Various dynasties ruled the Indian sub-continent and left behind enormous and rich cultural heritage that also included intellectually enriched research in the shape of various documents scripted in Urdu. In order to provide efficient access to this knowledge, analysis though digitizing the existing work is the need of hour. In addition to digitization, efficient search mechanisms also need to be implemented to provide users a rapid access to the queried information. In most cases, the digitized documents are complemented by manually assigned tags which not only is a time consuming task but also provides a very limited search facility. Automating the transcription of these documents using Optical Character Recognition (OCR) systems is also challenging due to the very complex cursive nature of Urdu text. To overcome these limitations, a keyword spotting based information retrieval system for document images is introduced in this study. The proposed technique relies on two major modules, document indexing and retrieval. Images of documents are segmented into partial words (ligatures) and identical partial words (PWs) are grouped into clusters. We introduce the concept of considering each (partial) word as a unique shape and a set of shape descriptors is extracted to characterize the PWs. The clusters of PWs are used to index a given set of documents. During retrieval, the query word presented to the system is matched with the clusters in the database and all documents containing instances of the query word are retrieved and presented to the user. The system evaluated on a set of printed Urdu documents in Nastaliq font realized promising precision and recall rates.
In the era of ubiquitous information and abundant data, the ability to extract relevantinformation from scattered, obscure, and heterogeneous data sources is of paramount importance. Despitethe considerable richness of ethnic cultural resources, they are often characterized by significant heterogeneity and fragmentation. Semantic segmentation technology can effectively leverage the power of computers to extract image features and compile feature databases, enabling the fusion of heterogeneous knowledge. To this end, this paper proposes an image classification method that combines object semantics and deep learning algorithms to identify object areas within an image and retrieve matching images from a database based on implicit semantic information. Through the integration of national cultural resources, this approach enables content retrieval and visual semantic segmentation. The use of multi-layer perceptrons for supervised learning facilitates the generation of object semantic templates to address the challenge of object semantic similarity. The resulting templates can then be applied to classify images, leading to a high classification accuracy rate for binary semantic images. Experimental results demonstrate that the proposed method enables automatic feature comparison with target images in the feature library during the retrieval of ethnic cultural resources, allowing for the mining of previously undiscovered and valuable knowledge. Finally, the best matching results and relevant information are output to users, enabling the effective use of these resources
The semantic relations between entities in documents play vital roles in many applications such as information extraction, and information retrieval. The problem of extracting the semantic relations in Vietnamese documents is more difficult because Vietnamese textual analysis tools are in progress. In this paper, we propose an approach based on syntactic dependency feature between provisions in each sentence and the Support Vector Machine classifier to extract the semantic relations between entities in Vietnamese documents. Our results show that syntactic dependency method is more stable and accuracy (F-measure) than the regular expression method especially for long and complex texts of Vietnamese legislative documents.
The main function of information retrieval (IR) system is to obtain efficient and exactly a minimum subset of document that is related to user concern. Synonymy and polysemy act as a barrier for natural language processing algorithms due to overestimation and misrepresentation. The proposed model uses the implicit of higher rank structure in combing terms with document to optimize the identification of relevant document based on terms used in queries with an enhanced automatic indexing approach has been suggested. The study benefited from the use of Term Frequency Inverse Document Frequency (TF-IDF) method to assign weight for each term in the document. Each document is presented as a vector of weight in the space. Also, the user query is represented as vector of weight. Finally, a Singular Value Decomposition (SVD) approach has been used in which a huge weight of term-document matrix is factorized into collection of vectors for approximation of the original matrix. The cosine similarity is also used to determine the closed vector of document to the user query. In regard to English information retrieval, It was observed that TF-IDF showed higher performance before term percentage 0.3 while Latent Semantic Indexing (LSI) was more stable than TF-IDF, especially in terms of the use of word association.
In Recent years, the application of machine learning approaches to conventional IR system evolve a new dimension in the field. The emphasis is now shifted from simply retrieving a set of documents to rank them also for a given query in terms of user's need. The researcher's task is not only to retrieve the documents from the corpus but also to rank them in order of their relevance to the user's requirement. To improve the system's performance is now the hot area of research. In this paper, an attempt has been made to put some of most commonly used algorithms in the community. It presents a survey on the approaches used to rank the retrieved documents and their evaluation strategies.
Question Answering Systems (QASs) have emerged as a good alternative for information seekers to retrieve precise information over the Internet. A good amount of research has been done to improve the performance of QASs across several languages, including European and Asian languages. However, Arabic, a morphologically rich Semitic language spoken by over 422 million people, has not seen similar development in the field of question answering. This article reviews the developments taking place in Arabic QASs as well as the challenges faced by researchers in developing Arabic QASs. After conducting an extensive literature survey of a number of English and Arabic QASs, this article classifies them according to several criteria. The most commonly used architecture for the development of an Arabic QAS, known as pipeline architecture, has been presented. In order to encourage and support the new researchers and scholars in conducting research in Arabic QASs, a list of techniques, tools, and computational linguistic resources, required to implement the components of the presented pipelined architecture, are described in this article in a simple and persuasive manner. Finally, the gap analysis between the research in Arabic and English QASs has been performed and accordingly, some future directions for research in Arabic QASs have been proposed.
The ability to control data and information through the Internet can be challenging. Preliminary analysis showed that some literal modification may occur to some words of the Quran in the electronic versions that span the Internet. Such small modifications may not be noticed by public audience. The holly book of Quran includes a unique feature in that its worldwide copies are all identically similar. The 114 Suras and all verses and words in them are preserved in the exact form. As such, we designed and evaluated a model and a tool to preserve the integrity of the wording in the Quran through generating a Meta data related to all letters in the Quran preserving the counts and locations. Such data can be used eventually in the same way CRC or hash algorithms are used in security to check the integrity of a disk and its data file where any small change in the data will result in a different hash value. We conducted several experiments to evaluate the different parameters and challenges that can impact the automatic authentication process of Quranic verses based on information retrieval and hashing algorithms.
A Mongolian Information Retrieval System based on Solr is proposed in this paper. The system implements the retrieval of Mongolian data store in our local machine. Firstly, in this paper, we built a Mongolian corpus with one million words, which has been corrected manually. Secondly, after being transcoded, theses data was represented by Latin characters. Finally, we used Solr to build indexes and documents so that we do queries on millions of data within seconds.
In large-scale enterprises, vast amounts of textual information are shared across corporate repositories and intranet websites. Traditional search techniques that lack context sensitivity, often fail to retrieve pertinent data efficiently. Modern techniques that use a distributed representation of words require a considerable training dataset and computation, thereby presenting financial and operational burdens. Generative models for information search suffer from problems of transparency and hallucination, which can be detrimental, especially for organizations and their stakeholders who rely on these results for critical business operations. This paper presents a non-goal oriented conversational agent based on a collection of finite state machines and an information search model for text search from an extensive collection of stored corporate documents and intranet websites. We used a distributed representation of words derived from the BERT model, which allows for contextual searching. We minimally fine-tuned a BERT model on a multi-label text classification task specific to a closed-domain knowledge base. Based on DCG metrics, our information retrieval model using distributed embeddings from the minimally trained BERT model and Word Movers Distance for calculating topic similarity is more relevant to user queries than BERT embeddings with cosine similarity and BM25. Our architecture promises to significantly expedite and improve the accuracy of information retrieval in closed-domain systems without the need for a massive training dataset or expensive computing while maintaining transparency.
Arabic document indexing is a challenging process due to the complex morphological nature of the Arabic language. Methods of document indexing in the literature relied on applying morphological schemes to extract terms. These morphological schemes mainly depend on root extraction and stemming. This paper proposes a simple document indexing method based on selecting only definite words (that have the prefix AL, or it is acceptable to have this prefix). The words that preceding (and/or) succeeding these definite words are also considered. The proposed method is tested using the TREC-2001/2002 Arabic test collection. The proposed method outperforms selecting all terms, either without stemming, or stemmed by the Light10 stemmer, for example, indexing documents by selecting definite words and words that come after them enhances the Mean average Precision of the Light10 by 4.4%, and at the same time decreases the index size by 6.1%.
With the development of intelligent collection technology and popularization of intelligent terminals, multi-source heterogeneous data are growing rapidly. The effective utilization of rich semantic information contained in massive amounts of multi-source heterogeneous data to provide users with high-quality cross-modal information retrieval services has become an urgent problem to be solved in the current field of information retrieval. In this paper, we propose a novel cross-modal retrieval method, named MGSGH, which deeply explores the internal correlation between data of different granularities by integrating coarse-grained global semantic information and fine-grained scene graph information to model global semantic concepts and local semantic relationship graphs within a modality respectively. By enforcing cross-modal consistency constraints and intra-modal similarity preservation, we effectively integrate the visual features of image data and semantic information of text data to overcome the heterogeneity between the two types of data. Furthermore, we propose a new method for learning hash codes directly, thereby reducing the impact of quantization loss. Our comprehensive experimental evaluation demonstrated the effectiveness and superiority of the proposed model in achieving accurate and efficient cross-modal retrieval.
Nowadays, the rapid development of Internet technology drives the advance of the big data era, and along with the widespread use of smart devices and social media, multimedia data is growing explosively. With the increasingly complex and diverse needs of information exchange, collection and storage, the types of information have also evolved from traditional text information to diverse data forms such as pictures and video and audio, bringing different degrees of convenience to people's work and life and other scenarios. However, the huge amount of multimedia data also makes information storage and retrieval more cumbersome. How to realize the effective storage and efficient retrieval of data, so as to better utilize the value of multimedia data, is one of the challenges that academia and information industry are tackling nowadays. In this paper, we study the cross-media retrieval technology of text and image by Contrastive Language-Image Pre-training model based on natural language processing method. The cross-media pre-training idea proposed in this paper can be applied not only to text-image processing, but also theoretically to mutual retrieval of modal information of video and audio, etc.
With the steady growth of the Internet many search websites have emerged, however most of these websites only allow you to perform the search using a textual interface. A more intuitive method for the user would be to use melody snippets of a song to perform this search. In this paper, we present a two-stage query-by-playing (QBP) system using symbolic representations where from a melody snippet of a song it is performed the retrieval of a set of similar songs. On the first stage we perform the alignments and calculate its result rank, which may have draws. Stage two was created to help solving this problem, where a classifier-based filter is applied on the results, reducing the number of draws. A new methodology for evaluating QBP is presented, aiming to understand the impact of the number of notes in the query and some alternatives to verify the robustness of the methods considering wrong and missing notes. In addition, a new method based on machine learning (ML) is presented to filter the results. In the experiments this method always improves the performance of queries regardless of the noise.
In addition to digital text, the digital image has become an important data type used to support digital humanities research. An image retrieval system based on automatic image annotation (IRS-AIA) is therefore developed based on Mask R-CNN to enhance image retrieval performance through enriching the metadata of an image by the recognized object labels for digital humanities research. In contrast, the metadata of an image manually determined by a human is called subjective labels. To verify whether IRS-AIA can assist digital humanists in the effectiveness of image interpretation, counterbalanced design in quasi-experimental research was applied. A total of 14 users were divided into two groups to complete the designed image interpreting tasks by operating IRS-AIA and general image retrieval tool (GIRT) through crossing the use of two systems. Analytical results show that IRS-AIA could effectively assist users in interpreting image situations. The use of IRS-AIA could acquire better image retrieval precision and recall rates than GIRT. However, the image interpretation effectiveness between the use of GIRT and IRS-AIA does not achieve a statistically significant difference. Subject label and object label present distinct purposes in image retrieval and both labels could satisfy users' needs for different information retrieval ways.
In the present information age, providing correct information to the query with a limited concept is a challenging task. The modern adaptive Information Retrieval (IR) system is needed to provide valid information. Ontology-based on IR can provide a better solution. Ontology is a perception of shared conceptualization, represented by classes, properties, and objects. Ontology mapping offers a solution to integrate inter-domain knowledge. This paper presents a method for using ontologies to handle inter-domain query in information retrieval. Using ontology in IR for Query Expansion (QE) and document ranking seems to be the ultimate goal. The Multi-Domain Specific Ontology Mapping (MDOM) proposes the concepts derived from ontology for query expansion and information retrieval. The result shows the improvement in terms of better document ranking.
Information retrieval (IR) is a field of study that focuses on gathering, organizing, storing, analyzing, and accessing information. It is used in all applications where searching for information is required through the internet. Selection and ranking of relevant web documents to satisfy users’ information need in response to users’ queries is the most important task associated with IR. To better comprehend the user’s intent and create a more efficient query, the initial query has been reformulated by adding new terms. The semantic text-similarity techniques proposed by researchers in the past demand a significant amount of trained labeled data as well as user intervention; both of them are scarce, hard to capture, and difficult to maintain. In general, these techniques ignored contextual information as well as word order, which causes the problems like data sparsity and latitudinal explosion. Deep learning techniques are now being utilized to find text similarities. Various sentence embedding models exist today that can use vectors to represent complete sentences and their semantics. This helps the search engine to comprehend the context, intent, and different aspects of the user text. So, in this paper, a sentence embedding model based query reformulation (QR) has been proposed for improving document ranking performance using a universal sentence encoder (USE) and a cosine similarity measure. Four standard datasets CACM, CISI, ADI, and Medline are used to perform all the experiments. The outcomes demonstrate the superior performance of the USE-based QR system over the SBERT sentence embedding model by 4.48 %, 5.97%, and 7.2% and 2.1% for ADI, CISI, CACM, and Medline datasets respectively.
This paper introduces a novel approach for document re-ranking in information retrieval based on topic-comment structure of texts. While most information retrieval models make the assumption that relevant documents are about the query and that aboutness can be captured considering bags of words only, we rather consider a more sophisticated analysis of discourse to capture document relevance by distinguishing the topic of a text from what is said about the topic (comment) in the text. The topic-comment structure of texts is extracted automatically from the first retrieved documents which are then re-ranked so that the top documents are the ones that share their topics with the query. The evaluation on TREC collections shows that the method significantly improves the retrieval performance.
This studies paper explores how synthetic Intelligence (AI) technologies may be used to improve the efficiency of information retrieval from huge databases, as applied in a diffusion of industries. The paper will discuss the concept of AI as a tool to reduce information complexity, at the same time as inspecting current studies projects inside the subject and unique programs for the enterprise and patron worlds. It'll then discover the demanding situations that get up from applying AI and massive records technology, and description a suggested framework for first-rate practices. The paper will recall how AI may be utilized to enhance information seek outcomes in real-time, and provide an explanation for the diverse ways wherein this might be carried out. Subsequently, the paper will discuss the capability implications of the usage of AI for large statistics Retrieval efficiency, as well as its destiny applications.
Advances in information technologies and changes in scientific, social and technical spheres have revealed new opportunities for the development of human society. These opportunities known as “Digital Economy” and “Industry 4.0” require significant changes in both content and process of university education. To prepare successful graduates educators should align themselves with the 4th industrial revolution featured by remote employment and transferring of human labor functions to cyber-physical systems with artificial intelligence components. As a result, the interaction of human intelligence and the formal systems is increasing; new human-machine interfaces and formal knowledge representation models appear. Therefore, there is a need to study information transformations during human-machine interaction. The paper describes the study of semantic transformation influence on subject area representation using professional multilingual search queries in general-purpose search engines and scientometric databases. By qualitative and quantitative analysis, some linguistic patterns were distinguished and data retrieval skills development guidelines were provided.
From the last few years, database of digital images has advanced substantially along with the techniques for image processing. Today, the databases of digital image are found in an increasing number, that provide useable and effective access to image collections. Image databases are becoming larger and more prevalent as a result of the Internet’s spread and the accessibility of optical imaging technologies like digital camera systems and scanning of images, necessitating the development of image retrieval methods that are more productive and useful. The research focuses on feature’s selection for extracting them in view to enhance the result of content-based image retrieval system. Identification of image features, corelating them on the basis of their effects, and the influence of these factors on retrieval are all part of this process. Low-level visual features that address more detailed perceptual components of visual data are observed along with high-level features that underpin in image retrieval techniques. As a result, the research is attempting to review these elements for improving the efficiency of CBIR search results. Further, in order to recognize the wider conceptual features of visual data, various features can be integrated with one another.
This research aims to systematically analyze the causes of the digital divide among the elderly and propose a strategy that can bridge the digital divide among the elderly at three levels in an integrated way by integrating online and offline resources to improve the Internet access, usability and overall cognitive level of the elderly, based on the design and implementation of an elderly-friendly information system. Trial results show that the system can improve the elderly's ability to use mainstream applications and knowledge level of mobile Internet and help them to gradually cultivate information literacy, so that they could integrate into the information society in a better way.
The increasingly innumerable network information makes the way for traditional people to find information become inefficient and tedious, while the personalized information service system let the information come to people, which is an efficient competition tool in the information explosion era. However, the current user's interest in mining or personalized information recommendation technology is still of one insufficiency or another, so the personalized information recommendation technology needs to be studied more in depth. Ontology as a domain model provides people's common understanding of the concept of domain and of the conceptual hierarchy, meanwhile its applications effectively reduces its dependence on natural language understanding technology. This paper proposes an ontology-based personalized information recommendation method; while maintaining the advantages of the currently existing personalized recommendation technology, it increases the semantic information, improving the accuracy of information recommendations, to make the personalized recommendation technology more perfect.
The Levenshtein's channel model for substitution errors is relevant in information retrieval where information is received through many noisy channels. In each of the channels there can occur at most t errors and the decoder tries to recover the information with the aid of the channel outputs. Recently, Yaakobi and Bruck considered the problem where the decoder provides a list instead of a unique output. If the underlying code C ⊆ F2n has error-correcting capability e, we write t = e + ℓ, (ℓ ≥ 1). In this paper, we provide new bounds on the size of the list. In particular, we give using the Sauer-Shelah lemma the upper bound ℓ + 1 on the list size for large enough n provided that we have a sufficient number of channels. We also show that the bound ℓ + 1 is the best possible.
Information Retrieval (IR) is a significant area in web mining where the users procure their needed information from the web. The documents in the web are in the form of web pages. As there is a high usage of data in the web, researchers are intended to do research in the field of IR. Web query classification and web page classification are leading major role to classify the web documents in IR using classification algorithms, based on the user query. The classified documents are indexed using ranking algorithm. This research paper aims to study about various methods of IR to classify and index the relevant web pages respectively.
With the rapid development of cloud computing, the information increase rapidly. Cheap cloud storage and computing power accelerate the development of big data, and make the big data information collection and information retrieval become necessary. More than 50% of the big data is unstructured data, so it is stored in the form of file for the most part. Big data is divided into many blocks what stored in the server with some corresponding metadata of storage on the master server. How to collect the big data web and keywords and retrieve the information are been discussed in this paper.
This paper proposes an efficient information monitoring method in large-scale network, which adjusts its observation intervals according to the information sources' update intervals considering user utility decrease and monitoring cost. The usefulness of the proposal is shown by using computer simulations while the observation cost is neither extremely emphasized nor neglected.
With the increasing scarcity of resources, the need for energy efficient solutions has reached its pinnacle. Vehicular ad hoc networks (VANETs) is no exception and there is a need for exclusive solutions for energy efficient data dissemination and information retrieval. The proposed scheme classifies the vehicles into different types based on their storage and computation capacity. Cluster heads (CHs) are elected for each energy zone according to type of vehicle, and act as information center. Apart from CHs, the dissemination of data and access of information in efficient and correct manner is made possible with the help of vehicles having high processing capability. The broadcast storm problem that consumes high energy in terms of bandwidth, is solved using proposed game theoretic decision model. The proposed model gives suggestion to vehicles whether to disseminate the information or not based on the current energy situation of the network. Simulation results prove that the proposed model saves energy and time by minimizing the overhead and delay. The network energy is efficiently utilized by achieving higher packet delivery ratio and lower delay than other state-of-art protocols.
Using a multi-agent paradigm in an RFID-based health information system is a very promising approach in many domains such as emergency situations, supervising the medication of elderly people and even automatic diagnosis. We propose a hybrid multi-agent system, which uses both mobile and static agents for retrieving various medical records of patients who undergo medical investigations in different medical units. This paper presents an analytical and experimental evaluation of the algorithm applied in the information retrieval process, in order to reveal the parameters that play a role in the performance improvement of the hybrid multi-agent system.
This article presents the analytical and retrieval potential of visualization maps. Obtained maps were tested as information retrieval (IR) interface. The collection of documents derived from the ACM Digital Library was mapped on the sphere surface. Proposed approach uses nonlinear similarity of documents by comparing ascribed thematic categories and thereby development of semantic connections between them. For domain analysis the newest IT trend - Cloud Computing was monitored across time period 2007-2009. Visualization reflects evolution, dynamics and relational fields of cloud technology as well as its paradigmatic property.
In the field of cross language information retrieval, how to translate the query into the target language, namely query translation, is a fundamental problem. Because of the ambiguity phenomenon, query translation is always a challenge. Existing researches always rely on mining the text information, such as the contextual relationship or word occurrence. Different from existing research efforts, in this paper, we address the query translation issue by mining the visual information of images, and a new query translation method based on visual information (QTVI) is proposed. QTVI has three steps: image search, image set denoising, and translation candidate selection. In step 1, the query and candidate translation are associated with corresponding image set via image search. Since the resulted image sets from step 1 may be unclean, in step 2, we de-noise the image sets via clustering strategy. Finally, in step 3, the final translation is selected from candidates by constructing multi-class classifier based on cleaned image sets. Empirical experiments show that QTVI outperforms Baidu Translation and Google Translation for the query translation task.
Rough set theory is a useful mathematical tool that deals with vagueness and uncertainty in data. It has been applied to many computer scientific fields, such as data mining, machine learning, pattern recognition, and expert systems. The main objective of this paper is to investigate the applications of rough set theory in the field of information retrieval. By classifying and analyzing the existing approaches with regard to this topic, the advantages of using rough set theory become clear. Using rough set approach enables us to improve the information retrieval system performances in terms of document ranking, recall level and may provide more user oriented search strategies. Possible improvements are suggested as potential research directions.
In this paper, the algorithm for a novel image retrieval scheme to retrieve images is presented. We address the unique algorithm to extract the colour pixel features by the HSV colour space and the texture features of Mpeg-7 Edge Histogram Descriptor .The proposed scheme transfers each image to a quantized colour code using the regulations of the properties in compliance with HSV model and subsequently using the quantized colour code along with the texture feature of Edge Histogram Descriptor to compare the images of database. We succeed in transferring the image retrieval problem to quantized code comparison. Thus the computational complexity is decreased obviously. Our results illustrate it has merits both of the content based image retrieval system and a text based image retrieval system.
Virtual Reality (VR) has the potential to transform the way we work, rest and play. This promise comes with new challenges. One challenge stems from the interactive nature of immersive Virtual Environments (VEs). Placement of contextual information in VEs can be critical to the user experience. This poster describes the use of eye-tracking to alleviate usability issues surrounding information presentation in immersive VEs. Results from our experiments show that integrating eye tracking into a VE to dictate where and when textual information is presented can improve performance when searching for contextual information. In summary, the results show improvement in task performance when the new direct method is employed to reveal information in target regions based on gaze. This seems to hold true independent of the VE or the type of information questioned.
This paper presents a review of online systems for content-based medical image retrieval (CBIR). The objective of this review is to evaluate the capabilities and gaps in these systems and to determine ways of improving relevance of multi-modal (text and image) information retrieval in the iMedline system, being developed at the National Library of Medicine (NLM). Seven medical information retrieval systems: Figuresearch, BioText, GoldMiner, Yale Image Finder, Yottalook, Image Retrieval for Medical Applications (IRMA), and iMedline have been evaluated here using the system of gaps defined in [1]. Not all of these systems take advantage of the visual information contained in biomedical literature as figures and illustrations. However, all attempt to extract metadata about the image from the full-text of the articles and retrieve figures/images in response to a query. iMedline aims to advance the state-of-the-art in multimodal information retrieval by unifying image and text features in computing relevance. We discuss the shortcomings of these current systems and discuss future directions and next steps in iMedline toward context-based medical image retrieval.
This article introduces the shortcomings of the current search engine. A personalized information retrieval system model based on 3D image vision technology is presented. This paper proposes a personalized information search architecture based on intelligent agent. The method uses the upper computer to calculate and operate, the bottom computer to interact with the user and display the query results. The general framework of the software and the concrete implementation scheme of each function module are given. In this method, a semantically based association model is constructed using the existing markup data, and it is integrated with the visual characteristics of the image. Thus, the correlation feedback is efficiently completed on two levels. The experimental results show that the method is fast, accurate and reliable.
Open-domain Question Answering (QA) is a crucial task in natural language processing. QA systems typically follow two main steps: (i) identifying relevant passages and (ii) generating answer sentences from these passages. Among these steps, identifying relevant passages poses a greater challenge and requires further refinement. In this paper, we introduce two novel strategies to improve the performance of this step, including: (i) a new method for computing the similarity between questions and text passages, and (ii) the integration of pretrained and fine-tuned models. Empirical evaluations conducted on the Zalo 2022 dataset demonstrate the efficacy of our proposed methods, manifesting a notable 10% increase in recall compared to using the BM25 method alone, and a 6% increase in recall compared to relying solely on a fine-tuned cross-encoder model.
Three-dimensional scene information retrieval has became an important research topic in the subject of Mobile Robot Vision and is useful for many applications, such as Mobile Robot Navigation and Mobile Robot target tracking. This paper presents a 3D information retrieval system for Mobile Robot Vision, which relies on the spherical compound eye structure to get a spherical camera image acquisition platform. Novelties of our work are the spherical camera platform for image acquisition and the 3D information retrieval methods. The spherical camera image acquisition platform, which is based on the spherical compound eye structure, has advantages of a large field of view, multi-channel and high sensitivity. The 3D information is recovered by image feature extraction and tracking, information restored and surface optimization. The paper uses the SIFT algorithm, the ANN algorithm and the Ransac algorithm to extract and track features, adopts the SfM method and the PMVS method to get 3D scene information. In additional, the Poisson surface information retrieval method is adopted to optimize the 3D information retrieval result. Experiments results prove a great advantage of real-time and accuracy compared with other systems.
The research in this paper delivers concise information that contains answers to user questions. This paper reviews and compares three main QA approaches based on natural language processing, information retrieval and question templates by black-box approach using a classification scheme and scoring mechanism to assess and rank three example approaches.
Based on the information technology competence standards of foreign vocational teachers, this paper analyzes in detail the information technology content of vocational education teacher training in China, and combines the "National Training" project carried out by Gansu Province to provide information technology skills for 300 vocational teachers in cities and counties of Gansu Province. The investigation was carried out, and the principal component analysis method was used to analyze the factors of 42 competences. Finally, six competency factors were extracted, and the content system of information technology competency training for vocational teachers was constructed. The differences in information technology ability of vocational teachers in different backgrounds were analyzed. Finally, relying on the "National Training" project to conduct an empirical analysis of the training content system and verify the training effect.
By introducing cloud model, this paper presents a re-ranking method which improves the accuracy of the IR (information retrieval) while recall is preserved. It is rare in traditional Chinese information retrieval to consider uncertainty while calculating the related degree of the query and each document in the result set. This paper researches IR in a perspective of uncertainty by introducing cloud model, measures the relevance between the query and document by the uncertainty degree that using document represents the query, and then re-ranks the result set. Experiments on NTCIR-5 (the 5th NII Test Collection for IR Systems) document collection for SLIR (Single Language IR) show that this method achieves an 18.08% and 26.50% improvement comparing to the initial retrieval method without any re-ranking.
Horizon scanning is being increasingly regarded as an instrument to support strategic decision making. It requires the systematic examination of information to identify potential threats, emerging issues and opportunities to improve resilience and decrease risk exposure. Horizon scanning can use the Web to augment the acquisition of information, though this involves a search for novel and emerging issues without knowing them beforehand. To optimise such a search, we propose the use of relevance feedback, which involves human interaction in the retrieval process so as to improve results. As a proof-of-concept demonstration, we have carried out a horizon scanning exercise which showed that our implementation of relevance feedback was able to maintain the retrieval of relevant documents constant over the length of the experiment, without any reduction. This represents an improvement over previous studies where relevance feedback was not considered.
Lossy compression on scientific data is coming into prominence as the scientific workflow is hampered significantly by large amounts of data produced by high-performance computing (HPC) applications. State-of-the-art lossy compressors, such as SZ and ZFP, show promising rate-distortion efficiency. However, as the data storage burden and need for feature-preserving compression continue to grow, relying on unitary or single-stage compression is becoming insufficient for obtaining desirable data reductions and feature preservation. This paper aims to improve the compression ratio by taking advantage of information retrieval (IR), a well-established topic but underexplored in lossy compression for scientific data. We propose our lossy compression technique, called DPZ, based on multistage feature extractions, a commonly employed step in IR. Unlike the prior works where the compression is either done by predicting or bit-plane encoding, this work focuses on preserving the key data content from each stage to the maximum extent, ultimately elevates the compression ratio. With the application of discrete cosine transform, principal component analysis, and quantization, DPZ obtains the dominant features with the least amount of bits possible. Specifically, a knee-point detection and an explained variance variation method are designed for finding optimal tradeoffs. DPZ also employs a sampling strategy to reduce computational overhead and estimate compressibility and parameters before compression. We evaluate the performance of DPZ using real-world scientific datasets. Experiments demonstrate that DPZ achieves superior compression ratios through multi-stage retrievals and outperforms SZ and ZFP at medium to high accuracy on most of the evaluated datasets.
We propose the “Application Architecture” which realizes user-centric information discovery and retrieval. We call applications based on this architecture “Personal Information Style Applications”. Quite different from the concept of Universal Design, architecture provides customized information technology usage environments. We show the concept of Personal Information Style Applications and implementation approaches. We also discuss the basic technologies needed. Finally, we explain four use cases in order to elucidate the basic technologies.
In this paper, we present information retrieval as a powerful tool for addressing an imperative problem in the field of statistical machine translation, i.e., improving translation quality when not enough parallel corpora are available. We devise a framework, which uses information retrieval to create a synthetic corpus from the easily available monolingual corpora. We propose an improved unsupervised training approach with a data selection mechanism, which selects only the most appropriate sentences, thus reducing the amount of data, which is less related to the domain in the additional bitext. We also introduce a new method to choose sentences based on their relative similarity/difference from the query sentence. Using the synthetic corpus created by our method, we are able to improve state-of-the-art statistical machine translation systems.
Using either stems or roots as index terms offered considerable performance to Arabic Information Retrieval (IR) systems compared to the use of surface words for indexing. Many comparative works tried to find out the best from these two indexing approaches but until then, no of the two methods widely overtook the other. Each of the two index types performed better under different test circumstances in terms of recall and precision. In this paper, we propose a hybrid approach combining the two indexing units in a way we take the advantages from both of them and try to overcome their shortcomings. Then, based on some combining techniques, we assign a weight for each indexing unit and try to find out the best weighting values.
Information searches based on expert-seeking technology can prove time-consuming or unsuccessful if search terms do not turn up extrinsic identifiers in profiles and saved documents. In many such cases, knowledge brokers function as "humans in the loop," providing intrinsic enterprise knowledge to mediate between information seekers and expert sources-a fact that future collaborative information-seeking system designs should take into account.
Several deep supervised hashing techniques have been proposed to allow for querying large image databases. However, it is often overlooked that the process of information retrieval can be modeled using information-theoretic metrics, leading to optimizing various proxies for the problem at hand instead. Contrary to this, we propose a deep supervised hashing algorithm that optimizes the learned codes using an information-theoretic measure, the Quadratic Mutual Information (QMI). The proposed method is adapted to the needs of large-scale hashing and information retrieval leading to a novel information-theoretic measure, the Quadratic Spherical Mutual Information (QSMI), that is inspired by QMI, but leads to significant better retrieval precision. Indeed, the effectiveness of the proposed method is demonstrated under several different scenarios, using different datasets and network architectures, outperforming existing deep supervised image hashing techniques.
In recent years, the integration of researches in Computer Science and medical fields has made available to the scientific community an enormous amount of data, stored in databases. In this paper, we analyze the data available in the Parkinson's Progression Markers Initiative (PPMI), a comprehensive observational, multi-center study designed to identify progression biomarkers important for better treatments for Parkinson's disease. The data of PPMI participants are collected through a comprehensive battery of tests and assessments including Magnetic Resonance Imaging and DATscan imaging, collection of blood, cerebral spinal fluid, and urine samples, as well as cognitive and motor evaluations. To this aim, we propose a technique to identify a correlation between the biomedical data in the PPMI dataset for verifying the consistency of medical reports formulated during the visits and allow to correctly categorize the various patients. To correlate the information of each patient's medical report, Information Retrieval techniques have been adopted, including the Latent Semantic Analysis technique suitable for constructing a concept space on patient information. Then, patients are grouped and classified into affected or not by using clustering algorithms according to the similarity of medical reports projected in the concept space. Results revealed that the proposed technique reached 95% of effectiveness in the classification of patients.
The performance of biomedical information retrieval greatly depends on biomedical knowledge; however the knowledge of available medical knowledge base is often incomplete and out-of-dated. To solve the problem that incomplete knowledge bases cannot provide the medical knowledge required for biomedical information retrieval, the paper proposes an Entity Relation Aware Graph Neural Ranking model (ERAGNR), aiming to fully leverage the internal knowledge of the document to alleviate the problem caused by incomplete external knowledge bases. ERAGNR mines the relationships between biomedical entities in the document through entity relation extraction and combines them with external knowledge. It increases the semantic association and reduces the semantic gap between the query and the document. The method first constructs a knowledge-query graph and a document-entity graph, and then fuses the two graphs to obtain a knowledge-query-document-entity graph. In a multi-task learning framework that combines text retrieval and relation extraction tasks, ERAGNR employs a shared text encoder and a graph neural network. This enables ERAGNR to learn semantic matching patterns between queries and documents and recognize relationships between entities in the documents. As a result, the model can capture semantic matching signals between entity relationships in the context and queries. The experimental results show that ERAGNR outperforms the state-of-the-art models. Through biomedical relation extraction task, the model can learn the ability to capture the context of the entity relations in the document, so that the model can more accurately match the semantics between the query and the document.
For the effective information retrieval models, traditional unsupervised cross-modal hashing methods try to learn hash functions from the underlying structure, distribution, and topological information of the data in order to preserve the original data spatial structure. Hence, to conduct more efficient analysis, this research study proposes a novel redundant data retrieval model based on parallel batch algorithm. Batch parallel proofreading is oriented to large-scale text offline proofreading scenarios, and has powerful computing power provided by the cluster computing. With this operation, the data will be pre-processed to better clean the row data. Then, the hash model is integrated to consider the key as a divisor, the dividend as a maximum prime number p not greater than the length of the hash table m, and uses the remainder of the calculation result as the final hash function. Furthermore, the model is revised to construct the efficient data retrieval task. Through testing, the performance of the proposed algorithm is proven to be efficient.
Concept based search is a method that enhances information retrieval systems using semantic relationships. The recall in concept based search is relatively low. That low recall comes from the fact that it is not easy to represent a concept completely. Query expansion intends to fill a gap because concept representation is always partial. Query expansion improves the recall. In this paper we present an expansion method for a concept based information retrieval. Our method uses semantic relatedness to extend user query through an undirected graph of concepts.
Stemming has shown to be effective in many natural language processing (NLP) applications such as in document classification, machine translation, and information retrieval (IR). This paper compares the performance of nine stemmers for Arabic language on microblog IR. These stemmers include: Information Science Research Institute (ISRI), Tashaphyne, Khoja, AL-stem, Light10, Motaz, Assem, Farasa, and ARLStem. Each stemmer was studied independently using the EveTAR dataset on a specific information retrieval task to obtain relevant query tweets. The performance of the nine stemmers was evaluated using BM25, precision at 30, and Mean Average Precision (MAP). The results show that root-based stemmers (i.e. ISRI and Khoja) outperformed others.
Nowadays, Content Based Image retrieval (CBIR) has been one of the most interesting research areas among researchers of technology-giants and academic institutions. Various CBIR techniques in literature show that there are three key properties in image retrieval viz. color, texture and shape. In this paper a detailed classification along with the challenges and issues of CBIR techniques evolved during recent years covering various methods for segmentation, feature extraction, indexing and feature transformation are described. Also, a technique of combining feature transformation with clustering algorithm is proposed for efficient image retrieval.
Aim of document similarity is to retrieve the list of ranked documents based on a query document. In the literature, it has been shown that document similarity method first compute similarity scores between the query and the documents based on a score function. Thereafter, documents are ranked according to their similarity scores. In the literature, TextTiling algorithm shown was basically done in three stages: tokenization into sentence-sized units, score calculation and detecting the boundaries of the subtopic. In this paper, we have evaluated two different approaches for documents ranking and further we checked this achieved results with other approach based on machine learning Firstly, Documents are ranked based on standard score calculation i.e using the tf-idf concept. Secondly, Documents are ranked based on Textiling approach. TextTiles have already been integrated into a user interface in an information retrieval system and have been used successfully for segmenting Arabic newspaper texts, which have no paragraph breaks, for information retrieval. Textiling is a far better technique than the standard score calculation when we try to summarize the documents. This helps in the improving the retrieval performance of the system. In this paper, we have shown the comparative analysis of two approaches. Further, we also tried to incorporate the statistically machine learning approach in our results, for handling a broad range of problems in information retrieval (IR).
This study considers the solution of the scientific and practical problem of creating automated expert information support systems for decision-making in the field of project team formation in a safety-oriented system (SOS). The purpose of the study is to increase the efficiency of formation of project teams of safety-oriented systems by HR services, as well as to improve the procedure of recruitment and selection of personnel using modern information technologies. A conceptual model-scheme of the life cycle of the development of a specialized intelligent recruitment system for higher education applicants with special training conditions based on the index method of candidate evaluation is proposed. The structure and architecture of the information product are formed. The system consists of many components of HRIS support, which is developed for the information needs of the experts of the admission campaign of higher education institutions with specific learning conditions with subsequent management decisions. Automation of the selection process will provide rapid collection, processing, analysis and reproduction of information, development of human resources, improving the quality of education of applicants in higher education institutions with specific learning conditions. Information systems provide high speed data processing, information retrieval, recruitment optimization and candidate analysis. Automation of HR processes helps to reduce the complexity and cost of resources to perform tasks. The expediency of using artificial intelligence during the heuristic activity of experts is substantiated. The use of information resources, development and integration of information system functionality into a single module to optimize management and decision-making will allow the organization to more quickly and efficiently implement strategic goals and mission. New models, methods and technologies for processing materials and data of applicants for their evaluation in o... (Show More)
Text retrieval is the main means for people to obtain information from a huge text database. Current document retrieval methods do not consider the association between words and context, and cannot effectively retrieve information based on semantics. Aiming at the semantic feature extraction of text retrieval, this paper uses Bert, DeepCT and KNRM and its derivative model to improve the construction and feature extraction of similarity matrix, and proposes a text retrieval model based on DeepCT and Conv-KNRM. The goal is to give appropriate word weights to different keywords and improve the accuracy of text retrieval. In this paper, through experimental comparison, it is proved that this model improves the accuracy of text retrieval.
Information Retrieval-based Bug Localization (IRBL) and Mutation-based Fault Localization (MBFL) are two widely used static and dynamic fault localization techniques, respectively. IRBL takes less time and utilizes more static information of software, while MBFL achieves high accuracy and the results are not easily affected by coincidental correctness test cases. However, the granularity of IRBL is coarse and MBFL consumes a lot of time to generate and execute mutants. In this paper, we propose IRMBFL (Information Retrieval and Mutation Analysis Based Software Fault Localization), a software fault localization technique that combines information retrieval and mutation analysis. First, the suspiciousness of source code files is measured by calculating the text similarity between the bug report and the source code to extract the files which may contain bugs. Then, the extracted files are mutated and tested. Finally, the bug statements are located by analyzing the changes in the execution results of the test cases. The experiments are conducted on the Defects4J dataset and Einspect@n and EXAM are used as evaluation metrics to evaluate the performance of IRMBFL. The experimental results show that IRMBFL locates 14 and 3 more bug statements than BugLocator and Metallaxis for Einspect@n when n=1. IRMBFL outperforms BugLocator on all projects and outperforms Met-allaxis on 2 out of 6 projects in terms of EXAM. In addition, the average bug localization time overhead of IRMBFL is reduced from 73.87% to 99.78% than Metallaxis.
In this paper, we compare the performance of different lemmatization approaches for information retrieval over Turkish text collection. A lemma is simply the "dictionary form" of a word and lemmatization is the process of determining the lemma for a given word where different inflected forms of a word can be analyzed as a single item. We compared three different lemmatizer and one fixed length truncation approaches over Turkish text collection. The first one is based on morphological analyzer for Turkish using with finite state language processing technology; another one is Dictionary-based Turkish Lemmatizer (DTL), which uses radix-trie data structure; the third one is a simple dictionary based top-down parser and the last one is truncation of words at fix length. We have assessed the performance of lemmatizers on Bilkent University Milliyet collection, which contains more than 400K documents. The comparison of performance analysis was done by the well-known IR evaluation metrics and experimented in the IR system. The results we obtained show that the lemmatization process improves IR performance and we achieved the best results using with Turkish Lemmatizer that is DTL radix-trie data structure and it used the minimum number of terms in IR system.
NA
Traditional Information Retrieval (IR) models are based on bag-of-words paradigm, where relevance scores are computed based on exact matching of keywords. Although these models have already achieved good performance, it has been shown that most of dissatisfaction cases in relevance are due to term mismatch between queries and documents. In this paper, we introduce novel method to compute term frequency based on semantic similarities using distributed representations of words in a vector space (Word Embeddings). Our main goal is to allow distinct but semantically related terms to match each other and contribute to the relevance scores. Hence, Arabic documents are retrieved beyond the bag-of-words paradigm based on semantic similarities between word vectors. The results on Arabic standard TREC data sets show significant improvement over the baseline bag-of-words models.
Distributed estimation has attracted great attention in the last few decades. In the problem of distributed estimation, a set of nodes estimate some parameter from noisy measurements. To leverage joint effort, the nodes communicate with each other in the estimation process. The communications consume bandwidth and energy resources, and these resources are often limited in real-world applications. To cope with the resources constraints, the event-triggered mechanism is proposed and widely adopted. It only allows signals to be transmitted if they carry significant amount of information. Various criteria of determining whether the information is significant lead to different trigger rules. With these rules, the resources can be saved. However, in the meanwhile, some inter-event information, not that important but still of certain use, is unavailable to the neighbors. The absence of these inter-event information may affect the algorithm performance. Considering this, in this paper, we come up with an inter-event information retrieval scheme to recover certain untransmitted information, which is the first work doing so to the best of our knowledge. We design an approach for inter-event information retrieval, and formulate and solve an optimization problem which has a closed-form solution to acquire information. With more information at hand, the performance degeneration caused by the event-triggered mechanism can be alleviated. We derive sufficient conditions for convergence of the overall algorithm. We also demonstrate the advantages of the proposed scheme by simulation experiments.
The major problem we will solve in this paper is how to bring the user closer to his/her search needs and how to enhance the search query prior submitting it to the search engine. To accomplish this, we built a query paraphrasing model using Genetic Algorithm to produce the most accurate set of paraphrased queries that match the user's need by exploring different query paraphrased combinations using WordNet as a thesaurus. The experimental results show that the proposed system can provide more precision than the regular query paraphrasing.
In this paper, we challenged the traditional way of information retrieval and proposed information cartography in scientific research domain which generates an information map for users to mitigate information overload in current systems. A legible information map embodies not only the results retrieved by a fine-designed mechanism, but also the complex relationships between them. Our framework of information cartography consists of three parts: document representation, network reconstruction and network compression. A novel syncretic representation learning method(SR) was proposed to learn a low-dimensional representation for each document, where a double-layer Skip- Gram model was leveraged to absorb heterogeneous information simultaneously. We carried out a series of experiments on CORA dataset to demonstrate the satisfying performance of SR for tasks such as text classification and link prediction. We also showed the effectiveness of our network compression method in preserving connectivity while optimizing legibility. Finally, pilot user studies manifested that our information map indeed improved the experience of information retrieval.
Classification is a central problem in the fields of data mining and machine learning. Using a training set of labeled instances, the task is to build a model (classifier) that can be used to predict the class of new unlabelled instances. Data preparation is crucial to the data mining process, and its focus is to improve the fitness of the training data for the learning algorithms to produce more effective classifiers. Searching for the frequent pattern within a specific sequence has become a much needed task in various sectors. Feature selection is selecting a subset of optimal features. Feature selection is being used in high dimensional data reduction and it is being used in several applications like medical, image processing, text mining, etc. In the existing work, unsupervised feature selection methods using Artificial Bee Colony Optimization Algorithm, Bat Algorithm and Ant Colony Optimization have been introduced. We have compared these three algorithms and concluded that Bat Algorithm proves to be better in performance than the rest. The proposed system will use a novel method to select subset of features from unlabelled data using Bat algorithm with one of the clustering algorithm and develop an expert information retrieval system.
Due to rarely considering document popularity and personalization issues at the same time in the extraction of semantic features based semantic matching algorithms, the accuracy is low in the field of information retrieval. To solve the problem, an improved personalized information retrieval algorithm DSMN is proposed. DSMN bases on the deep struct semantic model (DSSM), uses independent recurrent neural network (IndRNN) to extract semantic features, and process long sequences. In DSMN, the self-attention mechanism is used to further extract the features, and the semantic similarity is calculated. By combining the semantic similarity with the processed user characteristics and document popularity, the relevance score of the query and the document is calculated to improve the efficiency of information retrieval. Experiments are done on four datasets, and the results show that the performance of DSMN is significantly better than the other state-of-the-art information retrieval algorithms based on semantic matching.
The speed and volume at which misinformation spreads on social media have motivated efforts to automate fact-checking which begins with stance detection. For fake news stance detection, for example, many classification-based models have been proposed often with high complexity and hand-crafted features. Although these models can achieve high accuracy scores on a targeted small corpus of fake news, few are evaluated on a larger corpus of fake and conspiracy sites due to efficiency limitations and the lack of compatibility with the actual fact-checking process. In this article, we propose a practical two-stage stance detection model that is tailored to the real-life problem. Specifically, we integrate an information retrieval system with an end to end memory network model to sort articles based on their relevance to the claim and then identify the fine-grained stance of each relevant article towards its given claim. We evaluate our model on the Fake News Challenge dataset (FNC-1). The results show that the performance of our model is comparable to those of the state-of-the-art models, average weighted accuracy of 82.1, while it closely follows the real-life process of fact-checking. We also validate our model with a large dataset from a real-life fact-checking website (i.e., Snopes.com), and the findings demonstrate the capability of the model in distinguishing false from true news headlines.
In recent years, to encourage citizens to use the E-Government system more widely, the number of innovative research topics in this area has grown tremendously to bridge the information gap between citizens and the government. However, recently, concerns have been increasing, especially in developing countries, as to how to simplify the submission of a request for an E-Government service and how to link these requests to build an online application without asking the citizen to input the data more than once or run around to submit their information in person. Thus, the challenge for the Government in this situation is to provide an e-system which is fast and simple for the citizen to use. This research proposes a method for simplifying the information retrieval process from the E-Government system. This method uses Natural Language Processing (NLP), concept indexing and data integration methods for data analyzing and building. To assist the retrieval of information, the government concept is used to match everyday language to the formal language which is used in the government's E-Documents. As well, the mapping is based on the enriched citizen's requests to structure a multi-layered E-Document. To demonstrate its usefulness, we applied our methodology to sample data of an online request for a vehicle registration procedure.
Relevance is one of the most interesting topics in the information retrieval domain. In this paper, we introduce another method of relevance calculation. We propose to use the implicit opinion of users to calculate relevance. The Implicit judgment of users is injected to the documents by calculating different kinds of weighting. These latter touch several criteria like as user's weight in the query's words, user's profile, user's interest, document's content and the document popularity. In this method, each user is an active element of the system, he searches documents and he makes treatments to provide relevant information to other users in the Network. This is similar as the peer-to-peer systems; unlike that, an element (user) have to manage automatically his data by creating a short view model of his most visited documents, and calculates his relative relevance about each one. The relative relevance is variable according each user, so the final relevance is calculated by the averaging of the elementary relevance of all users. Hence, the name of collaborative relevance.
Cross Language Information Retrieval (CLIR), is the process of retrieving relevant documents, where in the language of the given query is different from the language of the retrieved documents. CLIR systems allow the users to search and access documents in the language different from the language of the search query. CLIR systems have been divided into Monolingual CLIR, Bi-lingual CLIR, and Multilingual CLIR based on different languages of query and documents. The first step of the Cross Language Information Retrieval system is the text pre-processing of given text documents in to useful representations. Pre-processing is the set of tasks that convert the given text documents into a suitable format for any higher-level text related applications. This technique can be used to reduce the computational process, noise data, and irrelevant information from the given text documents. This paper discusses in detail the different pre-processing techniques such as dataset creation, tokenization, noise removal, stop word removal, stemming, lemmatization and finally term weighting of two languages dataset (i.e., Tamil and Malayalam), which is manually collected from BBC online website. Finally, the study investigates feature extraction techniques of Term Frequency- Inverse Document Frequency (TF-IDF). These techniques will help to design and model CLIR systems with high performance.
In recent years, medical images have become a constant part of our life. It is hard to imagine making a medical diagnosis without basic medical imaging apparatuses (CT, MRI, USG, etc.). In order to fully utilize its potential, the image data must be stored in the proper way, particularly so that it is easily accessible. The latter is particularly important in a medical diagnostics - it is desired that a physician should have quick access to the comparative image data. In this context the authors present the possibility of using linguistic approach to modeling of coronary arteries in semantic techniques of image retrieval. The proposed methodology takes into account, among others, the basic topologies of arteries (balanced artery distribution, right dominant or left dominant) as well as individual sections or ramifications of these arteries. For the purpose of analysis and indexing these visualizations appropriate graph and sequential grammars are proposed.
Current expectations from nowadays information retrieval systems (IRS) have grown beyond the “document contains these terms” requirement that was considered common sense 10–15 years ago. Nowadays systems are expected to return results that are relevant to the intended meaning of the query. In the general IRS usage scenario, the user is not really interested if the returned documents contain or not the words he entered in the query, but the user expects the returned documents to be relevant to the intended meaning of his query. The subject of analyzing the query (expressed as a sentence in natural language) in order to infer the meaning of the query, has spawned lots of research publications in the last 10 years, but without any large impact in the user search experience. We propose a system (the OntoSense User Interface) that guides the user in providing the query directly as a query ontology, thus providing the meaning of the query directly to the IRS. This approach is especially useful in mobile platform information retrieval scenarios where the query sentence is usually too short (very few words are used) for the IRS to match relevant documents.
Multilingual Information Retrieval System (MLIR) provides users to apply query in one language and retrieve the resultant documents in more than one language. Assessing the performance of these systems is a challenging task. Amongst the available metrics Precision is considered as the basic measure for assessing the performance of IR/MLIR systems. In the literatures a few metrics are available to analyse the performance of MLIR System. Therefore in this paper Precision oriented metrics are proposed based on the weight of languages and the results are very promising and demonstrated their existence in MLIR research domain.
The study investigated use of electronic resource by students of College of Business Administration and College of Information Technology. That is, it examined possible factors and problems in their searching habits, information seeking, use and retrieval in satisfying their needs. Therefore, the study focused on information seeking behaviour of students and barriers to utilizing online resources to execute academic tasks.
The web is increasingly moving towards structuring and to taking into consideration of semantics, particularly with XML and ontology. In addition, access to information requires the use of web tools for information retrieval (IR). Many methods from traditional IR were extended to structured documents. On the other hand, approaches have been proposed to regard specifically semantics in structured documents by using external semantic resources while collecting original documents on which it is necessary to provide semantic similarity measures in order to perform comparisons between concepts. Most previous approaches did not take into account the relations between concepts and are not tailored to the specific needs of learners. In this paper a semantic IR system of structured educational documents is proposed and adapted to the needs and preferences of the learner. This approach is based on a representation of the nodes of the document's tree and of the query through the semantic vectors of those concepts. Tests show the feasibility of the proposed approach.
The recent rise in the amount of unstructured data in digital format has given rise to the use of natural language processing techniques to understand the data. Organizations have started recognizing the potential in the unstructured textual data. Data from the internet as well as from the organizations' internal repository can help them to gain more insight into the market. Information collected from such sources provides valuable decision-making probability for the organizations. Keyword-based search is the most helpful form of the document retrieval process. The paper discusses such processes used in the current scenario as well as propose a new form of classification technique to be used for the process of document retrieval.
The methods of query expansion (QE) have achieved significant performance in information retrieval of electronic medical records (EMR). It is a pity that the direct addition of expansion terms may cause query drift, which decreases the precision of EMR retrieval. To solve the issue, we combine the methods of query reformulation and re-ranking to improve the performance of QE in Chinese EMR retrieval. First, the synonyms and hyponyms are extracted from a Chinese medical knowledge graph as expansion terms, and the weights of expansion terms are calculated with the combination of semantic similarities, category weights, and co-occurrence frequencies. Then the query is reformulated by these expansion terms, and the EMR documents are retrieved by the expanded query. Second, four categories of medical terms in top retrieval results are selected, and the performances of all combinations in re-ranking are tested to select the most proper terms for re-ranking. Then the original retrieval results are re-ranked by these terms. Experiments show that the algorithm can promote the effectiveness of EMR retrieval compared with baselines, which shows that the combination of query reformulation and re-ranking can differentiate expansion terms of different categories and maximize their effects.
Image retrieval is based on the description of image content, found in the target image collection with the specified characteristics or with the image of the specified content. The content of the image can be divided into two categories: visual content and information content. The visual content corresponding to the physical representation of the image, such as color, shape, texture, etc. while corresponding image semantic information content, such as the theme, characters, scenes, etc come under the category of information content. The emergence of the theory of sparse representation for computer vision and pattern recognition, and other fields has brought the profound influence. Image sparse representation is a well as a new image description model, the said coefficient of less than zero composition reveals the image signal of the main structure and essential attribute. Efficient representation of the model not only can obtain the observation signal, but also can extract the semantic information. This Topic through the introduction of sparse coding idea retrieval process can be divided into two steps: off-line training and on-line retrieval. By using the sparse dictionary online training algorithm for each image to complete dictionary training, using the image similarity evaluation algorithm based on sparse feature expression of query image and the database image similarity calculation, finally according to the similarity retrieval results, thus the ascension of systemic algorithm accuracy and efficiency.
Knowledge-intensive language applications, like open-domain question answering, call for NLP systems to retrieve related evidence in a document collection. A efficient and scalable retrieval system is an ideal component in these applications. Up-to-data research developed sequence-to-sequence framework on this task, eschewing any format of term-frequency or query-document similarity calculation. This line of work greatly improved the efficient of retrieval but with the sacrifice of scalablity. This work proposes a new framework based on the seq2seq architecture, which can solve scalability issues that exist in such methods. Experimental results on the public dataset Natural Question demonstrate the effectiveness of this approach.
With the rapid development of artificial intelligence and its wide application on the ground, more and more devices is embedded in intelligent question answering technology. The development of mobile Internet technology and the popularization of community platform provide us with huge and jumbled data. Credibility has become an urgent problem to be solved in question answering system. As far as the medical platform is concerned, there is a huge noise problem in the current general medical information retrieval. Based on this, we propose a trusted evaluation mechanism based on the information of the medical community platform. In order to search and select more accurate and credible answers, we propose an optimization method of search ranking based on credible evaluation mechanism. By analyzing the professional competence of information providers, this method trustfully ranks multiple answers in the question-and-answer set of medical communities, which largely solves the interference of “one-question multi-answer” phenomenon in the question-and-answer system on retrieval. At the same time, it uses department classification, subdivides medical directions, and further matches the areas of concern of information providers, so as to improve the accuracy and hit rate of the retrieval system.
The paper considers the spoken document retrieval problem. The proposed method uses a length of the longest common substring (LCS) algorithm for detection of query words. This method based on phoneme recognition of a spoken document. Also this method is compared with a text retrieval methods based on a vector space model. Use of a length of LCS and phoneme recognition improves retrieval effectiveness.
According to whether the program under test is executed, software bug localization methods can be divided into static bug localization and dynamic bug localization. Among them, Information Retrieval-based Bug Localization (IRBL) and Spectrum-based Fault Localization (SFL) are widely used static and dynamic bug localization methods, respectively. But the localization granularity of IRBL is coarse and the localization accuracy of SFL is easily reduced by the information which is unrelated to the bug. In order to refine the localization granularity of IRBL and improve the localization accuracy of SFL, this paper proposes ISBL (Combine Information Retrieval and Spectrum for Bug Localization), a statement-level software bug localization method based on information retrieval and spectrum. Firstly, the suspicious files are filtered using information retrieval technique, and then the suspicious files are used to reduce spectrum information for statement-level bug localization. To evaluate the performance of ISBL, experiments were conducted on the Defects4J dataset, and MRR and TOP@N were used as metrics for evaluation. As the experimental results show, for MRR, ISBL increased 3.0% and 3.1% compared to Ochiai and DStar, respectively; for TOP@1, ISBL locates 4 more bug statements than Ochiai and DStar.
Health depiction and image are indispensable for patient instruction and self-care. Images characterize an imperative feature of the entire medical knowledge. Presently there is hardly any attempt in automatically construction inclusive image bases for universal health information customers. We proposed an approach to automatically construct a consumer -leaning image library contain images of individual organs, diseases, drugs and other medical entities. Health check imaging has become an important tool not simply in document patient presentation and clinical findings, but also considerate and managing various diseases. Image data present tangible visual evidence of disease manifestation. The number of digital images that needs to be acquired, analyzed, classified, stored and retrieved in the medical centers is exponentially growing with the advances in medical imaging technology. The goal of this work we proposed medical image retrieval technique for pathology images that implements recent improvements in feature representation, efficient indexing, and similarity matching.
Web-Ear is an information retrieval system in which anyone can search "What person X told about the topic Y" on the internet or in the database of the system. If the search is made on the internet, first the system submits a query to Google search engine and retrieves a set of information. In order to isolate speech portions of retrieved data, an extraction process is required to be performed according to pre-defined rules set by regular expressions, and only after then we have the data that we want to present to the user.
Software developers produce a significant amount of knowledge, everyday facing a significant amount of engineering challenges and resolving them using a significant number of information resources. Once the task that they are facing is complete and the results of their effort are embedded into the source code this knowledge is very rarely shared with the software development community. As a consequence other software developers when faced with the same or similar problem have to solve it by themselves wasting a significant amount of time and resources every day. This paper proposes a Collaborative Information Recommender (CIR) system which intends to address the problem outlined above and facilitate the exchange of information between software developers. The proposed CIR system not only supports a direct reuse of the code related information found by other developers but also supports the creation and management of very problem focused social and knowledge networks.
Recently, NLP (Natural Language Process) is required in question-answering technology in many fields such as chatbots and AI speakers. Accordingly, companies and communities are conducting research on Open Domain Question Answering (ODQA), which is required to identify which question and answer questions should be used in a vast amount of data sets. In fact, in the case of question-and-answer learning, there are cases in which multilingual methods are included in most cases, but performance can be further improved by using a Korean-only model. Therefore, this paper focuses on information retrieval, which is a method that includes answers to questions in the ODQA Framework and tries to find a method unique to Korean. In this paper, we propose to apply the token for each query to the model using the Korean method using the BM25 (Best Matching) and DPR (Dense Passage Retrieval) models. To verify the above evaluations, KorQuAD v1 dataset is used. Accordingly, Korean BM25 can obtain the highest score for morpheme-based tokenization, and additionally, adding and maintaining other tokens according to the model performed well, and as a result of learning with the same model, DPR has more sub-tokens than the conventional method.
With the development of remote sensing imaging technology, the satellite information includes more abundant data, and the quality of satellite cloud image had achieved great progress. The satellite cloud images contain the valuable information for weather forecasting and early prediction of different atmospheric disturbances such as typhoons, hurricanes etc. The processing of content-based satellite cloud image has become one of the most important tasks for using the satellite cloud image information for continuous research. Content-based satellite cloud image processing and information retrieval is a very important problem in image processing and analysis field. In this research, a Content-Based Image Retrieval (CBIR) system has been developed using color, texture and shape as retrieval features from the satellite image repository. To extract the grey level/color properties of an image, histogram values have been used. Four functions of texture features have used such as (Entropy, Energy, correlation and contrast) and the shape features (area, perimeter and metric) have been extracted using the morphological operations. The images and the extracted feature vectors were stored in the database. Results obtained from the processing demonstrate the usability of proposed system.
This paper presents a knowledge graph-informed smart UX-design approach for supporting information retrieval for a wearable, providing treatment recommendations during emergency situations to health professionals. This paper describes requirements that are unique to knowledge graph-based solutions, as well as the direct requirements of health professionals. The resulting implementation is provided for the project, which main goal is to improve first-aid rescue operations by supporting artificial intelligence in situation detection and knowledge graph representation via a contextual-based recommendation for treatment assistance.
With the rapid development of Natural Language Processing (NLP), text matching has become the basis of many downstream tasks in NLP, and the study of text matching is of great research significance for solving tasks such as question and answer and information retrieval in NLP. Most of the current text matching methods tend to have problems such as mismatch of grammatical structures and insufficient interaction information in sentences. In order to solve the problems of insufficient interaction information and lack of features and ability to capture keyword and sequence information in text matching, this paper proposes a text matching method based on multi-layer coding and soft attention mechanism. The method first embeds the text and sends it to a gating module for processing, then sends the processed result to a module containing a combination of multilayer coding and soft attention mechanism for further operations such as multiple alignment, and finally sends it to a classifier containing a three-layer fully-connected network for predicting whether the input text pairs match or not. The two modules proposed in this paper are practically feasible, and comparison and ablation experiments have been conducted on the publicly available datasets LCQMC dataset and BQ dataset, and the experimental results show that the two modules improve the accuracy of text matching by 2.44% and 1.02%, respectively.
Recently, two improved methods have shown their advantages in browsing Earth Observation (EO) dataset. The first method is the Bag-of-Words (BoW) feature extraction method and the second is the Normalized Compression Distance (NCD) for assessing image similarity. However, they have not been compared so far for satellite image retrieval, which motivates this paper. Two retrieval experiments have been performed on a freely available optical image dataset and a SAR image dataset. Through these two experiments, we conclude that the BoW method performs generally better than NCD. Although it is a parameter-free solution for data mining, NCD only performs well for images with repetitive patterns like some homogeneous classes. In contrast, BoW method performs much far beyond that of NCD. In addition, NCD is computationally very expensive, which makes it infeasible to be applied in real applications. In contrast, BoW method is more realistic in practical applications in terms of both accuracy and computation.
The retrieval effectiveness of Query Expansion (QE) is very much dependent on the ability to accurately identify and expand core concepts which are truly representative of the intended search goal. Two characteristics of natural language queries which hinder the performance of query expansion for information retrieval are query length and structure. The varying lengths of a query translate to the number of core concepts that may exist and the possibility of there being multiple query intents embedded within a single query. On the other hand, the structure of queries reveals the linguistic properties which allows for the determination of whether they take the form of well-formed sentences or are simply bags-of-words which in the strictest sense are a series of words with no obvious relations amongst them. Whilst query lengths are easily assessed, we propose a two-level automated classification technique consisting of linguistics based and statistical processing for query structure classification. The proposed method has revealed high levels of classification accuracy on TREC ad hoc test queries.
During to past few years, social media have gained a pivotal role in crisis communication. Its usage has ranged from informing the public about the status of a crisis and what precaution need to be taken, to family members checking on the safety of loved ones. Despite the widespread use of social media in crises situations and the clear potential benefit from collecting potential critical information from social media, emergency management services (EMSs) are still reluctant to use social media as a source of information to improve their situational awareness. One of the reasons for the reluctance is that crises management are typically overloaded with information. Adding social media will only increase the information overload, and the EMSs risk being provided with more and possibly irrelevant information from social media sources. Furthermore, most automated social media analysis platforms are designed exclusively to classifying messages into crisis and not crisis-related categories. The platforms do not take into account the degree of relevance of social media information to the EMSs. Such relevance further depends on the crisis' status at a certain point in time, and the information gathered up to that point. This paper proposes an intelligent information retrieval framework from social media during crises. The developed framework combines two components: The first component classifies social media messages into separate topics representing the information needed in different situations. The second component decides which information to retrieve based on the information available and the status of the crisis. The framework is evaluated through a survey of EMSs. They agree with the framework in 70.85% of the cases about what information is needed.
We propose SATM, a Semantic Augmentation Transformer Model for ad-hoc retrieval. SATM adapts contrastive learning, which improves the performance of semantic similarity and the ranking results of ad-hoc retrieval. It can also augment the semantic representation of sentence embeddings. Specifically, we first use an unsup-ervised contrastive learning augmentation module to learn query similarity so that the projection head can accurately capture query semantics. Then we use the trained encoder network to map queries and perform semantic similarity calculations and rankings with document embeddings. We use BERT to extract the contextual representation of the sentence, and use the augmentation module to enhance the semantics and eliminate the anisotropy of sentence embedding. Experimental results show that in the TrecQA data set, SATM has 4% and 1.4% improvements in MRR and MAP over Bert-base.
When users search the Web it is difficult to return information that is as complete as possible. Also many short texts, such as tweets, are not self-contained and need to be explained since understanding them requires additional knowledge or related facts. Recent research focuses on the context of the user, not on complementing the search results, seeking to return more specific information for the search. Unlike previous works, this research focuses on the context of the retrieved data, attempting to enrich the returned information by incorporating complementary facts that give context to the information requested by the user.
The paper proposes a topic based clustering mechanism, which achieves information retrieval and sharing by delegating such responsibilities to those vehicles which have infrastructure access and are willing to provide the service to other neighboring vehicles. The paper leverages the name based transport protocol of Named Data Networking (NDN), builds the request and data routing according to the information topic names through the corresponding cluster head. The paper illustrates the details on how the topic clustering is set up, dissolved and split as needed. It also presents the procedures for a vehicle without infrastructure access to detect, join and leave a topic based cluster. The proposed mechanism has the advantages of preventing the Forwarding Information Base (FIB) from expanding too large and reducing the network bandwidth consumption by avoiding transporting the same content multiple times in serving different neighboring vehicles.
Social Information Retrieval can be interpreted as querying the private information spaces of others within one's social network. One of the crucial steps in such a search approach is to identify the set of potential information providers to route the query to. In this experiment, we compare various routing mechanisms based on topic models (Latent Dirichlet Allocation, LDA), Explicit Semantic Analysis (ESA), and traditional metrics like Term Frequency (TF) and Term Frequency-Inverse Document Frequency (TF-IDF) to identify expertise using a publicly available data collection with 1, 400 scientific abstracts including author information, queries, and relevance judgments. The abstracts are interpreted as knowledge profile in a social information retrieval scenario. Our results suggest that both LDA and ESA can solve the routing problem, whereas the LDA-based approach and a new ESA approach considering links between semantic concepts perform best on the tested dataset.
The hurdle problem in Cross Language Information Retrieval (CLIR) is the poor performance when compared to monolingual performance in terms of average precision. The main reasons behind the poor performance of CLIR are query term mismatching, multiple representations of query terms and un-translated query terms. In this paper, we are putting our effort to solve the given problem which is discussed in detail. The limitations are needed to be addressed in order to increase the performance of the CLIR system. By analyzing those methods the architecture for English-Hindi CLIR system is proposed. Pre and post query expansion is used to improve the performance of English-Hindi CLIR system using English and Hindi WordNet, Local Expansion using initial query, definition based pre query expansion and keyword ranking. The pre and post query expansion helps to improving the performance of English-Hindi CLIR system and based upon past experiences the proposed approach retrieves more relevant information. All experiments are performed on FIRE 2010 (Forum of Information Retrieval Evaluation) datasets. The experimental results show that the proposed approach gives equal/better performance of English-Hindi CLIR system compared to monolingual performance and also helps in overcoming existing problems and outperforms the existing English-Hindi CLIR system in terms of average precision.
Given the area of their research, the students of the Faculty of Electrical Engineering and Computing (FER) need the latest scientific information from both domestic and foreign journals. That is why the Croatian Ministry of Science, Education and Sports has provided them with the access to various databases of scientific and technical journals. Furthermore, the Faculty of Electrical Engineering and Computing is a provider of one of the most important databases for engineers - IEEE Explore. Students can access these databases either from their homes or from the Faculty premises; they can also attend seminars on information retrieval. The goal of this article is presenting the research on students of the Faculty of Electrical Engineering and Computing, their usage of available databases and their familiarity with the search tools for those databases.
Under the guidance of scientific and technological innovation concept, new technology and algorithms are applied to information retrieval and real estate registration system. These excellent algorithms and hardware technology make our file management more convenient, intelligent, and efficient. In this paper, researches on the establishment of intelligent file management system and document retrieval were done based on big data environment and with the combination of the project development experience of the Land and Resources Bureau of Nanjing. First of all, this paper focuses on the defects of Java EE framework which is proposed based on the composite Spring-Struts-Hibernate (SSH) frame system and aims to optimize the performance of SSH and promote its efficiency. Secondly, the growing number of archives is reducing the retrieval efficiency of archives management website. In this paper, the ant colony optimization algorithm is employed to classify the data mining and applied to the quick retrieval of big data. At the same time, the complexity of the optimized algorithm and retrieval efficiency are analyzed. The results show that the retrieval time and efficiency are improved in the big data environment.
Hawking's argument about non-unitary evolution of black holes is often questioned on the ground that it doesn't acknowledge the quantum correlations in radiation process. However, recently it has been shown that adding `small' correction to leading order Hawking analysis, accounting for the correlations, doesn't help to restore unitarity. This paper generalizes the bound on entanglement entropy by relaxing the `smallness' condition and configures the parameters for possible recovery of information from an evaporating black hole. The new bound effectively puts an upper limit on increase in entanglement entropy. It also facilitates to relate the change in entanglement entropy to the amount of correction to Hawking state.
A distributed information retrieval of information retrieval is one of the important research direction. With the rapid growth of WEB information, information is more and more, this information can not be processed. So how to only part of data retrieval, this is distributed information retrieval needs to solve one of the major problems. This paper focuses on the distributed information retrieval, distributed information retrieval, this search engine analysis, to the search engine's analysis of all kinds of search engine is analyzed, and then their respective advantages and disadvantages, at the end of the development of distributed information retrieval technology prospects described.
Stemming is an essential step in various Natural Language Processing (NLP) applications and is used to reduce different variants of the query words to a standard form to avoid the vocabulary mismatch issue in Information Retrieval (IR) systems. Due to specific grammatical rules and complex morphological structures, finding an effective stemming algorithm in Urdu is a challenging task. Although, several stemming algorithms have been proposed for the Urdu text stemming; however, none of them extract the stem from multilevel inflected forms. In this context, according to the best of our knowledge, this is a first effort towards the proposition and evaluation of a novel Urdu Text Stemmer (UTS) that can deal with multi-level inflection forms in Urdu text. The experimental evaluation of the proposed scheme has been conducted on the text-based and word-based custom-developed corpus. The proposed stemming technique is rigorously evaluated and compared with state-of-the-art stemming algorithms. Experimental results demonstrate that UTS outperforms existing Urdu stemmers and achieves an accuracy of 94.92% and 91.8% on word corpus and text corpus, respectively. We also evaluated our proposed system in an Information Retrieval application for Urdu, using the Collection for Urdu Retrieval Evaluation (CURE) dataset. Our approach for information retrieval outperformed and improved both recall and precision metrics.
In this paper, a texture based retrieval method is proposed in HEVC compressed domain. The proposed method can be used for either video retrieval of HEVC coded videos or image retrieval of HEVC I-frame coded images. In I-frame coding various prediction modes are utilized to spatially predict pixels of a block. The selected prediction mode and the size of prediction unit for coding of a block indicate the way, in which the pixels of the block are related to their neighboring pixels and they can considered as texture features. Thus, we measured the texture similarity of coded I-frames in HEVC based on the similarity of the prediction modes and block size histograms. This retrieval method achieves 0.34 ANMRR in videos with high resolution, which is better compared to the other compressed domain retrieval method based on previous standard H.264/AVC that have achieved 0.45 ANMRR in the same image database.
Ticket annotation and search has become an essential research subject for the successful delivery of IT operational analytics. Millions of tickets are created yearly to address business users' IT related problems. In IT service desk management, it is critical to first capture the pain points for a group of tickets to determine root cause; secondly, to obtain the respective distributions in order to layout the priority of addressing these pain points. An advanced ticket analytics system utilizes a combination of topic modeling, clustering and Information Retrieval (IR) technologies to address the above issues and the corresponding architecture which integrates of these features will allow for a wider distribution of this technology and progress to a significant financial benefit for the system owner. Topic modeling has been used to extract topics from given documents; in general, each topic is represented by a unigram language model. However, it is not clear how to interpret the results in an easily readable/understandable way until now. Due to the inefficiency to render top concepts using existing techniques, in this paper, we propose a probabilistic framework, which consists of language modeling (especially the topic models), Part-Of-Speech (POS) tags, query expansion, retrieval modeling and so on for the practical challenge. The rigorously empirical experiments demonstrate the consistent and utility performance of the proposed method on real datasets.
Retrieving relevant medical data promptly is essential for both improving patient care and scientific discoveries. To compare the performance of Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Deep Neural Networks (DNNs), and a unique fusion approach called Random Multimodel Deep Learning (RMDL), this study addresses the application of deep learning models in medical document retrieval. The findings of this research emphasize the value of RMDL's multi-model fusion and imply that integrating several learning modalities can have positive effects in the medical field. improved document retrieval systems have the potential to provide medical personnel with faster access to pertinent information, which could result in more precise diagnoses, more focused treatment regimens, and ultimately improved patient outcomes. Improved retrieval also makes it easier to collect information for medical research in an efficient manner, which speeds up breakthroughs across a range of domains and aids in the creation of individualized medicine strategies. Ultimately, the most effective model is assessed using performance evaluation parameters.
This paper presents an analysis of Query Optimization and Information Retrieval in Multi-lingual Environment. Data plays an important role in Information retrieval. In today's global environment, databases should support multilingual text data efficiently and effectively. All the government records or portals today are required in regional language. The retrieval depends on the type of query passed and how the optimizer behaves with the amount and type of data to be retrieved. Hence it is imperative that the information systems support the storage and management of multi-lingual data effectively. The primary requirement to achieve this goal is that the principal data repositories should support storage and retrieval of Multi-lingual data. Our initial experiments indicated considerable performance degradation when comparisons where made between English and Gujarati language. This paper explores the work carried out by various researchers in Multilingual environment and our preliminary analysis for the same for Indian languages with emphasize on Gujarati language.
The IDF (Inverse Document Frequency) term weighting method is a classic treatment of a term’s significance in information retrieval and text analytics. IDF can be derived from the information-theoretic Kullback-Leibler (KL) Divergence and has given rise to competitive methods such as TF*IDF and Okapi BM25, which is the default scoring function of ElasticSearch. We developed a new information metric called DLITE and derived from it an alternative to IDF, namely iDL, for term weighting and scoring in ranked information retrieval. In a series of experiments we conducted on multiple benchmark Text REtrieval Conference (TREC) collections, iDL methods consistently outperformed BM25, a very competitive baseline, for ad hoc retrieval. We outline the theoretical properties of DLITE that support the effectiveness of iDL. As a general information measure, we expect DLITE to be applicable in many other areas of big-data analytics and machine learning where further research will be valuable.
This paper expounds the key technologies of the ontology, focuses on the overall structure design, system function, ontology construction and retrieval process of the system, and finally discusses the key problems to be solved in the system design: ontology management and service, information resource acquisition and document pre-processing, etc.
In this paper, we investigate the retrievability of datasets and publications in a real-life Digital Library (DL). The measure of retrievability was originally developed to quantify the influence that a retrieval system has on the access to information. Retrievability can also enable DL engineers to evaluate their search engine to determine the ease with which the content in the collection can be accessed. Following this methodology, in our study, we propose a system-oriented approach for studying dataset and publication retrieval. A speciality of this paper is the focus on measuring the accessibility biases of various types of DL items and including a metric of usefulness. Among other metrics, we use Lorenz curves and Gini coefficients to visualize the differences of the two retrievable document types (specifically datasets and publications). Empirical results reported in the paper show a distinguishable diversity in the retrievability scores among the documents of different types. CCS CONCEPTS • Information systems → Retrieval models and ranking.
Paper currency is one of the most in-demand and long-established payment modes across the globe. People suffering from visual disabilities often face difficulties while handling paper currencies. Over the years, assisting technology has been rekindling itself to serve the aged and disabled person more aptly. Image processing methods and other sophisticated technologies, like Artificial Intelligence, Deep Learning, etc., can be employed to identify banknotes and fetch other valuable pieces of information from them. This paper proposes a framework that focuses on an integrated approach to retrieving data from the paper currency’s uploaded image. The current version of the framework focuses on identifying the authenticity of the paper currency and classifying it according to its value. This work is an initiative to help visually impaired people to use paper currencies without assistance from other individuals and support them in living independently.
This paper presents a query-based information retrieval system for assistance pf the visually impaired people. It can recognize the known faces along with their timestamp. It can process the query and reply to it based on its recordings. Visually impaired users need help of sighted person for recognizing people around them unless they speak and start the conversation themselves. This dependence limits the socialization of the visually challenged people. Such situations may lead to anxiety and cognitive load. Many systems have been designed as aid for such problems, but they lack the information storing and a query-based model to take decision based on the information. This solution uses local binary pattern-based histogram for recognition of known faces. The system provided an accuracy and average precision of 82.4% and 85% respectively. The vital information is stored and delivered in response to a visually impaired person’s query via audio.
The Cloud age, characterized by a revolution of Data center infrastructures with powerful resources, has leveraged the "as-a-service" paradigms. Nowadays, the Web of Things (WoT) proposes the abstraction of real world entities in a virtual Web avatar, to acquire, process and present real-time information with the ability to connect with the real-world, and to control real things. A new generation of services can arise from these complementary paradigms. One of them is crucial for a daily basis interaction with this new smart world, Information Retrieval (IR), mostly in the form of search engines can also evolve into more powerful tools. Based on that, a new architecture of this kind of services has to be defined by the synergy and challenges that WoT imposes. We propose an IR as a service IRaaS architecture for the new era of the Web of Things. It considers the high dynamics in WoT which leverages a significant amount of changes in the IR collection to merge efficiently conventional IR concepts and the cloud power. Our IRaaS approach takes into account the big-data characteristics of WoT concerning velocity, volume, volatility, and the variety of data. The architecture has two main pillars: the indexing and analysis block, and the querying and retrieval block. We propose building it driven in three dimensions: search scope, resources, and data type. From these perspectives, we show and compare the related works and researches.
With the rapid expansion of information on the internet nowadays, the query results produced by Relational Database (RDB) can no longer satisfy the complex search requests made by users. Traditional information retrieval modules generally have problems such as low retrieval efficiency, poor scalability, and low intelligence. In this paper, this paper proposed an intelligent information retrieval module based on the Learning to Rank (L TR) to optimizes the relevance score in the information retrieval process. Experimental results conducted in production environment show that our proposed method improved accuracy of relevance score by 15.84% and accelerate the retrieval speed by 4.15 times. In addition, the interpretability of the retrieval results and the selection of key features are also analyzed and realized.
This study is about a domain cased-based knowledge base model that can improve the level of human value judgement and decision for information retrieve as reducing a ambiguity in language. The big data problem occurred which is decreased the user confidence in search result by a dramatic increase of data or information, and The increasing use of mobile device, is emphasized to increase the simplicity that reduce the human involvement for searching and deciding of result. Therefore, in this study, for increasing the information retrieve performance in area of needing the high-level decision. Indeed, this study has the objective as search for improved performance that required high-level decision-making in the field. For showing high-level concepts or knowledge, in a specific domain area, we used TBox and RBox search and inference mechanism for solving the ambiguity in language. In order to improve search performance which represented concepts, on the processing of knowledge base design and quality through the use of qualified, and is to validate the configuration. Drugs that require expertise in the field of knowledge base Search performance analysis of the knowledge base, Keyword-based search with the ambiguity of the language problem can be solved.
We consider the problem of accurately and efficiently querying a remote server to retrieve information about images captured by a mobile device. In addition to reduced transmission overhead and computational complexity, the retrieval protocol should be robust to variations in the image acquisition process, such as translation, rotation, scaling, and sensor-related differences. We propose to extract scale-invariant image features and then perform clustering to reduce the number of features needed for image matching. Principal Component Analysis (PCA) and Non-negative Matrix Factorization (NMF) are investigated as candidate clustering approaches. The image matching complexity at the database server is quadratic in the (small) number of clusters, not in the (very large) number of image features. We employ an image-dependent information content metric to approximate the model order, i.e., the number of clusters, needed for accurate matching, which is preferable to setting the model order using trial and error. We show how to combine the hypotheses provided by PCA and NMF factor loadings, thereby obtaining more accurate retrieval than using either approach alone. In experiments on a database of urban images, we obtain a top-1 retrieval accuracy of 89% and a top-3 accuracy of 92.5%.
This paper mainly brings out the details related to a prototype biodiversity information retrieval system that has been setup using the distributed Grid-Cloud resources of GARUDA Grid project, India. The overall experiment has been done with the help of open source biodiversity databases. The structure of these relational database tables are not standardized and are hosted on a variety of Database Management Systems (DBMS) at different Virtual Machines (VMs) which in general been assumed as geographically distributed. The front end of the end user system is an HTML interface which captures and redirects the user query to the application engine which has been built using python and made functional in master grid node. According to the received input, the python program interprets the data and generates a query in the Structured Query Language (SQL). This generated query is sent to the distributed remote database servers which channelizes to the local DBMS of the cloud's virtual machine and executes the SQL query. The end results are retrieved back by the master grid application engine and been displayed in a new HTML page.
Managing and developing competencies or skills are vital to professional development of an individual. Many tertiary education institutions are therefore focused on developing curriculum that will help the graduating students acquire skills that are aligned with industry practice. Industry skills frameworks such as Skills Framework for the Information Age (SFIA) define the professionals required for an IT professional. Therefore, mapping the curriculum competencies to the industry skills framework has a dual purpose of aiding the educationists to improve the curriculum, and the students to plan the courses according to their career plan. Existing mapping methods are manual and painstaking processes. In this paper, we present an automated solution based on text analytics techniques to map the curriculum to industry framework and provide a visual based analysis to discover the strengths and weakness of the curriculum. We evaluated our solution model on an undergraduate core curriculum; Bachelor of Science (Information Systems Management) degree program BSc (ISM), offered by the School of Information Systems (SIS), Singapore Management University (SMU) and Skills Framework for the Information Age (SFIA).
Text mining is thought to have a high commercial potential due to the significant amounts of unstructured text data produced on the Internet. The practice of obtaining previously undiscovered, comprehensible, potentially useful patterns or knowledge from a corpus of text data is known as text mining. In this study, we attempt to extract the structured information from the text and then use various machine-learning models to categorize the data. We then look for the model that provides the highest level of classification accuracy.
Microblogging sites like Weibo, Tumbler, Pinterest, and Twitter have been identified as significant resources for current events including social, political and emergency cases. During disaster situations, these sites are very advantageous for collecting information in real-time. During these events, many people make way to microblogging sites but very few of microblogs published are relevant to the information needed. Hence, it is important to make useful methodologies that can extract relevant microblogs from the huge chunk of data posted on these sites. In this work, one of the methodologies is applied for retrieving tweets for a distinct situation. The accuracy of retrieval and the relevance of tweets retrieved have also been assessed as a portion of this work.
Locating buggy files is a time consuming and challenging task because defects can deflate from a large variety of sources. So, researchers proposed several automated bug localization techniques where the accuracy can be improved. In this paper, an information retrieval based bug localization technique has been proposed, where buggy files are identified by measuring the similarity between bug report and source code. Besides this, source code structure and frequently changed files are also incorporated to produce a better rank for buggy files. To evaluate the proposed approach, a large-scale experiment on three open source projects, namely SWT, ZXing and Guava has been conducted. The result shows that the proposed approach improves 7% in terms of Mean Reciprocal Rank (MRR) and about 8% for Mean Average Precision (MAP) compared to existing techniques.
Object retrieval aims at retrieving images containing objects similar to the query object captured in the region of interest (ROI) of the query image. Boosted by the invention and wide popularity of SIFT image features and bag-of-visual-words image representation, object retrieval has progressed significantly in the past years and has already found deployment in real-life applications and products. While existing object retrieval methods perform well in many cases, they may fail to return satisfactory results if the ROI specified by the user is inaccurate or if the object captured there is too small to be represented using discriminative features and consequently to be matched with similar objects in the image collection. In order to improve the object retrieval performance also in these difficult cases, we propose in this paper an object retrieval method that exploits the information about the visual context of the query object and employ it to compensate for possible uncertainty in feature-based query object representation. Contextual information is drawn from the visual elements surrounding the query object in the query image. We consider the ROI as an uncertain observation of the latent search intent and the saliency map detected for the query image as a prior. Then a language modeling approach is employed to devise a contextual object retrieval (COR) model. There, the relevance score is determined based on the search intent scores that are inferred from the uncertain ROI and the saliency prior. The usefulness of the contextual information for object retrieval and the effectiveness of the proposed COR model are demonstrated and evaluated on three representative image datasets.
Facing tremendous volume of semi-structured XML and non-structured free text, network information retrieval is one of the most research hotspots in dealing with these data more efficiently, precisely and uniformly. Many traditional IR methods ignore text semantics and their labeling result has usually only one level, lacking of context expression as well, therefore structure extraction from free text and its conversion to XML format are studied, with a CRF based algorithm SIECRF provided. Experiment results are analyzed, showing its efficiency to extracting text structure and has a good application future.
Welcome to RCIS, the “Research Challenges in Information Science” community and its annual conference! In this short article, we introduce newcomers to this field and to its community with an overview of what RCIS is and how it positions with respect to other venues existing in the domain, like CAiSE, MODEL, and ER, highlighting its legacy. We also take this opportunity to enumerate and define the eight major RCIS areas of scientific contributions and exemplify them with some representative papers published during this 13th edition or before, which we hope to guide researchers and practitioners towards making their future RCIS submissions successful and impactful in the RCIS community.
With the rapid development of the Internet, while it facilitates users to obtain information, it also increases information overload. Although a lot of data have been divided into categories, it is still a big challenge to retrieve effective information from thousands of categories and their subcategories. For professional business, we need more efficient information organization and interactive interface to reduce the complexity of information retrieval. This paper designs an intelligent question-and-answer system for science and technology intermediary services based on knowledge graphs, which uses knowledge graphs to store domain data, combines intelligent question-and-answer technology and website design technology to realize efficient information query. On the basis of building the knowledge graph database of professional domain, the problem of information efficient search is effectively solved by combining natural language processing with relational graph.
Peer-to-Peer Information Retrieval System (P2PIR) is a system that can query multiple information retrieval systems and merges ranked results list into a single result of documents. Classical methods are generally based on linear combination schemes. A major shortcoming of the classical methods is that there is no defined way to study dependencies and interactions existing among relevance criteria. In this paper, we propose a result merging method based on the Choquet Integral, called Choquet-Based Merging (CBM). The experimental results obtained on the test collection provided by TREC Contextual Suggestion track shows the effectiveness of our proposal.
The synonyms of domain-specific terms are of value to information services in that domain, such as information retrieval, question answering, etc. The method of obtaining synonyms entirely by manual annotation is costly and inefficient. Using the open source word vector corpus, through word similarity calculation, the similar words of the term word are automatically obtained, and then manual screening can be performed to obtain synonyms efficiently and at low cost. Experiments in the field of library and information show that this method can quickly and effectively obtain similar words of a large number of domain-specific terms.
In recent years there has been tremendous growth in the computational capabilities of devices, content and the retrieval methods. There is great interest now in the mobile web. Mobile web refers to access to the web via a browser or application from a mobile computing system. The mobile computing system can be a mobile computer, smart phone or even a mobile phone. The access to the web from a mobile device especially the search for information is growing in popularity every day due to the availability, cheap cost and convenience of access. Studies have shown that the users are not satisfied with the state of the art in mobile browsing especially in the search and retrieval process. The potentially large volume of content that can be accessed, low power limitations of cellular band width, variable and small screen sizes pose considerable challenges to mobile users. This has necessitated great interest and research in the domain. This paper proposes a middleware based system that retrieves content dynamically from the web based on the user context, personal preferences and uses adapted snippet clustering algorithms for re ranking the query results. The architecture of the mobile information retrieval system is given along with the discussion on the prototypes used to validate the results. The initial results are promising and show considerable potential for research in the domain.
Query is an important element that provides a specification of the users' information need. Many Studies have found contexts within query that strongly influence the interpretation of a query. In this paper, we propose to use cliques as query context which refers to context words within the query. This approach refers to cliques as query context which is particularly useful for the selection of relevant term relations to avoid topic drift. Our experiments on several collections show that the new approach bring significant improvements and more effective on collections.
On-site score processing software for cheerleading projects based on distributed architecture considering data retrieval is studied in the paper. In the designed model, the data retrieval is the theoretical basis. Intelligent information retrieval is the most cutting-edge science and technology at present. Through simulation, extension and expansion, the retrieval system can be made more and more intelligent, involving social news, literary knowledge, scientific knowledge and other disciplines. The lack of system intelligence will result in information not being systematic and complete. To deal with this challenge, this paper applies the distributed architecture algorithm to the system and consider the scenario of the score processing software for cheerleading projects. Through the implementation of the model, the system is validated.
In this paper, we present an unsupervised approach for estimating the effectiveness of image retrieval results obtained for a given query. The proposed approach does not require any training procedure and the computational efforts needed are very low, since only the top-k results are analyzed. In addition, we also discuss the use of the unsupervised measures in two novel rank aggregation methods, which assign weights to ranked lists according to their effectiveness estimation. An experimental evaluation was conducted considering different datasets and various image descriptors. Experimental results demonstrate the capacity of the proposed measures in correctly estimating the effectiveness of different queries in an unsupervised manner. The linear correlation between the proposed and widely used effectiveness evaluation measures achieves scores up to 0.86 for some descriptors.
The Internet of Medical Things (IoMT) is used to sense and deliver vital signs for healthcare monitoring, so rapid medical information retrieval is crucial. The information-centric model can help shorten delays of IoMT-based medical data delivery, but IoMT nodes cannot perform the information-centric model due to their limited resources. In this paper, we integrate IoMT with fog clouds and propose a fog-assisted information-centric IoMT framework so that fog nodes can help perform the information-centric model and reduce latency of IoMT-based medical information retrieval. In the framework, consumers can exploit reverse-paths and request aggregation to acquire medical information of multiple subjects from the nearest provider via one information delivery procedure. Moreover, the framework addresses the consumer mobility issue to guarantee receipt of medical information. Finally, this framework is evaluated to verify its advantages.
During the last few decades many Arabic question answering systems have been developed. These systems may extract answers from texts or web-pages. None of these systems make use of question answering database where user can present questions in natural language which differ from the stored questions. The proposed system uses information retrieval approaches to get to the closest answers to the input question, so the system gives partially or totally correct answers. The Latent Semantic Indexing (LSI) is implemented to enhance the resultant selections. Arabic natural language processing is used in the proposed system along with LSI.
Cross-language information retrieval is one of the important tasks in the field of information retrieval. Existing cross-language neural retrieval methods usually use single-task learning, and the single feature capture model limits the performance of neural retrieval models. To this end, a cross-language retrieval method based on multi-task learning is proposed, using a text classification task as a secondary task, capturing feature information of both tasks simultaneously using a shared text feature extraction layer so that it learns the feature patterns of different tasks, and then inputting the feature vectors into the neural retrieval model and the text classification model to complete the two tasks, respectively. In addition, the external corpus introduced by the text classification task also plays a role in data augmentation to a certain extent, further increasing the level of feature information. Experiments conducted on four language pairs from the CLEF 2000–2003 dataset show that the present method significantly improves the text feature extraction and thus enhances the neural retrieval model performance, increasing the MAP values of the neural retrieval model by 0.014-0.203 and increasing the model convergence speed by 22.82% on average.
Bug localization involves using information about a bug to locate affected code sections. Several automated bug localization techniques based on information retrieval (IR) models have been constructed recently. The "gold standard" of measuring an IR technique's accuracy considers the technique's ability to locate a "first relevant method." However, the question remains -- does finding this single method enable the location of a complete set of affected methods? Previous arguments assume this to be true, however, few analyses of this assumption have been performed. In this paper, we perform a case study to test the reliability of this "gold standard" assumption. To further measure IR accuracy in the context of bug localization, we analyze the relevance of the IR model's "first method returned." We use various structural analysis techniques to extend relevant methods located by IR techniques and determine accuracy and reliability of these assumptions.
Natural Language Processing (NLP) is an Artificially Intelligent (AI) mechanism that allows computers to intelligently analyze, comprehend, and derive meaning from human language. In natural language text processing, common words like ‘a’, ‘the’, ‘is’, ‘an’, etc. are known as a stopwords. They are typically considered having no informative value. It is proved that one of the major benefits of removing stopword in NLP text-based processing is the reduction of the text in the corpus by 35 - 45%, without compromising on the efficiency of the target application performance. There are many stopword lists existing for Non-Indian languages like English, Arabic, French and German. Even for a few Indian languages like Hindi, Sanskrit and, Tamil substantial lists are available. But as of date very little research work is reported for one of the widely used Indian language namely Gujarati. As per our survey, for the Gujarati language, two major approaches have been suggested for stopword identification. The first approach is giving a static generic stopword list, and another approach is a Rule-based approach. The major drawback of these method is their inability to handle neologism. In this paper, we have suggested domain-specific, robust and dynamic stopword list identification mechanism developed for documents written in the Gujarati language. In our proposed approach, we take the top "N" words as seed words based on their frequency and later add other "M" similar context word which are identified by word embeddings. Further the effectiveness of removing these listed (N+M) stop words was checked by applying the stopword removal preprocessing phase in the Text Classification (TC) and Information Retrieval (IR) applications. In TC model, the feature vector reduces by approximately 16%, and on other hand, the accuracy of the TC model increased by nearly 3 %. The experiments also found, removal of the these stop words in IR application, increased the Mean Average Precision (MAP)... (Show More)
The reform and modernisation of public sector based on wide application of information-communication technologies (ICT) is considered as one the key elements of futher development of information society in the Republic of Serbia. The trends of development of many e-Government services for many countries in the world, indicate the necessity of application natural language processing - NLP and in e-Government services Republic of Serbia. When performing many of the natural language processing tasks it is needed that all forms of a word with the same meaning has the same form. In general, many documents that are available e-Government services are not structured, so from them is very difficult to isolate some forms (knowledge) that exist in them. The process of finding useful information from such documents in Serbian language, which normally represents highly inflectional language, is one of the key elements of the modern information society in the Republic of Serbia.
Digital world is coming, were data as become big data with ever increase in large volume of digital information available in terms of text documents. This tends for data extraction, enrichment, analysis and retrieval of text documents which are in the form of unstructured nature becomes a major problem in search engine. Traditionally text documents are the source of storing our information; either personal or professional. Today text documents are generating at very high speed, and need to be process the data on-time to upgrade the search engine. It is also important for organizations including private and public which have been collecting large volume of domain-specific text document information, which may contain national intelligence, education, medical information, business and marketing. In this paper we present a system that enriches the information retrieval process of text documents in search engine from unstructured data and bringing the big data and data analytics world into educational sector and make the best of both worlds by using the latest cutting edge technology deep-structured semantic modeling with text hashing and proposing a next generation search engine.
Historic documents such as Chinese calligraphy and old newspapers usually were handwritten or printed in poor quality so that an automatic optical character recognition procedure for scanned document images is difficult to apply. Thus efficient pattern matching techniques are required in order to do content-based information retrieval based on user's queries. In this paper, a fast pattern clustering and image matching procedure is proposed to do image/pattern search in a historic document image based on user's query images. The image summary extracted from the document image is constructed so that a set of distinct image clusters are formed. A couple of distance measures that calculate distance between image patterns are also proposed to evaluate their cluster similarities. By precise pattern matching and hierarchical image clustering, our experimental results show that an online query image can produce accurate and faster results than traditional approaches for a broad range of historic document images.
In a world that seeks perfect results of any search query, an information retrieval system that produces an accurate and relevant output is desired. However, because of the famous semantic gab problem of image representation, a Content Based Image Retrieval (CBIR) system faces some difficulties, since it highly depends on the extracted image features as basis for a similarity check between the query image and database images. This purposed approach overcomes these difficulties with the aid of the most fast growing technology, namely Deep Learning. In addition, it explores the effects of merging the features extracted from the latter layers of the deep network to achieve better retrieval results. The experimental results demonstrate the effectiveness of the proposed scheme in terms of the number of relevant retrieved images of the query results, and the mean average precision, while keeping low computational complexity since it uses an already trained deep convolutional model called AlexNet. Thus in turn, a reduction in the complexity that combines training a deep model from the scratch has been achieved.
A user query for information retrieval (IR) applications may not contain the most appropriate terms (words) as actually intended by the user. This is usually referred to as the term mismatch problem and is a crucial research issue in IR. Using the notion of relevance, we provide a comprehensive theoretical analysis of a parametric query vector, which is assumed to represent the information needs of the user. A lexical association function has been derived analytically using the system relevance criteria. The derivation is further justified using an empirical evidence from the user relevance criteria. Such analytical derivation as presented in this paper provides a proper mathematical framework to the query expansion techniques, which have largely been heuristic in the existing literature. By using the generalized retrieval framework, the proposed query representation model is equally applicable to the vector space model (VSM), Okapi best matching 25 (Okapi BM25), and Language Model (LM). Experiments over various data sets from TREC show that the proposed query representation gives statistically significant improvements over the baseline Okapi BM25 and LM as well as other well-known global query expansion techniques. Empirical results along with the theoretical foundations of the query representation confirm that the proposed model extends the state of the art in global query expansion.
Currently, the correct selection of tokens from incoming information is one of the important issues in such areas as machine translation, information retrieval, information extraction from text, and information security. Algorithms for extracting tokens from texts are called tokenization. In this study, a tokenization algorithm has been developed that works on the basis of a knowledge base to extract lexemes from a text.
One of the biggest problems facing Web-based Information Systems (WIS) is the complexity of the information searching/retrieval processes, especially the information overload, to distinguish between relevant and irrelevant content. In an attempt to solve this problem, a wide range of techniques based on different areas has been developed and applied to WIS. One of these techniques is the information retrieval. In this paper we described an information retrieval mechanism (only for structured data) with a client/server implementation based on the Query-Searching/Recovering-Response (QS/RR) model by means of a trading model, guided and managed by ontologies. This mechanism is part of SOLERES system, an Environmental Management Information System (EMIS).
In many practical settings, the user needs to retrieve information messages from a server in a periodic manner, over multiple rounds of communication. The messages are retrieved one at a time and the identity of future requests are not known to the server. In this paper, we focus on the private information retrieval protocols that ensure that the identities of all the messages retrieved from the server are protected. This scenario can occur in practical settings such as periodic content download from text and multimedia repositories. We refer to this problem of minimizing the rate of data download as online private information retrieval problem.Following the previous line of work by Kadhe et al. we assume that the user knows a subset of M messages in the database as side information. The identities of these M messages are initially unknown to the server. Focusing on scalar-linear settings, we characterize the per-round capacity, i.e., the maximum achievable download rate at each round. In particular, we show that for the setting with K messages stored at the server, the per-round capacity of the scalar-linear setting is C1= (M + 1)/K for round i = 1 and Ci= (2i -1(M + 1))/KM for round i ≥ 2, provided that K/(M + 1) is a power of 2. The key idea≥of our achievability scheme is to combine the data downloaded during the current round and the previous rounds with the original side information messages and use the resulting data as side information for the subsequent rounds.
Most educational institutions have adapted to the mode of online teaching which has resulted in an increase of online video recordings. Learner community can be benefited with the ability to retrieve required information from the online class recordings. In this paper, we propose a methodology for converting video transcript data into useful information repositories for the purpose of retrieving class transcripts relevant to user's information needs. We focus on the online video recording transcript data. We also discuss challenges in transcribing which are crucial to understand preliminary processing. Our dataset consists of transcripts from diverse subject domains deeper experimental insights. We use interactive transcripts obtained from ASR (automatic speech recognition) services and non-interactive human generated transcripts. State-of-the-art methods for keyword retrieval: Latent Dirichlet Topic Modelling (LDA), Term Frequency (TF.IDF) and Text Rank (graph based) are applied on the video transcript data. Further, cosine similarity metric is applied to obtain the similarity measure between the transcript documents and keywords.
This paper presents a dataset that can be used for evaluating query suggestion algorithms in textual information retrieval. The dataset is public and offered free of charge to the information retrieval research community. The data was gathered in an experiment that lasted more than 2 months and to which participated a number of 119 users, mainly faculty students. The dataset contains web browsing history and query history (submitted to the Google search engine) from all these users. The data is indexed in a database and downloadable in a database dump format. The dataset is very useful for evaluating general query suggestion algorithms by themselves (in a standalone manner) or against Google's MPC query suggestion algorithm. At the same time, the dataset supports building and testing personalized query suggestion algorithms that consider the user context/profile when computing query suggestions.
Semantic and keyword web based technique is becoming a generic issue in an application of Information Retrieval (IR). Most of the researchers used different web techniques for finding relevant information and find the keyword based search, which are not able to fetch the relevant search result because they do not know the actual meaning of the term or expression and relationship between them in the web search. In this paper, semantic and keyword based web search method have been applied on the different web search engines. The selected search engines such as semantic search engines (Google, Yahoo, Wikipedia) and keyword search engines (Hakia, Bing, DuckDuckGo). Performance is based on their precision ratio and natural language queries. Various queries was input on different search engine and output of the documents was classified a relevant documents and non-relevant documents. Precision ratios were calculated in the final retrieved documents on each web search engines. Also defined some popular semantic and keyword search engine features.
With the advent of data ecosystems finding information in distributed and federated catalogs and marketplaces becomes more and more important. One of the problems in data search and search in general is the mismatch between the terminology of users and of the searched items, be it dataset metadata or web pages. The paper proposes an agent-based approach to document expansion (ADE). The idea is to represent documents with agents that exploit local information collected from user searches and relevant signals to improve the representation of the document in a search index and subsequently to improve the search performance of the system. The agents collect terms from relevant queries and perform topic modeling on these terms and publish different variants expanded with the topic terms to the search index. We find that the approach achieves good improvement in search performance and is a valuable tool because is places no burden on the information retrieval pipeline and is complementary to other document expansion and information retrieval approaches.
An overview of the features of the design and development of the Virtual Library information system was conducted. A new approach is proposed for designing and developing the Virtual Library information system for preservation and development of e-books in the format of MARC 21. The model of information system Virtual Library is proposed.
In order to improve the intelligentized level of high efficiency teaching, a teaching optimization method based on intelligent information technology is proposed. The optimized intelligent information processing technology of college teaching includes the module of teaching resource information dispatching, the database module and the intelligent information processing module, and the design of the teaching resource information scheduling model of universities and colleges. The adaptive feedback equilibrium configuration method is used to optimize the allocation of teaching resources in Colleges and universities. Based on the fuzzy partition scheduling method, the intelligent processing of teaching information in Colleges and universities is carried out. The access model of the university teaching resources distributed large database is constructed, and the university teaching large database access and information retrieval are adopted by the driver configuration program. The intelligent information processing technology is used to optimize the teaching methods of colleges and universities. The test results show that this method is used to optimize the design of teaching methods in Colleges and universities, the ability to dispatch and balance the information of teaching resources in Colleges and universities is improved, the intelligent information management of College teaching is realized, it has the higher intelligence level of the teaching methods and the stronger ability of information processing than traditional method.
The needs for automatic semantic information indexing and retrieval systems have raised owing to the rapid growth of video content. Above all, in the information sources of the video, textual information has an important role in high level semantics. This paper presents the strategy for automatic person indexing and retrieval system using overlay text in the television news interview video sequences. For recognition the overlay text and extraction the information, the proposed methods are based on many of the accepted production rules of the television news and the temporality of the video sequences. The experimental results on Korean television news video show that the proposed method efficiently realizes the automatic indexing and retrieval system.
For developers, the debugging process is a huge consumer of time and effort. They receive many bug reports that they must address by reproducing the bugs and reviewing the code for errors. Therefore, an automated system to help the developers in these tasks is very much required. This paper presents a system to find potential solutions in the Stack Overflow (SO) forum of questions and answers focusing on programming issues. It finds the similar problems, which have relevance to the reported bugs in a bug report, analyzed by the experts in SO. We combine two similarity measures to rank relevant SO posts that suggests the solutions for software bugs. Our experimental evaluations on several open-source projects show that the combination of similarity measures performs better than applying the similarity measures individually.
Healthcare practitioners are increasingly using search functionality embedded in Electronic Medical Record (EMR) software to search for relevant evidence summaries at point of care. We introduce a learning to rank approach that exploits information carried in EMR data and UpToDate user accounts to (significantly) improve ranking results, compared to a comparable model that does not exploit such features.
The ability to find relevant legal information is a fundamental component of a legal experts and AI. We propose a model for legal information retrieval that exploits the capabilities of a legal ontology enriching it with knowledge extracted from Wikipedia.
Discerning relevant data is becoming more difficult, time consuming, and costly as the amount of data available dramatically increases. Currently, consulting firms strive to use data to support client's business decisions with evidence. To be effective at this, consultants must consider the applicability of both internal and external data libraries to their clients' requirements. Frequently, evaluating the applicability of data sets is a manual process, which can be costly to the firm and the client. This paper describes a technical approach to automate this process. Specifically, it details the structure of a software application, named UVa Open Miner, capable of assessing the applicability of data sources to client projects. This UVa Open Miner aims to maximize the scale and diversity of candidate data sets, increase the relevance of data found, and maintain manageable computational complexity. UVa Open Miner consists of two segments: mapping and matching. The mapping component text mines web pages to identify an ontology of keywords describing the business requirement. This enables users to handle diverse business requirements from various industry verticals. The matching component scores data sets based on a relevance factor obtained from the ontology map. To validate the application, subject matter experts provided business requirements for a problem in their domain, and validated the application's results. Professionals in environment science, political science and policy-making fields found the application to be useful. Therefore, the application, along with the framework used, can be refactored into a reusable solution for consulting firms to use for their clients.
Document Retrieval has seen significant advancements in the last few decades. Latest developments in Natural Language Processing have made it possible to incorporate context and complex lexical patterns to document representations. This opens new possibilities for developing advanced retrieval systems. Traditional approaches for indexing documents suggest averaging word and sentence encoding to form fixed-length document embeddings. However, the common bag-of-word approach fails to incorporate the semantic context, which can be critical for understanding document-query relevancy. We address this by leveraging Bidirectional Encoder Representations from Transformers (BERT) to create semantically rich document embeddings. BERT compensates the limitations of the Term Frequency Inverse Document Frequency (TF-IDF) by incorporating contextual embeddings. In this paper, we propose an ensemble of BERT and TF-IDF for a document retrieval system, where TFIDF and BERT together score the documents against a query, to retrieve a final set of top K documents. We critically compare our model against the standard TF-IDF method and demonstrate a significant performance improvement on MS MARCO data (Microsoft-curated data of Bing queries).
In this paper, we propose an architecture for personalizing information retrieval (IR), exploiting the interactions between the user and the social network. We use an extension of a Dung argumentation framework to show how the precision of the personalized information retrieval system could be improved. We use also social media and search history to define the user-profile which is represented by a restriction of a Description Logic.
In this paper we describe a system of retrieving information from artwork based on textual cues, descriptive to relative art pieces, made available through the metadata itself. Large datasets of artwork can easily be mined by using alternative queries and search methodologies. In the most common search methodology a text-based query using a keyboard is performed. We are proposing a method for searching, finding and recommending digital media content based on pre-set metadata text queries organized in two categories, then mapped to speech sentiment cues extracted from the emotion layer of speech alone. We also account for the difference in sentiment expression for male and female speakers and further suggest that this differentiation may improve system performance.
Spoken document retrieval (SDR) has long been deemed a fundamental and important step towards efficient organization of, and access to multimedia associated with spoken content. In this paper, we present a novel study of SDR leveraging the Bidirectional Encoder Representations from Transformers (BERT) model for query and document representations (embeddings), as well as for relevance scoring. BERT has produced extremely promising results for various tasks in natural language understanding, but relatively little research on it is devoted to text information retrieval (IR), let alone SDR. We further tackle one of the critical problems facing SDR, viz. a query is often too short to convey a user's information need, via the process of pseudo-relevance feedback (PRF), showing how information cues induced from PRF can be aptly incorporated into BERT for query expansion. In addition, such query reformulation through PRF also works in conjunction with additional augmentation of lexical features and confidence scores into the document embeddings learned from BERT. The merits of our approach are attested through extensive sets of experiments, which compare it with several classic and cutting-edge (deep learning-based) retrieval approaches.
Information Retrieval Systems (IR) is using different indexing techniques to retrieve information such as, Inverted files, and Signature files. However, Signature files are suitable for small IR systems due to its slow response, while inverted file have better response time but its space overhead is high. Moreover, inverted files use B±trees for single-word queries. In this paper, a new indexing structure called TB±tree to be used in the design of inverted files for large information retrieval systems. The TB±tree is a variant of the B±tree that supports single key-word queries and phrase queries efficiently. In TB±tree algorithms which represent each key-word stored in the index by a numeric value, and this numeric value can be used as encryption and inforce security. The numeric value for each keyword is stored in binary format, which may reduce the size of the index file by 19%. The records in TB±tree may be of variable length.
The ineffectiveness of information retrieval systems is mostly caused by the inaccurate query formed by a few keywords that reflect actual user information need. One well known technique to overcome this limitation is Automatic Query Expansion (AQE), whereby the user's original query is improved by adding new features with a related meaning. It has long been accepted that capturing term associations is a vital part of information retrieval. It is therefore mainly to consider whether many sources of support may be combined to forecast term relations more precisely. This is mainly significant when frustrating to predict the probability of relevance of a set of terms given a query, which may involve both lexical and semantic relations between the terms. This paper presents a approach to expand the user query using three level domain model such as conceptual level(underlying Domain knowledge), linguistic level(term vocabulary based on Wordnet), stochastic model ME-HMM2 which combines (HMM (Hidden Markov Model and Maximum Entropy(ME) models) stores the mapping between such levels, taking into account the linguistic context of words.
In this paper, we study how to perform XML query expansion effectively from the high quality pseudo-relevance documents. A solution for selecting good expansion information is presented, in which various features impacting weight, such as term element frequency, term inverse element frequency, semantic weight of tag and level information, are analyzed and those term with high weigh value are selected as expansion term. Experiment results show that proposed expansion method is feasible. Compared to original query and traditional expansion method with no structure features considered, our method achieves better retrieval performance.
As enormous volume of electronic data increased gradually, searching as well as retrieving essential info from the internet is extremely difficult task. Normally, the Information Retrieval (IR) systems present info dependent upon the user's query keywords. At present, it is insufficient as large volume of online data and it contains less precision as the system takes syntactic level search into consideration. Furthermore, numerous previous search engines utilize a variety of techniques for semantic based document extraction and the relevancy between the documents has been measured using page ranking methods. On the other hand, it contains certain problems with searching time. With the intention of enhancing the query searching time, the research system implemented a Modified Firefly Algorithm (MFA) adapted with Intelligent Ontology and Latent Dirichlet Allocation based Information Retrieval (IOLDAIR) model. In this recommended methodology, the set of web documents, Face book comments and tweets are taken as dataset. By means of utilizing Tokenization process, the dataset pre-processing is carried out. Strong ontology is built dependent upon a lot of info collected by means of referring via diverse websites. Find out the keywords as well as carry out semantic analysis with user query by utilizing ontology matching by means of jaccard similarity. The feature extraction is carried out dependent upon the semantic analysis. After that, by means of Modified Firefly Algorithm (MFA), the ideal features are chosen. With the help of Fuzzy C-Mean (FCM) clustering, the appropriate documents are grouped and rank them. At last by using IOLDAIR model, the appropriate information's are extracted. The major benefit of the research technique is the raise in relevancy, capability of dealing with big data as well as fast retrieval. The experimentation outcomes prove that the presented method attains improved performance when matched up with the previous system. (Show More)
As massive data is stored in cloud datacenters, it is necessary to effectively locate interest data in such a distributed environment. However, since it is difficult to create a visual vocabulary due to the lack of global information, most existing systems of Content Based Image Retrieval (CBIR) only focus on global image features. In this paper, we propose a novel image retrieval framework, which efficiently incorporates the bag-of-visual-word model into Distributed Hash Tables (DHTs). Its key idea is to establish visual words for local image features by exploiting the merit of Locality Sensitive Hashing (LSH), so that similar image patches are most likely gathered into the same nodes without the knowledge of any global information. Extensive experimental results demonstrate that our approach yields high accuracy at very low cost, while keeping the load balanced.
This thesis studies a user interest model which can correspond with every user by collecting some personal information of users and optimize the retrieval results by search the original query expansion automatically. After putting the inquiry keyword into the search engine, the system will filter and sort the retrieval results according with the relevant of the user's interest model and finally present it to users. The experiments show that using this method can improve the precision ratio and recall ratio and realize the personalized service of the information index for users.
Students are confronted with a great amount of information to handle and filter and that's why they must be information literate to cope with the problem of information overload. Information overload could cause stress and confusion and even anxiety in human behavior. Anxiety is known from psychological literature as disorder in human's behavior that manifests through phobia. In relation with that people with information anxiety often feel that they could not filter all relevant information and that during information retrieval they would not be able to evaluate information which creates anxiety. People with information anxiety live in constant fear that information that come to them could be dangerous and that they must protect themselves. Besides information anxiety there are different kinds of anxiety that are in connection with that for example computer anxiety, Internet anxiety, social anxiety, technological anxiety etc. For the purpose of exploring information anxiety among students of Faculty of philosophy in Osijek the survey was conducted. Although students are exposed to different media and they feel information overload as a problem, students still cope well with it and according to research results information overload does not cause information anxiety among students' population.
In order to better cope with the explosive growth of network teaching information more accurate extraction of user required education information. In this study, a college English teaching information recommendation system based on multiple mixed criteria fuzzy algorithm is designed. Through English educational information retrieval, database design and recommendation display application, higher recommendation information can be better extracted by combining user preferences. Through the system application test results, it can be seen that the English educational information recommendation system involved in this article can achieve a wider range of content coverage. In addition, when the number of system users increases to 500, the accuracy of system users’ discovery is stable at 95%, and the recommendation recall rate is always above 90%. The recommendation effect of education information system is good, it fully shows that the recommendation effect of the English education information system proposed in this paper is good.
With the growth of digital Arabic documents specially in information retrieval (IR) and natural language processing (NLP) applications, identification of irregular plurals which are commonly called broken plurals (BP) in modern standard Arabic becomes very urgent issue. Broken plurals are formed by imposing interdigitating patterns on stems, and singular words cannot be recovered by standard affix stripping stemming techniques. Identifying broken plurals is an important and difficult problem which needs to be addressed. In information retrieval, deriving singulars from plurals is referred to as a stemming. The process of stemming can be achieved by removing the attached affixes from a given word. To the best of our knowledge, all existing Arabic stemmers are unreliable and still under research. Consequently, this paper proposes an approach which identifies broken plurals without the need to perform the stemming process on any given word. The well known decision tree system (WEKA J48) is applied to build a classifier (model) on a very huge Arabic corpus as a training data which is pre-processed and prepared as a piece of this work. The built classifier is evaluated using unseen test set. The obtained results reveal that a very promising broken plural recognizer could be designed and implemented for NLP applications.
This paper provides a Chinese question and answer retrieval method based on dynamic programming algorithm. This paper aims to unify the question and answer process, which was originally divided into three sub steps, into the same process, and integrate the idea of state machine in formal language to model the entity recognition process and the question and answer retrieval process with the same model. Entity recognition is modeled as a transfer process from the initial state 0 to state 1 of the system, The question and answer retrieval process is modeled as a transition from state 1 to state 2. To ensure the accuracy and breadth of answers, and to unify the whole question and answer process from a macro perspective. In addition, this paper models the transfer process of the target entity in the question answering process as the state transfer process in the state machine, models the transit entity as the intermediate state, and models the final answer as the termination state. On this basis, the dynamic programming algorithm is used to support the overall decision making of the transition process between all states, while ensuring the operating efficiency of the system.
This work presents the development of information technology and the selected application of Micro Electro-Mechanical Systems (MEMS) as an example of integration of information system applications in the area of ecology. The development of information systems is presented, which is shown as subjecting it to a number of transformation methods and procedures. The development of information technology over the last century has been shown. It showed how common MEMS technology has become as an effect of the development of information systems. One of the potential applications of MEMS in an ecological context has been presented. An example of using this technology is a weather station. The information obtained from it is particularly important in the case of heavily polluted environments.
This study proposes an algorithm where edge shape information is retrieved using a vote based rearranged chain code. The proposed algorithm rearranges and normalizes the chain code of edges to create a maximum vote based normalization chain code whose correlativity is reinforced. As it obtains cyclical maximum values from the chain code, rearranges them and flips code values according to the frequency, it is resilient against rotation and scaling noise and its retrieval speed is fast. As a result of a test, it was demonstrated that the proposed algorithm can retrieve rotated images better than the existing chain code based algorithm.
With the development of science and technology, the artificial intelligent information retrieval has become the norm. At present, this kind of information search contains keyword matching and browse the situation analysis, a combination of feedback on each other. To this, in this paper, "Apple's growth" in the Journal of library resources search for example, the keywords are divided into five types, using neural network to evaluate the similarity between the five kinds of searching information and keywords, on the basis of the above analysis, the mathematical process of analysis of the browsing situation is studied by using the principal component analysis (pca). The results show that the mathematical analysis process of the dynamic feedback artificial intelligence information retrieval technology needs to be repeated feedback, and the joint function, to ensure the effectiveness of the retrieval information.
This paper introduces a content-based, data-centered data search system for large-scale databases. Firstly, this paper designs the system from data source tracking module, data mining module and system warning module respectively. Then the software design flow chart and information retrieval algorithm are analyzed, and the software part of multimedia information retrieval system is designed. A fusion algorithm of central classification index and local classification index based on B+ tree is proposed to further improve the efficiency of classification. Experiments show that this method has high retrieval speed and accuracy.
Computer information extraction algorithms automatically extract valuable information from a large amount of unstructured and non-standard text, such as person name, location, attribution, organizational structure, date, etc. This article focused on the research of computer information extraction algorithms based on English corpora. Firstly, this article conducted simulation and performance testing experiments on an English corpus to evaluate the performance of the algorithm proposed in this paper. In the simulation and performance test experiments of this paper, it can be seen that when the dataset of the English corpus was 500, the total precision of the proposed algorithm was 91%, while the total precision of the neural network algorithm and the traditional algorithm were 81% and 70%, respectively, which were lower than the total precision of the proposed algorithm. Moreover, the precision, recall and F-value of the proposed algorithm were higher than those of the neural network algorithm and the traditional algorithm. In the experiments in this paper, the proposed algorithm outperformed the traditional algorithm and neural network algorithm in several evaluation indicators, such as precision, recall and F-value. In addition, the proposed algorithm was more solvable in dealing with problems such as linguistic diversity and complexity of grammatical structures. The proposed algorithm is helpful to promote the development and advancement of computer information extraction algorithms. This paper has provided ideas for exploring more advanced deep learning models and integrating a variety of computer information extraction algorithms.
This paper discusses an approach of university unstructured information resources management system implementation. Firstly a new idea of unstructured information management emphasizing automatic analysis is briefly introduced, and then the reasons of using Grid in this method are explained. Finally this paper analyzes its system architecture and introduces its prototype system in detail. To solve these problems, one kind of Grid-based university unstructured information resources management software infrastructure is designed. The software infrastructure uses Grid technologies to share organization's CPU abilities and realize open, agile and integrated information system, uses UIMA to integrate unstructured information analysis technologies, and insists using intelligent process to increase veracity of information analysis.
This work presents an Information Retrieval system specifically designed to manage Ancient Egyptian hieroglyphic texts taking into account their peculiarities at both the lexical and encoding level for its application in Egyptology and Digital Heritage. The tool has been made freely available to the research community under a free license and, to the best of our knowledge, is the first tool of its kind.
The proliferation of information technology news websites on the internet has resulted in an abundance of content that can be quickly and easily published. There are numerous websites in Thailand that offer information technology news, which provides great benefits to both students and IT employees in the country. These websites are valuable resources for keeping up with the latest technological advancements and for sharing knowledge among IT professionals. However, searching through this massive amount of material to find relevant news in the Thai language can be a challenging task. Traditional search engine approaches rely on general keyword searches to locate pertinent information, but this method can prove problematic when dealing with extensive content in the Thai language. In response, this paper proposes an information retrieval system designed and developed specifically for Thai-language information technology news. The system employs the K-means algorithm and support vector machine for classification, serving as a pre-processing step for text classification used in the indexing process of the information technology news retrieval system. Experimental results indicate a recall value of 68% and precision of 95%. The F-measure performance was found to be 79%.
The information retrieval process involves the multiple links such as determining goals, selecting the retrieval systems, and also formulating strategies. Based on this, the construction of the smart library system based on the book information retrieval is studied in this paper. With the improvement of people's search requirements, search engines are constantly adapting to users' requirements. In addition to some simple specialized search and classified navigation, there are also more professional search engines. To improve the search accuracy and construct the efficient system, this paper proposes the intelligent information retrieval model and designed the smart library system. The sensing models and the data analytic pipelines are combined to study the performance of the designed system. The proposed system is implemented on hardware platforms to show the performance.
We propose and evaluate a Cross-language Information Retrieval model (CLIR) based on the extraction and the translation of Formal Concepts avoiding queries and/or documents translation. The contribution of this work is the unified formal framework that integrates Formal Concept Analysis (FCA) and information retrieval for effective CLIR. The model is indexing bilingual documents using bilingual Formal Concepts extracted by a FCA. Moreover, the use of noun phrases, in addition to keywords, as indexes is studied. We use two comparable collections: an Italian-French collection and an English-French collection. To evaluate our model, we use three Information Retrieval models: TF. IDF, BM25 and Language Model. Finally, we study the query expansion results. Our main finding suggests that Formal Concept Analysis is effective to align Formal Concepts from different languages. Results indicate that our model performances are comparable to a words translation approach and better than a words embedding approach.
It requires a comprehensive concept of modern information technology to further advance the course reform in the context of information culture, which mainly contains three ways. In essence, it requires to make a shift from the concept of information technology as a tool to the concept of information technology as the culture.
This paper presents "Associative Image Search", a new image retrieval scheme and its specific engineering application, which enable value creation from big data. The main aim of the associative image search is the realization of information retrieval that enhances the potential for serendipities by providing users with new awareness. Thus, this paper presents the details of research for realizing associative image retrieval. Furthermore, as an example of its applications, a Biomimetics image retrieval platform is also introduced in this paper. By associatively and collaboratively using data accumulated in the fields of biology and material science, the Biomimetics image retrieval platform enables acceleration of their knowledge sharing in different research fields. From retrieval results actually obtained from this platform, there is discussion of the potential of serendipities such as new knowledge emergence.
Many of the criminal cases analysed by the Prosecution Office of the Federal District and Territories are repetitive and processing them can be streamlined by providing similar previous cases as template. We investigate the use of information retrieval techniques to enable automated identification of similar cases and evaluate if semantic search performs better than lexical search in the task of assisting legal opinion writing. As a proof of concept, syntactic indexing (TF-IDF and BM25) and semantic indexing (Latent Semantic Indexing - LSI and Latent Dirichlet Allocation - LDA) techniques were evaluated using document collections from two public prosecutors offices. In addition, we evaluate model enrichment with the use of recorded data about the cases, and also with the legal norm citations observed in documents. Baseline document collections sampled from full document collection from two public prosecutors offices were used for model evaluation utilizing Normalized Discounted Cumulated Gain (NDCG) as metric. We conclude that there is no significant performance difference between semantic and syntactic indexing techniques. In addition, we observe no significant performance gain with model enrichment. We chose the BM25 technique as more adequate because it has a good balance between performance and simplicity.
The necessity of a system that supports query-based searching with the presence of more than 7,154 world languages also increases as the accessibility of information across languages increases. The most challenging task is gathering knowledge in various natural languages, which calls for a lot of resources like databases and digital libraries and others. Nowadays, accessing information in multiple languages is also one of the problems. In this multilingual world it is common to see language barriers while seeking information in our native language. This is especially true for under-resourced Ethiopian language, specifically for Wolaytta language. Therefore, the main goal of Cross-lingual Information Retrieval (CLIR) is to enable users submitting a query in one language and retrieve relevant documents in another language. The development of Wolaytta-English CLIR enables Wolaytta language speakers to write queries in their native tongue in order to access and retrieve the various information sources that are available in both Wolaytta and English. The translation of the query phrases from Wolaytta to English is done using the Neural Machine Translation (NMT) approach. To do this, a total of 31,102 Wolaytta and English parallel corpus had been gathered, pre-processed and employed in NMT and CLIR experiments at the sentence level. For a bilingual run of Wolaytta and English, the suggested system achieved an average precision of 0.75 and recall of 0.73. Though the performance of the system was affected by the size and quality of the corpus, the result obtained for the small corpus is encouraging. Improving the performance of the CLIR system by implementing stemmer and POS tagger for Wolaytta language will be the future work.
The World Wide Web is widely used as a global standard for collecting information. Millions of Internet pages provide information on the existence of users worldwide. To extract the relevant web page, a search engine application is applied to the online search because of the important information through search queries entered by the user or page rank modules. The size and complexity of the World Wide Web are growing. That’s why it needs to be improved as the pages of information are more important in terms of better user experience who enters a query in a search engine. Website traffic analysis and network users in the development of Internet usage are critical in the world. The way of growth point to extract intelligence information resources knowledge and develop a variety of client and server-side tools. The traditional Web search engines take a long time to browse hundreds of thousands of users to return search results. Online library search engines and other large file libraries (such as customer support database product specification database, News Release Archives, News archive of articles, etc.) become difficult and expensive to file in manually. To overcome the problem, in this work, Sequential Hypertext Induced Topic Search (SHITS) method for web search is proposed and the role of web association is to provide web search engine’s different implementations of relevant results. The proposed method is using web search based on user behavior analysis using the Information Retrieval (IR). The web-based database of semantically related concepts is connected in Uniform Resource Locators (URL). It also uses the SHITS algorithm to extract useful keyword frequency from a huge set of words on each page and keywords. The proposed algorithm is used in many optimizations and the user’s useful results are obtained.
With the rapid increase in the volume of biomedical publications, developing an efficient search strategy to retrieve relevant biomedical documents that match the user search intention is a tremendous challenge. This paper proposes a novel pseudo relevance feedback technique which combines MeSH terms and UMLS concepts to improve the performance of retrieving biomedical documents from MEDLINE. The MeSH terms that annotate the feedback documents and the UMLS concepts derived from the feedback documents are used to help retrieve publications relevant to the user query. Extensive performance studies using the OHSUMED collection show that the proposed pseudo relevance feedback scheme using both MeSH terms and UMLS concepts improves the retrieval performance by 43.3% over the approach based on unexpanded query in terms of mean average precision. We have integrated the proposed strategy into G-Bean, which is publicly accessible at http://bioinformatics.clemson.edu:8080/G-Bean/index.jsp.
Information retrieval systems embed temporal information for retrieving the news documents related to temporal queries. One of the important aspects of a news document is the focus time, a time to which the content of document refers. The contemporary state-of-the-art does not exploit focus time to retrieve relevant news document. This paper investigates the inverted pyramid news paradigm to determine the focus time of news documents by extracting temporal expressions, normalizing their value and assigning them a score on the basis of their position in the text. In this method, the news documents are first divided into three sections following the inverted pyramid news paradigm. This paper presents a comprehensive analysis of four methods for splitting news document into sections: the paragraph-based method, the words-based method, the sentence-based method, and the semantic-based method (SeBM). Temporal expressions in each section are assigned weights using a linear regression model. Finally, a scoring function is used to calculate a temporal score for each time expression appearing in the document. These temporal expressions are then ranked on the basis of their temporal score, where the most suitable expression appears on top. The effectiveness of the proposed method is evaluated on a diverse dataset of news related to popular events; the results revealed that the proposed splitting methods achieved an average error of less than 5.6 years, whereas the SeBM achieved a high precision score of 0.35 and 0.77 at positions 1 and 2, respectively.
The cloud computing system, which operates via the Internet, provides large, scalable computer resources to users as a service. This technique has attracted significant attention from the scientific and corporate worlds alike. However, the notion of cloud computing lacks methods for knowledge discovery and information retrieval. Put differently, clouds need intelligence and autonomy. Web services are a crucial component of Service Oriented Computing (SOC) in cloud computing environments. Within a cloud context, a single Web service cannot get specific and tailored information from other Web services. Web services may be successfully composed with the aid of agents. This project’s primary objective is to offer a method of information extraction in a cloud environment using web services and the multi-agent system idea.
This paper aims to discuss how a linguistic-discursive approach can contribute to semantic computing field, especially to information retrieval process. A brief history of devices and methods used to storage and share knowledge reveals different strategies that aim to optimize users' searches through increasingly fast and accurate answers. The modern data-intensive technologies, the increased computational, and the data storage resources knowledge, that can be shared and accessed instantly, demand new discussions about the contributions of language theories to information retrieval. For this, it is analyzed a document that composes an innovative platform of knowledge management in agriculture, applying concepts offered by Greimas and his group in order to extract relevant information that can support the search engines. This scientific project related to the study of meaning generation associated to knowledge domain, to semantic computing approach become to be an important tool to help in the search of computer science advances, especially for bringing about continuous improvement within organizations and making them more competitive.
Recently, the multi-stage reranking framework based on pre-trained language model BERT can significantly improve the ranking performance on information retrieval tasks. However, most of these BERT-based reranking frameworks independently process query-chunk pairs and ignore cross-passages interaction. The context information around each candidate passage is extremely important for relevance judgement. Existing relevance aggregation methods obtain context information through statistical method and lost part of semantic information. Therefore, to capture this cross-passages interaction, this paper proposes a context-aware BERT ranking framework that utilizing abstractive summarization to enhance text semantics. By utilizing PEGASUS to summarize both sides of candidate passage accurately and then concatenate them as the input sequence, BERT could acquire more semantic information under the limitation of the input sequence’s length. The experimental results of two TREC data sets reveal the effectiveness of our proposed method in aggregating contextual semantic relevance.
We are living in the world of computers. This modern era deals with a wide network of information present on web. A huge number of documents present on web have increased the need for support in exchange of information and knowledge. It is necessary that user should be provided with relevant information about given domain. Traditional Information Extraction techniques like Knowledge Management Solutions were not so advanced that they can lead to extraction of precise information form text documents. It leads to the concept of Semantic Web that depends on creation and integration of Semantic data. The Semantic data in turn depends on building of Ontology. Ontology is considered as backbone of Software system. It improves understanding between concepts used in Semantic Web. So, there is need to build an ontology that uses well defined methodology and process of developing ontology is called Ontology Development.
As with the excessive information growth in the web, retrieving the exact segment of information even for a simple query, has transformed to a difficult and resource expensive state. Specially, in e-learning domain it is vital to search knowledge frequently and focusing on a limited well defined search space. IPedagogy is a question answering system which works with natural language powered queries and retrieve answers from selected information clusters by reducing the search space of information retrieval. In addition, IPedagogy is empowered by several natural language processing techniques which direct the system to extract the exact answer for a given query. System is evaluated with the use of mean reciprocal rank and it is noted that system has 0.73 of average accuracy level for 10sets of questions where each set is consisted of 35 questions.
With the widespread use of convolutional neural networks (CNNs) for image analysis and retrieval, it has become increasingly important to efficiently index and search the large feature space generated by these networks. This study provides a comprehensive review of various indexing techniques used for extracting CNN features. CNNs are commonly used in deep learning for image classification, object detection, and image retrieval. The main challenge in CNN-based image retrieval is the high-dimensional nature of the CNN features. Indexing techniques can be used to efficiently store and retrieve these features. In this study, different indexing techniques are discussed based on data structure, indexing scheme, feature type, and retrieval approach. This paper also provides an overview of the applications of these techniques, their pros and cons. The study will be useful for researchers and practitioners who want to use CNN features for image retrieval and need to select an appropriate indexing technique for their application.
The continuous need to process semi-structured data in the more connected and semantic web requires a retrieval model that can truly reflect the user's intention and capture a user's understanding. As a semantic network shows great potential in representing the inherent structure of information in a document, recent studies have attempted to apply semantic networks into information retrieval. While many of the recent works on semi-structured data retrieval focused on the use of field structure within the data. Solely relying on the field structure is insufficient to portray the user's understanding, which is represented through the use of specific query terms. In this study, we seek to overcome this limitation by utilizing a semantic network to model semi-structured data and apply a graph-based semi-structured data retrieval model. Using both a popular testing environment and a real-life query data, we compare the performance of the suggested model with various competitive state-of-the-art retrieval models. The study's findings demonstrate the strength of the proposed model while providing intriguing opportunities for further application of the model.
Due to the trend of individualization and adaptation of e-Learning, more and more SCORM-compliant teaching materials are developed by institutes and individuals in different sites. Also, cloud computing environments are emerging as powerful infrastructures to support e-Learning applications. Therefore, how to rapidly retrieve SCORM-compliant documents on cloud computing environments has become an important issue. Creating an index from folksonomies has been investigated in previous researches; however, the involved uncertainty has not been addressed. This paper focuses on the fuzzy index creation problem for learning content retrieval. A bottom-up approach to constructing the fuzzy index is proposed. The index creation method has been implemented, and a synthetic learning object repository has been built on a Hadoop cloud platform to evaluate the proposed approach. Experimental results show that this method can increase precision of retrieval.
This work introduces the first prior case retrieval models for Turkish courts. We investigated the rulings of the Court of Cassation of Turkey. Since law professionals have to find texts they are interested in from enormous legal databases and computers can process large amounts of text swiftly, information retrieval algorithms are helpful for law professionals. The information retrieval algorithms utilized in this work are recurrent neural network autoencoders, and the combinations of recurrent neural network autoencoders with BM25 algorithms. The combination of the long-short term memory autoencoder with ATIRE BM25 achieves the best scores on our dataset.
In this paper we adapt standard information retrieval techniques to a novel task, the mandatory regulatory review of public comments on proposed rule changes. The vast number of public comments exceeds the responsible agency’s ability to manually review in the time allowed. Therefore, the agency requires an automated approach to efficiently sort and process the comments. To rank the public comments’ relevance to rule sections, we implement a vector space model and compare the results to experts’ reviews. We perform experiments over several indexing techniques to improve semantic relevance, splitting the regulatory document based on textual formatting, text length, and a hybrid method combining these two techniques. To improve the accuracy of our predictions, we test various synonym lists generated from a domain-specific ontology, as well as variations of standard stopword lists. By applying the relevance search as a multi-class classification problem, we find the method that most closely matches human reviews, achieving respective normalized discounted cumulative gain and mean average precision scores of 0.83 and 0.75 on our test data set.
To improve the performance of network education information resources system, this paper proposes ontologybased similarity algorithm and semantic query expansion method. We mainly study ontology reasoning and define reasoning rules based on knowledge point ontology. Combining with various conceptual relationships in knowledge point ontology, we define corresponding rules and verify them by experiments, which lays a foundation for semantic extension. Then, a domain ontology-based information retrieval model is proposed based on the above content. By separating the relationship between concepts, the semantic similarity and correlation are calculated by using the conceptual hierarchy and association relationship and considering various factors, the semantic similarity and correlation are synthetically quantified into the semantic association degree for semantic retrieval. Finally, the prototype system of semantic retrieval is designed and validated, and the simple retrieval of resources is realized.
Retrieval of the semantic data becomes a time consuming and a highly tedious task. Ontology plays a maj or role in the retrieval of semantic data. In this paper the researcher has presented the methodology of music domain ontology construction using protegé 5.0 which is the best and the most commonly used Editor, intended to enhance information retrieval. The researcher has developed the string_ ontology in music domain using protege 5.0 and the working of ontology has been tested using the Descriptive Logic ontology query language. The novelty of this paper is that the researcher has constructed this string ontology in music domain from scratch and no such ontology has been constructed earlier.
Cross-modal hashing is popular for its high efficiency, low dimensionality, low storage overhead in information retrieval, and its ability to effectively characterize the consistency of high-level features of multi-source heterogeneous data. In this article, we present a fresh method that utilizes an intra-modal classifier that employs discrimination loss to learn intra-modal discriminations based on classification targets of image-text pairs. Meanwhile, the modality classifier serves as the discriminator in a GAN framework, using adversarial loss to reduce heterogeneity between different modality representations. To measure the efficiency of our method, we evaluate our proposed approaches against 8 other methods on three public benchmark datasets and show that our approaches outperform them in both I2T and T2I query.
In the realm of legal artificial intelligence (AI), the spotlight has been cast on its remarkable precision and efficiency, especially in tasks such as similar case retrieval where the identification of pertinent cases in response to a given query is of paramount importance. This task, distinct from traditional text retrieval, presents a set of unique challenges that necessitate the availability of high-quality, annotated datasets to facilitate efficient model training. The intricacies of handling extended queries and candidate documents, coupled with the varied interpretations of similarity, further compound the complexity of this endeavor. This study introduces an innovative training approach, combining dense and sparse retrieval methods. Utilizing a sparse retrieval model, we extract unlabeled data from extensive legal cases. Subsequently, a dense retrieval model screens this data, merging it with labeled data to create pseudo-labeled data, iteratively training until convergence. The results demonstrate exceptional performance in the Chinese law retrieval task dataset, showcasing a notable 3.66% precision enhancement and a substantial 3.62% improvement in mean average precision (MAP). However, the dataset’s imbalance across different charges of cases poses a challenge, potentially affecting retrieval performance for long-tailed legal cases. Nonetheless, these outcomes signify accelerated and more efficient retrieval of similar cases for legal professionals. Additionally, they provide high-quality references for non-legal individuals lacking expertise in the field.
Data Leakage is the biggest issue, in this digital world. Big organizations keep consolidated data of personal information of employees, users, customers, clients, etc. Even there are many algorithms used for privacy preservation to protect the sensitive or confidential data, data leakage becomes an uncontrolled threat in the digital world. The purpose of this research is to analyze the performance of data leakage detection system used in the information retrieval system in web application. In this paper, we consider the models such vector space model and Interaction information retrieval model. The idea is to check the semantically similar documents in the Web for leaked data. The result inferred that Interaction Information Retrieval Model is better than Vector Space Model.
The growth of digital cameras has resulted in the creation of huge collections of personal photographs and video. Thus, in the last few years personal information management (PIM) has become an important topic in the information retrieval area, in which it is playing an interest role in managing and retrieving personal collections. Therefore, many previous studies have tried to visualize these collections in different ways. This paper aims to discuss the importance of visualization techniques that have been proposed in the PIM field in general, and it shows the main factors that have influence on visualizing an interactive PIM system interface for large collections of photographs and video in particular. The factors were identified through reviews of related work. The collated factors form a conceptual framework of interactive PIM system interfaces for photograph and video collections. The aim of the framework is to ensure full understanding about the personal information lifecycle and thus leads to the production of a PIM system that satisfies its intended users.
In this paper, a literature information retrieval system based on data node database retrieval is proposed. The graph database Neo4j is used for storage, and the user searches through keywords and uses feature values to conduct in-depth searches. The system presents information to users in the form of graph nodes, and comprehensively and systematically explains the principles, methods, and methods of English translation of traditional Chinese medicine. Based on the discussion of this book, combined with the characteristics of TCM English and the principles that should be followed in TCM English translation, this paper studies and analyzes the translation methods of TCM English. Transliteration, free translation, literal translation, free translation and annotation and other translation methods can be used in TCM English translation.
With the popularization of computers and the development of network technology, Internet information services have gradually penetrated into all aspects of people's lives and are fundamentally changing people's traditional way of life. At present, the algorithm does not calculate the similarity of multi-node information resources when allocating and recommending multi-node information resources, which leads to long allocation time of multi-node information resources, low accuracy of information resources allocation and low coverage of recommendation list. To solve this problem, a multi-node information resources allocation recommendation algorithm based on collaborative filtering is proposed. The quality of information resource classification will directly affect whether the existing information resources can efficiently serve the information demanders, so the research of information resource classification technology has important practical value. Collaborative filtering is the most widely used and successful technology in recommendation system. Collaborative filtering algorithm is used to integrate and process the information resources of nodes, and a scoring model of information data is constructed. By using the scoring model, the nearest neighbor set of node information data is found for prediction scoring, and the similarity calculation is used to complete multi-node information classification. It can quickly build an available system to predict users' potential information needs with only a small amount of historical score data between users and items, and has the advantages of simplicity, ease of use and high accuracy. In order to improve the predictability of information resources, this paper proposes a compensatory resource quality evaluation method, which organically analyzes, evaluates and separates huge and chaotic resources, and integrates the quality characteristics of resources with feature extraction algorithms and classification algorithms. In order... (Show More)
In the recent years the use of smartphones have exponentially gained importance while shopping especially while shopping through online stores, existed both inland and abroad. This trend has been in huge practice by the millennial generation of 90's that have grown with these modern hand held mobile gadgets as their primary source of entertainment and communication. However, a little information is available as to how the retailer or store management informs or guides the walk-in customer while shopping in multi-channel scenario. The paper investigates both attractiveness and effectiveness of various information triggers (NFC, QR codes etc.) available that help the customer while shopping in the real physical store environment by utilizing customer-owned smartphones. The 191 participants were participated during the study and the results of laboratory experiments have been presented and based on these results recommendations have been proposed. For future research, the concept has been presented that explores the computer related attribution style for willing usage of various trigger technologies like NFC or QR codes.
Bengali documents are increasing on the World Wide Web and it is becoming a overwhelming problem for the increasing large number of web users to reviewing and reduce the information. Many researches have been conducted in the field of Natural Language Processing for English documents and in order to serve with satisfactory accuracy. This research work proposed a simple and powerful extraction based method for summarizing of the Bengali text documents. The system could summarize a single document at a time. The ultimate objective of the proposed methodology helps readers to get summary and insight of the Bengali documents without reading revealing the in-depth details. In the proposed Bengali documents summary generation method there are four features: Preprocessing, Sentence Ranking and Summarization, Combining Parameters for Sentence Ranking, Summary Generator. The results of performance evaluation show that the average scores of Precision, Recall and final scores are 0.80, 0.67, and 0.72 respectively.
The workshop aims to bring together researchers and practitioners to review and discuss ways of providing effective access to large-scale collections of cultural heritage content. The scale, variety and availability of cultural heritage content, combined with the variety of user groups with respect to background knowledge, specialist experience and needs is challenging in the context of existing access methods. In particular, we consider going beyond keyword search in large-scale cultural heritage digital libraries, in support of exploration and discovery. Our purpose for the workshop is to consider the opportunities and challenges presented by new and existing technologies, as well as the needs and experiences of diverse user communities. Our goal is to assess the current state-of the-art, to identify opportunities and establish future research priorities, informed by the combined knowledge and experience of academics and practitioners.
Information retrieval (IR) has become more and more important in our daily lives. But there are few graphical interfaces or tools that can represent complex data querying and the searching process. This paper proposes a visual tool for multi-attribute data retrieval. The tool provides a Voronoi treemap combined with a selected path tree (SPT) on one window to conduct information retrieval according to different attributes of data, which are organized by a Voronoi treemap, and the other window is used to show retrieval results synchronously. Herein, SPT is used to record user's selection of attributes and is visually superimposed on Voronoi treemap. It may be composed of different levels of nodes in Voronoi treemap and it records the operations of attributes - Union, Intersection and Complement- during the attributes selecting process. Users can also adjust the SPT interactively according to displayed results on the other window. Finally, the paper gives some examples of selecting historical relics in 3D digital museum design system based on double touch-screen. The results prove the usability and effectiveness of our work.
Building robust information revival systems demands employing efficient natural language processing and morphological analysis techniques. These techniques are commonly exploited to find syntactic and semantic matches between users' queries and their corresponding documents. Word stemming is one those techniques that has been widely employed in Information Retrieval systems, namely to increase their recall. A lot of research work has been conducted to evaluate English stemming techniques. However, a little attention has been given to Arabic stemmers. In this research work, we present a comprehensive review of state-of-the-art Arabic stemming techniques and compare between them according to a variety of criteria. In addition, we classify existing Arabic stemmers into four categories: Root-based, Affix Removal, Rule-based, and Context-based techniques. We review seven of the most commonly used Arabic stemming algorithms that fall under these categories, and provide a comparative analysis and evaluation between them according to the goal, input, employed approach, and output of each technique. We conclude this study by proposing our idea of building a hybrid Arabic stemming approach that combines multiple stemmers and exploits a new set of rules to better stem Arabic words.
Due to the increasing amount of electronic information, a majority of laypeople (ordinary people with no professional medical knowledge) now rely on the Web to seek for health information for self-diagnosis. However, research has shown that current search engines are failing to deliver effective search results due to the inability of laypeople to formulate good queries because of lack of domain knowledge and unfamiliarity with the medical vocabulary and concepts. This article attempts to address this by proposing a Selective Collection Enrichment approach that uses three different external resources to enrich a user query, thus generating three different expanded queries. In addition, we deploy pre-retrieval query performance predictors to select an expanded query that is most likely to perform better when retrieving on a local collection being searched. Furthermore, we evaluate the effects of combining several pre-retrieval query performance predictors' scores using data fusion techniques for Selective Collection Enrichment. Our empirical evaluation shows marked improvement in the retrieval performance in terms of nDCG@10 when the Selective Collection Enrichment approach is deployed.
In this manuscript we are presenting an efficient framework for retrieving the information from the Semantic Web using an Ontology-based approach. The present web is comprised of unstructured information which is extremely difficult to be retrieved by the search engine. The semantic web covers up this bug by exploiting the ontologies on the web and making the information machine understandable to be effectively retrieved by the search engine.
Recently, serious food safety events have emerged frequently and food safety issues have caused wide public concern all over the world. To find the needed food safety information for users from the growing information over the Internet, this paper presents an ontology-based semantic retrieval model used in food safety domain information retrieval. Firstly, food safety domain ontology is constructed to represent food safety domain knowledge, which has many advantages, such as sharable, reusable, and scalable. So it is very appropriately to describe the semantic relationships between food safety domain concepts. Then the lexicon for words segmentation is expanded based on the food safety domain ontology to return more accurate preprocessing results of queries. Finally semantic query expansion and sorting algorithm of search results based on concepts similarity computation model are implemented so that more relevant results can come before irrelevant retrieval results. The experiments show that the precision of the retrieval method with the proposed model is higher 25.2% averagely than that of the traditional retrieval method.
The main goal of our research is to invent a new Web Image Search Portal with more innovation. We aim to search and retrieve the matched images from the uploaded online images with the help of Auto hidden tag method using the RANSAC algorithm through Open Computer Vision. We implemented a new system using the object detection process in an image using the openCV technique. When a user posting their images, if they forget to comment on all the objects present in the image also our proposed system will detect all the object present in the image using object detection algorithm, then this object will be extracted from the image and recognized with large dataset by using RANSAC algorithm. This system will be very useful to the end user in required image retrieval process, even images which are not labeled or tagged by the upload user also retrieved in this system. Auto hidden tagging process with visual feature extraction and the comparison is the main technique used in this system.
Nowadays, communication between patient and doctor during an appointment has changed significantly owning to the opportunity that medical portals provide. Whether or not necessarily appreciated by the doctors, the patients became more aware of the first symptoms' suggesting a particular disease and the medical procedures that apply as a standard. Estimating amount of reliable factual medical information in a document is carried out by parametrizing space of digital documents and dividing it into subsequent layers that represent distribution of the system responses computed as random variables to a query about medical information. Analyzed are the following attributes: dynamism of decrease of query words numbers in the documents, precision, recall in the metric space layers, their mutual correlation and specifically the amount of reliable medical information in the documents. Sensitivity of estimators is explored in order to determine the final decision about further browsing digital documents of the metric space for more medical information that satisfies the user's need. For identification of the true positive information in the space layer and then, in each document of this layer, matching of medical terminology with the document contents, is processed following binary Boolean search space model.
In this paper, we propose an extended IR vector space model by introducing some semantics concepts inherent to YAGO ontology (A large ontology derived from Wikipedia and Word Net), then an adaptation of Bees Swarm Optimization algorithm (BSO) that was developed previously for classical IR, to the new context. In the offline indexing step we define a representation based on semantics annotations taken from YAGO ontology, for the documents and the queries. Afterwards, we integrate the appropriate structures obtained at the first stage in the online interrogation process. The experimental tests have been performed on Reuter's corpus and compared to those previously obtained. The preliminary results clearly show that this pathway promises to provide efficient results for real time IR and deserve to be further deepened.
With the rapid expansion of information in the network environment, it brings many challenges for information retrieval staff. It seems to be a pair of contradictions between the searchers' retrieval skill and the rapid expansion of information. On the one hand, the website developers must carefully consider arranging the retrieval key words in order to ensure the users to be able to meet their needs. On the other hand, the users should also accurately utilize retrieval key words to obtain information which they need. Frequently, after retrieval, the users have already lost the interest to the website or they have wasted a lot of precious time. According to the similar users' searching habits, information tips in the search box attempt to provide a suggestion list to the users, which can help users to ensure their own demands as soon as possible, and then help users to quickly, accurately and conveniently search the information which they want. It can not only improve the users' satisfaction, but also reduce the website developers' workload. This function can make the user interface more friendly and help users to avoid typing wrong words. In the paper, the authors discuss Ajaxrelated technologies, and information tips in the search box model based on Ajax is established. Finally, this model is implemented by using Ajax.
Developments in the technology and the Internet have led to increase in number of digital images and videos. Thousands of images are added to WWW every day. To retrieve the specific images efficiently from database or from Internet is becoming a challenge now a day. As a result, the necessity of retrieving images has emerged to be important to various professional areas. This paper proposes a novel fuzzy approach to classify the colour images based on their content, to pose a query in terms of natural language and fuse the queries based on neural networks for fast and efficient retrieval. Number of experiments was conducted for classification and retrieval of images on sets of images and promising results were obtained. The results were analysed and compared with other similar image retrieval system.
With the purpose of saving the developing time of software engineers and promoting the work efficiency of programs, the research on automated source-code summarization (SCS) has become necessary in recent years, i.e. generating language descriptions for source code. To date, there exist two categories of SCS methods: information retrieval (IR)-based SCS and neural-based SCS. The latter is the mainstream method at present, however, this line of work suffers from the drawback of incapability to generate low-frequency words, which potentially degrades the performance. To tackle this predicament, we in this paper propose an IR-enhanced neural SCS method RetCom to improve the prediction of low-frequency words through leveraging both structural-level and semantic-level code retrievals. Furthermore, we figure out a token-level context-dependent mixture network to fuse different information sources, i.e. original code, structurally most similar code, and semantically most similar code. Finally, extensive experiments are performed to validate our proposed RetCom using two real-world datasets. Compared to several baseline methods, the experimental results show that our method does validly capture more low-frequency words to conduct a superior performance.
This paper discusses the major issues of publishing, writing, use of standards, and search engines for mathematical contents on the web. We also suggest two hypothetical features, first one describing a concept of a rich text box called `Equation Box' (EBox) that provides a facility to writing text and math equations in line. The second feature describes the underpinning working of a MIR (Mathematical Information Retrieval) system. The paper has been divided into four parts. The first part focuses on available web standards for publishing mathematical expressions and reusing them between different web applications, the second part focuses on various interfaces, commonly known as `Equation Editors' provided to write and edit mathematical expressions, and their related problems. The third part deals with Digital Mathematical Libraries (DMLs) which provide collection of such resources to users, and finally the fourth part deals with MIRs, available as Open Source, and their effective and efficient use to tackle typical problems of Information Retrieval in this particular domain.
This study uses statistical inference to compare the performance of three text models used for bug localization in collaboration systems: Vector Space Model (VSM), Latent Semantic Indexing (LSI), and Latent Dirichlet Analysis (LDA) on the method level. After the three models are compared we confirm that VSM is the superior model. We then, point out which external factors i.e. methods lengths, queries lengths, methods documentation comments, products names and components names mentioned in bug reports affect VSM performance. We conclude that VSM performance is positively correlated with most of the tested factors. We believe our results can be helpful to: (i) text models developers, to understand the strengths and limitations of VSM for future development; (ii) bug localization programmers using classical VSM, to understand improved ways to prepare methods extracted from big data collaboration systems and (iii) bug reporters, to follow the most efficient methods presented in this work in reporting bugs to enhance the information retrieval process.
Recent evolution in computer science and web technology result in continually expanding resources on the web that pose unprecedented challenges in Big data to access it with the existing framework. In order to access the vastly increasing resource more effectively, clustering methods are required. Resource Description Framework (RDF) which is used to describe the resources on the web. Results suggest that linking resources in a semantic manner enhance searching capabilities over the web such that using keywords as linking information can be used to calculate the semantic similarity. Consequently, the traditional retrieval system often suffers from scalability and inefficiency problems when processing or analyzing such large-scale data. Hence, the basic idea is to propose a framework that retrieves the information effectively from the Resource Description Framework (RDF) using SPARQL ENGINE. Furthermore, we purpose pruning method that improves accuracy by quickly filtering out the false alarms (whenever there is a change in the source) and automatically update the RDF document that can be further used for semantically retrieving the root source content of the particular resource. To improve its scalability and efficiency in a big data environment, this framework is implemented on Hadoop, a widely-adopted distributed computing platform.
This paper discusses other ways that can be used to obtain information on the translation of Indonesian Alqur’an and hadith that is by using Information Retrieval Technique which the process is tokenizing, stopword removal and steamming. The stemming method used in this study is ECS (Enhance Confix Stripping). The database consists of hadith documents Bukhori Muslim numbered 7008 number of traditions and Al Quran translation 30 Juz from the Ministry of Religious Affairs of the Republic of Indonesia. The resulting system can speed up the search process on the Indonesian translation of the Qur’an and Hadith and obtain relevant information.
The paper deals with utilization of neural networks for information retrieval. It is focused on reduction of text document space by Hebbian neural networks. The Hebbian neural network with Oja learning rule with linear activation function reduces term space into much lower dimension and gives good results for text document dimension reduction and retrieval. The aim of this paper is to try to increase the retrieval evaluation by F-measure that applies different nonlinear activation functions to the output layer of network. Results show better F-measure when applying other nonlinear activation functions instead of applying classical linear activation function. The results were verified on the collection of 50 documents and 100 terms, where documents were clustered into five different clusters. For each dimension the Precision, Recall and F-measure were computed and the results were depicted graphically.
This paper describes a study on the adaptation of information retrieval (IR) using Term Frequency-Inverse Document Frequency and data visualization and to ease the searching for hadith process. The distribution of fabricated hadith that happened nowadays could negatively affect religion, Muslims, and Muslims' faith. The information technology's advent provides the platform for any applications and systems by indexing and IR to access the hadith's contents. However, the hadith's nature of scattered and disordered causes unpleasant usage of the indexing. The present IR method uses only the straightforward keyword search that does not highlight the details, significance, and relevance of the results retrieved. Therefore, this research expected to provide the progressive web-based application as the platform for users to locate hadith by entering any keyword as an input. It is also visualizing the most relevant source of hadith to enable users to make a better decision for them to read. The proposed solution's functionality and usability were tested, and as a result, it is proven that all the system features were functioning well and managed to receive 78.4% acceptance of system usability.
Clothes products contain many characteristics which are difficult to describe in keywords, such as texture, shape, or the relationship between object and space. Based on this issue, we develop an image-based visual clothing retrieval system, which extracts and uses the features of clothing images to find objects that are difficult to describe by text, or without text annotations. We integrate techniques from image processing and information retrieval, and develop our retrieval system which provides region-of-interest visual search. Experiment on an image database with about 1891 clothing images shows that our approach is effective and efficient.
Communication networks have become a fundamental part of many critical infrastructures, playing an important role in information delivery in various failure scenarios triggered e.g., by forces of nature (including earthquakes, tornados, fires, etc.), technology-related disasters (for instance due to power blackout), or malicious human activities. A number of recovery schemes have been defined in the context of network resilience (with the primary focus on communication possibility in failure scenarios including access to a particular host, or information exchange between a certain pair of end nodes). However, because end-users are becoming more and more interested in information itself (regardless of its physical location in the network), it is appropriate to complement the well-defined framework of network resilience with one that addresses information resilience, and to introduce definitions of relevant disciplines and measures, as proposed in this paper.
In modern society, to train students with higher information literacy is one of the important goals of higher education in the world. Compared with Europe and America and other developed countries, China started late in the education of college information literacy and the effect of information literacy education is not ideal. This study analyzes the content of information literacy education and development status and finds the relevant constraints. Based on the advanced experience from abroad, the paper proposes some suggestions on the system construction of information literacy curriculums in Chinese colleges. Namely, by rational design of curriculum system and training path, to enhance the students' information, knowledge, ability, awareness and moral construction, and promote the comprehensive improvement of information literacy of college students.
Existing search engines index web pages as a whole and use them for information retrieval, which leads to irrelevant documents being returned to users. This paper proposes a new indexing approach for solving this problem by 1) using VIPS algorithm for page segmentation, 2) filtering out the function blocks through several heuristic rules, 3) clustering feature blocks into different sub-documents and indexing them respectively. For three user queries, the initial results retrieved from Google are compared with the search results of improved indexing system, which shows that our approach gets a higher performance in terms of precision and F-measure.
We propose to extend the concept of private information retrieval by allowing for distortion in the retrieval process and relaxing the perfect privacy requirement at the same time. In particular, we study the trade-off between download rate, distortion, and user privacy leakage, and show that in the limit of large file sizes this trade-off can be captured via a novel information-theoretical formulation for datasets with a known distribution. Moreover, for scenarios where the statistics of the dataset is unknown, we propose a new deep learning framework by leveraging a generative adversarial network approach, which allows the user to learn efficient schemes from the data itself. We evaluate the performance of the scheme on a synthetic Gaussian dataset as well as on the MNIST, CIFAR-10, and LSUN datasets. For the MNIST, CIFAR-10, and LSUN datasets, the data-driven approach significantly outperforms a nonlearning-based scheme which combines source coding with the download of multiple files.
Construction of vocabulary file for Myanmar information retrieval (MIR) is an important task during index retrieval operation. Although generating vocabulary file using dictionary and orthography have been existed for Myanmar language, the performance depends on the coverage of the dictionary and training dataset and can cause out of vocabulary (OOV) problem, leading to lower precision and recall in information retrieval. Resources are sparse for Myanmar language and it is difficult to find a comprehensive dictionary for any domain. In this paper, we propose a new method for similarity percentage measurement based automatic construction of vocabulary file, a variant of Levenshtein distance without the use of any additional data e.g., training corpus. The development of the automatic generation of vocabulary files from raw Myanmar texts to use in indexing of MIR task. The experimental results showed that the method is highly effective for Myanmar language government domain.
With the inception of World Wide Web, the amount of data present on the internet is tremendous. This makes the task of navigating through this enormous amount of data quite difficult for the user. As users struggle to navigate through this wealth of information, the need for the development of an automated system that can extract the required information becomes urgent. This paper presents a Question Answering system to ease the process of information retrieval. Question Answering systems have been around for quite some time and are a sub-field of information retrieval and natural language processing. The task of any Question Answering system is to seek an answer to a free form factual question. The difficulty of pinpointing and verifying the precise answer makes question answering more challenging than simple information retrieval done by search engines. The research objective of this paper is to develop a novel approach to Question Answering based on a composition of conventional approaches of Information Retrieval (IR) and Natural Language processing (NLP). The focus is also on exploring the use of a structured and annotated knowledge base as opposed to an unstructured knowledge base. The knowledge base used here is DBpedia and the final system is evaluated on the Text REtrieval Conference (TREC) 2004 questions dataset.
Music Information Retrieval (MIR) is an interesting area of investigation. The MIR research aims to develop new techniques for processing musical information and searching music databases by content. Therefore, robust retrieval and matching techniques are required. This paper devises a more practical and efficient approach to MIR by investigating a variety of statistical and signal processing-based features, such as Fast Fourier Transform (FFT), Linear Predicative Coding coefficients LPC, Pitch and the wavelet analysis. The features were tested by using different measures of melodic similarity to achieve a better search in musical databases. The paper uses the recall measure to evaluate the retrieval results. It indicated a poor retrieval quality with statistical features and LPC parameters and it is improved when we used the FFT coefficients, and finally using wavelet coefficients caused a significant improvement in its value.
In this digitalera, there is an enormous increase in the digital documents due to the use of the internet. Despite the hype, in the processing of digital documents, the problem of polysemy interprets more than one meaning for a single word. Hence the polysemy related queries will adversely affect the relevant result retrieval process. The paper addresses the polysemyissue on information retrieval paradigm and used clustering as a tool for improving the accuracy and appropriateness in the information retrieval process. This paper proposed a novel approach that generates a coherent group for the query topic. The proposed model is a hybrid of the k-means algorithm and the hierarchical approach named as HK clustering model to utilize the efficiency of both the strategies and overcome the drawbacks of each other.
This paper proposes a new Information Retrieval Model based on Possibilistic Networks. The model structure integrates most relevant term to term dependence relationships. The approach used to extract the set of these dependencies focuses on local dependencies between terms within each document. The relevance of a document to a query is interpreted by two degrees: the necessity and the possibility. The necessity degree evaluates the extent to which a document is relevant to a query, whereas the possibility degree evaluates the reasons of eliminating irrelevant documents. These two measures are also used for quantifying terms-terms links and terms-documents links. Experiments carried out on three standard document collections show the effectiveness of the model.
Modern enterprises in the increasingly competitive market environment of the demand for office information management system has increased rapidly. This study proposes a comprehensive design and implementation strategy for the shared office information management system. The three-layer architecture is developed, including front-end interface layer, business logic layer and data storage layer, and combines Visual Studio Code and Node.js tools. The experimental results show that the system can effectively meet the core business needs including document management, project management, task allocation, calendar and planning, team communication and information retrieval, and significantly improve the office efficiency of the enterprise. Although this study provides rich insight into system design, certain limitations remain that point the way for future research.
Myanmar News Retrieval involves searching for and extracting pertinent documents or data depend on a user's query or information need. Neural Information Retrieval (Neural IR) leverages the power of neural networks to enhance the effectiveness of this retrieval process. In a crucial information retrieval (IR) task, neural methods are employed to rank documents in response to a given query. Neural models are utilized in various information retrieval (IR) contexts, including ad-hoc retrieval, recommender systems, multimedia search, and conversational systems aimed at producing responses to natural language queries. The evaluation metrics will be utilized to gauge the effectiveness of the retrieved documents deemed relevant to the given query. This paper introduces a Kernelized Neural Ranking Model (KNRM) designed specifically for ad-hoc retrieval tasks. Through comprehensive analysis of datasets, we analyses that conversational questions exhibit complex phenomena that are absent in current reading comprehension datasets. Our model focuses on adopting a deep architecture specifically at the query term level to support relevance matching.
The sensor data are collected in regular interval and securely stored in cloud. In order to provide security this research work aims in proposing SK-IR: secured keyword based retrieval. In security mechanism the symmetric key encryption scheme AES is being used to secure the sensor data. The data are retrieved from cloud based on its keywords, scores and file location which are available in the posting list. The hash function is implemented to the posting list to enhance the security. The posting lists are stored in unknown cloud server location and it protects the data from getting hacked. In this paper we studied the performance of encryption methodology over unstructured database by choosing Hbase/Hadoop platform. Since HBase can handle the huge volume, variety, and complexity of data used on the Hadoop platform.
Big data strongly demands a network infrastructure having the capability to efficiently collect, process, cache, share, and deliver the data, instead of simple transmissions. Such network designs show the requirements of energy efficiency, availability, high performance, and data-aware intelligence. To meet these requirements, we adopt the information-centric networking (ICN) approach, where data are retrieved through names and in-network caching is utilized. However, as the typical existing ICN architectures, content centric network (CCN) cannot efficiently utilize the caches for data sharing because of the on-path caching strategy, and network of information (NetInf) demonstrates the resolution latency for data retrievals. To design an efficient and effective ICN architecture for big data sharing, we combine the strong points of CCN and NetInf, where information islands (IOIs) and management plane are utilized for direct data retrieval and global data discovery, respectively. We provide a reference architecture and propose an aggregatable name-based routing (ANBR), which can naturally enable consumers to retrieve the closest copy of information. In this network, each piece of data can be cached at one IOI at most once, which greatly improves the efficiency of cache usages. The consumers first try to retrieve the data in the local IOI, and then try to globally retrieve it from the closest IOI, holding the copy of the data if necessary. We investigate the impact from the key factor, IOI size, to the energy consumption of ANBR. It shows that energy consumption first decreases and then increases as the IOI size increases, and the optimized IOI size can be found for deployment. Furthermore, we study the relation between the optimized IOI size and the average retrieval times for the data. The result shows that the optimized IOI size increases as the average retrieval times increase.
This paper introduces a new weighting scheme in information retrieval. It also proposes using the document centroid as a threshold for normalizing documents in a document collection. Document centroid normalization helps to achieve more effective information retrieval as it enables good discrimination between documents. In the context of a machine learning application, namely unsupervised document indexing and retrieval, we compared the effectiveness of the proposed weighting scheme to the `Term Frequency - Inverse Document Frequency' or TF-IDF, which is commonly used and considered as one of the best existing weighting schemes. The paper shows how the document centroid is used to remove less significant weights from documents and how this helps to achieve better retrieval effectiveness. Most of the existing weighting schemes in information retrieval research assume that the whole document collection is static. The results presented in this paper show that the proposed weighting scheme can produce higher retrieval effectiveness compared with the TF-IDF weighting scheme, in both static and dynamic document collections. The results also show the variation in information retrieval effectiveness that is achieved for static and dynamic document collections by using a specific weighting scheme. This type of comparison has not been presented in the literature before.
Cloud Computing is getting prevalent and as development is speeding up, more and more sensitive information is being centralized onto the cloud, such as e-mails, personal health records, company finance data, and government documents etc. The data owners and cloud server are not in the same trusted domain for the data outsourcing. This may put the outsourced unencrypted data at risk, leading to leakage of sensitive information to unauthorized entities or even it can be hacked by the cloud server. To avoid leakage and to achieve security and maximize the usability, we propose the advance techniques of both Cryptography and Information Retrieval communities, in the spirit of "as-strong-as-possible" security guarantee. The paper focuses on online secure information retrieval using genetic algorithms. It increases information retrieval relevancy. Vector Space Model is used for information retrieval based on the similarity measurement between query and documents. Documents with high similarity to query are judge more relevant to the query and should be retrieved first. Also, main aim of this paper is to increase the relevancy over exact match in secure encrypted domain. The process aimed at finding the effective and best searching technique over cloud data with the enhanced security.
This article proposed a new approach for automatically correcting queries over Multi-XML, called MXDR(Multi-XML Distributed Retrieval). We first classed multi-XML documents by a clustering method, and elicited the common structure information. Then generated certifiable structured queries by analyzing the given keywords query and the common structure information of XML datasets. We can evaluate the generated structured queries over the XML data sources with any existing structure search engine.
The Kohonen self-organizing feature map (KSOFM) also known the Kohonen self-organizing map (KSOM) under comes the artificial neural network. An important characteristic of ANN is ability to learn. Learning is a process by adjusting the weight with respect to input features so that the actual output response will matches to the desired output. The role of learning based network is that the point distributes themselves across the input space to selected or mapped group of similar input vectors, while the output nodes compete among themselves. Once network trained, the network produces a low dimensional data clustering of the input value that preserves the ordering of the actual structure. Thus nodes can be recognizing groups of similar input vectors. It generates a mapping of the input vector to the output layer, which depends on input vectors and overall result in dimensionality reduction of the input space. Follows the “winner-take-all” policy means which cluster unit weight matches more closely with the input pattern is considered the winning neurons so that pattern classification process take place. It is basically based on sumup of actual sequence and final junction point.
With the growing rise of digital platforms such as facebook, twitter and others that attract billions of people around the world, it has gained a lot of traction as a source of unstructured data. The volume of unstructured data is expected to expand at a 60 percent annual rate in the future years. The emergence of social media has facilitated the expansion of unstructured data. Thanks to social media, everyone may now express and share their thoughts and opinions on a wide range of topics. Infodemiology is one such popular and rapidly expanding information retrieval method (i.e., information epidemiology). As per the concept, a set of approaches that study data, particularly health data on the internet, for the purpose of public health studies and policy. Web data is analyzed to detect disease outbreaks faster than traditional surveillance in infoveillance (also known as information surveillance). This research has created a prototype software model for extracting spatiotemporal diseases from diverse data sources in this vein. To identify prospective sickness instances, the proposed method gathered tweets and other postings and used NLP semantic algorithms with transformers called BERT. The prototype system is tuned to improve disease detection using rules given by dissimilar semantic similarity algorithms. The prototype’s output is a visual representation of likely disease forms and spread, which is a straight IE produce from Twitter and other sites.
This study develops a web information retrieval system using the fuzzy relations in the indexing and ranking portions of standard web retrieval methods. The system was developed, including crawler, indexer, ranking portion, and user search structure. The BK-products of fuzzy relations with closure/interior properties are used to construct a fuzzy thesaurus and further to retrieve the relevant documents. The results of the system are evaluated and compared with the existing search methods.
The purpose of finding the Semantic Similarity is to find a degree of match between the aspects which are conceptually similar but not lexically. It is an important issue in the field of web information retrieval which requires retrieving a set of documents that are semantically related to a given query posed by the user. For the efficient information retrieval from the web that involves the collection of documents related to a domain, it is required to explore various facets of semantic similarity and design their respective quantitative metrics. These aspect are spread across various features like individual /Isolated words, terms (multi-word expressions/phrases/domain specific terms), sentences, entities, documents of the same language, documents of different languages. This paper takes into account the various similarity aspects explored in the literature and their respective possible quantification measures.
In this digital world, there is no lack of digitize text, image, audio and video for any subject but extracting the conceptual information relevant material is very difficult and achieved by different information retrieval(IR) technologies. Boolean Matching Model, Probabilistic Model, Extended Boolean Model, Fuzzy Set Model and Vector Space Model are some of the widely used models for information retrieval. Among these, vector space model (VSM) is one of the classical and widely used model which works based on the mathematical concepts of linear algebra. In VSM, queries and documents are the vectors in the highly multidimensional space and terms are used as dimensions to construct the index key to represent the documents. In this paper, we consider different approaches of vector space model for information retrieval and compare the results to find a neat understanding of term count model, classical vector space model and normalized vector space model. All the three models works based on the concept of term frequency (tf) and inverse document frequency (idf). Here the vector space retrieval technology we have applied on odia text to get an overview how VSM works on regional language like Odia. Odia (officially changed the name from Oriya to Odia in November, 2011) is one of the language mainly spoken by the people live in Odisha, the south east state of India. This Model can also be used for text summarization and data mining purpose.
This paper addresses an important problem of semantic representation of documents for information retrieval in a data integration system. Quite often search query on documents seek relevant information. Conventional methods of feature extraction do not capture relevance but rather focus on term matching for query processing. Challenges of semantic representation of documents lie in identification of important features. Most of the techniques for identifying important features, transform original data to a different space. This gives a sparse matrix which is computationally expensive. So we come up with an alternative approach based on CUR matrix decomposition. This technique finds important documents and important terms in order to improvise the query processing. Experimentation results prove the efficacy of this approach on five data sets.
The Arabic language is expanding in the world. According to UNESCO, the Arabic language is spoken by more than 422 million native speakers around 29 countries and among 1.6 billion Muslims worldwide use it to perform their daily prayers. The presence of the Arabic language on the internet grew around 6.091% in the last fifteen years (2000-2015), it is the highest growth of the ten top online languages. Therefore, the number of Arabic documents increases rapidly. This calls for the necessity to improve Arabic Information Retrieval (IR) techniques. Many researchers agree on the benefits of both stemming and lemmatization in IR, primarily with highly inflective languages, short documents and limited space for storing data. The chief purpose of the current study is assessing the impact of stemming and lemmatization on Arabic IR. In this paper, we illustrate several concepts of Arabic morphology, including stemming and lemmatization algorithms. Then, we highlight the use of these latter and their benefits for different Arabic IR systems. Finally, an experiment is conducted to calculate the occurrence of all Quranic surface word, stem, and lemma forms by searching their similarities in both Classical and Modern Standard Arabic resources. In doing so, recent and efficient analyzers AlKhalil Morpho Sys and MADAMIRA are used.
Relevance feedback (RF) allows users to be actively involved in the information retrieval process and has been widely used in various information retrieval tasks. While most existing RF methods in content-based image retrieval (CBIR) focus on visual features of individual images only, in this paper we formulate the relevance feedback process as an energy minimization problem. The energy function takes into account both the feature aspect of each image and the manifold structure among individual images. The solution of labelling images as relevant or irrelevant is obtained with the graph cuts method. As a result, our method enables flexibly partitioning the feature space and labelling of images and is capable of handling challenging scenarios (or queries). Experimental results demonstrate that our proposed method outperforms the popular RF methods.
Retrieval of privatized information through deep web signifies that data extraction from the private email accounts of the users such as Gmail, Yahoo, rediff etc. Now a days most of the people keep their private information in their accounts and every user have more than one account of different service provider. All these service providers do not have adequate internal searching facility so it is tough for the individual to search its privatized contents on their own different accounts in mess-up scenario. We have analyzed through literature survey there is no proper architecture available to solve this problem. The functioning of proposed framework is divided into two parts, the first part of the architecture is based on retrieving of privatized information from the different private domains after authenticating the user on host domains and the second part is based on user authentication to domain mapper. In order to solve the particular problem we introduced the method of privatized information retrieval in deep web scenario.
Interaction with users is a powerful strategy that potentially yields better information retrieval for all types of media, including text, images, and videos. While spoken document retrieval (SDR) is a crucial technology for multimedia access in the network era, it is also more challenging than text information retrieval because of the inevitable recognition errors. It is therefore reasonable to consider interactive functionalities for SDR systems. We propose an interactive SDR approach in which given the user's query, the system returns not only the retrieval results but also a short list of key terms describing distinct topics. The user selects these key terms to expand the query if the retrieval results are not satisfactory. The entire retrieval process is organized around a hierarchy of key terms that define the allowable state transitions; this is modeled by a Markov decision process, which is popularly used in spoken dialogue systems. By reinforcement learning with simulated users, the key terms on the short list are properly ranked such that the retrieval success rate is maximized while the number of interactive steps is minimized. Significant improvements over existing approaches were observed in preliminary experiments performed on information needs provided by real users. A prototype system was also implemented.
Web service discovery is one of the major research thrust areas in the field of computing environment. From last decade, a good number of researchers are contributing their thoughts in finding the best available service from a pool of services that can satisfy the user's requirement. Different researchers have adopted different methodologies and ideas to visualize their noble thoughts in the field of web service discovery. Information retrieval methods are one of them. Utilization of IR methods in service discovery approaches makes the discovery process efficient. In this paper, we focus on those service discovery approaches which employ information retrieval methods for the purpose of automatic discovery. This paper provides a survey of how these approaches differ from each other while discovering a service.
Digital libraries in the scientific domain provide users access to a wide range of information to satisfy their diverse information needs. Here, ranking results play a crucial role in users' satisfaction. Exploiting bibliometric metadata, e.g., publications' citation counts or bibliometric indicators in general, for automatically identifying the most relevant results can boost retrieval performance. This work proposes bibliometric data fusion, which enriches existing systems' results by incorporating bibliometric metadata such as citations or altmetrics. Our results on three biomedical retrieval benchmarks from TREC Precision Medicine (TREC-PM) show that bibliometric data fusion is a promising approach to improve retrieval performance in terms of normalized Discounted Cumulated Gain (nDCG) and Average Precision (AP), at the cost of the Precision at 10 (P@10) rate. Patient users especially profit from this light-weight, data-sparse technique that applies to any digital library.
In this paper, we present a Geographical Information Retrieval system, which aims to automatically extract and analyze touristic information from photos of online image collections (in our case of study Flickr). Our system collect all the photos, and the related information, that are associated to a specific city. We then use Google Maps service to geolocate the retrieved photos, and finally we analyze geo-referenced data to obtain our goals: 1) determining and locating the most interesting places of the city, i.e. the most visited locations, and 2) reconstructing touristic routes of the users visiting the city. Information is filtered by using a set of constraints, which we apply to select only the users that reasonably are tourists visiting the city. Tests were performed on an Italian city, Palermo, that is rich in artistic and touristic attractions, but preliminary tests showed that our technique could successfully be applied to any city in the world with a reasonable number of touristic landmarks.
With the increase in amount of information being made available in digital format, information retrieval is a challenging task. Currently there exists a gap between organizations, volunteers and NGOs for volunteering work. There has been an upsurge of NGOs, non-profit events and corresponding independent volunteers or organizations willing to interconnect especially during these pandemic times. There is a need to fill this gap and connect the stakeholders minimizing the emergency response times. This paper proposes a novel design and implementation of an OCR based application for Automated NGO connect using machine learning. Phases implemented include image de-noising, binarization, data extraction and data conversion. The framework integrates deep learning based Tesseract OCR with image processing module and Data visualization module. The proposed model can be extended to other application domains as well for research purposes.
The Levenshtein's channel model for substitution errors is relevant in information retrieval where information is received through many noisy channels. In each of the channels there can occur at most t errors and the decoder tries to recover the information with the aid of the channel outputs. Recently, Yaakobi and Bruck considered the problem where the decoder provides a list instead of a unique output. If the underlying code C ⊆ \mathbb F 2n has error-correcting capability e, we write t=e+l, ( l≥ 1). In this paper, we provide new (constant) bounds on the size of the list. In particular, we give using the Sauer-Shelah lemma the upper bound l+1 on the list size for large enough n provided that we have a sufficient number of channels. We also show that the bound l+1 is the best possible. Most of our other new results rely on constant weight codes.
In the domain of automotive manufacturing, specification documents represent intricate descriptions detailing every aspect of a product, design, or service. Conventionally, these specifications demand the deployment of expert teams to manually identify crucial data from the extensive documentation. The need to automate the extraction of candidate information from these documents is increasingly pressing in this industry. This research encounters two central challenges: Firstly, the queries for the specifications input by users are typically concise and ambiguous; secondly, not every word in a query carries the same significance. In response to these challenges, we propose LeCAR, which exploits contextual data to clarify query sentences and concentrate the search scope. Our experiments validate that the proposed method outperforms existing techniques that employ pre-trained language models, all without necessitating additional training data.
Semantic video retrieval is a key application in today's networked world. With the increasing proliferation of digital video contents, efficient techniques for analysis, indexing and retrieval of videos according to their contents have become ever more important. This paper proposes the new method of STAR (Semi-supervised clustering Technique with Application for Retrieval of video) in the web mining application using the data mining algorithms. This Process is performed by the content based image retrieval and other distance functions. Conventional distance functions are Mahalanobis Distance, Euclidean Distance, and Bregman Distance are used to evaluate the distance between Point-to-Point and Point-to-Centroid. The combined effect of MKBoost and Bregman K-means clustering algorithm will give promising results in the retrieval of images from video structural semantics. Experimental results show that the proposed algorithm is more effective and efficient than the existing algorithm.
Idea mining is a new and interesting field in the areas of information retrieval research. The thoughts of people are helpful to improve strategic decision making. This paper demonstrates the efficient computational methods of idea characterization based concept by extracting the interesting hidden data from unstructured texts which come in many forms and sizes. It may be stored in patents, publications, reports, documents, Internet etc. We briefly discussed a number of successful text mining tools and text classification to extract the idea with a combination of idea mining measures.
Due to the increasing daily of users documentation in the global communications network, the management, and control of its information in it have been challenged. How to extract useful knowledge in an Internet heterogeneous environment is essential. Information retrieval techniques can retrieve user information needs in a large amount of data. Search engines are the first selection of users to find information. In this search, Web crawler plays a key role in search engines. A web crawler is a script that routinely scans the web. In this paper, a information retrieval method will be presented using a combination of vector space modeling and language statistical model to improve the retrieve of related documents. The proposed approach is based on the structural similarity of the document corpus. Measuring the similarity of the input term is based on the shortest path between each term and the next term. In this method, the number of nodes, edges and links is calculated. Finally, the similarity using the path feature and cosine similarity is measured. Available documents the proposed method by the crawler of the web is compiled from various Wikipedia pages. The results of the conceptual retrieved of documents show that 84% accuracy, 88% recall and average precision of 56% have improved compared to other methods.
Inverted index is an important data structure used in Information Retrieval operation, which enable all retrieval engines to easily facilitate full-text search. In this paper, Map Reduce algorithm is used for the construction of inverted index, so as to enable it to work in a parallelized manner and also make the data structure to support large scale document corpora. Here, we have considered crime articles related to women and children drawn from various English newspapers all over India. The paper aims to cluster the news articles according to a specific type of crime committed in a parallelized manner. We proposed a Hadoop based framework integrated with R environment that preprocess the corpus and stores the news articles and process it with the Map Reduce algorithm which identifies the type of crime like sexual harassment, physical abuse, emotional abuse, rape, murder and cluster it accordingly. We observe that the proposed method outperforms the other conventional methods and is more suited for batch processing applications.
For software engineers to find all the relevant program elements implementing a business concept, existing techniques based on information retrieval (IR) fall short in providing adequate solutions. Such techniques usually only consider the conceptual relations based on lexical similarities during concept mapping. However, it is also fundamental to consider the contextual relationships existing within an application's business domain to aid in concept location. As an example, this paper proposes to use domain specific ontological relations during concept mapping and location activities when implementing business requirements.
The relationships between the law and fact, and the use of the facts to support a legal conclusion requires practice. This study introduces a technique called the "legal analysis of data extraction" which helps students/law practitioners to quickly assess a law article on the basis of the extracted attributes, and then apply their understanding in order to support a legal conclusion. To ease the processing of the law text data and retrieval of important information (entities) like names of Judges, appellants, respondents, judgment dates, etc. by utilizing NLP processes and ML techniques. This study demonstrates a novel approach of advanced information retrieval in the law articles leveraging natural language processing and machine learning techniques which are productized and used on live data as well. In this custom NER, aggregate accuracy of 95% is achieved using text classification followed by a regex fallback and flagging mechanism. This study consists of the following sections- a) Introduction b) Related Work c) About Dataset d) Methodology and approach e) Fallback mechanisms f) Results and Evaluations g) Conclusions & Future Scope with h) References.
The past decade has seen the rapid proliferation of low-priced devices for recording image, audio and video data in nearly unlimited quantity. Multimedia is Big Data, not only in terms of their volume, but also with respect to their heterogeneous nature. This also includes the variety of the queries to be executed. Current approaches for searching in big multimedia collections mainly rely on keywords. However, manually annotating every single object in a large collection is not feasible. Therefore, content-based multimedia retrieval -using sample objects as query input - is increasingly becoming an important requirement for dealing with the data deluge. In image databases, for instance, effective methods exploit the use of exemplary images or hand-drawn sketches as query input. In this paper, we introduce ADAM, a novel multimedia retrieval system that is tailored to large collections and that is able to support both Boolean retrieval for structured data and similarity-based retrieval for feature vectors extracted from the multimedia objects. For efficient query processing in such big multimedia data, ADAM allows the distribution of the indexed collection to multiple shards and performs queries in a MapReduce style. Furthermore, it supports a signature-based indexing strategy for similarity search that heavily reduces the query time. The efficiency of ADAM has been successfully evaluated in a content-based image retrieval application on the basis of 14 million images from the ImageNet collection.
This research focuses on the application of artificial intelligence in the modern design field and proposes a solution to build an information visualisation design platform based on natural language processing technology. The profound impact of AI on various fields brings opportunities and challenges to the design field, which designers must be able to fully grasp in order to improve their design and work efficiency. In the research process, the framework for building a design platform based on NLP technology, an in-depth analysis of the key features of the extracted design information and their intrinsic relationships, and a rational planning and implementation validation of the process, as well as a comparative analysis in terms of computational methods for feature extraction, are presented. The visualisation platform is built based on user-oriented and data-driven principles of inter activity and diversity of expression. This study explores the applicability and effectiveness of natural language processing algorithms in the design field, expanding the scope of research on natural language processing techniques and providing an effective reference for the research and practice of combining artificial intelligence with modern design.
Decision-making activity is carried out in an every organization to solve the problems or to take the decisions. To make an effective decision, decision maker needs relevant and meaningful information. But to retrieve meaningful information from such a huge database, decision maker needs background knowledge about the domain. Practically for a decision maker having background knowledge about each and every domain is not possible. Due to this meaningful information remains hidden in the database itself. Decisions made out using such irrelevant and meaningless information leads to irreparable damage and harm to the reputation of organization. Hence to retrieve the meaningful and relevant information, background knowledge about the domain is necessary. To solve this problem, ontology is used as a source of domain knowledge. By using ontology, meaningful information is retrieved from the database to help in taking decision. In proposed system, ontology is used to represent the background knowledge about the domain. Use of ontology improves the relevancy and meaningfulness of the retrieved information. Such meaningful and relevant information is used to improve the effectiveness of decision making. Experimental analysis of the results shows that, results obtained by using proposed approach are more meaningful and relevant as compared to existing approach.
X-secure and T-private information retrieval (XSTPIR) is a form of private information retrieval where data security is guaranteed against collusion among up to X servers and the user's privacy is guaranteed against collusion among up to T servers. The capacity of XSTPIR is characterized for an arbitrary number of servers N and arbitrary security and privacy thresholds X and T, in the limit as the number of messages K → ∞. Capacity is also characterized for any number of messages if either N = 3, X = T = 1 or if N ≤ X +T. Insights are drawn from these results, about aligning versus decoding noise, dependence of PIR rate on field size, and robustness to symmetric security constraints. In particular, the idea of cross subspace alignment, i.e., introducing a subspace dependence between Reed-Solomon code parameters, emerges as the optimal way to align undesired terms while keeping desired terms resolvable.
Visual information retrieval has become a major research area due to increasing rate at which images are generated in many application. This paper addresses an important problems related to the content-based images retrieval. It concerns the vector representation of images and its proper use in image retrieval. Indeed, we propose a new model of content-based image retrieval allowing to integrate theories of neural network on a vector space model, where each low level query can be transformed into a score vector. Preliminary results obtained show that our proposed model is effective in a comparative study on two dataset Corel and Caltech-UCSD.
The paper describes Spoken Document Retrieval system based on Speech Recognizer, Vector Space Model and high-order Markov chain phonemic transcribing method. Relevance of a document and an query is estimated by a weighted cosine measure. Phonemic transcribing allows to transform a recognized text of a spoken content to phoneme sequences. This phoneme sequences are used for retrieval by text user's query which is transformed to phoneme sequences too. Words matching is performed on phoneme-level based on Kullback-Leibler divergence.
Information retrieval (IR) returns a relative ranking of documents with respect to a user query. Learning to rank for information retrieval (LR4IR) employs supervised learning techniques to address this problem, and it aims to produce a ranking model automatically for defining a proper sequential order of related documents based on the query. The ranking model determines the relationship degree between documents and the query. In this paper an improved version of RankGP is proposed. It uses layered multi-population genetic programming to obtain a ranking function which consists of a set of IR evidences and particular predefined operators. The proposed method is capable to generate complex functions through evolving small populations. In this paper, LETOR 4.0 was used to evaluate the effectiveness of the proposed method and the results showed that the method is competitive with other LR4IR Algorithms.
A new image retrieval system is proposed that combines the bag-of-words (BoW) model and Probabilistic Latent Semantic Analysis (PLSA). First, interest points on images are detected using the Hessian-Affine keypoint detector and Scale Invariant Feature Transform (SIFT) descriptors are computed. Graph-based visual saliency maps are then employed in order to detect and discard outliers in image descriptors. By doing so, SIFT features lying in non-salient regions can be deleted. All the remaining reliable feature descriptors are divided into a number of subsets and partial vocabularies are extracted for each of them. The final vocabulary used in the BoW model is obtained by the concatenating the partial vocabularies. The resulting BoW representations are weighted using the TF-IDF scheme. Finally, the PLSA is employed to perform a probabilistic mixture decomposition of the weighted BoW representations. Query expansion is demonstrated to improve the retrieval quality. Overall a 0.79 mean average precision is reported when the saliency filtering was applied on SIFTs and the BoW plus PLSA method was used.
The user of a social website is often interested by information about events that have interested other users, especially his friends, and that appear remarkably in their researches, exchanges and sharing. In this paper, for a better description of the user and his interests, we propose: (i) a geosocial user profile that takes into account the event aspect of user information needs, (ii) a search process integrating the user profile for a better personalization and adaptation of the search results.
The software engineering based on components is an evolving branch of software engineering. The evolution in data mining and information retrieval techniques forms the basis of the approaches to component retrieval. This has paved way to new techniques to be used for efficient storage, retrieval and management of component repository and storage systems. Such information retrieval caters to the needs of rapid delivery and intelligent computations on Big Data can be used as well for recommender systems for the areas like component based development.
Over the years, advancements in the work have been incorporated towards professor relationships and data collection techniques to model a wide variety of human activities and behaviors. Most sensor data come from smart devices such as cleaners and cleaners that provide the ability to manipulate and extract data from monitoring data for monitoring and healthcare. Due to the high popularity and use of smart devices as respondents, performance recognition systems are more accurate and easier to use. Identifying smooth designs is a difficult task in contrasting environments and scattering tube data. This work presents a knowledge-based model based on job descriptions that reflects the work of the worker. The knowledge model is based on two new approaches: to consider a functional degree scheme between measuring sensor energy and functional consciousness to model controversial sensor data and establish the relationship between them. Low efficiency (easy work) and high level (weak). In this article we make a case why ontology can contribute to blockchain design. To support this issue, we make an analysis translate the tonnage ontology and some of its representations into the smart contracts that enable it implement traceability restrictions on the original traceability feature and Ethereum blocking platform.
In the context of medical document retrieval, users often under-specified queries lead to undesired search results that suffer from not containing the information they seek, inadequate domain knowledge matches and unreliable sources. To overcome the limitations of under-specified queries, we utilize tags to enhance information retrieval capabilities by expanding users' original queries with context-relevant information. We compute a set of significant tag neighbor candidates based on the neighbor frequency and weight, and utilize the most frequent and weighted neighbors to expand an entry query that has terms matching tags. The proposed approach is evaluated using MedWorm medical article collection and standard evaluation methods from the text retrieval conference (TREC). We compared the baseline of 0.353 for Mean Average Precision (MAP), reaching a MAP 0.491 (+39\%) with the query expansion. In-depth analysis shows how this strategy is beneficial when compared with different ranks of the retrieval results.
Cross-language information retrieval (CLIR) is a subfield of information retrieval dealing with retrieving information written in a language different from the language of the user's query. The domain of CLIR is crucial in the future as vast amount of content in the web is in English. There is a need for mechanisms that can retrieve the content from English and translate it to the native language. This paper proposes a text summarization-suffix tree algorithm for the overall process of CLIR. The idea is to summarize the content and re-rank the results based on clustered coefficients. This method can improve the efficiency of the content retrieved and afford a great flexibility to the user. The key contributions of this paper are to explain the approach and discuss the key results.
Recently, unstructured data like texts, documents, or SNS messages has been increasingly being used in many applications, rather than structured data consisting of simple numbers or characters. Thus it becomes more important to analysis unstructured text data to extract valuable information for usres decision making. Like OLAP (On-Line Analytical Processing) analysis over structured data, Multi-dimensional analysis for these unstructured data is popularly being required. To facilitate these analysis requirements on the unstructured data, a text cube model on multi-dimensional text database has been proposed. In this paper, we extended the existing text cube model to incorporate TF-IDF (Term Frequency Inverse Document Frequrency) and LM (Language Model) as measurements. Because the proposed text cube model utilizes new measurements which are more popular in information retrieval systems, it is more efficient and effective to analysis text databases. Through experiments, we revealed that the performance and the effectiveness of the proposed text cube outperform the existing one.
Most traditional information retrieval systems are based on single terms indexing. However, it is admitted that semantic content of a document (or a query) cannot be accurately captured by a simple set of independent keywords. Although, several works have incorporated phrases or other syntactic information in IR, such attempts have shown slight benefit, at best. Particularly in language modeling approaches this is achieved through the use of the big ram or n-gram models. However, in these models all big rams/n-grams are considered and weighted uniformly. In this paper we introduce a new approach to weight and consider only certain types of N-grams "compound terms". Experimental results on three test collections showed an improvement.
Since the rapid progress in deep learning in recent years, many language models have achieved significant results in various information retrieval (IR) tasks. Passage ranking plays a vital role in this field, and the neural network models significantly outperform the traditional method. However, fine-tuning the pre-trained model to the downstream task may be influenced by the fact that there are differences between the two tasks. And traditional methods also have their advantages. In some cases, the performance of BM25 is obviously better than the deep learning model. This paper discusses the results of the deep learning model linearly combining with BM25 and adds noise to the model for enhancing the finetune performance. We conduct experiments on the MS MARCO dataset to show convincing results.
In order to effectively improve the reader's writing ability, this article has established a Chinese composition example sentence retrieval system based on expressions, and relevant composition example sentences can be retrieved according to the writing keywords and expression methods. This article first constructs a expression clause set, and annotates 11 classic expressions such as narration and description. Then it uses the BERT model to automatically identify the expressions, and finally constructs an example sentence retrieval system based on Solr. Experiments show that the performance of example sentence retrieval is better than that of using keywords directly.
Search interface is an important component of information retrieval system. It acts as medium which connects users with an information retrieval system (IRS) and offers a platform for interaction and communication of users' information needs to IRS. Search interface needs to be robust and embedded with features that act as aid-in-hand to users, in particular to novice users to enable them to translate their information needs to meaningful and semantically rich queries. The present study attempts to identify and assess features and facilities of search interface of seven prominent academic databases, i.e., Cambridge Journals, JSTOR, Sage Journals, ScienceDirect, SpringerLink, Taylor & Francis, and Wiley Online Library. The emphasis is laid on the searching features like simple search, advanced search, browsing, access points (sign in, register), user assistance, alerting services, web 2.0 tools, etc. To achieve the maximum exploitation of academic databases, the study, on the basis of experience gained during assessment, suggests features that should be available on search interface of academic databases. This may prove helpful for academic database designers for designing a better user-friendly academic database.
Given a priori knowledge, hyperspectral target detection aims to locate objects of interest within specific scenes by utilizing differences in spectral characteristics among various land covers. However, for those traditional model-driven detectors with monotonic analytical mode, they perform mediocrely in the disassembly of hyperspectral image (HSI) data, failing to cope with real scenes with complexity. The discrepancy between fixed model assumptions and HSI data severely reduces detection effects, leading to the inability of such methods to mine deep-level features and adapt to the variability of imaging scenes. To overcome the limitations of traditional methods, we propose a chessboard-shaped topological framework for high-dimensional data structures to disassemble an HSI from both spatial and spectral dimensions adaptively. With hyperspectral target detection refined into an information retrieval task in a topological space, a chessboard-shaped topology for hyperspectral target detection (CTTD) is proposed. In the topological space, latent and hidden data features of original images are presented in an intuitive way. Therefore, the differences in both spatial and spectral dimensions between the two classes of objects, namely, target and background, are specifically amplified and exploited to perform the information retrieval task with superior performance. Extensive experimental results on benchmark HSI datasets demonstrate that CTTD can efficiently adapt to the variability of real scenes while extracting abundant and detailed information for accurate target localization. Moreover, both detection effects and computational efficiency exhibited by the proposed method provide strong support for its popularization in practical applications.
It is important to find suitable partners in order to form successful collaborations between companies and university researchers. We consider finding the partners by calculating the similarity of the documents such as scientific papers and patents. We focus on weak (unique) links of researchers as the local similarity of their documents, instead of strong links as the global similarity of the documents. In the present paper, we propose a system that matches partners using documents such as research papers and patents. Given a query, the proposed system outputs a graph of unique research in retrieved documents. Each node in the graph corresponds to a word with a document frequency of two. Two words connected by an edge occur in the same two documents, and neither word appears in other retrieved documents. The edge is labeled with the names of the researchers involved in the documents in which the two words appear. Experiments are conducted using graphs output by the system.
The richness of Cultural Heritage and Natural History is abundant. Many of the cultural heritage collection in Library, National Archive, and Museum in the form physical object or digital format in a different type of media (text, image, audio and video). One cultural heritage object can have the relationship with other objects in different media format and do not mention query term explicitly. Using the various media format causes problems in search. A monolithic search engine like Google, Bing, Google Image, Youtube, or Findsounds only retrieve one media format. Besides, the search result of the existing search engine is less relevant and incomplete in searching cultural heritage. Several multimedia information retrieval techniques used in building the relationship using ontology like ontology based search, content-based search with ontology and hybrid search with ontology. This paper proposes Concept-based Multimedia Information Retrieval System (MIRS) with ontology using Indonesia's cultural heritage dataset to increase relevance and completeness of the system. Concept-based MIRS using manually built thesauri or by extracting latent word relationship and concept from the Ontology that provides definition and formal structure for describing the implicit and explicit concepts and its relationship in cultural heritage documentation. Ontology-based Semantic similarity measure is defined which measure the semantic relationship between document based on the likeness of their meaning. The search results indicate that the document being retrieved becomes highly relevant, more complete, enrich the keyword and in varying media formats when is compared to existing search engine results such google, bing, google image, youtube and findsounds in specific domain.
A diverse and heterogeneous data environment, an excellent cross-media semantic retrieval model is the key to enhancing the interactivity of digital libraries and promoting the upgrading of their knowledge services. On the basis of analyzing the theoretical concept of deep learning, characteristics of deep structure, related technologies, and correlation between cross-media semantic retrieval and deep learning, this study constructs a cross-media semantic retrieval framework based on deep learning. Research suggests that the combination of deep learning and cross-media semantic retrieval, as a new information retrieval model in digital libraries, helps to solve the problems of searching for semantic information across media and efficiently processing complex dimensional data, greatly improving the efficiency of data retrieval and integration. It will inevitably accelerate improving knowledge service levels in the era of big data. The study provides reference for improving the effectiveness of cross-media semantic retrieval for library users and improving library knowledge organization and services.
This poster provides an analytical model for examining performances of IR systems, based on the discounted cumulative gain family of metrics, and visualization for interacting and exploring the performances of the system under examination. Moreover, we propose machine learning approach to learn the ranking model of the examined system in order to be able to conduct a “what-if” analysis and visually explore what can happen if you adopt a given solution before having to actually implement it.
As the syntax based information retrieval yields very low precision, the researchers are moving to the semantic based information retrieval. The great challenge of the information retrieval system is to retrieve the related relevant information from the heterogeneous web resources. This paper aims for the retrieval of the relevant geo-spatial concepts from the geo-spatial ontology which aids application in quality testing of water and flood prediction. In the geo-spatial concept space, the retrieval of relevant geo-spatial concepts depends on the distance separating the two geo-spatial concepts and the context of the concepts. Research in geo-spatial information retrieval has used either the semantic distance between the geo-spatial concepts or the context of the concepts for computing the relevancy. This paper proposes HCC (Hybrid Conceptual Context) algorithm which aims at retrieving relevant geo-spatial concepts by considering both the semantic distance and the context on which the concepts occur in the geo-spatial space. This work computes the semantic similarity by measuring the semantic distance using Euclidean and Manhattan distance measure. This computation is carried out in two steps such as retrieval of semantically similar geo-spatial concepts and the retrieval of context dependent geo-spatial concepts. Semantic similarity computation in HCC algorithm retrieves contextually and semantically similar relevant geo-spatial concepts using Euclidean distance and Manhattan distance measures. Experiments have been done using Ordnance Survey Master Map datasource and geo-spatial ontology. The precision and recall show Manhattan based semantic similarity computation improves relevancy by 10%.
In this paper, we propose a semantic query expansion approach by extending the query-regularized mixture model to include latent topics and apply it to spoken documents. We also propose to use context feature vectors for spoken segments to train SVM models to enhance the posterior-weighted normalized term frequencies in lattices. Experiments on Mandarin broadcast news showed that this approach offered good improvements when applied on spoken documents including relatively high recognition errors.
Full text search in the legal domain is not enough, because there is a gap between common sense and legal knowledge. We want to present some possible directions to solve the problem of full text search in legal domain. The gap can be bridged using a model that mixes tags from Eurovoc Thesaurus Schema Ontology and legal ontology in order to enrich information retrieval capabilities in the legal domain.
In recent years, cross-modal retrieval has a wide range of applications in fields such as information retrieval and multimedia processing, where the key is to find relevant data in another modality through one modal query. In cross-modal graphic retrieval, mutual attention mechanisms are commonly used to interact image and text features. However, this interaction mechanism makes it difficult to extract separate visual and textual features for subsequent indexing steps, especially in large-scale retrieval tasks. Furthermore, in image representation, non-semantic information in image features can lead to inaccuracies in the alignment of image and text features. Therefore this paper proposes a Transformer and Feature Enhancement Method (TSFE) for Image-Text Retrieval. The method better fuses the underlying features and the higher level features by using a hierarchical Transformer coding structure on two separate branches, image and text, to proofread and localise the final output features. The Feature Enhanced Attention (FEA) module is designed in order to infer non-semantic information in the image and enhance the features of salient regions. Channel-space attention and max pooling is introduced on top of Transformer encoder self-attention to filter redundant and interfering information from channel and spatial dimensions. Experiments on Flickr30k dataset and MS-COCO dataset in this paper show improved performance in cross-modal retrieval, thus validating the effectiveness and sophistication of the proposed method.
Recently the collection of varied digital image databases has increased. Many users have found that searching and retrieving required images from large collections are very difficult tasks, and successful and effective retrieval methods were developed to provide an effective and efficient search and retrieval process. In most content-based image retrieval (CBIR) methods, various visual features have been considered indirect to retrieve the images from databases. Content-based medical image retrieval (CBMIR), like any CBIR method, is a technique for retrieving medical images on the basis of automatically derived image features, such as colour and texture. Although a number of methods and approaches have been suggested, retrieval performance remains one of the most challenging problems in current CBMIR studies, this due to the well-known `semantic gap' issue that exists between machine-captured low-level image features and human-perceived high-level semantic concepts. To bridge this gap, much research has been proposed. This study proposes two expansion methods to increase and enhance precision of the retrieval model; both of our proposed methods depend on top-ranked images. Our first expansion method is to reformulate the new expand query image based on mean values for features of top-ranked images, while the second method is based on selecting the most important features only. The expansion process was performed on eighteen colour features and twelve texture features extracted from two common medical images datasets, Kvasir and PH2. Experimental results show that our proposed methods performed better and have a precision of 95.8% and 80.7% for Kvasir and PH2 data set, respectively.
The explosive growth of data, images in the World Wide Web makes it critical to the information retrievals. Image retrieval has been recognized as an elementary problem in the retrieval tasks and this exercise has got a wide attention based on the underlying domain characteristics. For instance, in social media data encompasses of noisy, diverse, heterogeneous, interconnected data. To confront these numerous characteristics and employ image retrieval the widely accepted deep architecture concept is utilized with the help of natural language latent query features. In this paper, we are introducing a novel approach for image retrieval task which collaboratively make use of the technicalities of natural language processing and deep architecture.
Tourism, as a basic component of social and economic development and urban construction, is also the core content to meet the spiritual needs of residents. Therefore, on the basis of understanding the current integrated development form of the Internet and tourism, this paper analyzes how to build a recommendation information retrieval model with personalized tourism based on the Apriori algorithm, aiming at the improved algorithm proposed based on big data.
The process of efficiently indexing large document collections for information retrieval places large demands on a computer's memory and processor, and requires judicious use of these resources. In this paper, we describe our approach to constructing such an index based on the vector-space model (VSM). We review the stages involved in generating an index, for weighting the index terms, and for representing documents in the VSM. We explain our choice of data structures from the parsing of the document collection through the generation of index terms, to generation of document representations. We explain tradeoffs in our choice of data structures. We then demonstrate the approach using the OHSUMED data set. Our results show that even with only a modest amount of main memory (4 GB), large data sets such as the OHSUMED data set can be quickly indexed.
With advancement in artificial intelligence and machine learning, web information retrieval has evolved from manual to automated to intelligent systems making significant strides in building tools and techniques to enable automated information retrieval from the websites. In this paper, an automated web information retrieval framework with Faster region based convolutional neural network (R-CNN) is proposed. The framework automatically navigates to the specific product page in a retail website and implements leading edge open-source tools for automated retrieval of product details from the retail website. Information retrieval by using partial tree alignment (DEPTA) machine learning technique is used as reference for comparison. The results reveal an improved precision and recall as compared to DEPTA. The future research in web information retrieval space will to design and develop self-healing capability of web information retrieval framework to adapt to dynamic changes in the website layout.
Health related publications have been growing at a considerable rate over the years. They are archived at public or private repositories for research or decisionmaking. In developing countries there is an increase in the need for technological infrastructure and funding towards research that enables the voluminous unstructured data to be effectively identified, stored, analysed and visualized to enable prompt decision making. Analysing data in real-time will assist knowledge seekers and researchers in timely access hence quick approach to solutions. We propose a web based, low-cost and user-friendly health information dissemination tool based on machine learning algorithms that analyses full-text publications sequentially and cluster related documents for ease of access. Information retrieval aspect of the model is enhanced through use of a semi-supervised approach that optimises topic selections during search operations. As future works, we propose to scale-up the prototype for bulky data processing and apply it to big data environments.
The use of digital images is exponentially increased due to the availability of various image capturing devices and the provision of transmitting them with less expense. Digital images are used for various purposes such as personal, entertainment, medical diagnosis, fashion design, forensic analysis, etc. The retrieval of relevant images from a huge volume of images is a tremendous task. Content-based image retrieval (CBIR) is a solution to retrieve relevant images from a pool of images by considering a query image. In a CBIR scheme, the user will give a query image and the system supposes to return a set of the image which are very similar to the query image. The CBIR schemes use classification models and the visual features of the images while ranking and retrieving the images from the image pool. This manuscript discusses the overview of a CBIR scheme, its application domains, the existing approaches in this domain and various datasets available for the research in this domain. The manuscript has also listed a set of challenges in this domain so that future works can be focused on this area.
Conventional classification method has good performance when applied to small datasets; however, their applicability to unknown data sets is weak. To serve education information services in the cloud computing (CC) environment, this study presented a cross-source education information classification model based on cloud computing technology. This work outlined mode of CC service and its mode of deployment and provided a processing method of multisource information depend on CC centre integrated with the features of information distribution in CC environment. First, function and structure of CC platform were analyzed. Second, this research provided a summary of feature extraction process of educational information utilizing technology of data mining and provided an approach for classifying educational information depend on text features by analyzing different types of educational information resources. Finally, a model for classifying cross-source academic information in a cloud computing setting was developed. The comparison of experiments proved that the approach presented in this paper has the ability of efficiently categorizing multisource educational information under CC platform. This approach outperformed other traditional categorization method of Support Vector Machine (SVM) in terms of efficiency and also accuracy.
One of the major problems of modern Information Retrieval (IR) systems is the vocabulary problem that concerns the discrepancies between terms used for describing documents and the terms used by the researchers to describe their information need. In this paper, we propose to use the well known abstractive model -Latent Semantic Analysis (LSA)- with a wide variety of distance functions and similarity measures to measure the similarity between Arabic words, such as the Euclidean Distance, Cosine Similarity, Jaccard Coefficient, and the Pearson Correlation Coefficient. LSA statistically analyze distribution of terms belonging to a large textual corpus to elaborate a semantic space in which each term is represented by a vector. In our experiments, we compare and analyze the effectiveness of this model with the different measures above in two cases: with and without Stemming, for two testing data: the first is composed of 252 documents from several categories (Economics, Politics, and Sports), and the second contains 257 politics documents only to test the influence of the variety of the used corpus. The obtained results show that, on the one hand, the variety of the corpus gives more accurate results; on the other hand, the use of the Stemming gives us more accuracy in some cases and the opposite in the others.
Query suggestion is an effective human-computer interaction (HCI) approach in information retrieval. According to human vision system, the image retrieval method using visual query suggestion (VQS) can provide a friendly query interface to solve the query ambiguity problem. In this paper, the content-based image retrieval (CBIR) system is realized first. Human-computer interaction used VQS to obtain users' intention. In this step, user submitted a query keyword to the system. VQS is utilized to provide a list of suggestions, each containing a keyword and a collection of representative images. If the user selects one of the image suggestions, this image will be viewed as the key image. Then CBIR system will retrieve the image sets to return the similar images based on the similarity of image features. In relevance feedback, user scored each returned image by the slider to optimize retrieval results. A friendly query interface is designed to carry out HCI in our system and the experimental result shows the proposed method can improve the average recall and precision efficiently.
This paper presents an approach to deal with the generalized Question Answering (QA) system. This work demonstrates an end to end system that is capable of dealing with the open domain questions i.e. not particular to any genre. For this research different technologies like beautiful Soup is used to scrape the web, MS-Marco dataset is used to train and shorten the document using transfer learning. Document/answers obtained using these processes are validated using an exact match with the Gold answer available in the dataset. This automatic evaluation method fails due to linguistic ambiguities inherent in the data. Taking limitation of this automatic evaluation method as a challenge, in this work an heuristic approach is proposed to deal with the Questions and answers that are dependent on time. A metric is proposed that uses current timestamp and top-n predicted answers from QA system as input. This approach is further tested on the publicly available GNQ(Google Natural Question) dataset. Out of that we have tested for 100 odd pairs and achieved an accuracy of 78% for the test data. The proposed heuristic approach appears to have the great prospect of developing into a beneficial scheme for gathering pinpoint, credible, and specific information in an accurate, real-time and efficient manner.
Many algorithms in informatics require a set of objects with similar properties to be grouped (clustered) on the basis of some predefined criteria. The proposed technique involves hierarchical merging in which software, responsible for solving the entire problem, is enhanced with highly parallel networks in hardware accelerators. Additional improvements are achieved with the aid of support methods that are sort and verification of object intersections that may also be autonomously used for other types of information processing and database management. It is shown and experimentally proved that the proposed solutions are efficient. They can be used in such areas as health care, statistical data manipulation and so on.
With the increase of a large numbers of Tibetan information, Tibetan text processing has become popular and important. Tibetan hot topics extraction has become one of the Tibetan information analysis tools. This paper describes a method of the hot topics extraction from Tibetan text. First, construction of the dataset is described. Second, Tibetan word segmentation is presented. Third, the feature selection and the text representation are conducted. The classical TFIDF is used to calculate the weights of features. At last, statistical-based method is utilized to extract the hot topics. The experiment shows it can extract the topics effectively and the results can reflect the characteristics of hot topic category. It is helpful and meaningful for text classification, information retrieval as well as construction of high-quality corpus.
With the development of internet technology, the information in the website increases sharply. In view of the problem of the user's great desire for intranet information retrieval and the inefficiency of the intranet information retrieval service provided by web search engines, in this paper, we study the architecture, key technologies and implementation of the intranet search engine system. We designed and implemented an intranet search engine system based on Lucene. The system contains information gathering module, indexing module, searching module and system interface module, which can index and search many document formats, such as html, word, excel, pdf and so on. Experiments show that the system has a good indexing and retrieval efficiency and performance, which can provide intranet information retrieval service for users effectively.
Enormous amount of outsource information are stored and retrieved across the global. During that process some difficulties are raised to maintain security while providing retrieval and searching procedures. Due to security concerns, sensitive data is protected by encryption before moving to the cloud. Normally precise information retrieval is difficult over encrypted cloud data. To overcome these issues, we propose an Enhanced Multikeyword Top-k Search and Retrieval (EMTR) scheme; it achieves good accuracy and high efficiency. First, the document can be efficiently estimated using inverted indexing. Then the user could query by any number of queried keywords appearing in the document which evaluate the relevance scoring of the document and to the search query to retrieve relevant information from cloud storage. Furthermore, we establish a new ranking procedure to retrieve highest ranked documents (i.e., most relevant) in the data set that brings considerable speedup over inverted weighted indexing. Our analysis demonstrates that the proposed scheme achieve high accuracy, improved search quality and efficient data retrieval with high speed.
Corporate reputation is an economic asset and its accurate measurement is of increasing interest in practice and science. This measurement task is difficult because reputation depends on numerous factors and stakeholders. Traditional measurement approaches have focused on human ratings and surveys, which are costly, can be conducted only infrequently and emphasize financial aspects of a corporation. Nowadays, online media with comments related to products, services, and corporations provides an abundant source for measuring reputation more comprehensively. Against this backdrop, we propose an information retrieval approach to automatically collect reputation-related text content from online media and analyze this content by machine learning-based sentiment analysis. We contribute an ontology for identifying corporations and a unique dataset of online media texts labelled by corporations' reputation. Our approach achieves an overall accuracy of 84.4%. Our results help corporations to quickly identify their reputation from online media at low cost.
In this paper, we are interested in aggregated search in XML documents. Our goal is to retrieve the best set of XML elements to be returned. We present a structured information retrieval model based on the Bayesian networks theory. The networks structure provides a natural representation of links between a document, its elements, and its contents. In this model, the user's query starts a propagation process to recover the XML elements. Thus, instead of retrieving a whole document or a list of disjoint elements that are likely to answer partially the query, we attempt to build a virtual document that aggregates a set of elements that are relevant and complementary. We evaluated our approach using the INEX 2009 collection and presented some empirical results for evaluating the impact of the aggregation approach.
Entity relation extraction, as the core task and important link in the fields of information extraction, natural language understanding and information retrieval, aims to extract structured entities from a large amount of unstructured text and to extract semantic relations between entities. The entity relation extraction technology based on deep learning has gradually surpassed the traditional method based on features and kernel functions in terms of the depth of feature extraction and the accuracy of model. This paper proposes a multiple model fusion strategy, adds the Convolutional Neural Networks (CNN) on the basis of the original model of Bi-directional Long Short-Term Memory (Bi-LSTM), uses the syntactic structure information of syntactic information and the shortest dependent path separately as input of the experiment, and introduces the attention mechanism to study the dependency relations between the words to capture the internal semantic information of a sentence. The experiment shows that the effect of entity relation extraction based on the fusion of multiple information and attention mechanism is significantly improved than that of a single information model.
Different from conventional media type, chemical structural formula (CSF) is a primary search target as a unique identifier for each compound in the research field of medical information retrieval. This paper introduces a graph-based CSF retrieval system, PharmKi, accepting the photos taken from smartphones and the sketches drawn on tablet PCs as inputs. To establish a compact yet efficient hypergraph representation for molecules, we propose a graph-isomorphism-based algorithm for evaluating the spatial similarity among graphical CSFs, as well as selecting dominant acyclic subgraphs on the basis of overlapping analysis. The results of comparative study demonstrate that the proposed method outperforms the existing methods with regard to accuracy and efficiency.
The Internet in big data trend contains huge amounts of image data.In this paper, we study how to make the image have the ability of information retrieval in the semantic level and data organization ability. Based on the keyword search and content-based retrieval, not only the intelligent retrieval based on semantic level is put forward but also crossing the semantic gap in information retrieval based on semantic knowledge, the similarity measure of image feature extraction and image query technology.
In conditions of the information society development, there is one of the most important tasks remains - to solve the problem of effective search and collection of the information. This is crucially important due to a growing diversity of information sources focused on developing different areas of human activities. Thus, the aim of the study is to analyze the prospects for use and improving of modern methods of information search as an integral part of the informatization process of all areas of human scientific activity, including historical research. During the analysis of the principles of the functioning of information search systems and a number of scientific researches in the field of information search, the prospect of using the precedent methodology in the framework of the improvement of search methods was established and the feasibility of using it to improve the accuracy of document evaluation was emphasized. It was found that precedents is able to solve a new or unknown problem by using or adapting a solution to a known problem by using experience gained in solving such problems. Therefore, was assumed about the prospect of using this technique as part of improving the information search system and as a necessary component of the process of informatization as part of development of historical research.
Recently, a continuous-time k-winners-take-all (kWTA) network with a single state variable and a hard limiting activation function and its discrete-time counterpart were developed. These kWTA networks have proven properties of finite-time global convergence and simple architectures. In this paper, the kWTA networks are applied for information retrieval, such as web search. The weights or scores of pages in two real world data sets are calculated with the PageRank algorithm, based on which experimental results of kWTA networks are provided. The results show that the kWTA networks converge faster as the size of the problem grows, which renders them as a promising approach to large-scale data set information retrieval problems.
The current technological innovation is witnessing a substantial surge of interest focused on the creation of so-phisticated tools that facilitate natural language interfaces for databases. This emerging trend is driven by a compelling aim: to bridge the existing accessibility gap that separates the average user from the technical accessibility of traditional databases. A key requirement for achieving this goal is the conversion of everyday natural language queries into structured SQL, a step that transforms technical operations into user-friendly ones. Central to this pursuit is recognizing that natural language is the most intuitive and innate mode of human communication. By leveraging the capabilities of language models, this study effectively demonstrates the translation of natural language requests into structured SQL queries. Upon utilizing this model, the paper also showcases an interactive web application to demonstrate a use case of the system, paving the way to make databases more accessible, user-friendly, and impactful across a broader spectrum of users.
The extraordinary boom of data keeps to undertaking the conventional methods to information usage. the use of synthetic intelligence (AI) and gadget studying (ML) is reworking how information is captured and used, as businesses increasingly more undertake a zero-believe approach to garage and retrieval of records. this article explores the ability of AI-enhanced data capture and retrieval to assist groups unleashes the price of their facts with the intention to maximize performance. It covers the basics of AIbased facts capture and retrieval, inclusive of photo popularity, speech reputation, optical character recognition, and herbal language processing (NLP). It additionally examines the effective implications of AI-improved statistics use, in addition to capacity troubles which can prevent or put off its adoption. sooner or later, the thing affords insights into how organizations can equip themselves with the essential AI-based skills to optimize their facts use and selection-making processes.
Vocabulary mismatch is a significant issue when it comes to query-based document retrieval in the medical field. Since the documents are typically authored by professionals, they may contain many specialized terms that are not widely understood or used. Traditional information retrieval (IR) models like vector space and best match-based models fail in this regard. Neural Learning to Rank (NLtR) and transformer models have attracted significant research attention in the field of IR. Recent works in the medical field utilize medical knowledge bases (KB) that map words to concepts and aid in connecting several words to the same concept. In this paper, we present various Siamese-structured transformer and knowledge-based retrieval models designed to address the retrieval issues in the medical domain. The experimental evaluation highlighted the superior performance of the proposed retrieval model, and the best one, based on the UMLSBert ENG transformer, achieved best-in-class performance with respect to all evaluation metrics.
The knowledge graphs are structured data utilized for information retrieval purposes. Entity alignment using multi-modal supplementary information plays an important role in knowledge graph integration. However, if the supplementary information is missing or incorrect, it can negatively impact the retrieval of information. If we can quantify the usefulness of the information for retrieval as a degree of importance, the influence of unimportant supplementary information can be reduced. In this study, we proposed a method that quantifies the importance of each piece of information by using a probability distribution. Our proposed method improves an existing method by 7.7% and 7.3% in H@1 on two datasets (FB15K-DB15K, FB15K-YAGO15K). Qualitative experiments also showed that the importance of information quantified by uncertainty successfully captured data that was not useful for information retrieval. Our qualitative experiments also show that the importance of information, quantified by uncertainty, effectively captures data that is not beneficial for information retrieval.
Applying deep neural network to learning to rank is an attractive emerging issue in information retrieval (IR). However, most of existing deep neural network based methods cannot even beat traditional methods regarding ranking precision, since the adopted raw text of query and documents contain a significant amount of irrelevance words. To overcome this problem, we propose a new deep neural network based method directly using extracted IR features, which is more robust and correlated to the final ranking task. Besides, our method takes both local and global perspectives on documents and query into account to produce an accurate ranking. Consequently, two separated neural networks are involved in considering the local and global views explicitly, and then the ranking decision is provided by the interaction between the extracted features of these networks. Experiments on two popular benchmarks MQ2007 and MQ2008 are carried out to verify the performance of the proposed method, and the experimental results show our approach significantly outperforms all of the state-of-the-art methods.
In the canonical private information retrieval (PIR) problem, a user retrieves a message from a set of databases without allowing any individual database to obtain any knowledge regarding the identity of the requested message. This perfect privacy requirement may be too stringent in many cases, and the user may only wish to control the amount of the privacy leakage to below a given level, and in return, can retrieve the message at a lower communication cost. In this work, we study the tradeoff between the download cost and the amount of privacy leakage under the maximal leakage metric. A new scheme is proposed by allowing a more flexible query structure and probability distributions in a code previously proposed by Tian et al., which utilized a fixed query set and a uniform distribution. It is shown that the optimal probability distribution in the proposed scheme has a particularly simple structure, which leads to a closed form achievability bound for the optimal tradeoff between the download cost and the privacy leakage. The proposed scheme includes several known schemes, such as those proposed by Lin et al., by Samy et al., and by Jia, as special cases.
Due to the explosive growth of digital images, new efficient and effective methodologies and tools are needed in the image retrieval field. Compared to the content-based image retrieval approach that suffers from the semantic gap, the text-based image retrieval approach has demonstrated its efficiency in retrieving relevant images for a given query. However, this approach suffers from some limitations. For example, query keywords could not match to the textual content of the document or only some images in a document are relevant to the given query. Therefore, a major challenge of the text-based approach is how to improve the image retrieval accuracy without using the image itself, i.e., by using the surrounding information (context) such as the document structure, the links, etc. To achieve this challenge, some works proposed to explore hyperlinks (explicit links) between documents to re-rank images, while more recent works proposed to automatically build implicit links between images and exploit them in the retrieval process. The aim of this paper is thus to compare the exploration of implicit links versus explicit links, either in image ranking or re-ranking. The Image CLEF 2011 collection on Wikipedia shows that not all top-ranked results are interesting to create and analyze linkages between images. In fact, only the aggregate ranking metric makes notice of the fact that linkages improve image retrieval. We also discover that the retrieval strategy–text-based retrieval with no links, implicit link-based re-ranking, or explicit link-based re-ranking–has a significant impact on the efficiency of the query process.
Corpora are important resources for several applications in Information Retrieval (IR) and Knowledge Extraction (KE). Arabic is a low resourced language characterized by its complex morphology. Furthermore, most existent Arabic language resources focus on Modern Standard Arabic (MSA). This paper describes KUNUZ a multi-purpose test collection composed of voweled and structured classical Arabic documents. Its goal is to provide a unique benchmark for assessing applications in several areas of document engineering including IR, document classification and information extraction. The documents are also translated in English to allow Arabic-English cross-lingual IR and machine translation. As far as IR is concerned, we follow the standard topic development and results sampling used in international campaigns. The paper, describes the process of topic development, results pooling and relevance judgment. It also analyses the results of some processing tools and IR models used in the runs. In order to enhance the results of our experiments, we also proposed to combine the results based on a meta-search approach using Support Vector Machines (SVM) classification.
The indexing based by ontology us not in biomedicine and used to retrieve the data in an efficient manner. By using ontology based method it improves the excellence of the effect and also make simpler for the user interaction and aware of the complexity. Ontology creation is done by using many tools. The most recently used tools are Protege, OWLGrED, TopBraid and SWOOP. Ontology can be used in many of the fields such as enterprise system, health, tourism, agriculture, education, agent processing system and biomedical domain, etc. Many researchers designed ontology for their research either manually, automatic or semi-automatic way to improve the retrieval or extraction accuracy. This paper presents the overview of these three types of ontologies designed for the domains such as Agriculture, Biomedical and Health, Education and Tourism. This paper also presents the tremendous growth of ontology based research in the last five years and reviewed the most cited research papers which really pave the way to the budding researchers for capturing the knowledge in ontology.
With the rapid development of Internet and multimedia technologies, video data has become one of the most important information in the Internet. In this paper, we have implemented a vertical video retrieval system based on mobile cloud network called V2 RMC in which key words or a video clip is submitted by a mobile terminal. Three enabling techniques are involved such as the similarity of the video, the package-based data transmission and high-dimensional index support video data set filtering. The experiment's results show that effectiveness of the system.
Log files are the history books of a computer system. In particular, they tell a good portion of the security related events and menaces that a system has to withstand and, sometimes, fails to resist to. Therefore, log files' analysis can be valuable to system and security administrators if the difficulty of extracting the relevant information of the different kinds of data and formats can be surmounted. And if we consider a huge system in terms of users, services and accesses, the difficulty of the analysis task rises enormously. In infra-structures where there is more than one server and communication link, it is possible for the administrator and security team to configure all systems to record their logs in a central huge repository, but the search for abnormalities is quite impossible without specialized tools. As we believe that the collected information on log files can be valuable, we use information retrieval open source tools to index the log files' fields and search for patterns of suspected behavior, which may indicate a system intrusion. Our development allows queries based on variables introduced by the analyst. The preliminary results obtained when using log files from an academic institution indicates that our approach is effective and can be used as a security aid.
In today's digital age, there is a need to digitize ancient images and documents, the ancient periods from stone inscriptions reveals the cultural history and helps to preserve and emphasize our country's heritage A recognition system aids in the analysis and digitization of traditional cultures and heritages. In this case, digitization entails the recognition of ancient images in both online and offline modes. This paper discusses a variety of strategies and techniques for retrieving, classifying, and extracting information from ancient inscriptions in a variety of languages.
We consider the problem of resource selection in clustered Peer-to-Peer Information Retrieval (P2P IR) networks with cooperative peers. The clustered P2P IR framework presents a significant departure from general P2P IR architectures by employing clustering to ensure content coherence between resources at the resource selection layer, without disturbing document allocation. We propose that such a property could be leveraged in resource selection by adapting well-studied and popular inverted lists for centralized document retrieval. Accordingly, we propose the Inverted PeerCluster Index (IPI), an approach that adapts the inverted lists, in a straightforward manner, for resource selection in clustered P2P IR. IPI also encompasses a strikingly simple peer-specific scoring mechanism that exploits the said index for resource selection. Through an extensive empirical analysis on P2P IR testbeds, we establish that IPI competes well with the sophisticated state-of-the-art methods in virtually every parameter of interest for the resource selection task, in the context of clustered P2P IR.
Search engines help users locate information within large stores of content developed for human consumption. For example, users expect Web search engines to direct searchers to websites based on the content of the site rather than the site address, and future video search engines to return video clips based on the actions recorded in the clip rather than filenames and donor tags. Search engines are developed using standard sets of realistic test cases that allow developers to measure the relative effectiveness of alternative approaches. The NIST Text Retrieval Conference (TREC) project has been instrumental in creating the necessary infrastructure to measure the quality of search results for more than 20 years, and has thus helped fuel the recent explosive growth in search-related technologies. This article is part of a special issue on NIST contributions to IT.
The holy Quran as a scripture and the source of law must be believed by more than one billion Muslim in the world. To be a good Muslim, someone must be able to understand the contents of the Quran and underlie all of their activity to the guidance that is in the Quran. However, the holy Quran is scripture that is very long document and derived in Arabic. Thus it can cause ordinary people those who do not understand Arabic find it increasingly difficult to find specific topics to learn the contents content of the Qur'an. This research aims to develop a web-based verse search system for Al-Qur'an. This paper proposed Quran information retrieval software which is integrated with clustering algorithm to ease Muslim to discover the relevant information in Quran verse by grouping the Quran verse into their own similar group. This proposed hybrid Quran information retrieval is composed of two main parts namely Quran indexing and Quran information searching.
In this paper, we propose a technique for retrieval of printed Kannada words from a digital repository based on Gabor wavelets and structural features. Gabor wavelets are employed to capture global properties of the underlying image whereas structural features are used to extract the local properties. We call the combination of these features as Glocal. An input document image is segmented into words and stored into a library. Then, Glocal features are employed to represent the words. Next cosine distance is used to measure the similarity between two words, based on it; relevance of the word is estimated by generating distance ranks. Then correctly matched words are selected at different distance thresholds such as 96%, 97%, 98% and 99%. Encouraging results are achieved in terms of average precision rate 74.27%, average recall rate 79.26% and F measure 75.62 % at a threshold of 97%.
This paper reviews the application of statistical language modeling in information retrieval to make web search process as efficient as possible. IR systems have been widely used for the task of ranking documents accurately for a given keyword query from the vast collection. Typically, web search engines are required to satisfy diverse information needs of user i.e. different users may have different search intention when they submit the same query to a search engine. Since there is a huge amount of user data available, it becomes possible to build user model so as to identify the more relevant documents based on individual users interest. This paper focuses on such a model suitable for IR especially for web search. It requires very less effort from the user side. The proposed user modeling would be helpful when the keyword query issued by the user is short and ambiguous i.e. most of the users provide inaccurate keyword query which is imprecise and they often under-specify their exact information needs. Thus query becomes ambiguous which needs to be understood by the retrieval system. Hence, personalization strategy needs to be adopted in order to solve these problems faced by the retrieval system. In order to personalize web search, the user profile model is formulated by using users browsing history. In the same way, past queries and clicks made by the user can also be used for building user search model. This approach is implemented and tested using real time user data.
Emotion cause extraction is a challenging task for the fine-grained emotion analysis. Even though a few studies have addressed the task using clause-level classification methods, most of them have partly ignored emotion-level context information. To comprehensively leverage the information, we propose a novel method based on learning to rank to identify emotion causes from an information retrieval perspective. Our method seeks to rank candidate clauses with respect to certain provoked emotions in analogy with query-level document ranking in information retrieval. To learn effective clause ranking models, we represent candidate clauses as feature vectors involving both emotion-independent features and emotion-dependent features. Emotion-independent features are extracted to capture the possibility that a clause is expected to provoke an emotion, and emotion-dependent features are extracted to capture the relevance between candidate cause clauses and their corresponding emotions. We investigate three approaches to learning to rank for emotion cause extraction in our method. We evaluate the performance of our method on an existing dataset for emotion cause extraction. The experimental results show that our method is effective in emotion cause extraction, significantly outperforming the state-of-the-art baseline methods in terms of the precision, recall, and F-measure.
In this paper, we propose a similarity measurement method based on the Hellinger distance and square-root cosine. Then use Hellinger distance as the distance metric for document clustering and a new square-root cosine similarity for query information retrieval. This new similarity/distance also bridges between traditional tf_idf weighting to binary weighting in vector space model. Finally, we conduct a comparison on performance between this method and the one based on Euclidean distance and cosine similarity. And from the results, we clearly observe that the precision and recall are improved by using the sqrt-cos similarity.
With the continuous growth of the electronic data and the expansion of World Wide Web, users are flooded with information for a single search query. Most commonly, the results that arrive for the search query ignore the context. Context has several dimensions such as users context, query/document context, spatial/temporal context. Though context is not a new idea in building intelligent systems, Contextual Information Retrieval is a biggest challenge in IR domain.This paper proposes news article retrieval using Convolutional Latent Semantic Model (CLSM). CLSM extracts the contextual features present in the query and finds the relevant documents and ranks them based on their relevance with the given query. CLSM was experimented with the clickthrough data of a commercial search engine and has been proven for its context-sensitive results and efficiency. In this paper, we discuss the feasibility of using CLSM for extracting news articles based on the context present in the query from a static news article repository.
This paper presents a research on product-specific web information gathering concept that intends to improve web searching through conventional search engines. Search engines are useful for searching general information; however, the results are not always accurate because they return the results based on key-words occurrences, but not accurateness. Further, users must filter and organize the information accordingly to make the information meaningful. Users must also visit multiple websites independently to check whether the information is relevant. These processes are tedious and very time consuming especially for users who search for specific products information. Hence, we propose a web information gathering model that aims to provide a standard for developing product-specific web searching tools. It was applied in a mobile application called Gold-Trader. The mobile application provides gold and silver prices information to individual or personal traders. It can help individual or personal traders to monitor and compare gold and silver prices from multiple websites without the needs to visit them individually.
Semantic Relatedness is widely used in various domains such as DNA sequence analysis, knowledge representation, natural language processing, data mining, information retrieval, information flow etc... Computing semantic similarity between two entities is a non-trivial task. There are many ways to define semantic similarity. Some measures have been proposed combining both statistical information and lexical similarity. It is difficult for a measure that performs well in a given domain to be applied with accuracy in another domain. A similarity measure may perform better with one language than another. Word is supposed to be not only similar to itself but also to some of its synonyms in a given context, and some words with common roots. Our approach is designed to perform query matching and compute semantic relatedness using word occurrences. It performs better than classical measures like TF-IDF, Cosine etc... Although it is not a metric, the proposed similarity measure can be used for a wide range of content analysis tasks based on semantic distance and its efficacy has been demonstrated. The measure is not corpus dependent so it can establish directly the se-mantic relatedness of two entities.
This study presents an approach to generate a bilingual language model that will be used for CLIR task. Language models for Bahasa Indonesia and English are created by utilizing a bilingual parallel corpus, and then the bilingual language model is created by learning the mapping between the Indonesian model and the English model using the Multilayer Perceptron model. Query expansion is also used in this system to boost the results of the retrieval, using pre-Bilingual Mapping, post-Bilingual Mapping and hybrid approaches. The results of the experiments show that the implemented system, with the addition of pre-Bilingual Mapping query expansion, manages to improve the performance of the CLIR task.
In this paper, we present an unsupervised method to find the covert properties of the product on E-commerce web sites. Our method is generic because we do not have to depend on web site restricted pattern. The method works by 3 algorithms which are Page Type Recognition, List Page Clustering and Query Relationship Discovery. Experimental results show that it can acquire accurate rate of 96% and recall rate of 94%.
The aim of information retrieval (IR) process is to respond effectively to user queries by retrieving information that are better meets their expectations. A gap could exist between user information needs and his/her defined context as the level of the user's expertise in the search domain is directly influencing the query richness. The information retrieval system (IRS) must be intelligent enough to identify information needs and respond effectively to meet the expected needs, regardless the level of user's expertise level. This process is difficult and it remains a major and open challenge in the domain of IR. IR models integrate many sources to achieve an effective retrieval system. Semantic IR is an environment in which semantic techniques were applied to sort the documents according to their degree of relevance to the query. The present work proposes a hybrid model to rank documents. The proposed model is based on a query likelihood language model and the semantic similarity between concepts to assess the relevance between query-document pairs. Concepts were extracted by projection on WordNet ontology then word sense disambiguation was conducted. A semantic index was built to validate the proposed model. The conducted empirical experiments show that the proposed model is outperformed the compared benchmarks in the measured IR metrics.
Retrieving information from network traffic is a fundamental task for network administrators to deeply analyze network traffic, and can be used in many applications such as network monitoring, anomaly detection, and network system optimization. However, it remains a challenging issue toefficiently retrieve information from large-scale network traffic. Moreover, many existing methods and software such as NetFlow and WireShark don't support to retrieve the content information of the network traffic or lack the power of handling the large-scale network traffic. To solve this problem, we proposed a novel framework and built a system named LTARS on this framework. In addition, we designed an algorithm called CFS algorithm and utilized by our system to improve the efficiency of retrieving. We conduct the experiments on many real-world datasets and the experimental results demonstrate that our method outperforms many existing state-of-the-art methods especially for extremely large-scale information data.
Nowadays, users of computers store a lot of text documents. This requires fast and precise searches over documents. The goal of Information Retrieval (IR) models is to provide users with those documents that will satisfy their information needs. The core of such models is the document representation used in the indexing of documents. Traditional IR models handle the frequency of query terms. The disadvantage of these models is that they exclusively consider terms in the query and ignore similar terms. This paper proposes a topic based indexing approach to represent topics associated to documents. Documents are modeled by using clustering algorithms based on natural language processing. As result of this proposal is a document-topic matrix representation denoting the importance of topics inside documents. In a similar way, each query over documents is converted into a vector of topics. Thus, a similarity measure can be applied over this vector and the matrix of documents to retrieve the most relevant documents.
Word Sense Disambiguation (WSD) has become a popular method for solving the ambiguous meaning of the words in Information Retrieval (IR) field area. Under the Natural Language Processing (NLP) community, WSD has been described as the task which able to select the appropriate meaning among the ambiguous meanings to a given word. Among three approaches, supervised based, unsupervised based and knowledge based approaches to WSD, this paper focuses on both supervised based and knowledge based approaches by proposing new Jaccard coefficient-based WSD algorithm to overcome the vocabulary miss match problem. WordNet and corpus external knowledge resources are utilized as the sense repositories by linking up with the new WSD algorithm to consider additional semantic for WSD. According to sample testing, IR system with new WSD algorithm attains more about 20 percent of total accuracy rate than traditional IR system.
Launched in 2007, UK PubMed Central (UKPMC) was originally released as a “mirror” of the US PubMed Central repository. The feedback from our users and our experience of running the service prompted the organisations comprising UK's principal funders of biomedical and health research to commission a new phase of development with aspirations to build the premier resource for biomedical and health research. This paper summarises the evolution of UKPMC and discusses the exploratory search facilities in the new web interface.
In recent years, due to the growing popularity of the human network behavior, we meet the new generation of big data era. This makes the emergence of characteristics that large data storage capacity and fast business growth. For large storage capacity, instability, low retrieval time caused by large data information, this paper demonstrate a distributed storage and server clustering approach to solve the problem of tradition storage capacity, high-availability of retrieval and servers. For the arrival of the era of big data, how to improve the retrieval time, the storage capacity and stability of system, these provide for the future construction of a guiding role.
An efficient mechanism for managing and retrieving relevant documents from heterogeneous data sources can enormously enhance the data analytics process. Semantics-based information retrieval approaches have emerged recently to use implicit knowledge for such purposes. Conventional search engines generate results by utilizing the intrinsic linkages included within the underlying data model (with minimal human involvement). However, it is equally essential to consider the necessity for retrieving documents in a private data environment based on domain knowledge. In this paper, we describe IIIT Bangalore’s contribution to building a full-fledged datalake framework founded on semantics-based approaches to host and analyze the data in various government sectors. This paper elaborates (a) an ontology-based big data management system/architecture, (b) a distributed heterogeneous data storage, and (c) a document retrieval solution in which an ontology acts as an intermediary to bridge the gap between what the user wants and what search retrieves. The proposed architecture is found to be promising based on the initial feedback received from the end users of the system.
One of the main tasks in Information Retrieval is to match a user query to the documents that are relevant for it. This matching is challenging because in many cases the keywords the user chooses will be different from the words the authors of the relevant documents have used. Throughout the years, many approaches have been proposed to deal with this problem. One of the most popular consists in expanding the query with related terms with the goal of retrieving more relevant documents. In this paper, we propose a new method in which a Cell Assembly model is applied for query expansion. Cell Assemblies are reverberating circuits of neurons that can persist long beyond the initial stimulus has ceased. They learn through Hebbian Learning rules and have been used to simulate the formation and the usage of human concepts. We adapted the Cell Assembly model to learn relationships between the terms in a document collection. These relationships are then used to augment the original queries. Our experiments use standard Information Retrieval test collections and show that some queries significantly improved their results with our technique.
This paper deals with the problem of Information Retrieval System (IRS) that integrate the user profile that contains novelty dimensions such as psychological state, emotional state, mental state and facial expression. In this paper, we propose an information retrieval system based on human behavior. The architecture of the proposed system is described in details. This IRS will be integrated in EMASPEL E-Learning Framework.
Given the prior information of the target, hyperspectral target detection focuses on exploiting spectral differences to separate objects of interest from the background, which can be treated as information retrieval (IR) task in machine learning (ML). Most traditional detection methods work in the original feature space and rely heavily on specific assumptions, which cannot guarantee effective extraction of features for the target and background in hyperspectral images (HSIs). Mass estimation (ME) is a base modeling mechanism that has been proven to effectively solve problems in IR and is not restricted by specific assumptions. In this article, we propose a novel target detection method through ensemble-based IR with ME (EIRME). By directly deriving the ordering from a sample set to rank data points, ME provides a simple and straightforward ranking measure to ensure that points similar to the given target are far away from dissimilar points. For the estimation of mass distribution, the proposed method utilizes a tree-structured mapping to generate a feature space, in which the separability of the target and background is further improved. In particular, to break through the technical difficulty that the direct migration of IR methods with mass measure cannot specifically meet the high-precision requirements of target detection in HSIs, we develop a specialized measurement, topological mass, which innovatively combines the mass measure with tree topology to quantify the spectral difference for detection output. Moreover, the IR with ME based on parallel measurements through ensemble trees provides a robust solution with better generalization capacity and higher precision for hyperspectral target detection, facilitating practical applications. Experimental results on benchmark HSI datasets prove that the specialized measurement that we developed successfully overcomes the drawbacks of the direct migration of IR methods with ME and exhibits unique advantages. In additi... (Show More)
Ancient Chinese dance resources are so vibrant and diverse that they are tremendously useful and exploitable. Historical heritage should be treated under the premise of 'knowing' and then 'changing'. Motivated by this, this paper attempts to use digital technologies to analyze and manage ancient Chinese dance resources and on this basis to facilitate their understanding and reuse. In this way, various and correlated information can be integrated based on evolvement patterns of this particular example of Chinese civilization in history. The dance resource information management platform efficiently and effectively integrates relevant digital resources based on a specific theme to form a resource set featuring the coverage of various types of materials, thereby significantly improving the utilization of digital information resources and realizing the three-dimensional teaching in dance classes. This platform helps users facilitate information integration in teaching design as well as helping them address the diversity of learning and application.
Shape Based Image Retrieval (SBIR) is a technique to retrieve images from the database by extracting the shape features of the object. In SBIR, shape features are extracted and then similarity matching is carried out based on some measure. Pyramid of Histogram of Oriented Gradients (PHOG), an improvised version of Histogram of Oriented Gradients (HOG), is very efficient for calculation of feature vector of an image. In this paper, PHOG along with Locality Sensitive Hashing has been exploited as a novel approach to SBIR. Locality Sensitive Hashing (LSH) is used for bit-level similarity matching.
In Natural language processing, one of the fundamental tasks is Named Entity Recognition (NER) that include identifying names of peoples, locations and other entities. Applications of NER include catboats, speech recognition, machine translation, knowledge extraction and intelligent search systems. NER is an active research domain for the last 10 years. In this paper, we propose a knowledge extraction framework to extract Named entities from Sahih AlBukhari Urdu translation which is a world known Hadith book. The proposed framework is based on finite state transducer system to extract entities and process the Hadith content using Part of Speech (POS) tagging. Conditional Random Field, an ensemble based algorithm, processes the extracted nouns for NER and classification. In the future, we aim to implement the proposed framework to rank the hadith content and apply the Vector Space Model.
In a research paper multiple citations and authors opinion about them are mentioned. Our primary focus is to identify which opinion is given on which citation and secondly to identify the exact sentence due to which citation has occurred in the citing paper. We have worked on two parallel approaches one is sentence level similarity, in which we try identifying the sentence(s) due to which citation has occurred. Other approach is article level similarity in which we identify the citation category and map anchor text to its citing article. We are able to show that an automated system can be created for identifying whether a sentence (without citation mark) is part of the citation text. In future we wish to extend this work with larger corpus and more sophisticated techniques.
The amount of information available on the internet is growing exponentially. The majority of this information is ambiguous by nature, and information retrieval (IR) systems typically return unrelated information when a typical web user tries to find relevant data. In this paper, we proposed a contextual query expansion technique (CQEB), which allows us to select only relevant documents and then only relevant terms from those documents. In order to establish the connection between retrieved documents and query keywords, the CQEB method makes use of BERT based deep neural word embeddings. We compared CQEB with the Glove embedding based QE technique. Extensive testing on test datasets from CACM and CISI reveals that our suggested method, CQEB, performs better than the standard query expansion (QE) techniques. Our experimental analysis demonstrates that, in 96% of the cases, the proposed method CQEB outperforms the alternative strategies in terms of F-score.
Patent retrieval is a long query task whose aim is to retrieve all documents related to patent applications. However, current approaches face with the term mismatch problem, leading to low retrieval performance. To deal with this issue, we propose a novel automatic query construction approach based on semantic concept importance for effective patent retrieval. In this approach, natural language processing techniques are firstly adopted to analyze patent long query inputs. Then, candidate query concepts are generated according to the concept features. Further, a concept importance-based query construction algorithm is presented to select the representative query concepts. Experimental results on the standard patent dataset demonstrate that our proposed approach can significantly outperform other state-of-art methods.
This paper probes into how to improve Information Retrieval by changing the feature distribution of the text. It introduces Cloud Model theory into Latent Dirichlet Allocation(LDA) Model and build a new feature selection system. LDA Model is used to mine the underlying topical structure. Each topic is associated with a multinomial distribution over words which are semantic related. But there is doubt that themes are relevant with each other in the light of semantics. Based on LDA model presented probability distribution of vocabulary in text, the new system with Cloud Model theory can automatically simulate feature set whose contribution degree is high in the text. Results show this feature set has less features but higher classification accuracy, thus obviously better than currently popular feature selection methods. If the query is matched to words with high contribution degree, the more these words are, the more relevant the article searched out is with the query. NTCIR-5 (the 5th NII Test Collection for IR Systems) collections of Experiment on SLIR (Single Language IR) show that this method achieves an obvious improvement compared with some other methods in IR.
The construction of university campus public information system is one of the basic units in the establishment of wisdom city, and it is the inevitable outcome of the information society evolved to the system integration stage. It provides a reasonable, efficient and humane modern quality service. This is conducive to the resources integration, sharing management and interactive applications, so as to improve the efficiency of learning, living and services. The construction of university campus public information system mainly concentrated in the identity authentication and authorization(construction of the One-Campus-ID Card system), resource sharing of digital network space construction, learning service oriented public information system (such as the integration of information resources and information database query), integration of the information service, teaching interactive information service design and other subsystems.
Performance of the recognition free approaches for document retrieval, heavily depends on the exact or approximate matching of images (in some feature space) to retrieve documents containing the same word. However, the harder problem in information retrieval is to effectively bring semantics into the retrieval pipeline. This is further challenging when the matching is based on visual features. In this work, we investigate this problem, and suggest a solution by directly transferring the semantics from the textual domain. Our retrieval framework uses (i) the language resources like Word Net and (ii) an annotated corpus of document images, to retrieve semantically relevant words from a large word image database. We demonstrate the method on two languages - English and Hindi, and quantitatively evaluate the performance on annotated word image databases of more than a Million images.
The conversion of unstructured big data into knowledgeable information has been the hotspot of search applications today. Nearly 75% of queries issued to Web search engines aim at finding information about entities. In an ideal case, the user wants to know the relations existing between the data objects. Conceptual knowledge graph provides an efficient way for exploring such relations. Past researches relied on knowledge bases like DBpedia to build such graphs. In this paper, we introduce a method that automatically extracts the key aspects of search query from the Wikipedia corpus. The extracted relations are dynamically expressed as a knowledge graph. Additionally, the system returns the list of results i.e., Wikipedia documents ranked in the order of their relevance in response to the search query. Thus, the proposed system can be viewed as an information retrieval system that leverages knowledge graph to provide more promising information to the user.
The paper presents a formal approach to the semantic interpretation of scientific and technical texts. Semantic proximity expression by means of a linguistic variable and its application to the information retrieval and documents classification are shown.
Impact Statement:Conventional digital cameras record images in spatial domain while single-pixel imaging (SPI) measures them in temporal domain. Here we present a 4-pixels imaging scheme, which retrieves an image partially in space and partially in time, by adopting a quadrant detector in an SPI system. While yielding images with equal quality, this system is 4 times faster than a standard SPI system. Our work can be useful for applications where comprise between spatial and temporal domains is needed. To retrieve an N pixel image of a scene, conventional digital cameras record the information using an N pixelated detector with one measurement, while single-pixel imaging achieves it by sampling the image in a particular basis and recording the light intensities of N temporal measurements with a single-pixel detector. Besides these two schemes, it is also possible to obtain the same spatial information with an X pixels detector and N/X measurements, where the pressure of information retrieval is distributed to both spatial and temporal domains, rather than condensed in one domain. In this paper, we investigate this possibility and present a 4-pixels imaging scheme, which retrieves an image partially in spatial domain and partially in temporal domain, by adopting a quadrant detector to a single-pixel imaging system. While yielding images with equal quality, the proof-of-principle system we built is four times faster than a standard single-pixel imaging system, demonstrating the feasibility of the proposed scheme. Our work exploits the flexibility of space-time information retrieval and can be useful for applications where comprise between spatial and temporal domains is needed.
Review comments are one of the main building blocks of modern code reviews. Manually writing code review comments could be time-consuming and technically challenging. Recently, an information retrieval (IR) based approach has been proposed to automatically recommend relevant code review comments for method-level code changes. However, this technique overlooks the structured items (e.g., class name, library information) from the source code and is applicable only for method-level changes. In this paper, we propose a novel technique for relevant review comments recommendation – RevCom – that leverages various code-level changes using structured information retrieval. RevCom uses different structured items from source code and can recommend relevant reviews for all types of changes (e.g., method-level and non-method-level). Our evaluation using three performance metrics show that RevCom outperforms both IR-based and DL-based baselines by up to 49.45% and 23.57% margins in BLEU score in recommending review comments. We find that RevCom can recommend review comments with an average BLEU score of ≈ 26.63%. According to Google’s AutoML Translation documentation, such a BLEU score indicates that the review comments can capture the original intent of the reviewers. All these findings suggest that RevCom can recommend relevant code reviews and has the potential to reduce the cognitive effort of human code reviewers.
Information retrieval is used to retrieve the information acTo retrieve an N pixel image of a scene, convecording to the user query. In the existing model the information retrieval is done by analyzing the whole document to answer a query and the terms related to the query are extracted. The indexing weight is applied to all the terms and finally it provides the response to the user. In the existing model they did not take the context into consideration so the information cannot be retrieved efficiently. In this paper, we propose a context-sensitive document indexing approach for information retrieval. The content carrying terms and background terms are separated by using lexical association. Indexing weight is calculated for content carrying terms. The term having the highest indexing weight is considered as the most salient sentence and these sentences are extracted and the document summarization is done. Then according to the user query the information is retrieved, the query is considered as the keyword. Then this keyword is matched with the summarized document. Once the keyword is matched, the particular sentences were extracted by using indexing algorithm. Finally these sentences are provided as the responses for the user. By using this approach, the information retrieval can be done effectively.
This paper introduced a LDA-based pseudo relevance feedback (PRF) model for cross language information retrieval. To validate the performance of PRF techniques in CLIR task, we conducted cross language query expansion experiments based on a self-constructed CLIR system, the LDA-based PRF model was applied before or after the query translating process, namely the pre-translation-PRF, the post-translation-PRF, and the combined-PRF strategy. We also compared this model with the classical VSM-based PRF algorithm. Experiment results showed that the proposed LDA-based PRF method was effective for improving the performance of CLIR.
The tasks, challenges, and techniques of Information Retrieval (IR) should reflect the structure of the underlying document-query sets, and the needs of the domain. Are document-query sets obtained from the enterprise domain fundamentally different from standard research corpora gathered from the web? In order to identify, understand, and characterize such structural differences, we build a framework using point set topology to analyze document-query sets. Our framework tailors topological notions such as subbasis, cover, compactness, towards IR. Unlike previous topological approaches, we use the reverse of the relevance map to topologize the set of queries, not the set of documents. We show that the topological approach exposes sharp differences between enterprise and web-collected standard research document-query sets. These differences readily motivate research into new retrieval tasks that are of commercial importance in Enterprise Information Management (EIM).
Keyword information retrieval is the mainstream way of information retrieval at present. Users need to scan a large amount of text information in the search results to find the information they want, but the process causes inefficiency and poor user experience. In fact, images in the web pages can provide a direct-viewing and fast retrieval way. Through the research on the features of web subject-related image and achieve its automatic identification and extraction, we can display it in the thumbnail together with the page title and summary in the results pages. It can help users filter and browse information in a more convenient way. This paper establishes a web image attribute system from both HTML attributes and external attributes. Then through corresponding automatic extracting algorithms and data analysis it succeeds to gain 16 feature rules of web subject-related image, and finishes building a web subject-related image feature model. The model proves to obtain more than 99% of the extraction rate and filtering rate while applying to the sample data, demonstrating its value in web information retrieval.
This work presents an information retrieval architecture developed for the Santa Catarina State Telemedicine System. This architecture employs DICOM Structured Reporting, controlled vocabularies for data catalogization and a specially developed search engine for data indexing and storing. Results of our case study show that searches can be performed much faster with the proposed search mechanism and that the precision of results is acceptable in most cases. In some searches, irrelevant items within the 15 first results were identified. This occurred partially because search terms found in the additional free text observations inserted into the findings reports were treated with the same relevance as formally diagnostically relevant items of the DICOM SR structure and, partially because the semantics of negations associated to search terms in the findings reports were not taken into consideration.
This paper presents a prototype of lyrics retrieval system that enables users to browse lyrics that relevant with the reflection topics and pictures that relevant with the lyrics. In this research, we use Christian Reflection from Santapan Rohani (http://santapanrohani.org). Santapan Rohani is the translation of Our Daily Bread (formerly RBC) devotional publications into Indonesian Language. There are many Gospel (Christian) songs that could be relevant with the topics of devotion. The lyrics that use in this research from hymn books used in Church services in Indonesia (Kidung Jemaat-KJ, Pelengkap Kidung Jemaat-PKJ, and Nyanyikanlah Kidung Baru-NKB). The text processing done in this research is tokenization, word transformation, stop words removal, and stemming. The methods used in this research are TF-IDF form term weighting, and Cosine Similarity for similarity measure. From the experiments, the precision of information retrieval must be improved. When threshold similarity=0.2 with query expansion using scripture/bible reference verses of the day, precision = 16.82%, and without query expansion, precision = 17.59% when threshold similarity = 0.25.
This paper presents an assessment of the difficulties involved in implementing visual information retrieval (VIR) systems for veterinary images such as x-rays. Particularly, we examine cases where the x-ray exchange is of a casual nature, such as x-rays photographed and stored as JPEG images, so that they can be exchanged and shared more readily with other professionals in the field. While this practice is very convenient for the exchange of information between veterinary professionals, the resulting images contain imperfections such as glare, angular distortion, and background noise. These imperfections can become a stumbling block when applying VIR algorithms intended to analyze and process these images.
Classical search engines rely mainly on keyword matches. More advanced search engines and information retrieval systems use personalization techniques to enhance the relevancy of the results. Those approaches assume that the current search of a user is directly related to his/her profiles and/or past navigation/search experience. However, this assumption does not hold in all cases. In this paper, we are proposing an information retrieval approach that is not making any assumption about the user. It offers a straightforward way to choose and navigate the available interpretations. Our approach relies on a navigation model that is based on a new paradigm called multi-space interpretation index and projection operations. We have derived a mathematical model of a Web information retrieval system based on the new paradigm. This concept naturally distinguishes between the different interpretation layers (each layer is a separate space). Therefore, it offers to users the flexibility to navigate the interpretation spaces in such a way that suites better their information needs.
The presence of retrieval information and sentiment analysis may quicken the evaluation of the lecturers and the result of an ongoing issue during the course can be known directly and accurately. This evaluation is conducted every semester by completing the questionnaire. The results of the sentiment analysis are used as the evaluation for lecturer teaching improvement based on the required search results. Currently, the lecturer's assessment is still done manually (with hardcopy) by the LPMI department. Information retrieval is the process of taking users' search requests (queries) according to the likelihood of relevance to the input query. The generalized Vector Space Model (GVSM) uses vector space concepts in determining the relevance of the user input (query) to the document set. Sentiment analysis is the process of document classification. In this research, the sentiment analysis was divided into positive, negative, and neutral classes using the Naive Bayes Classifier (NBC) method. The classification process begins by dividing the document into training data and test data. The search results using the 43 most commented on lecturers rated with 5 keywords using the GVSM had a precision rate of 100% and the recall of 100%, while NBC's sentiment analysis had a precision or truth rate of 72% and an error rate of 28% with a long time of searching and sentiment analysis averaged 21.8 seconds. Based on that data, it can be concluded that the search and sentiment analysis applications can run properly and appropriately.
Translingual Text Mining (TTM) is an innovative technology of natural language processing for building multilingual parallel corpora, processing machine translation, contextual knowledge acquisition, information extraction, query profiling, language modeling, contextual word sensing, creating feature test sets and for variety of other purposes. The Keynote Lecture will discuss opportunities and challenges of this computational technology. In particular, the focus will be made on identification of language pair phenomena and their applications to building holistic language model which is a novel tool for processing machine translation, supporting professional translations, evaluation of translingual systems efficiency and also for improving the process of teaching foreign languages regardless of the level of students' proficiency. Some components incorporated to machine translation systems rely specifically on language-pair phenomena like for example language recognizer, or word deliminer. Nowadays, declarative programming is becoming widely used just for describing language pair phenomena by graphical formalism. Translation irreversibility represents a unique language and system independent asymmetrical translation innovative technology, which will be presented during the Keynote lecture.
This paper represents a new technique for building a relevance judgment list for information retrieval test collections without any human intervention. It is based on the number of occurrences of the documents in runs retrieved from several information retrieval systems and a distance based measure between the documents. The effectiveness of the technique is evaluated by computing the correlation between the ranking of the TREC systems using the original relevance judgment list (qrels) built by human assessors and the ranking obtained by using the newly generated qrels.
Blogs have been popular social networking platforms in recent years. Blog opinion retrieval is one of the key issues that needs to be solved. In this paper, we investigate if the Condorcet fusion and the weighted Condorcet fusion can be used for effectiveness improvement of blog opinion retrieval. The experiments carried out with the data set from the TREC 2008 Blog track show that the Condorcet fusion is effective and the weighted Condorcet fusion, with its weights trained by linear discriminant analysis, is very effective. Both of them outperform the best component result by a clear margin.
Combining the resulting lists of multiple information retrieval (IR) systems has been known to outperform, in many cases, the best of the individual systems. However, it remains a challenging question to know what combination method to use and in what conditions the combination system can perform better than its individual systems. In this paper, we use an information fusion paradigm: Combinatorial Fusion Analysis (CFA) to study these issues. We take the TREC dataset as our experiment data and use the rank-score characteristic (RSC) function to measure the cognitive diversity between different individual systems. Results from our experiment demonstrate that: 1) combined system can improve performance only if the individual systems have relative good performance and are diverse, 2) there is no guarantee that the combined system performs better when more individual systems are added, and 3) rank combination is better than score combination in majority of the cases when the diversity between two individual systems measured by the RSC function is large enough.
With the increasing popularity of digital documents in the information era, large scale document-term matching is critical for document similarity tasks. However, the traditional automatic term matching method is mainly from the perspective of classification, which ignores the macroscopic and ambiguous characteristics of domain terms. This paper proposes a document-term matching method based on information retrieval (IR-DTM). We utilized an automatic data collection method, which can effectively alleviate the problem of insufficient data and greatly reduce the manual workload. Then, we realized semantic matching between macro terms and micro documents by extending the words in terms, thereby effectively improving the accuracy of document-term matching. Experimental results show that our method can perform effective automatic matching of domain terms and case documents. The effectiveness of IR-DTM is evaluated by R@1, R@5, and R@10. The R@10 percentage points on PolProv1.0, EcoProv1.0, and TechProv1.0 datasets can reach 84.6%, 92.5% and 93.2% respectively.
Concern localization refers to the process of locating code units that match a particular textual description. It takes as input textual documents such as bug reports and feature requests and outputs a list of candidate code units that need to be changed to address the bug reports or feature requests. Many information retrieval (IR) based concern localization techniques have been proposed in the literature. These techniques typically represent code units and textual descriptions as a bag of tokens at one level of abstraction, e.g., each token is a word, or each token is a topic. In this work, we propose multi-abstraction concern localization. A code unit and a textual description is represented at multiple abstraction levels. Similarity of a textual description and a code unit, is now made by considering all these abstraction levels. We have evaluated our solution on AspectJ bug reports and feature requests from the iBugs benchmark dataset. The experiment shows that our proposed approach outperforms a baseline approach, in terms of Mean Average Precision, by up to 19.36%.
Without personalization is a serious limitation of today's information service, in view of which and the advantage of the agent technology, a kind of user interface profile based on multi-agent has been designed in order to make information retrieval easy, and its concrete description has also been given. The agent technology has provided us a kind of completely different information service pattern, which probably can improve the efficiency of information retrieval significantly.
A combined sensor and communication network can provide important support for first responders and emergency workers in dangerous environments. The innovation proposed here is use of SDN network control to dynamically accommodate, through an open API, competing resource demands for three distinct but closely associated needs: communication among people, environmental monitoring, and information retrieval. The SDN-controlled configuration realizes the desired flexibility and robustness. This paper outlines the physical and software-defined architecture of a quasi-linear communications-sensing relay network, with an SDN controller, that can be a major asset in emergencies or for work in dangerous environments.
In today's world with ever increasing amount of text assets overloaded on web with digitized libraries, sorting out these documents got developed into a feasible need. Document clustering is an important procedure which consequently sorts out huge number of articles into a modest number of balanced gatherings. Document clustering is making groups of similar documents into number of clusters such that documents within the same group with high similarity values among one another and dissimilar to documents from other clusters. Common applications of document Clustering includes grouping similar news articles, analysis of customer feedback, text mining, duplicate content detection, finding similar documents, search optimization and many more. This lead to utilization of these documents for finding required information in a competent and efficient manner. Document clustering required a measurement for evaluating how surprising two given information are. This dissimilarity is often estimated by using some distance measures, for example, Cosine Similarity, Euclidean distance, etc. In our work, we evaluated and analyzed how effective these measures are in partitioned clustering for text document datasets. In our experiments we have used standard K-means algorithm and our results details on six text documents datasets and five most commonly used distance or similarity measures in text clustering.
The biomimetics to adopt a high-performance and high efficiency creature function for a new technology attracts attention. However, it is unknown which creature function helps the development of a new technology because the kinds of the creature are more than 1,500,000. Therefore, we propose an information retrieval system called Bio-TRIZ database by using TRIZ which attracts attention as a problem solution of the engineering. The Bio-TRIZ database is based on the Linked Data which contains not only engineering perspective data but also naturalist perspective data. Therefore, we can search the creature function by different approaches. We can expect technology transfer from nature to engineering to realize a sustainable lifestyle as spiritually rich society which does not depend on energy consumption by using the proposed system. We evaluated the proposed system through plural case studies. Then, we confirmed that our proposed system was effective at a viewpoint of the serendipity information retrieval for realizing sustainable lifestyle.
Data retrieval is a key process of acquiring information as per requirement. Now days, the necessity of proper information has increased. The most basic tools which provide this service are browser. It traverses the data as per user's query and gives the search results of all related information. Hence, it becomes a time consuming process to find required information. In this paper, the focus is done over content based data mining using ontology and text feature extraction. Content based data mining process focuses on domain of the data. Ontology, itself is a domain based data set information system that will help to achieve required data retrieval in a more appropriate way. The proposed systemuses k means clustering algorithm for creation of flat clusters. Flat clusters are the primary classification or clusters of data that are used for various further processing. For more appropriate data retrieval, this system uses text feature extraction algorithm. This algorithm will help to reduce the noisy data from data sets. A noise free data will help to perform better data retrieval process.
Developing an information management system for a dynamic organization is a challenging task. The fundamental back-end architecture, especially design of databases and data retrieval queries play an important role in determining the overall efficiency of the information system. A poor database design will lead to increased maintenance and inefficient retrieval of data will lead to decreased performance. This paper introduces CampusStack code named Camps, a campus information management system primarily meant for academic institutions built on top of open source technologies. The primary goal of CampusStack Project is to develop systematic design patterns for increased efficiency and minimized maintenance. The key contributions of this paper are two fold i) to introduce the design of scalable entity attribute value pair data modelling on top of relational data model to mitigate the effects of software change requests and the maintenance. ii) introduce CampusStack and its core components.
In this paper, toward a brain-computer interface for information retrieval, we performed preliminary experiments on post-saccadic event related potential (ERP) during web browsing. Two channel electroencephalogram was analyzed with five subjects during simple string retrieval tasks using web browser with active eye movements, i.e., saccadic eye movements. After the saccadic eye movements the ERP signals were observed in target string retrievals, whereas not observed in standard string retrievals. The classification algorithm was applied and the average decoding performance of single-shot post-saccadic ERP could be 78.2% and 77.6% in recall and precision, respectively. The eye-tracking was also studied and the eye gaze points were confirmed to be on and around target strings when the ERP was elicited. This result suggests that the post-saccadic ERP with eye-tracking leads to a possibility of the use in the brain-computer interface for information retrieval.
Introducing mobile learning can add value to the overall learning activities. Electronic learning helps learning institution to extend the contents and contexts of information sharing and knowledge delivery beyond its traditional mechanism. The changes enable students to participate more in the process of learning, like ability to customize learning to the personalized mobile based interface. Mobile learning are also able to empower students in generating information among students themselves. The study examines data continuum starting from big data concept, information sharing, and pervasive knowledge in mobile learning & teaching process. As of big data can play critical role in profiling each of student's need and habits of students' learning patterns can contribute in providing visualization of pervasive knowledge based on student's interest. We have deployed literature analysis approach from peer reviewed journals for contents analysis. Mobile learning with big data enabled can be expected to maximize impacts on students' learning experiences.
Automated information retrieval and text summarization concept is a difficult process in natural language processing because of the infrequent structure and high complexity of the documents. The text summarization process creates a summary by paraphrasing a long text. Earlier models on information retrieval and summarization are based on a massive labeled dataset by the use of handcrafted features, leveraging on knowledge for a particular domain, and concentrated on the narrow sub-domain to improve efficiency. This paper presents a new deep learning (DL) based information retrieval with a text summarization model. The proposed model involves three major processes namely information retrieval, template generation, and text summarization. Initially, the bidirectional long short term memory (BiLSTM) approach is employed for retrieving the textual data, which assumes each word in a sentence, extracts the information, and embeds it into the semantic vector. Next, the template generation process takes place using the DL model. The deep belief network (DBN) model is employed as a text summarization tool to summarize the textual content. In addition, the image description is generated for the visualized entities that exist in the images. The design of BiLSTM with the DBN model for the text summarization and image captioning process shows the novelty of the work. The performance of the presented method is validated using Giga word corpus and DUC corpus. The experimental results referred that the proposed DBN model outperformed the compared methods with the maximum precision, recall and F-score. The image captions are compared with a predefined set of captions that exists for the image and the performance is evaluated using the BLEU metric.
As the number and size of image databases grows, accurate and efficient content-based image retrieval (CBIR) systems become increasingly important in business and in the everyday lives of people around the world. Accordingly, there has been a substantial amount of CBIR research, and much recent interest in using probabilistic methods for this purpose. However, there exist some shortfalls or limitations of these methodologies. Methods which boost retrieval performance by incorporating knowledge base inference engine have also been of interest. In this paper we describe a novel framework for performing content-based image retrieval using knowledge based inference engine. Given a user specified image query, the system first interprets the query using the knowledge based inference engine and then extracts a set of images, from a labelled corpus, corresponding to that query. The experimental results reveal that the performance of image retrieval can be surprisingly enhanced.
In order to improve the performance of feature extraction, this paper proposes a new retrieval model based on coupled feature extraction. In this paper, we preprocess the big data, process the big data by distributed fusion, analyze the characteristics of the big data, extract its statistical features, and construct the big data distribution structure model. By extracting the power user coupling feature, the multi-space memory distribution of user variable relation big data features extraction and retrieval is obtained, and the analysis model is optimized. Experimental results show that the retrieval model based on this method can improve the capability of retrieval and information access, which is helpful to improve the retrieval ability to a certain extent.
The Coronavirus crisis has cast a shadow over the education sector; As it pushed schools, universities, and educational institutions to close their doors, to reduce the chances of its spread. This raised great concern among those affiliated with this sector, especially students preparing to take important exams. All this pushed educational institutions to switch to E-learning, as an alternative that has long been talked about over the need to integrate it into the educational process. From this standpoint, the importance of electronic exams as an alternative to the traditional paper exams appears, there are many assessment methods in E-learning, one of these assessments is the essay. The study used a variety of similarity tests and used an Arabic data set of 40 student answers. The results indicate that the Arabic Automated Essay System will improve with Cosine similarity and Arabic WordNet. Automatic Arabic Essay Scoring using WordNet is higher in terms of precision than the Automated Arabic Essay Scoring without using WordNet dependent on the mean absolute error value and Pearson correlation. The results clearly show the Cosine similarity with Arabic WordNet has the lowest error. Cosine similarity with all stemming types has the lowest error compared with the Jaccard and Euclidean similarity. Euclidean similarity has the highest error.
Video synthetic aperture radar (VideoSAR) provides a continuously multidimensional phase-history acquisition, proved to be significantly attractive in the remote sensing-based information extraction field. In this paper, we propose a 3-D structure-from-motion (SfM) retrieval method based on the cylinder model from circular VideoSAR sequences. First, VideoSAR imaging characteristics on the cylinder model are analyzed. Then, SfM retrieval method with geometric prior knowledge is proposed using robust shadow information. Finally, experimental results utilizing circular VideoSAR fragment published by the Sandia National Laboratories indicate that the proposed method can achieve a higher accuracy, hence, the validity has been demonstrated.
With the growing amount of documents in the search index of information retrieval systems, the problem of ranking documents becomes crucial. The modern state of the problem leads to the point where machine learning becomes the most efficient way to optimize the ranking function. In this article investigated ranking function in information retrieval systems (IRS) and learning to rank problem. During the learning to rank process, IRS is defining the weight coefficients for simple rankers. The conducted researches are showing the approach for learning to rank problem LTR-MGA utilizing the hybrid method based on modified genetic algorithm and the Nelder-Mead method. This approach can be used to optimize a graded-metrics of ranking, such as NDCG. The efficiency of proposed method was proved, based on researches performed on LETOR data sets. The value of ranking quality measures was significantly increased after learning to rank process. Also the usage of modified genetic algorithms leads to reduction of time required for learning to rank comparing to traditional genetic algorithm.
Currently, the society is very critical and demands valid information transparency. Information transparency is needed to maintain good governance and can also prevent corruption. However, information on the sustainability of development programs carried out by the government is much unknown to the public. In addition, the government must be burdened with public dissatisfaction due to fake news (hoax). The Ministry of Public Works has tried to build a website to disseminate its programs. However, measurement of the quality of the website is needed. This study uses questionnaires with a Likert scale to obtain user interface assessment data in accordance with the WebQual 4.0 method. This study shows that the easiness of the website to use has the highest correlation with how helpful the website is. Public works ministry websites need to more frequently update valid information and promote transparency. The society also wants online complaint platform to make it easier to solve problems.
In the rapid growing and dynamic world, Information Retrieval is a paramount task. Though efficient Information Retrieval Systems exist, the advent of advanced computing techniques such as Artificial Intelligence impose a need to have more intelligent IR systems that can actually understand user queries. Interpreting user queries is also another major challenge in question answering systems. The proposed system is such an intelligent IR system that semantically extends and understands user queries to produce more accurate and relevant information. This model is useful to organizations or universities which employs this question answering system to answer user or student queries. This can be done by feeding information into a knowledge base, where the query data or the relevant information is retrieved. But what lacks in most of the question answering systems is its accuracy and precision in answering to the queries. Information retrieval is the major task in question answering systems.
This paper demonstrates a series of analyses to calculate new clusters of shared subject headings among items in a library collection. The paper establishes a method of reconstituting anonymous circulation data from a library catalog into separate user transactions. The transaction data is incorporated into subject analyses that use supercomputing resources to generate predictive network analyses and visualizations of subject areas searched by library users. The paper develops several methods for ranking these subject headings, and shows how the analyses will be extended on supercomputing resources for information retrieval research.
This article aims to present an instrument for phonemic transcription of Ukrainian texts that is necessary for phonetic studies, lexicographic work, and in text-to-speech conversion, machine translation, information retrieval, automatic speech recognition, language model transfer, and other natural language processing tasks. The tool provides an automatic scholarly transcription of Ukrainian texts into the International Phonetic Alphabet (IPA) and the Extended Speech Assessment Methods Phonetic Alphabet (X-SAMPA).
This paper introduces a retrieval system based on context rather than content. The system uses a set of vector spaces to represent the different contextual characteristics (position, time, sound environment, etc.). In this model both the current context and the items in the corpus are represented by vectors. We then use vector similarities to identify the relevant items given the context. In order to take into account the user's own perception of the context we also use learning techniques based on the user feedback. This paper also presents two applications we are currently developing, which make use of this retrieval system: a contextual adaptive User Interface selecting the right configuration profiles matching the current context as acquired by a computer, and an augmented reality application for iPhone suggesting to the user activities and places that could be of interest to him given its current environment.
We present scalable and parallel versions of Lipmaa's computationally-private information retrieval (CPIR) scheme [20], which provides log-squared communication complexity. In the proposed schemes, instead of binary decision diagrams utilized in the original CPIR, we employ an octal tree based approach, in which non-sink nodes have eight child nodes. Using octal trees offers two advantages: i) a serial implementation of the proposed scheme in software is faster than the original scheme and ii) its bandwidth usage becomes less than the original scheme when the number of items in the data set is moderately high (e.g., 4,096 for 80-bit security level using Damgard-Jurik cryptosystem). In addition, we present a highly-optimized parallel algorithm for shared-memory multi-core/processor architectures, which minimizes the number of synchronization points between the cores. We show that the parallel implementation is about 50 times faster than the serial implementation for a data set with 4,096 items on an eight-core machine. Finally, we propose a hybrid algorithm that scales the CPIR scheme to larger data sets with small overhead in bandwidth complexity. We demonstrate that the hybrid scheme based on octal trees can lead to more than two orders of magnitude faster parallel implementations than serial implementations based on binary trees. Comparison with the original as well as the other schemes in the literature reveals that our scheme is the best in terms of bandwidth requirement.
When searching for information with an information retrieval (IR) system, sometimes the results of the search documents provided by the system do not match the information needs of the user. Pseudo Relevance Feedback (PRF) based Query expansion (QE) tries to overcome these problems by adding words that are expected to improve retrieval results from top N ranked documents retrieved. The use of firefly algorithm (FA) as one of the optimization methods has been proven by the previous study to improve the performance of the IR system. However, in that study the weighting of words was done using the rocchio function of the Pseudo Relevant Document (PRD), so it is feared that the performance of IR system will be reduced if the number of relevant documents in PRD is little or none at all. Therefore, scoring by term relationship between query and PRD is used in this study combined with rocchio algorithm. The results of the study showed that usage of term relationship word co-occurrence or word similarity can improve the performance of the IRS that was previously built. In addition, word co-occurrence with jaccard have the best performance compared to the previous study and other combinations. FA itself was able to choose the optimal terms, even though the number of top N ranked documents increased. Furthermore, the combination of term relationship and rocchio algorithm can increase the convergence rate than the ones without rocchio algorithm.
Nowadays, most of information saved in companies are as unstructured models. Retrieval and extraction of the information is essential works and importance in semantic web areas. Many of these requirements will be depend on the storage efficiency and unstructured data analysis. Merrill Lynch recently estimated that more than 80% of all potentially useful business information is unstructured data. The large number and complexity of unstructured data opens up many new possibilities for the analyst. We analyze both structured and unstructured data individually and collectively. Text mining and natural language processing are two techniques with their methods for knowledge discovery form textual context in documents. In this study, text mining and natural language techniques will be illustrated. The aim of this work comparison and evaluation the similarities and differences between text mining and natural language processing for extraction useful information via suitable themselves methods.
This paper presents a comparative study for word spotting techniques according to holistic approach. So, the current work consists in experimenting word image segmentation, characterization and matching to show the most reliable techniques. The experimental process is done in the same printed and handwritten Arabic dataset. Our aim is to realize an effective system of information retrieval.
The rapidly increasing size of biomedical databases such as Medline requires the use of intelligent data mining methods for information extraction and summarization. Existing biomedical text-mining tools have limited capabilities for incorporating citation information during document ranking and for inferring topological and network relationships between biomedical terms. Often too much is returned during summarization leading to information overload. Furthermore, literature-based discoveries could be hard to interpret if the network is too complex. SEACOIN2.0 can incorporate citation information during document ranking and uses a unique association rule mining algorithm to generate multi-level k-ary trees. The multi-level trees facilitate efficient information retrieval, visual data exploration, summarization, and hypothesis generation. The system presents graphical summarization via multiple dynamic visualization panels and an interactive word cloud. LexRank algorithm is used to identify salient sentences in top abstracts related to the query. An average F-measure of 94% was achieved for document retrieval, and an average precision of 88% was obtained for identification of top co-occurrence terms. SEACOIN2.0 was also used to replicate previously published findings using the literature-based discovery and EMR-based PheWAS approaches. We present herein SEACOIN2.0 (https://newton.isye.gatech.edu/SEACOIN2/), an interactive visual mining tool for improved information retrieval, automated multi-level summarization of Medline abstracts, and literature-based discovery. SEACOIN2.0 addresses the problem of “information overload” and allows clinicians and biomedical researchers to meet their information needs.
Software is incrementally evolved as various new feature requests are implemented to meet users' requirements. To accelerate the incoming feature implementation, developers often utilize existing third-party APIs that encapsulate featurerelated functionality into simple APIs. However, it is non-trivial for developers to choose which APIs to use and where to use them in a target program since the search space of APIs and their usage locations are usually large. In this paper, we introduce a tool, MULAPI, to facilitate the decision of suitable APIs at potential usage locations for implementing the incoming feature requests. MULAPI combines feature localization and information retrieval techniques to accomplish API recommendation and usage location. Empirical studies demonstrate that MULAPI can effectively recommend correct APIs and their usage locations with higher precision than state-of-the-art approaches. The video of our demo is available at https://youtu.be/s3Cs5ltqdvs.
We offer an automated way of estimating the author of a song using only its lyrics content. To this end, we introduce a complete text classification framework which takes raw lyrics data as input and report estimated songwriter. The performance of the system is evaluated based on its classification and retrieval ability on a large dataset of Turkish songs, which was collected in this study. The results promote the use of such technique as a complementary tool in music information retrieval applications.
In a recent paper by Yaakobi and Bruck, the problem of information retrieval in associative memories has been considered. In an associative memory, each memory entry is associated to the neighboring entries. When searching information, a fixed number of input clues are given and the output set is formed by the entries associated to all the input clues. The maximum size of an output set is called the uncertainty of the associative memory. In this paper, we study the problem of information retrieval in associative memories with small uncertainty. In particular, we concentrate on the cases where the memory entries and their associations form a binary Hamming space or an infinite square grid. Particularly, we focus on minimizing the number of input clues needed to retrieve information with small uncertainty and present good constructions some of which are optimal, i.e., use the smallest possible number of clues.
Model-based CASE tools provide mechanisms to capture and store heterogeneous artifacts produced during the software development process. These tools incorporate a meta-model describing artifact types and traceability links. Although model-based CASE tools provide required means to create and link different artifact types, still the process of linking artifacts is primarily manual resulting in missing or broken traceability links. This paper proposes a novel approach to create and utilize a project-specific ontology derived from the textual and structural information available in the development artifacts to assist the traceability link creation process. We discuss the benefits and challenges of incorporating the proposed approach in a model-based CASE tool.
In the era of information burst, optimization of processes for Information Retrieval, Text Summarization, Text and Data Analytic systems becomes utmost important. Therefore in order to achieve accuracy, redundant words with low or no semantic meaning must be filtered out. Such words are known as Stopwords. Stopwords list has been developed for languages like English, Chinese, Arabic, Hindi, etc but standard stopword list is still missing for Sanskrit language. Identifying stop words manually from Sanskrit text is a herculean task hence this paper reflects an automated stop word generator algorithm based on frequency of word and its implementation to ease the task. To fine-tune the generated list still manual intervention by language expert is required thus following a hybrid approach. The paper presents the first of its kind, a list of seventy-five generic stopwords of Sanskrit language extracted from a data amounting to nearly seventy-six thousand words.
Question answering (QA) is the task of automatically answering a question posed in natural language. Its applied to several domains, and it is a specific type of information retrieval, that has three components such as question processing, information retrieval, and answer extraction. By analysing the user question, we intend to improve the precision of Question answering systems by focusing namely on the representation of the question itself as a bag of concepts, using the Explicit Semantic Analysis (ESA). In particular, it proposes an approach that decides the relevant answers based on a set of features that describe: (i) the classification of the question, (ii) the generation of the bag of concepts, as well as (iii) the extraction of the relevant answer from the candidate sentences. The representation of the user's question as a bag of concepts allows us to have the greatest number of relevant documents to this question.
The translation has an essential role in Cross-lingual Information Retrieval. Translation using a dictionary is reliable even though it has a limited vocabulary. Translation using google translate, in some cases, using different words used in document target words. The translation process causes word translation to be less accurate to get relevant documents. In this paper, we proposed a new translation approach by adapting google translate using a dictionary and word embedding in Arabic-Indonesian Cross-lingual Information Retrieval. The dictionary is the primary resource used for translation improved by Levenshtein distance and FastText for finding the correct word translation. Google translate is used to complete translation when the word does not exist in the dictionary resource. The proposed method archive a BLEU score of 0.47. This score is higher than the other comparison resource score. The proposed method successfully improves the translated query to retrieve more relevant documents in cross-lingual information retrieval based on this implementation.
The performance of an IR system is deteriorated by factors including short and vague queries put up by the users, ever increasing volume of documents on the web, users not knowing their exact information need etc. Relevance feedback (RF) and web search document clustering are techniques to improve the performance of an Information Retrieval (IR) system. Relevance feedback provides a method to get more relevant search result from an IR system using documents that are marked relevant by the user as a feedback to reformulate query. This refined query is then used to retrieve the documents. In document clustering approach, the search result is divided into thematic groups where documents of one group are similar to each other and dissimilar to the documents of other groups. This paper presents a report on the effectiveness of relevance feedback technique as compared to document clustering in context of web information retrieval and why document clustering is the most preferred approach.
Education Institutions, Recruitment Organizations, Government Authorities and Students are main stockholders in Education System. Education system relies upona lot of paper-work which is cheap but on environment it has negative consequences. Process of planning and execution of government schemes is inefficient, very slow which lacks even feedback mechanism. Stakeholders are required to do lot of paper-work for maintaining information, changing and transferring that information, forgery of information and storage of that crucial information and error detection and recovery from damage if takes place. For getting scholarships, recruitments, admissions related activities, recruitment in government services are not streamlined and needs same information of students which should be shared with efficiently using some technological advancements. Present day systems are based on paper based information flow which faces problems of delay of information processing, Information Retrieval, Delay in Response, Inefficiency and under effective performance of the system. Presently in most of the countries especially in developing countries all the stack holders of the system are not integrated and does not share same platform for information sharing and response which prevents. This makes the communication and information flow slow and non-coordinated which reduces the efficiency and effectiveness of the education systems. In this paper we are proposing a new framework in the form of information system for inclusion of each stakeholder in the system so that problems could be overcome and stack holder with the help of information and communication technology utilise the full potential and effective delivery products and services of the Education system to everyone including students and government. In this paper we propose the framework with complete information of development processfollowing software engineering concepts to solve most of the problems of present day education syst... (Show More)
In the past few decades many information's are stored in World Wide Web as web pages and documents, it is increased by huge amount of data so the problem persist for searching a particular document. There are various document image retrieval image methods are available such as classification, clustering and graph techniques are designed to detect the problems in document image retrieval system. This research is concerned with study and analysis of focusing document image retrieval and logo matching in journal database. There are various national and international journals are available in online to representing their logo in document. Retrieval framework is effective not only in improving retrieval performance in a given query session, but also utilizes the knowledge learnt from previous queries to reduce the number of iterations in following queries. This paper describes methods and techniques for document image retrieval analysis, logo matching and survey includes the current state of research problems in information retrieval, document image retrieval, relevance feedback and logo based text retrieval.
Query expansion is one of important technologies used to improve retrieval efficiency. Many studies focus on query expansion with relationships between terms only extracted from the single local domain corpus. In fact, because the single local domain corpus is relatively small, there exist many no-landing terms which have no candidates for query expansion resulting in low retrieval performance. Therefore, to address such problem, relationships between terms captured from Wikipedia are superimposed to the basic Markov network that pre-built using the local domain corpus. A new larger Markov network is formed with more and richer relationships for each term. A graph mining technology, clique, is implemented to measure inter-relationships in Markov network for query expansion. The proposed techniques of superimposed Markov network and clique-based query expansion are benefit to improve precision and recall of information retrieval and to reduce the risk of topic drift.
Spatial information delivery is extremely basic and quite important in the Held of geographic information science. Traditional spatial information delivery is system-oriented or system-centered. Accordingly users have to adapt to the system and complete the delivery of spatial information, and this delivery service mode suffers from personalization and proactivity problems. Aiming at the objectives of spatial information delivery, we give the process of spatial information delivery taking into count personalization and proactivity, and point out three main links: data source collecting, user profile modeling and information dissemination. Then, an analysis of delivery algorithms of spatial information is given subsequently. The adaptability of content-based delivery algorithms, collaborative delivery algorithms and hybrid delivery algorithms are analyzed in-depth respectively, and the application conditions of every kind of algorithms are given clearly.
Information retrieval systems typically use the vector space model in order to represent both the queries and the set of documents. Afterwards they use the cosine distance for ordering the documents that best match the search query. The same cosine distance is used in order to compute the distance between two documents represented in the vector space. This article proposes a new approach, in which both the documents and the search query are represented as a time series of words. As a consequence, the document is no longer seen as a `bag of words' but as a sequence of words in which the order is essential. On top of this model we will use a well known algorithm in time series, dynamic time warping (DTW), in order to compute the distance between a search query and a document or between two documents. In order to improve the model, we enhance the algorithm to take into consideration the semantic context by using WordNet when computing the distance between words. This semantic enablement integrates smoothly into the time series model that we propose and it permits us to go beyond the `bag of words' model into the semantic search area.
Graphical illustrations are often used in biomedical articles to convey statistical results, schematics, etc. They are frequently accompanied with superimposed text annotations, or figure-text. It is generally assumed that this figure-text provides information that complements associated textual metadata, such as the captions, or bibliographic citations for indexing the figures and enhanced retrieval quality. However, to the best of our knowledge nothing in the literature adequately supports the assumption of information gain. In this article, we report the results of a pilot study that compares image retrieval based on figure captions alone to that using figure-text in addition to captions. In the blind study two judges evaluated a set of figures retrieved on the topic of "lung cancer". We find that figure-text could help improve retrieval performance for specific queries.
Information Retrieval (IR) system finds the relevant documents from a large dataset according to the user query. Queries submitted by users to search engines might be ambiguous, concise and their meaning may change over time. As a result, understanding the nature of information that is needed behind the queries has become an important research problem. So, various search engines emphasize the web query classification. For the efficient IR system, this system proposes the Web Query Classification Algorithm (WQCA) by using NoSQL graph database. This system classifies the web queries into each characteristic and each predefined target categories. In web query classification, the input query is first classified into web search taxonomies (characteristics). Then, domain terms are extracted from the query, and each of them is classified into their relevant categories that are stored in the NoSQL database. By using categories from WQCA, this system finds the relevant document from the document collection. The vector space IR model is used in this system to retrieve the relevant document.
The Santa Catarina State Telemedicine and Telehealth System — STT/SC stores well over 2 million examinations and every month about 20 thousand new imaging exams are sent to the system database. As a significant part of the findings associated to these medical data are stored as text documents, the precise extraction of information is a difficult task. In the healthcare domain it is common to find different terms, some of them appearing as composed expressions, used to represent the same concept, whereas the existence of simple modifiers can turn an expression into another, different concept. This work presents an information retrieval architecture developed for the STT/SC. It consists in a module integrated into the STT/SC and was developed in order to support both experienced and inexperienced users to perform queries. Experiments were performed to evaluate the accuracy of the proposed system in real situations. Results show that the search can be performed more much faster and with acceptable precision.
The terabytes of information available on the internet creates a severe demand of scalable information retrieval systems. Sparse Matrix Vector Multiplication (SpMV) is a well-known kernel for such computing applications in science and engineering world. This raises need of designing an efficient SpMV. Researchers are putting their continuous effort to optimize SpMV that deal with wide class of sparse matrix patterns using various compressed storage formats, and algorithm for high performance computing devices like multi-core/many-core processor i.e. GPU. But, they have not focus on optimization of input vector, which is highly sparse for various applications. This paper presents a novel approach - Sparse Matrix Sparse Vector Multiplication (SpMSpV) to utilize sparse input vector efficiently. To demonstrate efficiency of the proposed algorithm, it has been applied to keyword based document search, where sparse matrix is used as index structure of text collection and sparse vector for query keywords. The proposed algorithm is also implemented over Graphical Processing Unit (GPU) to explore high parallelism. Implementation results over CPU and GPU both demonstrate that SpMSpV using Compressed Sparse Column (CSC) sparse format is more efficient for information retrieval applications that use highly sparse input vector.
This work presents a retrieval scheme for encrypted JPEG images based on Markov process. In our scheme, the stream cipher and permutation encryption are combined to encrypt JPEG images, which are then uploaded to a database server. After that, the server without knowing the original content can extract features from the transition probability matrices of the AC coefficients of encrypted query image, in which those coefficients are modeled by Markov process. With the multi-class support vector machine (SVM), the features of encrypted query image can be converted into a vector with low dimensionality determined by the number of image categories. The encrypted database images are conducted similarly. After low-dimensional vector representation, the similarity between encrypted query image and database image may be measured by calculating the distance of their corresponding vectors. At the client side, the encrypted images returned by the server are decrypted to the plaintext images using encryption key. The proposed scheme can preserve file compliance and file size for encrypted JPEG images, while providing privacy-preserving image retrieval.
Recent information retrieval (IR) systems answer a multi-modal query by considering it as a set of separate uni-modal queries. However, depending on the chosen operationalisation, such an approach is inefficient or ineffective. It either requires multiple passes over the data or leads to inaccuracies since the relations between data modalities are neglected in the relevance assessment. To mitigate these challenges, we present an IR system that has been designed to answer genuine multi-modal queries. It relies on a heterogeneous network embedding, so that features from diverse modalities can be incorporated when representing both, a query and the data over which it shall be evaluated. An experimental evaluation using diverse real-world and synthetic datasets illustrates that our approach returns twice the amount of relevant information compared to baseline techniques, while scaling to large multi-modal databases.
Malayalam is the administrative language of the south Indian state of Kerala and also of the Lakshadweep islands of west coast of India. This work explain an efficient monolingual information retrieval system for Malayalam using query expansion technique, which returns Malayalam documents in response to query in Malayalam. The proposed system uses synonym mapping to improve the efficiency of document retrieval. This technique help to overcome vocabulary mismatch issues by expanding user query with additional relevant terms and reweighting the terms in expanded user query. The developed system is domain independent and used in different Natural Language Processing (NLP) tasks in Classical language like Malayalam.
The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs. However, in addition to these semantic information, there are rich information embedded in source codes themselves. These source codes, if analyzed properly, can be a rich source for enhancing the efforts of software artifact retrieval. To this end, in this paper, we develop a feature extraction method on source codes. Specifically, this method can capture both the inherent information in the source codes and the semantic information hidden in the comments, descriptions, and identifiers of the source codes. Moreover, we design a heterogeneous metric learning approach, which allows to integrate code features and text features into the same latent semantic space. This, in turn, can help to measure the artifact similarity by exploiting the joint power of both code and text features. Finally, extensive experiments on real-world data show that the proposed method can help to improve the performances of software artifact retrieval with a significant margin.
In order to improve the retrieval performance in the educational resource database, this paper designs an retrieval mechanism called ERRM based on Lucene and topic index. Firstly, this paper introduces the full-text search engine - Lucene, and studies in depth its internal implementation. Then by the way of the topic index which is learned by the topic model, this paper researches how to successfully expand query semantics. Based on Lucene and topic index, this paper proposes a new retrieval mechanism for educational resources. After expanding the user query, the retrieval mechanism submits the enriched query condition to Lucene for full-text retrieval. The experimental results show that the proposed mechanism could obviously improve the retrieval performance and precision.
An improved algorithm of the topic relevance model is designed, and the cosine value of the angle between the space vectors is used to measure the similarity between the block documents. By analyzing the development status of innovation and entrepreneurship education, this paper proposes innovative and entrepreneurial talents that integrate MOOCs Cultivating new ideas for curriculum reform, expounding from the aspects of curriculum system, integration mechanism, resource platform construction, and innovation and entrepreneurship mentor team building. In addition, the efficiency of distributed file storage and retrieval system based on MapReduce is much better than standalone processing and MPI parallel system. However, there is still a threshold problem in order to comprehensively improve the quality of innovative and entrepreneurial talent training.
One of the key problems of information extraction is to design a cross domain extraction procedure that can adapt different domain topics and text formats. However, most information extraction methods focus on specific areas or only have limited scalability for semi-structured texts. We argue that the problem of cross domain information extraction is basically introduced by domain related features. For example, the features used for price extraction in e-commerce websites cannot be directly applied in the case of extracting salary for recruiting websites. In worst case, a whole extraction model is required to be implemented despite the fact that there are common characters for price and salary. In this paper we propose a cross domain solution by dismantling domain relevant features into sub-features that are less domain related. The sub-features include composite features (those can be represented with a combination of several other features) and atomic features (features that can't be dismantled). To manage the features effectively we propose a multi-level feature model by organizing the features as well as their relations. With this model, we give an information extraction method that can be quickly shifted when the extraction domain changes.
Information systems provide effective and efficient computer-based support for businesses. The increasing use of information systems has also brought about increase in security threats, especially in online transactions. This paper previews the basic concept of information systems, their components, and general schema. Furthermore, using a literature search approach, a survey of the various threats to online services and a critical analysis of the effectiveness of various security measures in online services is presented. Finally, some recommendation for online users and online information service providers are made to effectively fight any form of threats experienced in online transactions. Online service users need to adopt a security conscious culture in online transactions, constant adherence to password standards, and practice safe online habits. Online service providers must adopt ethically acceptable standards and use modern technologies to secure the services they provide for users.
We propose in this paper to exploit Arabic dictionaries to enhance Arabic Information Retrieval (IR). We use standardized LMF dictionaries. We first put forward to mine such dictionaries and to represent them into graph-based representation. This graph will also be mined with a hybrid approach that combines both linguistic and statistical techniques to extract useful knowledge for IR. We study how extracted knowledge from such resource and added to the initial queries can attentively affect the retrieval process and results. Several query expansion strategies are carried based on morphological, semantic and morpho-semantic queries terms relations.
Due to introduction of General Data Protection Regulation (GDPR) in EU; all the cloud hosted applications (span across multi geo location) wherein captures the personal data need to first identify data privacy protection (DPP) entities and handle it as per EU norms and regulations. The company's legal contracts or transactions in tie up with other parties (customers, partners, suppliers, etc.) are usually stored in to repository; on termination of the contracts either by agreement or mutual consent then the other parties' information are usually archived for historical reasons. The other parties are usually interested in knowing what all documents or transactions they were participated earlier and expects the data to be pruned on need basis. As these documents are unstructured in nature, this paper proposes a solution in identifying all DPP entities with in legal contract documents, index the corpus level accumulated knowledgebase, apply customized ranking algorithm for the retrieved legal contract documents based on DPP search query, derive DPP entities specific legal contract document dependency relation graph for which the parties are participating by using techniques from Information Retrieval, Information Extraction, Natural Language Processing (NLP) and Ontology.
Depending on how the query is formatted, content and material approaches are utilized to discover photos. Words such "keywords" and "captions" are employed by content search engines totag and locate photographs. Even if it seems like a nice concept, there are number of issues with it. First off, it is quite challenging to sum up picture content in a few words since imaged at a is so rich in information. Second, manually a notating photograph requires a lot of effort. When a collection contains a large number of photos, this operation becomes extremely thing, challenging, and costly. Third, characterizing the appearance of a picture is an extremely subjective task. Finding a picture may be more challenging since an authoring tool and a user could use distinct terms todescribe the same material in the same picture. Thus, content-based image retrieval (CBIR), a technique foridentifying related pictures in a database by examiningtheir visual contents, has become essential. This article explains how to use the Haar transform, K-means clustering, and the Euclidean distance to retrieve photos.
A new combination of multiple Information Retrieval approaches are proposed for book recommendation based on complex users' queries. We used different theoretical retrieval models: probabilistic as InL2 (Divergence From Randomness model) and language models and tested their interpolated combination. We considered the application of a graph based algorithm in a new retrieval approach to related document network comprised of social links. We called Directed Graph of Documents (DGD) a network constructed with documents and social information provided from each one of them. Specifically, this work tackles the problem of book recommendation in the context of CLEF Labs precisely Social Book Search track. We established a specific strategy for queries searching after separating query set into two genres Analogue and Non-Analogue after analyzing users' needs. Series of reranking experiments demonstrate that combining retrieval models and exploiting linked documents for retrieving yield significant improvements in terms of standard ranked retrieval metrics. These results extend the applicability of link analysis algorithms to different environments.
We consider the problem of private information retrieval through a wiretap channel II (PIR-WTC-II). In PIR-WTC-II, a user wants to retrieve a message (or file) privately out of M messages, which are stored in N replicated and noncommunicating databases. An eavesdropper observes a fraction μn of the traffic exchanged between the nth database and the user. The databases should encode the returned answer strings such that the eavesdropper learns nothing about the contents of the databases. We aim at characterizing the capacity of the PIR-WTC-II under these joint privacy and security constraints. We obtain an upper bound in the form of a max-min optimization problem. We propose an achievability scheme that satisfies the security constraint by encoding a secret key into an artificial noise vector using an MDS code. The user and the databases operate at one of the corner points of the achievable scheme of the PIR under asymmetric traffic constraints such that the retrieval rate is maximized under the imposed security constraint. The upper bound and the lower bound match for the cases of M = 2 and M = 3 messages, for any number of databases N, and any μn.
This paper presents a new computational backend model that supports Arabic document information retrieval (ADIR) as a dataset and OCR services. Therefore, different services that support document analysis, retrieving, processing including dataset preparation, and recognition will be discussed. Consequently, ADIR services provide general functions of the Arabic OCR to compose many other services in the OCR domain. Furthermore, the proposed work can provide accessing different methods of document layout analysis with a platform where they can share and handle such methods (services) without any setup requirements. One of the used datasets composed from 16,800 Arabic letters written by 60 writers. Each writer wrote each letter from Alif to Ya 10 times in two forms. The forms were scanned at 300 DPI resolution and are segmented in two sets: training set with 13,440 letters for 48 images per class label, and testing set with 3,360 letters to 120 images per class label Convolutional neural network (CNN) is used and adapted for Arabic handwritten letters classification. In an experimental test, we showed that our results outperform 100% classification accuracy rate on testing images. Therefore, the ADIR services provide a “service description”, which includes an interface and a server's URL. The interface allows communication process between clients and services. Although, in this article we evaluate IR results and compared them with respect to corrected equivalent.
Information Security Risk Management is one of the key factors in ensuring security of the Information Assets of any organization. Information Security Risk Management has gained more importance recently as information security breaches increase and information infrastructures are constantly being targeted by various attacks. The risk assessment activity in information security risk management helps in identifying the potential risks to the information assets. To perform proactive risk management, the attack related information is required to construct the attack patterns which could be used to predict the future attacks. Attack patterns can be stored as they help in extracting risk intelligence for effective risk management. Storing of attack patterns is achieved by constructing Ontology. Ontologies are used to formally represent domain knowledge. The ontology stores patterns of attacks against that target the confidentiality, integrity and availability of the information assets. The extraction of risk intelligence is done by mapping the log files of the currently monitored network activity with the stored attack patterns to predict forth coming attacks. The ontology created can also help in sharing attack related information among different organizations interested in performing Information Security Risk Management.
In order to provide appropriate text information in line with users' cognitive level, a new personalized retrieval method is proposed in this paper. At first user's personalized cognitive structure should be established and with this structure user's cognitive level can be represented formally. Then related concept can be extended around the keywords, and queried documents should be ranked according to the user's cognitive level and the content of the document. Experiments show that different users will get different search results even if they use the same keywords.
Shah, Rashmi and Ramchandran recently considered a model for Private Information Retrieval (PIR) where a user wishes to retrieve one of several Ä-bit messages from a set of n non-colluding servers. Their security model is information-theoretic. Their paper is the first to consider a model for PIR in which the database is not necessarily replicated, so allowing distributed storage techniques to be used. Shah et al. show that at least Ä+1 bits must be downloaded from servers, and describe a scheme with linear total storage (in R) that downloads between 2R and 3R bits. For any positive e, we provide a construction with the same storage property, that requires at most (1 + e)R bits to be downloaded; moreover one variant of our scheme only requires each server to store a bounded number of bits (in the sense of being bounded by a function that is independent of R). We also provide variants of a scheme of Shah et al which downloads exactly R +1 bits and has quadratic total storage. Finally, we simplify and generalise a lower bound due to Shah et al. on the download complexity of a PIR scheme. In a natural model, we show that an n-server PIR scheme requires at least nR/(n - 1) download bits in many cases, and provide a scheme that meets this bound.
In this paper we present a framework for extraction of inference rules from Turkish documents, such as "A solved B ~ A found a solution to B". Many natural language processing tasks, such as question answering, information retrieval and machine translation can benefit tremendously from inference rules. Our framework consists of three layers: construction of dependency trees; determining predicates and their complements and finally discovery of inference rules via clustering. We use Silhouette Index (SI) of the clusters as an automatic evaluation metric.
API example code search is an important applicationin software engineering. Traditional approaches to API codesearch are based on information retrieval. Recent advance inWord2Vec has been applied to support the retrieval of APIexamples. In this work, we perform a preliminary study thatcombining traditional IR with Word2Vec achieves better retrievalaccuracy. More experiments need to be done to study differenttypes of combination among two lines of approaches.
In order to improve the quality of life in today’s digital culture, one of the most important areas of health to consider is mental health. Over the past several years, mental health has become increasingly important as the digital landscape has gotten more complicated, and as more and more people seek ways to relax, connect with others, and find mental balance in their life. While there have been advances in technology, particularly in information retrieval and recommendation systems, very few of these advances have been applied to the mental health industry. In this paper, we bridge this gap by introducing a ranking system that suggests personalized videos for users with the intention of helping them improve their mental health. By updating the relative ranking of videos by marginal improvement for different users, then aligning the distribution of videos depending on the areas that the user needs the most help with, the website helps provide personalized and actionable mental health advice.
Music Information Retrieval (MIR) focuses on retrieving useful information from collection of music. The objective of research work in this paper is to explore clustering approaches which can be useful in automatically mining the content from Carnatic instrumental music. The content to be retrieved is the instrument that is primarily used to play the song. Carnatic music songs with ten different instruments namely, Flute, Harmonium, Mandolin, Nagaswara, Santoor, Saxophone, Sitar, Shehnai, Veena and Violin are considered as input. Mel Frequency Cepstral Coefficients (MFCC) and Linear Predictive Coefficients (LPC) features are used for representing music information. In the first step, visualization technique is used to explore the capability of different features in distinguishing Carnatic music with different instruments. Then different clustering techniques are used for understanding natural way of grouping among this instrumental music. A discussion on the comparison of instrument clustering results with different algorithms, combined with various features is also presented.
In data management for large-scale applications such as Peer-to-Peer networks, and Grid and Cloud Computing arise challenges in regard to the decentralization of the application and in regard to an increasing number of failures. A consequence of these conditions is an increasing retrieval time, inaccurate results and higher network consumption. A solution to restrain an increasing retrieval time and an increasing number of messages is the introduction of approximate queries. The introduction of approximate queries limits the querying of all nodes of a network to a subset of nodes in the cost of the results' accuracy. Thus, a conflict to provide a large-scalability lies in guaranteeing accurate data, in the provision of fast results and in a low consumption of network bandwidth. Therefore, we propose an information aggregation that is based on an analytic hierarchical process (AHP) to find a trade-off among the unpredictable factors of time, messages and accuracy. After a user defines the preferences for the retrieval process, the AHP-based algorithm makes autonomous decisions on each node. The algorithm decides autonomously about pruning the approximate queries to reach an optimal trade-off from a global perspective. Applying the self-regulated pruning of the approximate queries allows reducing the messages from an exponential in crease to a constant factor. At the same time, the retrieval time is reduced from a linear increase to a constant factor in regard to an increasing number of nodes. At the same time of reducing the retrieval time and network bandwidth, the AHP-based self-regulation guarantees certain level of accuracy.
In this paper, we present a scalable arrow detection technique for biomedical images to support information retrieval systems under the purview of content-based image retrieval (CBIR) and text information retrieval (TIR). The idea primarily follows the criteria based on the geometric properties of the arrow, where we introduce signatures from key points associated with it. To handle this, images are first binarized via a fuzzy binarization tool and several regions of interest are labeled accordingly. Each region is used to generate signatures and then compared with the theoretical ones to check their similarity. Our validation over biomedical images shows the advantage of the technique over the most prominent state-of-the-art methods.
Key phrases are sequence of words that capture the main topics covered in a document. The key phrases help readers rapidly understand, organize, access and share information of a document. In this paper, we present a preliminary study on key phrase extraction from Bengali documents using two important features, such as TF*IDF, phrase's first occurrence in the text. For this study, we design a prototype system which works as follows: extracts n-grams from a source article, identifies candidate key phrases, and finally ranks the candidate key phrases to select the desired number of key phrases. The system has been tested on a collection of Bengali documents selected from a Bengali corpus downloadable from TDIL website and the preliminary results on Bengali key phrase extraction have been reported in this paper.
Ontology is recently one of the hot issues in research community. Domain specific Ontologies are being utilized as a search engine on the web page with an objective to make searching on the web page substantially more efficient, especially when it is more important to find the right web page, than searching with usual keywords. Ontology can play a very important role in the process of creating as well as managing the knowledge. This paper addresses the important issues in developing domain specific ontology for agriculture domain. We propose a generic approach for agriculture domain ontology representing entities and their relationships. We have developed a small ontology using the suggested approach. Our work is significant as we have not found any significant work targeting ontology development in agriculture domain.
Personal digital assistants are designed to assist users in easy information retrieval or execute the tasks they are interested in. The conversational medium implies an additional level of intelligence but typically these systems do not support any reference to the user's past interactions. We propose a domain-agnostic approach that enables the system to address queries referring to the past by using an information retrieval approach to rank various entities for a given query. We also add semantic enrichment to the recall process by augmenting the entities with information from a knowledge graph and leverage that in the retrieval process. We mined user interactions for the Cortana digital assistant to extract queries with location and business entities and show that our technique can achieve an accuracy of 89.8% for such recall queries.
Value services for people living in the flood of information is a service that reduces the time for information retrieval. To reduce the information retrieval time, the system should provide only the content that the user needs. The system collects and analyzes all the information related to the user to select the content that the user needs. Particularly, in the case of academics, such information includes major field, final degree, occupational group information, and contents information used by the user. This paper analyzes the characteristics of people in academia by using KOSEN system and proposes a personalized recommendation system for them. KOSEN is a web site where Korean scientists from all over the world come together to share information and networking. In addition, this paper suggests the services and contents required by KOSEN members considering the residential areas due to the nature of KOSEN. To do this, we use a big data processing solution and describe the analysis result.
A library always tries to complete its book collection, yet visitors often find it difficult to search for an alternative book of an unfound book they search. Based on the questionnaire result, 83.3% respondents of library visitors take a long time in finding the exact data content of a book as an alternative of an unfound book. There are some books having similar title but the content is different. As a result, the visitors get it more difficult. For now digilib.uad.ac.id has not been able to give an alternative book when book searching done by visitors. The aim of this research is to ease the library visitors to find the exact reference book as an alternative book of an unfound book in the library by implementing Cosine Similarity method in detecting similarity.The research subject was the similarity detector system of the book data content in the library by using Cosine Similarity method. The data collection method was literature study, interview, and questionnaire. The system development phases included database design, interface design, flowchart design of book data content similarity and system evaluation with Blackbox Test, Alpha Test, and Accuracy Testing.This research developed a web-based application to detect book data content similarity by comparing the resume of a book that visitor searches with other books by using Cosine Similarity method based on the highest to the lowest score rank. System evaluation result with Blackbox test to librarians shows that the system runs well, the input can be received properly, and the output meets the expectation. On the Alpha Test evaluation, the system is also accepted by the users with 92.5% of respondent evaluation shows a highly positive response towards it. This application is reasonably used with the result of Accuracy Testing of system comparison of 82.14% relevant.
Multimedia Information Retrieval is one of the most challenging and novel issues. Search for knowledge in the form of video is the main focus of this study. In recent years, there has been a tremendous need to query and process large amount of data that cannot be easily described such as video data. There is a mismatch between the low-level interpretation of video frames and the way users express their information needs. This issue leads to the problem named semantic gap. To bridge semantic gap, concept-based video retrieval has been considered as a feasible alternative technique for video search. In order to retrieve a desirable video shot, a query should be defined based on users' needs. In spite of the fact that query can be on object, motion, texture, color and so on, queries which are expressed in terms of semantic concepts are more intuitive and realistic for end users. Therefore, a concept-based video retrieval model based on the combination of the knowledge-based and corpus-based semantic word similarity measures is proposed with respect to bridging semantic gap and supporting semantic queries. For evaluation purpose, TRECVID 2005 dataset is utilized as well.
Information Retrieval is about user queries and strategies executed by machines to find the documents that best suit the user’s information need. However, this need reduced to a couple of words gives the retrieval system (IRS) a lot room for interpretation. In order to zero in on the user’s need many a IRS expands the user query by implicitly adding or explicitly recommending the users further useful terms that help to specify their information need.Queries often do not comprise more than a handful of terms, which, in turn, do not sufficiently represent the user’s need. In this paper, we propose and demonstrate an approach that enables users to resort to implicitly more complex query expressions. We call these semantic structures concept blueprints. Furthermore, users have the possibility to define the blueprints on their own. The purpose of the blueprints is to spot more precisely the text passage that fits the user’s information need.
Information systems should be designed to allow users to express their information requests in a simple way and to retrieve information that they consider as relevant to these requests. In order to fulfil these requirements, information system designers face many challenges w.r.t. selecting appropriate technologies and deciding on a modelling approach for their system. In this paper we present a conceptual framework that can be used to analyse the “fitness” of (semantically enriched) information systems w.r.t. queriability and information retrieval quality. It can help designers to spot the strengths and weaknesses of various modelling approaches as well as managing tradeoffs between modelling effort and their potential benefits. We evaluate this methodology in a case study on different modelling approaches for annotating medical images.
Correlation among software artifacts (also known as traceability links) of object oriented software plays a vital role in its maintenance. These traceability links are being commonly identified through Information Retrieval (IR) based techniques. But, it has been found that the resulting links from IR contain many false positives and some complementary approaches have been suggested for the purpose. Still, it usually requires manual verification of links which is neither desirable nor reliable. This paper suggests a new technique which can automatically filter out the false positives links (between requirement and source code) from IR and thus can help in reducing dependence as well as incorrectness of manual verification process. The proposed approach works on the basis of finding correlations among classes using either structural or co-changed dependency or both. A threshold is selected as a cut off on computed dependency values, to accept the presence of structural and co-changed dependency each. Now the traceability links are verified using these dependencies. If atleast one of the structural or co-change information validates the link obtained from IR approach, then that link is selected as candidate link, otherwise removed. Different thresholds have been experimented and comparison of results obtained from IR and the proposed approach is done. The results show that precision increases for all values of thresholds. Further analysis of results indicates that threshold in the range of 0.3 to 0.5 give better results. Hence, the proposed approach can be used as complementary to other Improved IR approaches to filter out false positives.
This study investigated the public information sources in specific science or technology in Taiwan. A survey was conducted extensively in Taiwan to study civic scientific literacy. Participants of this study included 2,024 individuals who were selected by using probability proportional to size sampling method. Interviews were conducted and survey data were quantitatively analyzed by means of Logistic regressions and reported with the use of cross tables. The result showed that: (1)The primary ways to retrieve specific science or technology information sources used by Taiwanese adult citizens were Internet. (2)Young citizens possessing the higher education placed a high value to retrieve specific science or technology information on the Internet. (3)The age and educational level were strong predictors of Taiwanese citizens who are retrieving particular science or technology information by Internet.
The exponential growth of web documents has resulted in traditional search engines producing results with high recall but low precision when queried by users. In the contemporary internet landscape, resources are made available via hyperlinks which may or may not meet the expectations of the user. To mitigate this issue and enhance the level of pertinence, it is imperative to examine the challenges associated with querying the semantic web and progress towards the advancement of semantic search engines. These search engines generate outcomes by prioritizing the semantic significance of the context over the structural composition of the content. This paper outlines a proposed architecture for a semantic search engine that utilizes the concept of semantics to refine web search results. The resulting output would consist of ontologically based and contextually relevant outcomes pertaining to the user's query.
Semantic web is the technology which drives the syntactic search and there are a wide variety of applications available for tourism sector today which promotes the country's economic status. This paper concerned with the development of a model towards the semantic search and the result which is based on user's priority while searching the tourism domain of interest. From this proposed model, the conditional probability for the given input can be calculated and querying ontology to provide relevant information. This proposed model has been developed with use of Netica-J. The ontology is being created with Protégé which is the tool used as an ontology editor and the SPARQL is used for querying the ontology. The interface between the ontology and SPARQL is being made with the help of Jena.
The development of deep learning technology provides a new development direction for text retrieval. Researchers have applied deep learning techniques to different information retrieval objects and carried out rich studies on them, such as web pages, scientific literature, and scientific data. This paper selects 40 research papers on related topics in the past 10 years through a step-by-step selection and conducts a review on the dimensions of model input, model structure, and its performance. Firstly, according to the differences in methods, we divided the deep learning text retrieval model into four categories: DNN-based, CNN-based, RNN-based, and Transformer-based, and analyzed the classical model structure and retrieval effect of each category. Secondly, we analyzed and compared the application scenarios of different types of models, and summarized some classic retrieval datasets. Finally, we discussed the main challenges and future research trends of deep text retrieval. This review is expected to provide basic knowledge and effective research entry points for scholars engaged in deep learning text retrieval.
Booming web pages contain a lot of information, while they contain little content and much unrelated noise information, such as script code, links, advertising and so on. These unrelated noise information occupies a lot of space, which is not suitable for the transition to small mobile devices, data mining and information retrieval. Therefore, web information extraction technology becomes more and more important. However, most extraction methods cannot adapt various and heterogeneous web structure and have poor generality and extracting efficiency. In this paper, we propose a method which can adapt to the heterogeneity and variability of web pages and gets high precision and recall. Our method is based on DOM structure to divide one web page into several blocks, and extract content blocks with statistical information instead of machine learning repeating training and manual labeling, which gets a good performance in Precision, Recall and F1.
The advancement in the world of internet has made information retrieval an easy task. Search engines provide users the information they need but not in specific form, instead floods the user with lots of documents/web pages that contain the information required by user. User waste their time to scan all documents in order to retrieve the needed information. Question Answering System reduces this drawback by automating the whole process. In this paper a tourism domain Question Answering System is proposed that contains the information about the tourist places of five different cities of Rajasthan. The system follows the principles of information retrieval and natural language processing. The proposed system is experimented with the set of 300 questions and it is observed that the system showed 70% precision and 80% accuracy rate. Lastly, proposed QAS (Question Answering System) is compared with other three existing Question Answering systems: AnswerBus [29], START [4], and NaLURI [28].
In the recent years, the database access is done by the HTML form based interfaces easily. The textual data deal with the natural languages, knowledge acquisition and finally information retrieval. The data units are returned from the databases and encoded into result pages for human browsing. For the encoded data processable, they need to assign a label for each group. In this paper, an automatic annotation approach aligns the resultant data into a similar group of data units. For each group, we annotate the results and assign the label for it. Then survey and classify the search result records for evaluate their concept similarity and dissimilarity. Finally the annotation wrapper is constructed automatically.
Inferring relevance between microarray experiments stored in a gene expression repository is a helpful practice for biological data mining and information retrieval studies. In this study, we propose a knowledge-based approach for representing microarray experiment content to be used in such studies. The representation scheme is specifically designed for inferring a disease-associated relevance of microRNA experiments. A group of annotated microRNA sets based on their chemotherapy resistance are used for a statistical enrichment analysis over observed expression data. A query experiment is then represented by a single dimensional vector of these enrichment statistics, instead of raw expression data. According to the results, new representation scheme can provide a better retrieval performance than traditional differential expression-based representation.
Passage retrieval is a fundamental task in information retrieval research and has received extensive attention on academia and industry in recent years. BERT-based passage retriever adopts a dual-encoder architecture to learn dense representations of queries and passages for semantic matching, which has become an indispensable component of passage retrieval system. However, BERT-based passage retriever tends to ignore phrases and entities mentioned in sentences, which are critical of retrieval. To address this problem, we propose a word recovery method that introduces word-granularity information into BERT-based paragraph retriever. We evaluate our model on three public datasets in different domains including E-commerce, Entertainment Video and Medical. By augmenting BERT-based passage retriever with word recovery method, the retriever achieves consistent improvements in MRR@10 and Reca11@1000, of which MRR@10 increased by 1.2% in Entertainment Video, and Reca11@1000 increased by 0.6% in Ecommerce.
The amount of Arabic electronic information is growing drastically on the web. Statistics shows that the number of Internet users in the Middle East has increased enormously since the year 2000 due to increase in ICT awareness and its importance within Arab countries. As a result this has raised the need to find effective methods and techniques for allocating and retrieving the Arabic-based content from the web. This paper presents major Information Retrieval (IR) tools and techniques and it highlights few challenges in this regard.
artificial intelligence (AI) is an emerging era that has proven to have a fantastic capacity to revolutionize how facts are captured and retrieved. AI-enabled systems offer an expansion of abilities to streamline the system of accumulating, managing, storing, and retrieving data. AI-enabled structures provide several benefits, such as using predictive analytics and herbal Language Processing (NLP) to extract records more effectively and correctly and automated pass-referencing competencies to simplify locating records. AI-enabled systems make indexing and saving statistics easier, allowing users to look for the relevant data they need quickly. AI-enabled structures offer higher security measures and encryption techniques for defensive touchy facts. Functions like facial recognition, gadget mastering algorithms, and get admission to hold data safe and far from malicious actors. Such systems may control and store statistics securely, correctly, and managed. AI-enabled systems provide an excellent suite of features for streamlining the manner of seizure and retrieval of statistics. AI answers, including predictive analytics, natural language processing, and automated cross-referencing, allow groups to quickly get the right of entry to the relevant information they need, even by providing higher safety features for included facts. In destiny, this technology might also grow to be even greater advanced, assisting businesses in gaining facts to electricity predictive analytics, helping in advertising efforts, and enhancing the organizational operation.
Structures, algorithms, mathematical models and methods for forming multi-agent data search and analysis systems are proposed. The capabilities of systems using the multi-agent architecture are shown, which allow carrying out a more efficient analysis and problem-oriented information retrieval on the Internet as well as providing the user with various data and documents relevant to a given problem. In the proposed model, the search setting does not depend on a specific search system and is general, and in the format of the setting module it is possible to refine the settings for a separate search system. Keeping the settings of the communication standard between agents, it is quite easy to replace any node, add or remove, so you can get an easily modifiable and flexible system without changing the architecture of the system as a whole.
Study on image indexing and retrieval concepts have evolved the research further on Content Based Image Retrieval (CBIR). The CBIR system analyses the image content features such as shape, texture, and colour. In recent systems, these low level features are combined with high level semantics in order to achieve high performance. Analyzing the image content will increase the time of feature extraction and complexity of the retrieval process. The proposed system describes the content of an image region using Color and texture as visual features. An ontology model is constructed to analyze the contents. Content Analysis comprises image segmentation and annotation, which scrutinizes images by subdividing them into entities, selecting primary entities, and finally extracting feature signifiers possessed by these entities. To explore the mechanism of content analysis system, a new fused framework using content and model based annotations was proposed. The proposed integrated framework for CBIR system was experimented and the performance was assessed over a large image database with number of features, compared against traditional approaches from the literature. From the experimental results, it is evident that the proposed system performs substantially better and faster than the existing systems.
In order to extract keywords from cross-language documents as accurately as possible, especially for the language whose keyword extraction technology is not mature, a text keyword extraction method based on information entropy and TextRank is proposed to extract the accurate keywords from the translated Chinese documents. This method determines the basic importance of words according to the information entropy of words, and then uses the information entropy of words to vote iteratively through the TextRank algorithm. This method solves the problem that TextRank algorithm easily extracts frequent non key words as keywords. The experimental results show that the proposed method can extract keywords more accurately than TextRank in the processing of cross-lingual bilingual translated documents.
Code comments can effectively help developers comprehend programs. However, it is a challenging and time-consuming task to write good comments for source code. Therefore, automatic generation of code comments is a promising research direction. Recently, researchers have leveraged neural machine translation to generate comments from source code and achieved impressive results. Another line of work has tried to exploit information retrieval (IR) techniques and showed excellent performance improvement on this task. However, current retrieval-based methods usually involve complex retrieval and editing operations, which are difficult to implement. To tackle the problems, we propose kNN-Transformer, a simple end-to-end retrieval-based code comment generation method. Our method combines a simple nearest neighbor retrieval module and a powerful transformer-based model. When generating each token, the retrieval module estimates a probability distribution depending on the current translation context rather than obtaining the retrieved samples in advance. The experiment results on four widely used public datasets (two Java datasets and two Python datasets) demonstrate that our method outperforms all the baselines, and our k NN retrieval module brings significant improvement when similar code snippets are available.
Errors are behaviors that people avoid them by making every possible effort, because many accidents were adverse aftermaths of the errors. Therefore, some people regard errors as aftermaths. It is essential that we should study them with appropriate theory and method. The paper employs the theory and method of information resource management, it elaborates that error information possesses a variety of characteristics and attributes of information resources, it describes that error is an important part of the organization information resources, which can be collected, organized and developed according to the flow of acquiring information resource. Error information also can be applied to the training and learning of organization, and provide service for developing and changing in which opportunities of innovation and reformation can be found to enhance organizational performance.
Search reranking is regarded as a common way to boost image retrieval precision. The problem is not simple especially when there are multiple features to be considered for search, which often happens in image retrieval. This paper proposes the combination of Circular reranking and Time-based reranking methods for improving the precision of image retrieval. Circular reranking utilises multiple features of an image, usually textual and visual descriptions, for reranking. Initially, it will conduct multiple runs of random walk for obtaining initial search results. Secondly, two features of an image are exchanged for better mutual reinforcement which makes multiple keyword search possible. Lastly, reranked results are attained through exchanging the ranking scores among different features in a cyclic manner. Time-based reranking is based on the count of Time, View and Download, of an image. Time count is the time duration between opening and closing of an image. View and Download counts are the total number of views and downloads respectively for an image. In our approach, Time-based reranking is performed on the Circular reranked list for improving precision, appropriately combining features of both reranking methods, while retrieving images during search.
Music documents retrieval system in most websites are useful for users who want to search sheet music which consists of chords, notes, or lyrics in a printable sheet format. With the growth of music repositories, music classification based on musical similarity can help users to find the same kind of songs. One important aspect to define music similarity measure is chord progression. In this paper, we present an approach to extract chord progressions from song segmentation in chord sheet music documents. This approach uses signature files to encode chords information with preserved musical data. Then, we propose a novel model for chord sheet music similarity without beats information which composes of three computation levels: (1) chord similarity, (2) sequence similarity, and (3) music similarity. In addition, we improve efficiency by removing duplicated parts. For the evaluation, we set an experiment to compare our music similarity model with others. The results indicated that music similarity based on sequences of chords obtained higher precision than chord classification based on the frequencies of chords. Furthermore, our model can sustain effectiveness even if beats information are unknown. We found that segmentation of chord progression can be another important aspect of musical similarity.
The growing number of scholars and students of the Quran Tafseer and its science has led to the increase of computer-based researches. Additionally, the evaluation of these researches requires computer-based experiments to be performed with a real Quran Tafseer documents. The available Tafseer documents are in text file or image file and are in Arabic language whereas the majority of the computer programming is in English language. Furthermore, the researchers will have difficulty to compare their research with other researchers. Therefore, the requirement for standard Tafseer dataset is very necessary for the current researches. This paper proposed a Tafseer dataset that will be used by researchers in Quran Tafseer and information processing fields. The dataset is organized in XML format to provide simple access and usage from the computer applications. A computer program is developed to create the dataset in XML structure. By applying the XML language computer applications users will be able to access the dataset and manipulate the content in easily.
Android has become the most popular mobile operating system. Millions of applications, including many malware, haven been developed for it. Even though its overall system architecture and many APIs are documented, many other methods and implementation details are not, not to mention potential bugs and vulnerabilities that may be exploited. Manual documentation may also be easily outdated as Android evolves constantly with changing features and higher complexities. Techniques and tool supports are thus needed to automatically extract information from different versions of Android to facilitate whole-system analysis of undocumented code. This paper presents an approach for alleviating the challenges associated with whole-system analysis. It performs usual program analysis for different versions of Android by control-flow and data-flow analyses. More importantly, it integrates information retrieval and query heuristics to customize the graphs for purposes related to the queries and make whole-system analyses more efficient. In particular, we use the approach to identify functions in Android that can be invoked by applications in either benign or malicious way, which are referred to as publicly accessible functions in this paper, and with the queries we provided, identify functions that may access sensitive system and/or user data and should be protected by certain permission checks. Based on such information, we can detect some publicly accessible functions in the system that may miss sufficient permission checks. As a proof of concept, this paper has analyzed six Android versions and shows basic statistics about the publicly accessible functions in the Android versions, and detects and verifies several system functions that miss permission checks and may have security implications.
This virtual workshop on digital language archives - digital libraries that preserve, curate, and provide online access to language data - seeks to address the growing need. It will explore a broad scope of issues related to digital language archives. This includes challenges and opportunities, strategies and solutions for: facilitating depositing and improving access; information organization, architecture, and retrieval; quality assurance; usability; ethical issues; ways of encouraging reuse of deposited data in research, and education. Workshop is expected to support interdisciplinary collaboration among information professionals, linguists, educators, representatives of language communities (including indigenous and other underrepresented), other interested audiences.
Code summarization refers to the procedure of creating short descriptions that outline the semantics of source code snippets. Existing code summarization approaches can be broadly classified into Information Retrieval (IR)-based and Deep Learning (DL)-based approaches. However, their effectiveness, and especially their strengths and weaknesses, remain largely understudied. Existing evaluations use different benchmarks and metrics, making performance comparisons of these approaches susceptible to bias and potentially yielding misleading results. For example, the DL-based approaches typically show better code summarization performance in their original papers [1], [2]. However, Gros et al. [3] report that a naive IR approach could achieve comparable (or even better) performance to the DL-based ones. In addition, some recent work [4], [5] suggests that incorporating IR techniques can improve the DL-based approaches. To further advance code summarization techniques, it is critical that we have a good understanding of how IR-based and DL-based approaches perform on different datasets and in terms of different metrics. Prior works have studied some aspects of code summarization, such as the factors affecting performance evaluation [6] and the importance of data preprocessing [7], etc. In this paper, we focus on the study of the IR-based and DL-based code summarization approaches to enhance the understanding and design of more advanced techniques. We first compare the IR-based and DL-based approaches under the same experimental settings and benchmarks, then study their strengths and limitations through quantitative and qualitative analyses. Finally, we propose a simpler but effective strategy to combine IR and DL to further improve code summarization. Four IR-based approaches and two DL-based approaches are investigated with regard to representativeness and diversity. For IR-based approaches, we select three BM25-based approaches (i.e., BM25-spl, BM25-ast, BM25-alpha) and ... (Show More)
In general document collections system contains groups of documents with overlapping content. However, most information storage and retrieval systems process each document separately, causing shared content to be indexed multiple times. In this paper, we describe a new document representation model where related documents are organized as a tree that allowing shared content to be indexed just once. We show that how this representation model can reduce the size of an inverted index as well as the time to build it.
When dealing with large scale applications, data sets are huge and very often not obvious to tackle with traditional approaches. In web information retrieval, the greater the number of documents to be searched, the more powerful approach required. In this work, we develop document search processes based on particle swarm optimization and show that they improve the performance of information retrieval in the web context. Two novel PSO algorithms namely PSO1-IR and PSO2-IR are designed for this purpose. Extensive experiments were performed on CACM and RCV1 collections. The achieved results exhibit the superiority of PSO2-IR on all the others in terms of scalability while yielding comparable quality.
Due to today’s information overload, finding and retrieving desired data or information has become difficult. Due to this new phenomenon, users will have a harder time locating and retrieving relevant information. It is now the norm rather than the exception to have data and information in multiple languages. It is critical in Ethiopia, where huge amounts of Afaan Oromo data are produced daily. With massive document collections come archival and search issues, putting these data and information characteristics to the test. Afaan Oromo users could easily search for and retrieve data that was relevant to their needs and interests using a hybrid information retrieval system developed for Afaan Oromo. Performance of information retrieval systems (IRS) has been under-researched in the Afaan Oromo linguistic domain. The efficiency of these systems has been found to be significantly lacking. For these issues, the AOIR system’s performance was improved by integrating various IR system approaches. The prototype includes the current indexing and searching subsystems. On-line news articles (Oromia Broadcasting Network, VOA Afaan Oromo), websites, books, and the Afaan Oromo Bible were used to gather 1000 Afaan Oromo text documents for the experiment. To identify terms and vocabulary that contained content, the text was subjected to text operations such as tokenization, normalization, stop-word removal and stemming and calculated using tf-idf term weighting scheme. After the experimental analysis in Python 3.7, precision, recall, and F-measure were all greater than 96.6 percent. The polysemy issue continued to affect the system’s overall performance. The paper also advocates for more work on system performance optimization to advance the AOIR.
Cultural heritage plays an important role in preserving social characteristics and knowledge for future generations. To provide long-term access to these resources, many cultural materials are today archived digitally. The problem arises when each cultural archive, which has own a large database, has been collected with the same cultural types, but different proposes, Therefore, there are various metadata standards that come from each of these archives, making it difficult to enhance, refine, or even improve raw data. This necessitates the need for a novel framework to integrating various subjects and metadata standards as well as extracting relationship among archives for enriching information retrieval. In this paper, we propose a new approach for discovery semantic relations between entities from articles using Wikipedia and various cultural heritage archives as resources. There are (1) dictionary extraction patterns used for extracting terms and meaning for creating a cultural heritage dictionary and (2) semantic relation extraction for extraction relation following question words. For enriching cultural information, the method for enriching cultural heritage information with the result of semantic relation extraction is presented using semantic string similarity matching. An evaluation of different domains shows high performance of the proposed approach.
Now a day's micro-blogging site Twitter is rapidly gaining popularity among politician, celebrities, businessmen, academician and even ordinary people. Many users want to collect useful information from Twitter for possible future use. In this regard, the user requires a system that facilitates user to restore tweets and find them again with higher degree of relevance with user's query. In this paper, we propose a framework for tweets retrieval from Twitter. The system acquires information from Twitter by using Twitter search API and develops a corpus of user's contents by removing noisy and ambiguous elements from the retrieved collection of tweets. Further, we pose queries to obtain the results of the system. We find that the system return useful documents to the user in order of their decreasing relevance scores.
Unit testing could be used to validate the correctness of basic units of the software system under test. To reduce manual efforts in conducting unit testing, the research community has contributed with tools that automatically generate unit test cases, including test inputs and test oracles (e.g., assertions). Recently, ATLAS, a deep learning (DL) based approach, was proposed to generate assertions for a unit test based on other already written unit tests. Despite promising, the effectiveness of ATLAS is still limited. To improve the effectiveness, in this work, we make the first attempt to leverage Information Retrieval (IR) in assertion generation and propose an IR-based approach, including the technique of IR-based assertion retrieval and the technique of retrieved-assertion adaptation. In addition, we propose an integration approach to combine our IR-based approach with a DL-based approach (e.g., ATLAS) to further improve the effectiveness. Our experimental results show that our IR-based approach outperforms the state-of-the-art DL-based ap-proach, and integrating our IR-based approach with the DL-based approach can further achieve higher accuracy. Our results convey an important message that information retrieval could be competitive and worthwhile to pursue for software engineering tasks such as assertion generation, and should be seriously considered by the research community given that in recent years deep learning solutions have been over-popularly adopted by the research community for software engineering tasks.
Automated malware toolkits allow for easy generation of new malicious programs. These new executables carry similar malicious code and demonstrate similar malicious behavior on infected hosts. In order to speed up the efficiency of mal ware detection, discriminating a malware as known or a new species of malware has become a critical issue in the security industry. In this paper, we propose a new approach to precisely classify malicious executables by employing information retrieval theory. Dynamic analysis of a sample's sequence of Windows API function calls produces corresponding parameters and values which is used as input to a standard TF-IDF weighting scheme to identify malware families by their behavior characteristics. Irrelevance reduction is developed to filter out non-relevant features and improve accuracy of malware classification. Finally, a similarity measure is used to determine the most similar malware family to the tested samples.
Health literacy is necessary for achieving and maintaining good health and is listed as an important national goal. As text is generally cost-effective and efficient for delivering health information, medical information is provided over text in the past decade. Nowadays, virtual assistants and smart speakers have become popular, and information retrieval using this technology has increased substantially. We aim to find out the text/audio features that affect information comprehension and retention. We have collected a text corpus from multiple sources and conducted a pilot study on Amazon Mechanical Turk (MT) for generalizability. Worker's responses give an indication of a positive correlation with text difficulty.
In the biomedical domain large amount of text documents are unstructured information is available in digital text form. Text Mining is the method or technique to find for interesting and useful information from unstructured text. Text Mining is also an important task in medical domain. The technique uses for Information retrieval, Information extraction and natural language processing (NLP). Traditional approaches for information retrieval are based on key based similarity. These approaches are used to overcome these problems; Semantic text mining is to discover the hidden information from unstructured text and making relationships of the terms occurring in them. In the biomedical text, the text should be in the form of text which can be present in the books, articles, literature abstracts, and so forth. Most of information is stored in the text format, so in this paper we will focus on the role of ontology for semantic text mining by using WordNet. Specifically, we have presented a model for extracting concepts from text documents using linguistic ontology in the domain of medical.
This paper describes a software tool called Text Search, which allows you to identify a plurality of content based on a thematic analysis of the text. It works with English fiction texts. Input is a user's query in English in the form of a text that is subject to thematic analysis and compared with the thematic analysis of texts in the database. This program is a database that stores, processes and issues information at the same time. The interface is implemented using the C # language and Windows Forms components, a database with a specific language, Transact-SQL, which was created jointly between Microsoft and Sybase. This language is an alternative to SQL with more advanced functionality. Modeling an information system is an important process that allows you to make a qualitative assessment of the scale of the system, to examine all its processes, the data to be used, and communications. By demonstrating the use of different types of diagrams, the main purpose of the system's operation is identified: the identification of a plurality of content based on a thematic analysis of similar texts. A hierarchical sequence of temporary objectives which lead to the accomplishment of the main goal is presented. The main processes of the information system and the data flows that it uses are demonstrated. Sub-processes are demonstrated at the second level of decomposition. In addition, a tree-like structure of the process hierarchy is created. It demonstrates the main processes and tasks that arise during their implementation, as well as their relationship.
in recent decades, a new scientific direction related to optimal storage and retrieval of information, called the theory of information retrieval, has been actively developing. Specific examples of methods for storing and presenting data and information search algorithms corresponding to these methods arose in it: lexicographic, treelike, relational, etc. Search problems for them have specific types and modifications: for example, the problem of finding identical objects, the problem of proximity, including search and etc. They play the role of model problems for the selected methods of storing information and have been studied over the years. Each time, to solve them, special research funds were involved, which were of a limited nature. Further, the organization of information retrieval using an innovative information-graph data model is proposed.
The domain of information retrieval focuses on retrieving unstructured data, particularly text documents. Document search plays a pivotal role in modern information retrieval systems. Locality Sensitive Hashing (LSH) stands as one of the widely employed techniques for document search in high-dimensional spaces. LSH offers the advantage of theoretical assurance for query accuracy in multidimensional spaces. However, further enhancements can be made to LSH by incorporating additional steps. This paper introduces a new algorithm called Dynamic Locality Sensitive Hashing (DLSH) as an improved version of LSH. DLSH utilizes a hierarchical selection of LSH parameters (such as the number of bands, number of shingles, and number of permutation lists) based on achieved similarity to optimize search accuracy and increase its score. The proposed technique is evaluated using various tampered file structures, and its performance is assessed. In some scenarios, the matching accuracy of DLSH surpasses 95% when optimal parameter values are selected for the number of bands, number of shingles, and number of permutations lists in the DLSH algorithm. These results make the DLSH algorithm suitable for implementation in critical applications that rely on accurate searching, such as forensics technology.
LangArc 2023 virtual workshop on digital language archives - digital libraries that preserve, curate, and provide online access to language data - continues (after the initial LangArc 2021 workshop) addressing the growing need. LangArc 2023 explores a broad scope of issues related to digital language archives. This includes challenges and opportunities, strategies and solutions for: facilitating depositing and improving access; information organization, architecture, and retrieval; quality assurance; usability; ethical issues; ways of encouraging reuse of deposited data in research and education; and coursework and other training for information professionals that will develop and maintain digital language archives. This workshop is expected to support interdisciplinary collaboration among information professionals, linguists, educators, representatives of language communities (including indigenous and other underrepresented), students, and other interested audiences.
Computerized fact retrieval (AIR) is a computer-assisted technique of information collecting designed to help gather excessive-stage facts. This method employs techniques, computerized internet scraping, herbal language processing, gadget gaining knowledge of, and different technological advances to create a set of dependent information sources. The advantages of AIR include improved search capabilities that permit queries to be more efficaciously tailor-made to personal wishes, faster question processing to lessen question time, and advanced garage efficiency because of dependent information. Advanced facts are also an advantage of AIR as data can be organized in more meaningful methods, making it more straightforward to retrieve applicable statistics. But, there are a few drawbacks related to AIR, inclusive of facts privacy concerns due to the opportunity of information being misused, the possibility of unreliable consequences because of errors inside the computerized question method, and the possibility of information being of low first-rate because of inaccurate records seize or mistakes in the search engine algorithms. In conclusion, automated data retrieval is a burgeoning technology that offers some advantages compared to traditional techniques of accumulating facts. The generation provides excellent points, faster question processing, and stepped-forward garage performance. Still, it must be utilized in compliance with data protection regulations to ensure statistics privacy concerns are considered.
This paper analyzes the problems of excessive information resources and insufficient accuracy of data information resource representation and extraction. This paper proposes an analysis of multimedia information resource characterization extraction method based on unstructured database. First, the underlying structure of the database is optimized, and the data resource attributes are identified by combining with the data feature mining algorithm, and the low-level information resource features of the unstructured database are clustered, and the data representation extraction path is reasonably planned with reference to the information feature mining results of the underlying database. The covariance parameters in the extraction process of information resource representation are calculated with Gaussian function algorithm, so as to effectively check the extraction error of resource representation and realize the extraction method of multimedia information resource representation in unstructured database. Finally, the comparison experiment proves that the multimedia information resource representation extraction method based on unstructured database can effectively improve the efficiency and accuracy of information resource representation extraction compared with the traditional method.
In view of existence problems in present E government information system, E-government information system based on the knowledge management was supplied in this paper. And the overall construction design proposal of Platform was given. The characteristics of knowledge management and the function of each component in the structure is analyzed. An information recommendation algorithm is designed. The management, organization, mining, personalized recommendation of E-government information are realized. Experimental results show that the platform has good scalability with the application value.
In recent years, there has been growing interest in learning to rank. We considered the current state of learning to rank in information retrieval systems. We proposed an approach for learning to rank problem based on multi-criteria optimization using the method of Pareto optimization and Genetic Algorithms. The performance of the method has been investigated on test data collections, also a comparison with existing methods of learning to rank has been performed. Weight vector for a ranking function obtained in this research shows the effectiveness of the proposed method. Also we consider a possible direction of the further improvements of the learning to rank method proposed in this article.
During maintenance, software developers deal with numerous change requests that are written in an unstructured fashion using natural language. Such natural language texts illustrate the change requirement involving various domain related concepts. Software developers need to find appropriate search terms from those concepts so that they could locate the possible locations in the source code using a search technique. Once such locations are identified, they can implement the requested changes there. Studies suggest that developers often perform poorly in coming up with good search terms for a change task. In this paper, we propose a novel technique-STRICT-that automatically identifies suitable search terms for a software change task by analyzing its task description using two information retrieval (IR) techniques-TextRank and POSRank. These IR techniques determine a term's importance based on not only its co-occurrences with other important terms but also its syntactic relationships with them. Experiments using 1,939 change requests from eight subject systems report that STRICT can identify better quality search terms than baseline terms from 52%–62% of the requests with 30%–57% Top-10 retrieval accuracy which are promising. Comparison with two state-of-the-art techniques not only validates our empirical findings and but also demonstrates the superiority of our technique.
Both the oblivious transfer (OT) problem and the symmetric private information retrieval (SPIR) problem studies the scenario where a client retrieves information privately and securely from databases, i.e., the privacy of the client is protected from the databases, and the undesired information is protected from the client. The OT problem studies the case of one database plus additional noisy resources between the database and the client. The SPIR problem studies the case of multiple replicated and non-colluding databases. In this paper, we combine the two models and propose a new problem of oblivious transfer (OT) with two replicated databases and a binary erasure multiple access channel connecting the databases and the client. We first provide an upper bound on the OT capacity. We then propose a protocol which achieves the upper bound. Therefore, we obtain the capacity of OT for this model. In our achievability and converse proofs, we utilized the techniques from both traditional OT and PIR. Compared to schemes that utilizes only techniques from OT, we see a 100% increase in the achieved OT rate.
The appearance of new technologies allows new data processing techniques. Thus, many new data processing techniques make difficult to user to find pertinent information in suitable time, unless knowing what accurately is in search of, where and how getting it. This paper proposes a pervasive network based information filtering system that integrates user profile such as identity, preference and other important data. User profile is embarked in a RFID-SIM card in order to guarantee its privacy, flexibility, mobility and confidentiality. The overall system objectives are privacy, security and providing pertinent information to the user according to his profile at anytime, anywhere, and in any form. The design and implementation of the system is also presented.
A soft-information fusion process produces refined estimates of soft-information, such as natural language messages. Information resulting from a soft-information process can be used to retrieve related, relevant information from background (a-priori) knowledge sources using contextual “cues” contained in those messages, a process we call “Context-Based Information Retrieval (CBIR)”. These retrieval results can be used to aid further understanding, and other fusion operations (e.g., data association). CBIR process performance is dependent on the choice of algorithms and parameters for those algorithms, and it is crucial that these are chosen appropriately for the problem domain the CBIR algorithm is used to aid. In this paper an f-measure evaluation of two spreading activation algorithms and their parameters is given using a soft information fusion process in a counterinsurgency domain. This evaluation takes place in two phases. The first phase executes the algorithms over a range of values in order to determine how those parameters affect the performance of the algorithms, and to set these parameters for future use. The second phase compares the results of these algorithms using the parameter settings learned from the first phase.
This paper introduces "Boxcan", an information retrieval platform for aggregated objects in ID-based object management system based on GS1 EPCglobal architecture framework. Boxcan platform receives an EPC of a container, then provides a tree structure of current parent-child relationship between the EPCs of the container and its inner objects. This paper proposes the system design of Boxcan platform and its applications. The proposed system design is evaluated with a field test of a practical object management system including Boxcan platform. A technical challenge to realize this function of Boxcan platform, fast retrieval of current parent-child relationship of EPCs from EPCIS, is also discussed in this paper. The effectiveness of the caching mechanism of EPC's current parent-child tree, which is a solution to this technical problem, is also evaluated by an experiment in a comparison with direct querying to EPCIS.
Distributed Peer to peer systems such as Chord allow peers to perform efficient searches using object identifiers rather than keywords. More specifically, they use a specific structure with some hashing scheme that allows peers to perform object lookup operations getting in return the address of the node storing the object. Lookups are achieved by following a path that increasingly progresses to the destination. These systems have been designed to optimize object retrieval by minimizing the number of messages and hops required to retrieve the object. The disadvantage is that they consider only the problem of searching for keys, and thus cannot capture the relevance of the documents stored in the system. This common problem with existing traditional distributed hash table (DHT) is done because they usually ignore the information retrieval algorithms, and thereby rely on keyword based searches. In this paper, we first propose to augment the P2P DHT system Chord with mechanisms for locating data using the information retrieval system LSI to facilitate content-based full-text search in large distributed information systems. Chord-LSI uses latent semantic indexing (LSI) to guide content placement in a Chord such that documents relevant to a query are likely be collocated on a small number of nodes. During a search, Chord-LSI transmit a small amount of data and search a small number of nodes. Simulation results show that Chord-LSI model is 17% more effective than Chord models.
In this paper, we propose a novel method that performs Cross Language Text Categorization (CLTC) from the perspective of Information Retrieval. We present an input document in target language in the form of a query in source language. Then we retrieve the training documents in source language and find K most relevant results. At last, we use the class labels of the K results to predict the class of the input document. The only external resource required by our method is a bilingual dictionary. Experimental results show that our method gives promising performance, which is better than translation-based method.
Identifying personal, institutional or product names in a text is an important task for numerous applications. This paper describes a solution for the Consumer Products Contest organized at International Conference on Data Mining 2012. The goal of this competition is to determine the state-of-the-art methods to automatically recognize product mentions in a collection of web documents, and to correctly identify the product(s) that each product mention refers to from a large catalog of products. Our approach combines methods of information retrieval and problem specific heuristics.
Cross-modal retrieval aims at retrieving highly semantic relevant information among multi-modalities. Existing cross-modal retrieval methods mainly explore the semantic consistency between image and text while rarely consider the rankings of positive instances in the retrieval results. Moreover, these methods seldom take into account the cross-interaction between image and text, which leads to the deficiency of learning their semantic relations. In this paper, we propose a Unified framework with Ranking Learning (URL) for cross-modal retrieval. The unified framework consists of three sub-networks, visual network, textual network, and interaction network. Visual network and textual network project the image feature and text feature into their corresponding hidden spaces respectively. Then, the interaction network forces the target image-text representation to align in the common space. For unifying both semantics and rankings, we propose a new optimization paradigm including pre-alignment for semantic knowledge transfer and ranking learning for final retrieval, which can decouple semantic alignment and ranking learning. The former focuses on the semantic pre-alignment optimized by semantic classification and the latter revolves around the retrieval rankings. For the ranking learning, we introduce a cross-AP loss which can directly optimize the retrieval metric average precision for cross-modal retrieval. We conduct experiments on four widely-used benchmarks, including Wikipedia dataset, Pascal Sentence dataset, NUS-WIDE-10k dataset, and PKU XMediaNet dataset respectively. Extensive experimental results show that the proposed method can obtain higher retrieval precision.
Nowadays, there are many software repositories, especially on the web, which have many challenges to be automated. Duplicate bug report detection (DBRD) is an excellent problem of software triage systems like Bugzilla since 2004 as an essential online software repository. There are two main approaches for automatic DBRD, including information retrieval (IR)-based and machine learning (ML)-based. Many related works are using both approaches, but it is not clear which one is more useful and has better performance. This study focuses on introducing a methodology for comparing the validation performance of both approaches in a particular condition. The Android dataset is used for evaluation, and about 2 million pairs of bug reports are analyzed for 59 bug reports, which were duplicate. The results show that the ML-based approach has better validation performance, incredibly about 40%. Besides, the ML-based approach has a more reliable criterion for evaluation like accuracy, precision, and recall versus an IR-based approach, which has just mean average precision (MAP) or rank metrics.
Abstract-People use a variety of social networking services to collect and organize web information for future reuse. When such contents are actually needed as reference to reply a post in an online conversation, however, the user may not be able to retrieve them with proper cues or may even forget their existence at all. In this paper, we study this problem in the online conversation context and investigate how to automatically retrieve the most context-relevant previously-seen web information without user intervention. We propose a Context-aware Personal Information Retrieval (CPIR) algorithm, which considers both the participatory and implicit-topical properties of the context to improve the retrieval performance. Since both the context and the user's web information are usually short and ambiguous, the participatory context is utilized to formulate and expand the query. Moreover, the implicit-topical context is exploited to implicitly determine the importance of each web information of the targeting user in the given context. The experimental results using real-world dataset demonstrate that CPIR can achieve significant improvements over several baselines.
Given that conventional spatial information user profile models have not a clear classification because of lack of theoretical basis, the comprehensive information theory is introduced into spatial information use profile modeling area. From the point of view of linguistic, the classification method of user profile models based on comprehensive information theory is proposed in this paper. According to syntactic, semantic and pragmatic information, user profile models are classified into four categories: syntactic information based models, semantic information based models, pragmatic information based models and composite elements based models, which make a clear distinction of the classification of user profile models. Based on these researches, the adaptability of all models respectively is analyzed in-depth, and the application conditions of every kind of user models are pointed out definitely.
Generative information extraction for patent text relies on a text-to-structure encoder-decoder framework, facilitating the automatic construction of patent knowledge bases. Existing models challenge to capture the long-range semantic dependencies due to the decentralized semantic context within the patent text, diminishing the accuracy of information extraction. In this work, we present a Semantic-driven Generative Information Extraction (SGIE) framework oriented to patent texts by enhancing and associating semantic context, leading to an improved information extraction performance. Information networks obtained from the pre-trained language model are used as information representations in the encoder layer, and based on an AMR (Abstract Meaning Representation) parser combined with patent schema knowledge, the patent-AMR graphs are created and viewed as semantic representations. The semantic-enhanced information representations are acquired by alignment from the same level of semantic and information representations, so then the global information representation is gained by gated fusing all semantic-enhanced information representations. A patent scheme knowledge-constrained decoder is employed beam search to generate information based on the semantic order decided, so patent technical information is extracted. Experimental results show that the proposed model is highly accurate and efficient in patent texts and is promising.
Semantic information retrieval is a popular research topic in the multimedia area. The goal of the retrieval is to provide the end users with as relevant results as possible. Many research efforts have been done to build ranking models for different semantic concepts (or classes). While some of them have been proven to be effective, others are still far from satisfactory. Our observation that certain target semantic concepts have high co-occurrence relationships with those easy-to-retrieve semantic concepts (or called reference semantics) has motivated us to utilize such co-occurrence relationships between semantic concepts in information retrieval and re-ranking. In this paper, we propose a novel semantic retrieval and re-ranking framework that takes advantage of the co-occurrence relationships between a target semantic concept and a reference semantic concept to re-rank the retrieved results. The proposed framework discretizes the training data into a set of feature-value pairs and employs Multiple Correspondence Analysis (MCA) to capture the correlation in terms of the impact weight between feature-value pairs and the positive-positive class in which the data instances belong to both the target semantic concept and the reference semantic concept. A combination of all these impact weights is utilized to re-rank the retrieved results for the target semantic concept. Comparative experiments are designed and evaluated on TRECVID 2005 and TRECVID 2010 video collections with public-available ranking scores. Experimental results on different retrieval scales demonstrate that our proposed framework can enhance the retrieval results for the target semantic concepts in terms of average precision, and the improvements for some semantic concepts are promising.
Search engines use artificial intelligence to find potential leads. Early concepts in both fields are introduced in Bush and Turing’s publications, which are also utilized to define artificial intelligence and data mining in this article. The foundation for further exploration of intellectual notions of art and their application to information retrieval is a straightforward model of an information retrieval system. Pattern recognition, representation, problem solving and planning, reasoning, and learning are among the concepts covered. This paper finishes by outlining future directions for artificial intelligence research in data mining systems.
Currently, most knowledge graph question answering (KGQA) systems need to retrieve all entities when dealing with complex problems involving multiple entities and relationships, which results in high time complexity and resource consumption. The response time of KGQA systems is an important indicator for evaluating their performance. To address this issue, this paper proposes a Chinese KGQA system based on dense relationship retrieval. The system uses the Faiss index mechanism for vector similarity retrieval, quickly extracts the top K relationships from the pre-built vector relationship library, and constructs paths to reduce a large number of irrelevant semantic paths in most KGQA tasks, thereby improving the time cost and computational resource overhead caused by the exponential growth of path numbers in second-order and higher-order questions. By using this method, the response time of the QA system can be shortened to within 1 second with minimal loss of accuracy. This paper combines pre-training models to complete tasks such as text semantic similarity and entity mention recognition, and achieves an average F1 value of 71.3% on the CCKS2019-CKBQA test set. Comparing with other systems demonstrates the superiority of our method in terms of accuracy and efficiency.
This paper presents a method of retrieval information on ontologies that allow users to view a keyword or descriptor closely related concepts get it. The application is divided into two modules that are: storing ontologies in relational databases and search on that database using keywords. For storage was created a scheme based on ontological components and finding the words was performed with an algorithm that calculates the semantic closeness between the concepts of ontology, and then presents the results close to the target user's search algorithm using a ranking.
Multimedia information retrieval usually involves two key modules including effective feature representation and ranking model construction. Most existing approaches are incapable of well modeling the inherent correlations and interactions between them, resulting in the loss of the latent consensus structure information. To alleviate this problem, we propose a learning to rank approach that simultaneously obtains a set of deep linear features and constructs structure-aware ranking models in a joint learning framework. Specifically, the deep linear feature learning corresponds to a series of matrix factorization tasks in a hierarchical manner, while the learning-to-rank part concentrates on building a ranking model that effectively encodes the intrinsic ranking information by structural SVM learning. Through a joint learning mechanism, the two parts are mutually reinforced in our approach, and meanwhile their underlying interaction relationships are implicitly reflected by solving an alternating optimization problem. Due to the intrinsic correlations among different queries (i.e., similar queries for similar ranking lists), we further formulate the learning-to-rank problem as a multi-task problem, which is associated with a set of mutually related query-specific learning-to-rank subproblems. For computational efficiency and scalability, we design a MapReduce-based parallelization approach to speed up the learning processes. Experimental results demonstrate the efficiency, effectiveness, and scalability of the proposed approach in multimedia information retrieval.
Software Bug Localization (SBL) is a task of locating the buggy source code. There are various ways of doing SBL and one of them is static SBL which utilizes the power of Text Mining (TM) in association with software repositories. Most of static SBL models are based on Information Retrieval (IR) methodology in which bug report works as a query and source code as database. In this paper we review state of the art SBL models which uses text mining techniques as their back bone in conjunction with other techniques. Essential features are extracted and summarized with the help of tabular representation. Aim of doing this study is to find the gaps in previous SBL models for proposing a novel SBL model in future.
Graph ranking is one of popular and successful technique for information retrieval. However, conventional graph ranking has two shortcomings when deployed for social image search. First, as social tags are noisy and incomplete, using that, the initial ranked list of images is inaccurate and impacts the following visual re-ranking. Another tough issue is how to conduct query-sensitive image re-ranking when multiple visual feature sets are available. In this work, we propose a sparse multigraph ranking framework, in which multiple graphs built on different visual features are integrated to simultaneously encode the image ranking. In particular, a sparse constraint is imposed on the fusion of different features, hoping to select a compact yet informative combination of features for different queries. To deal with the highly noisy issue inherent in social tags, a tag refinement solution along with word embedding is utilized to derive the more accurate initial ranking list, which services as the supervision signal for the proposed graph ranking framework. Extensive experimental analyses and evaluations on NUS-WIDE dataset demonstrate the proposed method can achieve state-of-the-art performance.
In the implementation of e-procurement application in the government of the Republic of Indonesia, one of the important issues is how to find relevant questions in the archive of a question answering (QA) service with the question asked by the user. A common method used for finding relevant documents is representing text documents into vector space model (VSM). Relevance between query and documents can be calculated using document similarities theory, by comparing the deviation of angles between each document vector and the query (from user question) vector where the query is represented as the same kind of vector as the documents. The Vector Space model algorithms widely used are Term Frequency * Inverse Document Frequency (TF*IDF) and Latent Semantic Indexing (LSI), however both models have their respective limitation. Considering that problem, this paper proposed hybrid model that combines TF*IDF and LSI to fix some limitations on both. From the experimental results, it is found that the proposed model is outperform (P@1=0.67) compared to TF*IDF model (P@1=0.27) and LSI model (P@1=0.4) that stand alone.
We study, for the first time, the problem of content-based searching of time-series microarray experiments in large-scale gene expression databases. The problem is approached as an information retrieval task where an entire ex-periment is taken as the query and searched through a collection of previous experiments. The relevant experiments are required to be retrieved based on the content similarity rather than their meta-data descriptions. A comparison of different fingerprinting and distance computation schemes is presented over a retrieval framework based on the differential expression of genes in varying time points.
Open Information Extraction system (OIE) extracts textual tuples consisting of arguments and its relation within the input sentence. OIE can also be considered as an unrestricted variant of information extraction. Throughout the years these types of systems have become more and more popular. The main factor for considering such a system is that they are made for speed, then any other factor. Also, one of the major advantages of using such a system is that the relations that are extracted are human-readable. Relation extraction in the legal domain has always remained a challenge because there is no structured data available. hence making this process more challenging. Since we are working in the legal domain, extracting the relations that are present in the document is very important. This leads us to a system that is capable of assisting people in the domain. The introduction of such a system would be really important for professionals like judges, advocates and also for the common people if they want to have a quick glance at certain documents. The outcome of these systems can be really helpful for future downstream applications like question answering, sentence similarity etc.
With the advent of the era of big data, all kinds of data are fused together which form a new modality-multimedia data, how to find information in multimedia data become a key problem, so cross-media retrieval has drawn great attention in recent year. Precisely and effectively measuring the similarity between different modalities of data is a key step of cross-media retrieval. The traditional methods usually use Cosine distance or Euclidean distance, which is difficult to reflect the real similarity between different modalities of data. For address this problem, in this paper, we use a novel approach to replace the above two methods, called efficient manifold ranking (EMR), which is able to better reflect the similarity between different modalities of data. Experiments on Wikipedia dataset and the challenging XMedia dataset which includes 5 media types show the effectiveness of the EMR algorithm, as compared with the 4 state-of the-art methods.
College information includes educational administration information, human resources information, teaching facilities information, teacher resources information and student information, etc. In order to improve the ability of management and integration of university information, an optimal design method of university information management platform is proposed based on B/S framework and APP. It adopts statistical analysis method to collect, statistics and classify the information in colleges and universities, and uses MySQL to construct university information management database. The network design of university information management platform is carried out under the system of B/S structure. The information management platform of university is divided into five management modules: administration, department, student, curriculum, backstage and so on. The design of network component interface of information management platform is constructed based on TinyOS. Native app local application program and Web app web page application program are used to develop the mixed information management system. So that the university information management platform can run compatible on APP mobile terminal equipment and PC terminal equipment, with Java as the development language, the design of information management platform is realized on the platform of J2EE enterprise framework SSH. The designed information management platform can realize the network of university information and remote information management, and the system has good stable performance and strong ability of cooperative management.
User profiles and interests have become essential for personalizing information search and retrieval. Indeed, traditional Information Retrieval Systems (IRS) don't integrate the user in the search process. Also, users do not always find what they need after a single query. Instead, they often issue multiple queries, incorporating what they learned from the previous results to iterate and refine how they express their information needs. So we rely on this process to learn the user information needs without asking him explicitly. This is achieved by capturing his judgments on the retrieved results. We consider also, in the construction of the user interests, what he is looking for and what the user doesn't want to find in the future results to build interests that best match his information needs.
Automatic Query Expansion provides most suitable terms for retrieving relevant documents from datasets. In this paper, a new query expansion approach is proposed and implemented which is based on accelerated particle swarm optimization(PSO). Focus of the proposed technique is to find the most suitable expanded query in place of related expansion terms. The proposed technique uses Fuzzy logic to enhance the accuracy of accelerated particle swarm optimization with changing different parameters. The proposed technique is compared with another state of the art work on different performance parameters such as F-measure and Mean Average Precision. The efficiency of the proposed algorithm technique is also evaluated on two datasets as CACM and CISI. The results indicate that the proposed technique shows improved results compared to existing similar existing technique.
MetricAttitude is a visualization tool based on static analysis that provides a mental picture by viewing an object-oriented software by means of polymetric views. In this tool demonstration paper, we integrate an information retrieval engine in MetricAttitude and name this new version as MetricAttitude++. This new tool allows the software engineer to formulate free-form textual queries and shows results on the polymetric views. In particular, MetricAttitude++ shows on the visual representation of a subject software the elements that are more similar to that query. The navigation among elements of interest can be then driven by the polymetric views of the depicted elements and/or reformulating the query and applying customizable filters on the software view. Due to its peculiarities, MetricAttitude++ can be applicable to many kinds of software maintenance and evolution tasks (e.g., concept location and program comprehension).
Medical Information Retrieval (IR) aims to extract relevant medical information from the web, patients' records, electronic books, research articles etc. However, users are generally unfamiliar with medical terms and find difficulties expressing their needs. One interesting solution is to integrate fuzzy ontologies in order to achieve semantic interoperability and offer a way to handle vague and imprecise information regarding the medical field. In this paper, our aim is to perform an effective IR by extending its understanding of ambiguous medical concepts. We propose a fuzzy ontology-based medical IR approach which is composed of three components: (1) personalized medical ontology building, (2) contextualized fuzzification of the personalized medical ontology and (3) a query reformulation process based on the resulting fuzzy ontology. A prototype has been implemented allowing experimental evaluation of the proposal. We have studied how the query reformulation has led to a quality results improvement.
The internet, social media, and other cutting-edge technologies are all contributing to the exponential growth of digital information. This makes interpreting and applying this enormous amount of data both possible and challenging. Our research proposes a smart system that can convert unprocessed text into a knowledge graph in order to address this. This system makes use of the graph database Neo4j. Although there are currently available tools for linking entities and models to extract relationships, such as Spacy, NLTK, and Flair, their combined use is inefficient. Our suggested method seeks to efficiently blend entity linkage and relation extraction, allowing us to convert raw data into a knowledge graph. There are many real-world uses for this strategy, particularly in data analysis and decision-making.
Similar bugs are bugs that require handling of many common code files. Developers can often fix similar bugs with a shorter time and a higher quality since they can focus on fewer code files. Therefore, similar bug recommendation is a meaningful task which can improve development efficiency. Rocha et al. propose the first similar bug recommendation system named NextBug. Although NextBug performs better than a start-of-the-art duplicated bug detection technique REP, its performance is not optimal and thus more work is needed to improve its effectiveness. Technically, it is also rather simple as it relies only upon a standard information retrieval technique, i.e., cosine similarity. In the paper, we propose a novel approach to recommend similar bugs. The approach combines a traditional information retrieval technique and a word embedding technique, and takes bug titles and descriptions as well as bug product and component information into consideration. To evaluate the approach, we use datasets from two popular open-source projects, i.e., Eclipse and Mozilla, each of which contains bug reports whose bug ids range from [1,400000]. The results show that our approach improves the performance of NextBug statistically significantly and substantially for both projects.
Web 2.0 technologies put the user at the center of data production and introduce a strong social collaboration. Therefore, the techniques used in traditional information retrieval systems do not meet the requirements of users who want to take into account their social preferences. The idea reported in this paper is about including not only the social context of the user but also the one for the resource. In the social network that we consider, the user social context brings its interests, which are captured from a collaborative tagging system, while the resource social context is related to clusters of tags, obtained by classification method, and users' opinions according to their expertise level on the resource. The results of our experiment evaluation on real-world datasets (crawled from delicious folksonomy) demonstrate significant improvements over traditional retrieval approaches.
In the health domain, the adoption of computer systems introduces better services, reduces human errors, and provides reliable services with nearly zero down time. In general, data in computer systems is stored in coded format; however, certain data, like user comments, cannot be coded. Hence, it is stored in the form of free text. Based on the results of the performed literature review, it was identified that the free text contains invaluable information; however, extracting such information is a challenging task due to the complexity of the stored data. In this paper, a Latent Semantic Indexing (LSI) algorithm is developed and applied on The Health Improvement Network (THIN). The algorithm utilizes the computational power provided by the multi-processor/multi-core system in performing the IR process. Further to that, the paper investigates the representation of the patient's data in the Term Document Matrix (TDM) to enhance the accuracy of the retrieved information.
In this paper, we propose a method is to improve the performance of information retrieval systems (IRS) by increasing the selectivity of relevant documents on the web. Indeed, a significant number of relevant documents on the web are not returned by an IRS (specifically a search engine), because of the richness of natural language Arabics. For this purpose the search engine does not reach high performance and does not meet the needs of users. To remedy this problem, we propose a method of enrichment of the query. This method relies on many steps. First, identification of significant terms (simple and composed) present in the query. Then, generation of a descriptive list and its assignment to each term that has been identified as significant in the query. A descriptive list is a set of linguistic knowledge of different types (morphological, syntactic and semantic). In this paper we are interested in the statistical treatment, based on the similarity method. This method exploits the weighting functions of Salton TF-IDF and TF-IEF on the list generated in the previous step. TF-IDF function identifies relevant documents, while the TF-IEF's role is to identify the relevant sentence. The terms of high weight (which are terms which may be correlated to the context of the response) are incorporated into the original query. The application of this method is based on a corpus of documents belonging to a closed domain.